How about perspective learning? Thanks, Minet. Can everyone here? I Zoom can hear me. Can Zoom people hear me? Okay, great. So yeah, I'm talking about perspective learning. I actually gave a talk about a year ago on the idea of this. This was at Simons, the Flatiron Place. And now we actually have some results. So I'm pretty excited about that. This is the collaboration between me, a few amazing. Between me, a few amazing students, Ashwin, Rahul, and Rubing, who did almost all the work, and Pratik, who made sure it was right. My role has mostly been saying things like, is that right? And then being like, oh yeah, that's not quite right. And then they fix it. And this talk today is in memory. Sheldon Kaplas is my best friend's father who passed away in the middle of the night last night, which I found out this morning. So I decided to dedicate my talk in his memory. Okay, so approximately correct, probably approximately. Approximately correct, probably approximately correct in the future. Here's the outline. It's boring. The motivation. So, this is a question I ask myself all the time. Like, what the fuck is learning? And like, we say it a lot. Sometimes we say reinforcement learning or all sorts of things, but really, what is it? And my background, actually, my PhD is in neuroscience. So I think about it quite a lot. And I just want to say, like, every year there's a neuroscience conference. Every year, there's a neuroscience conference with about 30,000 people, and about one-sixth of it is devoted to learning and memory. So there's like 8,000 people a year all talking about what learning and memory is in natural intelligence. And that's been going on for like, I don't know, 30, 40 years. And I just want to say, like, it's not clear. Like what learning is in the actual world, there's really no consensus on it, how many things it is, all that stuff. But there is some stuff that is clear. So one, learning is an involved property. One, learning is an involved property. It enabled organisms to make better decisions on average in their niche about how to act in the future based on the past. So it's all about acting better in the future. And just to be clear, it's not about summarizing the past. Summarizing the past in its own right, you could think of as learning. It's just not what learning evolved to do. And this works because the future was at least partially predictable, right? There were certain patterns that you could learn. Certain patterns that you could learn. Some of them were static, but some of them were dynamic. And there's lots of different kinds of learning. Like just when you think about learning in a biological context, there's behavioral learning, like I'm learning to play tennis. That's a particular kind of learning. There's associational learning, like every time I hear a bell, there's food, right? I don't have to do anything, but I still learn that the bell means food. There's reinforcement learning. Every time I go over here, I get bitten by mosquitoes. Get bitten by mosquitoes. Okay, so that means I'm reinforced. Don't go over there. There's sensory motor learning. This one's kind of cool. It's unsupervised. Like, you look at, say, snow a lot, you'll start, or let's do clouds. If you look at clouds a lot, you'll start noticing, oh, some clouds are darker and some clouds are lighter and there's different kinds of clouds. So start being able to discern different kinds of things. You never got any feedback. There was never any behavior. You still learned something. And then finally, there's Something. And then finally, there's imitation learning. Like you do something like with little kids. I smile, then they learn how to smile, or I wave, they learn how to wave. All these things are happening biologically. They're all totally different mechanisms. They're all called learning. Some of them are supervised, some are reinforced, some are unsupervised, and they're very different things. I just want to point that out to like ground us in: we're not really studying all of these things. Maybe we're studying some of them, and it's worth formalizing them. Formalizing them, and his talk is about a particular model of learning that won't deal with all of this stuff. But how do we model learning? Well, what we call learning is a formal model in AI. There's a bunch of different things, packed learning, online learning, reinforcement learning. Just want to remind ourselves, it's not really learning. It's just a model. Models are wrong and useful, as George Fox liked to say. And so let's look at probably almost correct learning, arguably nearly 100 years old. There's theories. 100 years old. There's theories in the early 30s from probabilists talking about you have IID data, what you can learn about distributions. The workhorse of modern AI, so arguably it's very useful. The assumptions IID are obviously wrong and dumb in a way. So it's a model that's useful but wrong. Okay, so what if we want to do a model that maybe is more accurate? That's still going to be wrong, but somehow reflects. Still going to be wrong, but somehow reflects the realities of learning in the natural world better. So we start from different assumptions. First of all, our data we're not going to assume is from a random variable that's static, but rather from a stochastic problem. Secondly, the goal is going to be a dynamic objective, not a static. So you may change. You may want to be able to classify cars and houses one day, but you may want to classify dogs and cats a different day. The objectives will change. Objectives will change. This model is more complicated, so we know from the bias-variance trade-off, this is going to reduce some bias, but it's going to add some variance. So, sometimes it'll be worse, and sometimes it'll be better than not making these assumptions. Just want to give you some examples. These are going to be extremely simple examples just to illustrate kind of the flexibility of this and relate it to some other things. So, all these examples, we're just going to have Z is a Bernoulli process. Okay, so the first example is Z T is. So the first example is ZT is IID. At each point, you just sample the coin flip, heads or tails. Probability of this particular run of it being heads happened to be, I think, 0.9. So here's one run. Now, this is a special case of when learnings are dynamic, when it happens to be static, but here it is. Now we can make it slightly more complicated. Now what we're going to say is each ZT is independent from all the other ones, but it's not necessarily identically distributed. identically distributed. So in this particular case, what we said is on the even time points, the probability of a coin landing is p. Sorry, the probability of coin landing on heads is p. And on the odd times, it's one minus p. So that means you're going to get a random realization. It's going to look something like this, like on the odd ones, it'll tend to be heads, on the even ones, it'll tend to be tails, but it'll go back and forth. Okay, so this is obviously an extremely simple stochastic process. Stochastic process, but if you just use kind of classical methods and you don't know that this is going on, it's going to do arbitrarily bad. Right, so in this kind of scenario, I think comes up a lot in data. The next one is kind of more closely related to what people do in forecasting and stochastic process modeling. We just assume ZT, for example, is a Markov model. So now at each time, it's neither independent nor identically distributed. Independent nor identically distributed, but it's governed by Markov dynamics. And we could just say, like, the probability of it switching state, where state now is which distribution you're in, is governed by some parameter. So there's some probability that you switch from landing, like the probability that you land on heads is p to the probability that you land on heads is one minus p. And that's just the dynamic governance. And we set up the parameters here, and here's another random realization. It happens to look deterministic almost, but that is just a random draw. But that is just a random draw of a coin. And then here's a more general one, and this is closely related to reinforcement learning, which is Zt depends not just on the past data, but also on the past decisions. So the probability that the state switches from P to 1 minus P is a function of whether you got the last one right, not just what it was. Okay, so these are just four different scenarios that one could imagine. That one could imagine just changing the assumptions on the stochastic process, whether it's stochastic process or stochastic decision process. The first one is basically canonical PAC learning. The second one is slightly more complicated. The third one is more like stochastic process modeling. The fourth one's more like reinforcement learning. But they're all in the special cases, it's kind of general formalization. Okay, any questions about any of that so far? Okay, so we're most. So, we're mostly going to focus the rest of the talk on the setting where the ZTs are independent but not identically distributed. So, think about the canonical problems in lifelong learning, which is kind of where all this stuff came up. You have task A, and then after a bunch of trials, you now get task B, and a bunch of trials, you now get task C, and the distribution in task A and task B and task C are different, so the distribution is changing every sum number of trials. Now, the way those problems are normally formulated is there's Normally formulated is there's no ability to predict what the task will change to. Just you have task A, then B, then C. Think about split C FAR or split MNIS. It's just, there's a whole sequence of tasks, but there's no way to predict it. However, imagine instead you had a scenario that you're task A for a bunch of trials, then you switch to task B, then you switch back to task A, then you switch back to task B. Now, after a while, you can start being able to predict. After a while, you can start being able to predict which task you're in. If you don't try to predict it, the only thing you can do is retrospectively react. After you get a bunch of mistakes in task B, you realize, oh, this is task A, now I should switch this thing into task A. However, if you can predict that it's about to switch because there's a reliable pattern of switching, then you don't have to retrospectively respond. You can predict it and never make a mistake again once you've learned the two tasks. So this was the motivation. So, this was the motivation. This came out of this DARPA program we had, Lifelong Learning, where all of the performers, you look at the performance, and the performance curves, they would go down and come back up and go down and come back up very reliably every time the test switched. And we thought, maybe that, but like, think about I'm an animal in the real world, and one day I go to the watering hole and I see a tiger there at two. The next day, if I see the tiger there at two, See the tiger there at two, I might get eaten and not learn very much ever again. Whereas, if I can predict it'll be there at two, and therefore I show up at 1:30, I survive. So, this is like an evolutionary argument that it makes sense to try to predict when stuff is going to happen. That's at the organismal level. But there's something even more fundamental. People call it allostasis, which happens even at the single cell level. But think about, like, just for you, like, when do you eat? Just for you, like when do you eat? Do you eat when you've run out of nutrients or when your body has predicted in various ways that you will run out of nutrients in several hours if you don't eat now? That's predicting. And you're not born knowing exactly how long it will take to digest all the food you eat. That thing you learn over time, depending on your metabolic rate and your activity level, and stuff like this. This is ongoing in us constantly. We're constantly predicting. Constantly. We're constantly predicting what resources we'll need in the future and adjusting to that. Here's another one I really like. Let's say something happens and you get scared. Physiologically, what happens is your heart starts beating faster, sending blood more to your body. Why is that? So that you can take off if the scary thing is about to eat you. Right? So there's all these evolutionary things that happen across the hierarchy of animals, probably. Of animals, probably also plants, where we're in a constant state of predicting what we think is going to happen and adjusting our levels so that we can respond well in the future. So I'm not making some like crazy claim like, oh, maybe does this happen? Like, no, for sure, this always happens in every animal ever, probably in every single cell organism ever also. It's a very real kind of learning. Okay, so with that in mind, I would like to formalize some of this. I would like to formalize some of this. The data model, we're going to say we're focusing on supervised learning. So we have X and Y. Z T is a realization of the stochastic process, which we're going to say capital Z. We'll have the past is Z little T, is Z is less than little T, and the future is Z bigger than little T. So T is usually going to be now. Okay, now the hypothesis sequence, the deal is the thing you should do at any time changes because the distribution is changing, maybe the goal is changing. Maybe the goal is changing. So we no longer have a fixed hypothesis. This is very different, obviously, than PAC learning. It's also very different than other people who have recently generalized PAC learning to deal with stochastic processes, or like online learning, where you think about the best hypothesis, the best constant hypothesis that's worked. So we're saying there's a sequence of hypotheses that each time you can have a different hypothesis. And we're also going to say, so. So, I'm not going to worry about this line for now, just that your hypothesis is a sequence, and capital H is a set of all possible sequences of hypotheses. Okay? Now, our learner, it's just going to, I'll give you more details later, it's going to take that past and it's going to give you a whole hypothesis sequence for all future time and all past time. That's what its job is, just sample one little h out of this big capital of H. Okay. Okay, now here's some math where it starts getting a little bit interesting. We're going to say perspective loss is this thing, L bar, which is the sum of losses. The losses may be potentially indexed by time. And it's the sum of losses divided by the total number of time steps. And then we're going to take the total number of time steps to infinity. And so it's basically just the average loss. Okay, it's very closely related to risk and other things like that. For theoretical purposes, That for theoretical purposes, we're just going to say it's bounded and it's monotonically decaying in time. Now we can say the perspective risk is just the expected perspective loss. We could define it differently, but that's what we're saying. So the expected loss with respect to the particular history that you've observed so far. So because it's a stochastic process, this is going to depend on the past, right? So we have this expected risk. Sorry, we have this risk as the expected. Sorry, we have this risk as the expected loss, and then we can define the expected risk, which is the integrating out the history. This is really not different than kind of classical PAC learning, where you have a loss, your risk, which is the expected over the unseen data, and your expected risk where you integrate out the training data too. So it's all essentially the same, just slightly more complicated. I think the relationship is one of the things for now. Yeah, all I'm saying. Yeah, all I'm saying is Z is this tuple X C Y T Okay, so now we're going to define the optimal risk. It's whenever at any given time T, it minimizes or infamizes. I don't know if it infamizes a word, but just say infamizes the risk at all future times. And a Bayes optimal learner is the learner that selects. Learner is the learner that selects a hypothesis that has the optimal sequence at every time. Okay, so now we've defined the best you can do and what you care about. Okay, this is just a summary of all the stuff I said, but there's nothing new on this slide. So you guys may know that in the 70s and 80s, in machine learning, people were fighting about. In machine learning, people were fighting about the difference between strong learnability and weak learnability. So, colloquially, strong learnability means something is strongly learnable if with enough data, your learner, there exists a learner that converges to something that's close to optimal, with high probability. That's basically what we mean when we say pack learning. But till like around 1980, it wasn't clear what weak learnability really meant. And that's going to be if with enough data, you can do epsilon better than change. Enough data, you can do epsilon better than chance, but not necessarily arbitrarily well. And there was a fight about whether the strong learnability and weak learnability were the same. And it turns out that people prove that they are in fact the same using a boosting argument. So as long as you can, as long as there exists an algorithm that weakly learns, meaning it can do epsilon better than chance, you can have an infinite number of them and it'll asymptotically strongly learn as well. That's this fundamental result. This fundamental result. And so we wondered to what extent does that result depend on a bunch of these assumptions on IID stuff? And so we just define strongly perspective learnability in a similar way that basically a family of stochastic processes is strongly perspectively learnable. If any stochastic process Z from this family, there exists a learner that outputs a sequence of hypotheses and a finite time t, such that as long as t is bigger than that, you basically get this packed learning rule. Basically, get this PAC learning. And the key differences is now this risk, I mean, this is the same as, you know, everybody knows this quantity, but risk here is now integrated over the future. And you'll note to do well, you have to perspect not just what the future will be like, but what you will be like if you're the H, right? You have to predict how you're going to respond in the future to things and predict what the future will hold. Yeah. Okay, but what we did over is over the jaw in the panel. No. This is draws in the future. But I thought the risk was didn't expect sampling. Yeah. Oh, sorry. You're talking about this probability. Yeah. Yes. Yes. A few definitions, uh, last one. Yes. Yeah. So does like finite provides like infinite you could just as easily consider like an infinite provider. I think it discounts. And I don't know what this would get you necessarily, but I just want to understand like the implications. Yeah. So because this tau is going to infinity, this is the infinite future and it's not discounted. And that makes the theory easier. However, there are problems with doing it this way. For example, if the Way for example, if the stochastic process is a Markov chain and mixes, and like it mixes in such a way that all states are equally likely, then the Bayes optimal perspective loss is basically chance, and so you get nothing. So, we're in the midst of wrestling with how to deal with something like discounting, but this is like quite different from discounting right now. So, what about this guy? I mean, this guy seems like okay, Okay, there's a problem 20 years down the line. I don't necessarily care about solving whether or not I'm good. But yeah, we'll talk about that later. But something like that seems like would want to. And so what I'm saying here is, I guess, somehow everything in the future is kind of weighted equally. Yep. It means what you said. I mean, and yeah, there's pros and cons. One of the pros are like, if in this case two I talked about where each point is independent but not identically distributed, this all works out. And I'm pretty sure we can prove, we haven't proven yet, but if you do a discounted loss, it's all the same. Once you get to dependence and like indecisence, then you need to start dealing with discounted. To start dealing with discounted in general, but there's certain distributions for which you also don't. Like, if it doesn't mix, you're still okay in a bunch of different. Okay. Now here's week, I basically explained to you the concept. So just say with a much, with a bunch of data, you can do epsilon better than chance, where chance is just say predicting EOI, but not necessarily arbitrarily well. And so our first result is going to. And so, our first result is going to be that in this case where you have stochastic process, the strong and weak are not the same. But to get there, I need to talk about a few more things. One is we have to define a learner in this case. So remember, I said a learner outputs not just an age, but a sequence of ages. I'm going to say this is informally a time-agnostic EPRM, which is the empirical perspective risk minimizer. perspective risk minimizer eats all the past data and outputs the hat which is the same for all possible time okay and i'll talk to you a little bit more about how it's going to do that the time aware vprm this one's time aware it's going to output a hat with a different hypothesis at all at all times including infinitely far in the future um now oh shit that's impossible to read but Oh shit, that's impossible to read, but it's probably for the best. This is the formal definition of a time-aware ERM, and it's based on what we needed in order for the theorem to work. But basically, it's not going to select any hypothesis. It's going to select a hypothesis that like minimizes this max error over our sub-sequences. And that's really the key. Like, you have to be able to concentrate on sub-sequences and look at sub-sequences and find. And look at subsequences and find the minimum there. And so the difference between the time-aware and the time-agnostic is they're both going to be optimizing their thing. The time agnostic is going to have the constraint that every single H is the same. And the time-aware doesn't have that constraint. So, in both cases, they're trying to find this minimum, but they're doing it in a slightly different fashion. Okay. Okay, now here's the first theorem. There exists stochastic processes for which time agnostic ERM is not a weak perspective learner, but there also exists stochastic processes where time-agnostic ERM is a weak perspective learner, but not a strong one. So in other words, weak and strong are different here. And that's different from PAC learning, which you'd maybe kind of expect. The intuition behind that is given by this. Imagine the distribution of your data. Imagine the distribution of your data switching from this discrete thing to this discrete thing every other time. And which thing is switching to and from doesn't matter so much. But imagine the distribution is switching every time such a way that there is a hypothesis that works better than chance on both of them. So it's not perfect on either, but it is better than chance on both. Just like ERM, if you just applied it in the entire future, it would do better than chance. Future, it would do better than chance, so it's weak, and it would not do arbitrarily well, so it's not strong. So, that's the intuition. Okay, so that's nice, but what does that mean? Well, it means if we use the standard toolkit in PAC learning, like we're going to get not strong learners in this setting. And so we need somewhat more advanced ones. And that's partially why we have that definition of the time-aware thing. Before even explaining the Before even explaining the intuition of this theorem, I just want to say: like, the time-aware ERM is not actually complicated at all. Let's just talk about deep learning. You have an MLP, you have whatever features you have in your data, and you have another feature that's time. Now, you have to deal with that feature slightly differently, and I'll tell you about how to deal with it. But that's basically the thing we're saying. Like, you take your whatever deep learning is, or it doesn't have to be deep learning. Or it doesn't have to be deep learning. You could use sparse lasso, or I don't know what your algorithms were called, but whatever, some other stuff. I'm sure it'll apply. And you just add time, like actual the time of the data, the data was generated. It's not positional time, like where in the sentence the word was, but actual time. So like, just think about it in a real world setting. Imagine you have MRI images and you're trying to classify people on a particular diagnostic disorder. Normally, you just have the images and the Normally, you just have the images and their label, but imagine you also had the time that the image was taken. It turns out that's extremely valuable because things change in time, like the MRI magnet might change, the operator of the thing might change, the settings might change, the source code for compiling C in the machine that runs the MRI might change. You may think that doesn't matter. That shit totally matters for MRI imaging. All that stuff matters in changes stuff. And so if you know that. stuff matters and changes stuff. And so if you know that time is there, you can deal with it in ways that you can't deal with it if you don't have time. So that's the basic thing. Now, turns out the theorem says we can get a time aware EPRM as a strong perspective learner under two conditions. One's consistency, which basically means the hypothesis class that you're considering includes something that's like arbitrarily close to the Bayes optimal thing. To the Bayes optimal thing. And the other is uniform concentration. That's what I was talking about before: that there's a sub-sequence of losses that's asymptotically equal to this long-run average. If we get those two things, then we can prove the statement of a theorem, which is kind of impossible to read here. But it says for a stochastic process Z, if we have consistency, which is defined here, and we have uniform concentration, which is here, then Is here, then the time-wear ERM, which is defined here, is a strong perspective order. So that's the core thing. As long as your hypothesis class considers things that are approximately perfect, and you can approximate this long-run average with a sub-sequence, then you can get strong learners. Okay. However, as you know, ERM can't actually run, so does this matter at all? Does this matter at all? In our paper, which I can send you if you want, we prove a few special cases. Like, if the setting is periodic, meaning the distribution shifts every K time steps to something, you can prove it's there's strong learning. If instead it's the HMM, so the distribution is shifting according to a Markov model, then it can be perspective learning. I just want to say it's not like a normal. Like a normal HMM. Like the thing that's changing is the distribution, not like the latent state. So you can think of the distribution as a latent state, but it's like a little bit like the Baum-Welch and stuff, like the math doesn't quite work out. It takes a bit of finagling to show that this is all kind of still works out okay. Okay, so I thought we should run some experiments. So we're starting with something called reversal learning, which is Starting with something called reversal learning, which is like a very standard thing in studying brains. And it works like this: you train an animal on A, and you train it on not A, and then maybe A, and then not A, and A, not A. You see the extent to which they interfere. This is actually one of the first things that happened when in the 70s and 80s, people started in neuroscience and cognitive science started talking about catastrophic forgetting way before anyone in machine learning ever thought about lifelong. In machine learning, ever thought about lifelong learning and catastrophic forgetting, people in neuroscience realize that this happens. And one of my favorite examples in learning a bike. So, if you teach someone to ride a bike, then you do something called flipped handlebars, where when you turn to the left, the wheel goes to the right. And when you turn to the right, the wheel goes to the left. So you can set up a bike to have that, obviously. Teach someone to ride a bike, anyone, if they already know how to ride a bike, and you put them on this thing. And you completely cannot do it. You just like immediately fall on your ass. Immediately fall on your ass. It's actually much worse than if you'd never ridden a bike because you do the exact wrong thing. But after a while, you can learn it and then you learn it and then you get back on a normal bike and you immediately fall back on your ass. You cannot ride a real bike anymore. But you do it for a while and then you're okay and you keep switching back and forth and eventually you can learn how to do both. But they're catastrophically interfering in the sense that your performance goes to the worst possible thing instantly on the previous thing. Thing. Another one of these that doesn't involve any behavior whatsoever is flipped glasses. So, this is also crazy. You take glasses and you make it so what's up looks down and what's down is up. And you have someone wear them. It turns out if you wear that for about a week, you wake up one morning and you don't notice the difference. It just looks like up is up and down is down, even though you're still wearing it. Your brain has just switched. Then you take it off and you're all fucked up. Now. And you're all fucked up. Now, you look up and you're looking down, and vice versa. But you do that a bunch, and your brain figures it out again until you can deal with it. So, the point of all that is like catastrophic interference is a real biological phenomenon. It's not like made up. And this reversal learning thing is something that people study a lot because it's crazy and interesting. So, we just set up standard machine learning problems where there's reversal learning. So, the first one was this synthetic one. This synthetic one. Yeah, the first one is the synthetic one where it's just two-class classifier. Positive numbers are class one, negative numbers are class negative one. Then you switch it. So positive numbers are class negative one, and negative numbers are class positive one. And you switch back and switch back, et cetera. So that's the reversal linear paradigm in like synthetic reels. We also have it set up, say, for MNIST. So you train MNIST, but you label the zero images and one images product. The zero images and one images properly. But then, after a bunch of trials, you switch it. So now all the one images are labeled with a zero, and all the zero images are labeled with a one. And then after a bunch of trials, you switch it back, and you can keep switching back and forth. We also did it because the MNS is too easy, did it with CFAR, with a similar kind of thing. Okay. Now, what are the algorithms we're going to use? Just use a time-agnostic variant of MLP and CNN. So these are not the empirical risk minimization standards. The empirical risk minimization things. I mean, obviously, because we can't run that, but we're just running an MLP or CNN and then like saying whatever hypothesis it learned applied for the entire future. We're also doing fine tuning where we allow it to update its parameters every time step. And then we also have a timeware MLP-CNN autoregressive transformer. And the deal is the timeware MLP and CNN are exactly the same as MLP and CNN. The same as MLP and CNN, but there's another variable and it's time. The key is what we do with time is we pass it through a positional encoding, just like normal transformers do, and we just concatenate that to the input. Just totally trivial. And then we just run, you know, SGD as normal. Autoaggressive transformer is just the normal autoaggressive transformer. So take a regular transformer that was used for language models, but instead of Language models, but instead of positional encoding the relative position of the word, it's just the absolute time of the thing. People have published these before, so it's not really new. We're just using it. And the Oracle is the best you can do, the thing that gets base written. Okay, so here's the results. This is on the synthetic task. It's just one-dimensional. Here's the oracle here. Here's the MLP. Remember, it's like every 10 time steps, the test switches from. Pest switches from whatever the positives are plus one to the positives are negative one. What you can see is here's the transformers. We have two of them, but they're basically the same. And then here's a time augmented MLP that gets this additional variable of time. And you can see they both converge relatively quickly to the Bayes operal performance here, which has turned out as perfect. The MLP is always at chance, right? Because obviously it can't possibly learn anything. Obviously, it can't possibly learn anything. That's comforting. We kind of know what's going on there. Now we have the MNIST test that I described, and it's a similar deal, just zeros and ones, but the labels are swapped. And you can see they also converge. It takes a little bit longer. You know, this was 1D, this is like 700D or whatever. But the same deal, the CNN is not doing anything because it can't by construction. And then finally, CIFAR. We'll use wise ResNet for here. This was, I forget exactly what the CNN was, but here we use wide ResNet and we added time. And here's just autoregressive transformers again. Here we like kind of use the first layer of a CNN to pass input there. But you can see again, it converges to the Bayes optimal. So in all cases, even though the task is switching, it's learning. Though the task is switching, it's learning to predict that there's a switch, which it has to in order to not ever make a mistake or not make more mistakes than you would basically if you just had one task. And so it's learning to perspect, to know when it switches and showing that it gets it right. Yeah, Jeremiah. Here, the way we set it up is the Oracle. We set it up is the Oracle is learning the distribution. So it knows which distribution it which task it is, but doesn't know the distributions yet. So it's got to learn the parameters of the distribution first. So it's kind of like, it's not quite the oracle I said before, but it seemed like a, it's like the best lifelong learner thing you could kind of do with Oracle. Okay, so. Okay, so that's exciting. And now we thought, you know, as we've talked about over the last few days, well, if you're going to do stuff now, you have to talk about LLMs because otherwise, you know, my students don't get internships or whatever. So then we wondered, like, can LLMs perspect? And it seems like, I don't know, they're kind of perspecting as it is, right? They're predicting the next word, which sounds pretty perspective and not just the next word. It seems like they're getting perspective. And not just the next word, it seems like they can perspect like a lot of next words, like thousands of next words, and it all works out pretty good. So maybe they're perspecting. So we gave it this scenario where it's independent but not identical. So it's just switching. The probability of it's a coin flip every time, but the probability of it landing head switches from P to one minus P and back every. And back every in this case, every time step. And so we gave it like, we just gave it a bunch of that, told it that, and said, predict the next 10 coin flips. I'm going to see how well it does. And so here's how well it does. This, by the way, is what we call a time aware MLE. So because everything is so simple, we can estimate the parameters using MLE and then predict what will happen. Predict what will happen in the future. So, we can actually use MLE, which is probably optimal in this setting. And after like about 20 samples, it ends up doing perfect. So, in contrast, we use open AI and stuff, but here's on the open source things, Llama and Gemma, and they are at chance levels. Like, basically, they can't prospect at all. They're doing as bad as possible, essentially. So, LLMs can't. So, LLMs can't perspect as far as we can tell. Now, maybe that's because we did a bad job prompt engineering, because, like, obviously, the prompts matter. So, here's a prompt. Consider the following sequence of outcomes generated by two Bernoulli distributions, where all even outcomes are generated by a Bernoulli distribution with parameter p. And outcomes are generated by a Bernoulli distribution with parameter one minus p. And we gave it an example, and then I say the next 20 most likely sequences of outcomes are, and just record. Outcomes are, and just recorded the output. So we told it everything we could possibly tell it, basically. There's nothing it doesn't know. It shouldn't even have to learn anything. It should just be optimal now. But turns out, do you have a question? I do not, certainly. Try with spaces, I'm set there, not two spaces, okay. So that may screw it up, but there's another That may screw it up, but there's another reason to screw it up, which I'm about to show you, which is if we just tell it: generate outcomes of 10 Bernoulli trials where zero is generated with probability 0.75 and one with probability 0.25, it doesn't. So we had it do that, and then we calculated the probability and like for both Lama and Gamma, and they're basically they center around 0.5, not 0.75. Like it can't generate Bernoulli sequences that Bernoulli sequences that aren't from an unbalanced point. They can do it, it seems like they can do it from a balanced point, but even that, I'm not sure they're really doing that properly. There's an interesting question from my perspective is, if you told humans to do this, humans would do it very badly. Like even you humans would do very badly. In particular, like humans get wrong how long you get a sequence of zeros in a row or ones in a row. We tend to think you don't get that many, but it turns out, You don't get that many, but turns out, like, in a sequence of 100 point flips, you get a lot of like runs of like 10 or 15 in a row, which is kind of counterintuitive for us. So, I wonder if it's doing the same thing that a human would do, in which case it's like interesting, maybe. But my guess is it's dumber than humans, even, is my guess. But the point is, it can't generate Bernoulli sequence. It can't generate coin flips. So, even if it tokenized right, it would still get it wrong. If it tokenized right, it would still get it wrong. And that's a really interesting question. We played around with a few different ways of saying it, ask it to explain its work and stuff like that. It gets the work kind of right, but it gets the math wrong. Like, for example, we give it a sequence and it's like, okay, the first thing to do is estimate P. And it says, well, not in this case, but let's say they were nine ones out of 10. So P is 0.8. That's wrong. That's wrong. Sorry, you counted wrong. Now, maybe it could tokenize better. Maybe if we gave it zeros or ones, it could count properly. But, like, if it can't count that, it's going to have a hard time. So, okay, that's why it fails. What time is it? What time? 4.11. Okay. I'm not going to talk about that because. I'm not going to talk about that because we don't understand it that well yet. So now I'd like to deliberate some things, the pushback we've gotten over the last year. Like, isn't this just time series modeling? Like, isn't that what forecasting is? Like, think about weather forecasting. You have, say, Y is the weather and X is like all the other variables that you measure, and this is, you're trying to predict that. Kind of. Yeah, it's kind of just forecasting. What I would say is, well, our intent. Is what our intention was to take the PAC learning framework, which has like 100 years of theory and methods development, and figure out how to use it in the context of forecasting. Because like when I learned about time series modeling and forecasting, I learned like ARMA and ARIMA and Kalman filter. And then I was like, okay, how is that related to PAC learning? And it was like hard to see how it's related to PAC learning. So from my perspective, arguably this is time series modeling, but from like a PAC learning perspective. Modeling, but from like a PAC learning perspective. Is it online learning? Maybe the typical assumptions in theory for online learning is that there's an adversary or there's no distribution, right? And so the results you can get kind of are very conservative, I'd say. Here we're saying there is a distribution that you can somewhat predict in the future. So you can get much stronger results in that way. Is it continual lifelong learning? Maybe. Lifelong learning? Maybe. I mean, a lot of the continual lifelong learning work, you're told when the task changes, which is not really what happens in many real world scenarios, that you don't know the task has changed. Now, online meta-learning is basically lifelong learning, but you're not told the task changed. So maybe it's like, it is kind of like that. Maybe it's the same as that. I haven't ever seen it quite formalized this way. This way, and but it's pretty close to that in a way. Is it reinforcement learning? Well, here we're really concerned about one single long run trial, like single episode reinforcement learning. So it's like that, but also where the distribution of the goals might change. So it's arguably very similar to a kind of continual single episode reinforcement learning. And as far as I know, Learning. And as far as I know, there's actually no work on that. There's some work on single-episode reinforcement learning. There's some work on continual reinforcement learning where the distribution is changing over time, but not quite putting it together. And the other question is, can we just use a transformer for everything? Or should we? And like maybe, at least today, it seems like it kind of works. It works better than the other stuff we tried here. But I think actually what we're saying is transformers. Is transformers are doing a kind of perspection, and maybe the LLM ones can't quite do it exactly right, but I think there's a close connection to what transformers are trying to do, at least in language modeling, maybe not in vision, but in language modeling and perspection. And so I think there's like, hopefully, there's some work that we can do to try to explain the performance of transformers in the context of this kind of perspective. So, what we want to do next is prove what kind of stochastic processes exhibit these properties of strong and weak perspective learning. We already have like relatively simple, straightforward results on periodic things and HMMs, but we'd like better. We want algorithms that we can run that provably do perspective learning because the ERM ones really can't run. Obviously, they need to scale, and then we deploy them in the real world. We have a couple. We have a couple publications that are related to this. This one is just we submitted a NURIPS and it's not on archive yet because there were some mistakes when we submitted it. And so we're still fixing that. But if you want to pre-print, I can send it to you. And this is my lab. Here's acknowledgments. A lot of this was funded by this. And there's some other stuff also. Some other stuff, also. And then, this is my wife and kids. I'm very grateful to her because she's taking care of all three of my small children while I'm here screwing around with you guys. And I couldn't do that without her doing that. And I'm happy to take any questions. Oh, yeah. So the deal is basically, you just kind of think about like semi-parametric modeling. So as you get, oh, I'm sorry. Yeah, so the question is, why do the hypothesis classes need to be nested? So at each time, we're considering an increasingly large set of hypothesis classes. And it's just like semi-parametric modeling, where as you get more and more data, you can start considering larger, like more and more complicated. Larger, like more and more complicated things. Because as we get more time, we can fit it. So, without that, the theorems I think would still hold, but I don't think the rates would be good at all. And we don't have any rates yet anyway, but like it's to control how quickly, basically how big the hypotheses are when you have a small amount of data.  I'm not sure I understand the question. Oh, well, so imagine like you're test switching between A and B, A and B, A and B, and it turns out like it's a quadratic function. It's a quadratic function. So there's a linear component and a quadratic component. Like after the first bunch of trials, maybe you have enough data to estimate the linear component well, but not yet the quadratic component because you need more parameters for that. And so the way this works is as you get, when you have a small amount of data, we would only be trying to estimate the linear component and not worrying about the variance of the quadratic component. And then once you have enough data, we'll also try to estimate the quadratic component. So by doing that, you can control basically the generalization error. Basically, the generalization error with the small amount of data. Any other questions? Yeah. Yes. Yep. Yeah, it took us a long time. Yep. Yeah, it took us a long time to figure out the right notion of phase optimal because we're kind of combining PAC theory and stochastic processes. And that's the way that we found that seemed to make the most sense. So I have a question that is debated. That's the difference between the two. One of them a priori, or would you be in a situation simply that at the end of the day there is a very large hypothesis class that effectively accommodates all possible tasks? And it's only because even the data is one component of another. And therefore, coming to the second question is implementation to the network, I can begin with something very overpromised. Examples of this is. Right. Samples of these very large subspace, and at first they're one, and then the second they're a third. Right. So the way, at least, the theorem is set up is that the learner chooses a sequence of nested hypothesis classes so that it can converge relatively quickly to the right thing. I think if it didn't do that, it still could converge, but no one has told it that. Like it's given, you know, the full set and it's got a pick. The full set, and it's got to pick this thing for itself. And the rate at which the hypothesis classes grow is actually fundamental for the theorem. Like it can't grow too fast relative to the amount of data. And then, in terms of over-parameterization, that whole thing just confuses me. I don't know. I think what I meant related to his question. His question was typical of continual learning and Learning one approach is the opposite. I begin with a small model, and I keep growing it as new tasks. What I was just saying is the opposite, that you begin with a large one, but with every task, you're just learning components. So effectively, the nested set of hypotheses just merges on its own. But I don't need to pre-specify. Yeah, and so what I can say experientially when we're playing with live phone learning, if you allow the number, if you allow the representation capacity to expand as you get more tasks, that works really well. Other people do things like freeze a bunch of the parameters until they get more tasks away, which is kind of equivalent to the second thing you're saying. And yeah, I guess either could work. I don't have any deep insights. I don't have any deep insights.