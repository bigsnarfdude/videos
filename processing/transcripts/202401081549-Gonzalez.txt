This is joint work with Eugene Gorski, who's at EC Davis, and Jose Methed, who's at UNAM in Mexico City. So I apologize in advance, it's a little bit technical, but hopefully we can get through it. There we go. All right, so thankfully, Sarah just described what the Hecke algebra is for all of us. So I'm going to tell you about the affine Hecke algebra, which hopefully some of you have seen. And it basically starts off the And it basically starts off the same way, right? We're going to think of this as an algebra that's generated by two families of generators. We have the Ti's, which are just the same Heck algebra generators you just saw. But then we're going to also incorporate a new family of generators, new polynomial move generators, called CJs, and their inverses. And they're going to be satisfying the following relations, right? So these are just the Peck algebra relations that we just saw in type A. And in addition, we want the Z's to commute with themselves, and to satisfy this. With themselves and to satisfy this additional relation with the T's. And why do we care about the affine-Hecke algebra? Well, because this is, we're going to build this double dick path algebra essentially out of it. So this is kind of a baby example. So we're going to say that a representation of the affine-Hacket algebra is calibrated if all these Zj operators are simultaneously diagonalizable with non-zero eigenvalues. So a mode. Values. So, a motivation for such a definition, if you're familiar with the representation theory of complex having simple Lie algebras, right, is how do you study it? We studied the Carton, which is a maximal commutative sub-algebra. This is a very analogous kind of construction. You want to say, all right, what's a commutative sub-algebra in here that I can use to diagonalize my representation? So that's kind of a motivation for this. And so, one question that you can ask is: what are the calibrated representations of the affine Heck algebra? How are they classified? Classify. And luckily, a room ram did this for us quite a while ago. And he told us that to do this, we need to study certain sequences, which we're going to call calibrated. And they're just going to be sequences of essentially Qt polynomials that satisfy some condition. You don't need to memorize this condition. This is just, again, the technical part of this. But it's telling you when to think of these as essentially weights, and they're just some conditions. And they're just some conditions on the weights. And then we can define on these sequences an action of the symmetric group, right? So we're going to say that the transposition acting on these sequences, so W with an underlying, is the sequence. And it's going to be admissible, an admissible transposition, if it satisfies this condition. Again, a technical component, you don't need to remember it. Just basically, it's telling you when you can act by these SIs. And the action is just, you flip the I and the I plus first component. You flip the i and the i plus first component in the usual way. The reason this matters is because this is going to allow us to define an equivalence relation on sequences. And so then we're going to say, all right, let me consider vw to be a vector space that is spanned by all possible sequences that are equivalent under this symmetric group action. That again is some technical condition, but all these sequences that are equivalent to this sequence W. And just keep in mind that I'm indexing my sequences. Keep in mind that I'm indexing my sequences backwards. I apologize for that, it's a little bit annoying, but there are reasons for this. And it's a theorem of Ram that, first of all, whenever you start with a calibrated sequence, you can explicitly define a representation of the affine-Hecke algebra, a calibrated finite-dimensional irreducible representation of the affine-Hecke algebra on this particular vector space. This is the explicit definition of the representation. Again, I'm just Again, I'm just showing you to tell you that, hey, look, it's really clear, right? And in particular, you'll notice that the Z's act by just scaling essentially by weight Wi, right? The Wi's are just some Qt coefficient. So these are really eigenfunctions. The second result of his is that not only can you always construct a representation starting with a calibrated sequence, but the converse is true, that any finite-dimensional, irreducible, calibrated representation is. Irreducible calibrated representation is always going to be of this form. So this is a full classification. And moreover, no matter which calibrated representation you give me, the joint eigenvalues are always going to be calibrated sequence. So this is really just a beautiful description of all the calibrated representations of the affine Heck algebra. So what we want to do is do exactly the same kind of you know a classification result for this thing called the double dig path algebra. Called the double dig path algebra. And so to do that, let me explain a little bit what this algebra is. So this double dig path algebra, which I'm going to call BQT, and its polynomial representation, which I will define shortly, originally arose as a central component in the proof of the celebrated shuffle theorem of Carlson and Mellet. I'll define it shortly, but let me just motivate it a little bit. It's very, very intricately related to the shuffle and the elliptic algebra. It actually contains these algebra. Algebra actually contains these algebras, the sub-algebras. It's related, although a little bit unclear, to the Daha. It's a cousin of the double Athena-Heke algebra. It's intimately related to Skein theory and Kamon-Brzonski homology of toroid knots. McDonald polynomials, there's a geometric representation of those on the Hilbert scheme of points in C2. So there's a lot of geometry showing up behind this. But despite the fact that it's connected to a lot of very interesting mathematics, the only known representation for BTT. Representation for BTT until this paper that I'm going to talk about is just this polynomial representation. And so our goal was, let's find, first of all, define what new calibrated representations of BQT are, and then we'll classify them. So let me define it. So BQT is the non-mutable algebra with generators. We're going to say Ti, these are just going to be Hecke algebra generators. ZJ's, affine Hecke algebra generators. So these two we've seen before. generator. So these two we've seen before. We have d plus and d minus. We want to think of these as raising and lowering operators. I'll draw a little picture that will make sense. And this is just the commutator of the raising and lowering operator. It's there because it's helpful for the relations. So you want to think of this really as kind of a quiver where at each level, 0, 1, 2, 3, you have an affine Hecke algebra of level n, right? So here you have the t's and the z's, they're acting locally. The T's and the Z's, they're acting locally. You have a little affine heck algebra of level N. The Phi is just an extra generator that lives there. And the D plus and the D minus kind of move you from one level to the next. This is infinite. It goes on forever. And then it has a bunch of relations. Don't worry about them. They're just there again for show. All to say is that this algebra is infinite dimensional and quite complicated. Let's just accept that. And take that as a black box. And pointedly, I should say it's the positive half. I should say it's the positive half because my z's are not invertible. So these are really just polynomial generators, not like in the affine height algebra. What is the polynomial representation of this, of BQT? Well, it's basically you want to take just your usual ring of symmetric polynomials in X's, and you want to take a tensor product with a polynomial ring in Y's. So these are not symmetric, and these are symmetric. And you want to take a big direct sum of moles. Want to take a big direct sum of all of this. So it's some big infinite dimensional vector space of symmetric functions tensored with polygonal moves and y's, essentially. And the representation, again, this is explicit. This is what lies at the heart of the proof of the shuffle theorem. It's some explicit operators in terms of Demogergoustic operators and platistic operators. The formulas would take the whole page to write. Again, complicated representation, but it's very, very explicit. And this was all that was known. So, how can we expand? So, how can we expand this? Well, the first thing we needed to do was we needed to actually make the algebra bigger, which seems a little bit counterintuitive. And so, we extended the algebra by a new family of treating operators, delta operators, indexed by symmetric polynomials. In this particular case, we're going to take power sums to index these. And you'll notice that they basically commute with everything except the raising operator. That's the only one that's different. Everything else, these are basically commuting with everything. And what they will do, Everything. And what they will do is essentially they'll allow us to chop up our weight spaces into one-dimensional pieces. That's why we need to cut them. On symmetric functions, if you restrict this to a polynomial representation, they're really just McDonald's operators if you know what these are. All right, so here's the first definition that's new, which is what is in a calibrated BQT representation. What's very similar in the sense that we're going to start with a graded representation in order to make it easier. In order to make it easier. And what we request is that it has a basis of simultaneous eigenvectors for both the Zi's, which were the same as in the Affine-Heck algebra, but in addition, also for these new delta operators. Again, we just needed them to chop our spaces into finer pieces. With the same conditions as before, we want the eigenspaces to be one-dimensional and the eigenvalues of these ZI's to be non-zero. So we just have a big family of eigenvectors that are going to be nice. Nice. And so, what's the idea is that we're going to, rather than having calibrated sequences for the affine-Heck algebra, what we want to do, the way we're going to classify our representations, are by looking at calibrated chains in certain posets instead. But again, this is all kind of building off of what a rubric did. So, again, I told you it's pretty dense. So, in order to do this, we need a weighted postet. So, we're going to say that a weighted postet is just going to be a graded postet. Is just going to be a graded poset with a collection of symmetric functions that assign essentially to every element lambda in my poset E some symmetric, some Qt value, right? With the condition that if mu covers lambda in my poset, right, then there's some addible weight, we're going to call that x, that satisfies this relation. It gets some technical condition. But what this is basically telling me is that in my poset, I want to think of there's a path from lambda to mu index. There's a path from lambda to mu indexed by x. The reason we need this is because that's exactly how we're going to think of chains in our postet. So, our chains, we're going to index them by some element in E, and then some path from lambda to mu, and that path is given by addable weights, and those, therefore, we can combine all those weights into a sequence. So, a chain is just some starting point plus a sequence, such that at every step, Sequence, such that at every step, obviously you're atom. These are the last two definitions, and then we'll get to the main thepologies. So in the Affine-Heck algebra case, we had the notion of a calibrated sequence. The analog for us is a good chain. So we're going to say that a chain is good if, again, you have some technical condition on the weights. They're not multiples of T's. And we're going to say that a weighted post-it is excellent if whenever you have... Whenever you have a two-step chain, which is to say you start with an element, x is addable for y, and then, or for lambda, and then y is addable for lambda union x. So you can first add x, and then you're allowed to add y. Some technical condition on the weight, and some other condition, where this is not true, or this, sorry, either this is true or this is true, or you can also first add y and then add x. That's what that last condition is saying. So, anyway, you can define. Anyway, you can define particular sets of weighted postets and particularly types of weights. So the main theorem that we are interested in, oh, it's hard, jumping ahead of myself. So why is this a good definition? Like, why do we care about excellent posets? Well, the first reason is that if you're in an excellent postet and you look at a chain in an excellent postet, you actually recover a calibrated sequence. So again, we're trying to generalize this construction of Room and RAM. So this is really the right definition in order. So, this is really the right definition in order to recover the classical setting. And so, let me give you an example. So, if you let E be the poset of partitions, then you can find a weighting on this poset by just taking the weight of a partition to essentially be the sum of the Qt contents of the boxes. So, here, R is the row, C is the columns. Just look at each box. You count Q to the row minus 1, T to the column minus 1, you sum over all of this. Column minus one, you sum over all of this, this gives you some sort of Qt polynomial, and this is the weight. And with this particular definition, you end up with an excellent postet. How much time do I have left? Five minutes? There's less than five minutes. Okay, I'll hear it. All right, so let me get to the main theorem. So if we let E be an excellent weighted poset, then playing the same game, we can let our vector space be the span of all possible good chains. In my poset, we take a direct. In my postet, we take a direct sum of these because we want it to be graded. We define some explicit action where the t's and the z's are really just acting the same as in the rune-ram's construction. This is acting by some sort of eigenfunction. And then the d plus and the d minus act in some different way, right? The raising and lowering operators are a little bit harder. And let me maybe very quickly point out that these d pluses do depend on a choice of coefficients, but I'll address that shortly. But I'll address that shortly. So, here's the main theorem. So, if you start with an excellent postet, then no matter which excellent postet you give me with the previous construction, you can always construct a calibrated EQTX representation. Sorry, that representation will be calibrated if and only if these coefficients in the formula for the raising operator satisfy this very specific condition. And actually, the choice of coefficients doesn't really matter. Choice of coefficients doesn't really matter because, as long as they satisfy this equation, the representations themselves won't be isomorphic. So, that choice is a little bit artificial. So, this gives us one direction. Given a poset, you can construct a representation that is calibrated. The converse is also true, which is to say, if you start with an arbitrary calibrated representation, subject to some extra conditions, then you can always construct a win-opus-it from it. And again, if you add a little bit, a few more. Again, if you add a little bit, a few more conditions on a representation, then the post-it is actually excellent with an isomorphism between the representation and the representation you would get from the post-it itself. So this is kind of the converse direction. It's not a perfect converse direction, again, because we have a few extra conditions that we have to add, but this is essentially a classification of the calibrated representations for this algorithm. So maybe let me very quickly give you some examples. Give you some examples. So we can construct interesting finite-dimensional representations depending on which posets you pick. In the case of the poset of partitions, we actually recover the polynomial representation that showed up in the proof of the Shuffle theorem, which shows that the representation is calibrated. But you can construct a bunch of things out of this, right? So you can construct ideals and co-ideals, dual posits. We classified the hom spaces between the representations. We defined the tensor products of the representations. This was actually quite difficult. This was actually quite difficult. And using this, we ended up with a geometric interpretation of the Rth tensor product of the polynomial representation in terms of parabolic H-security spaces. So maybe in one minute, let me say, why would you care about this? Because obviously, this is a lot of representation theory. So, from a combinatorial perspective, one of the things that I think is interesting is that I mentioned the elliptical algebra lives inside BQT. Call algebra lives inside BQT, right? This double dig path algebra. And there's a lot of work recently due to Anna Kuhn and all her cattle animal friends. She's somewhere here. Where they've proven a lot of things by constructing, by basically proving a lot of combinatorial information by using the polynomial representation of the elliptic call algebra, which is contained inside the polynomial representation of BQT. So if we have a huge family of new BQT representations, that gives us, as a consequence, a huge family. Gives us, as a consequence, a huge family of new elliptic Hall algebra representations. So, you can ask: what is the combination of works behind all these new representations? And how does that connect to all the theory that we already know is super interesting for this one very specific choice of representation? So, that's it. That's my talk off and now. What's the case? What? You know what might be end? What? No, what might be the end? I don't necessarily have a question for this. Okay, okay, good. That was all of the technical stuff. So no questions associated. Yeah. I'm just wondering, like, the DICTAP algebra is actually the idea was from the DICPAP, and then the D plus and the D minus have something to do with the DICTAPs. So how about with this, like, extended version of that? Version on that. When you look at the data. Yeah, yeah, yeah. Okay, so this is. So I lied a little bit. Right, the algebra that you're talking about is AQT. This is the one that actually showed up inside the proof of the shuffle theorem. What we're defined was this algebra BQT. And the reason I've also called it the double dig path algebra is because it contains AQT and is contained by BQT. So these algebras are to sum. So these algebras are, to some extent, they're infinite dimensional, so this is why this doesn't, it's actually not that weird. But these algebras, in some sense, are equivalent, but this is a simple algebra. So unfortunately, trying to play the same game directly with this one was much harder because trying to define the delta operators here and extending this by delta operators was like not immediate. So the answer is we don't know yet. There should be some sort of formulation just because these algebra are 100% equivalent, it's just not clear. I have no idea. I don't know what the parallel algebra is. So maybe remotely. Do you have more slides in your I emailed you last? Oh, thank you. I got my gun. Sorry. No, I can't. Yeah, yeah, yeah. Yeah, yeah, it was okay. Okay, I don't want to have anyone. Right, right. I got two tips to stay. Oh, okay. I think it's a good idea. Sorry about that.  You want to go down together? Just stare at it. What kind of garbage is this? Okay, so for our next talk, we have Jennifer Elder. He'll be talking about cyclists of being on permutations, an analysis of maps and statistics in the fine stack definition. Is this still recording? It is recorded. I've never stopped. All right, so as you can see from my co-author list, there are a few of us. There are a few of us in the room, not just me. So, this talk is going to have a lot of math. I am going to skip over a lot of the details. It is a 15-17 minute talk. But it's also kind of the story of the impact of workshops like this, because this group was formed at the Research Community and Algebraic Cognitorics Conference at ICERM two years ago, where I had just graduated from my PhD. I had just graduated from my PhD, was suffering from a little bit of burnout, and having a really great research group like these workshops help form was extremely helpful for me personally and for my career. So our first paper, I'm introducing to you without telling you how it connects to cyclic sieving. This was the first project that we worked on. This was the first project that we worked on together. So, a few brief definitions. Consider a set, an action on that set, and some sort of discrete statistic. Homo Messi happens whenever the average of the statistic over the entire set matches the average of that statistic on each orbit of the action. Our set that we focused on was Sn, and then we looked at all of And then we looked at all of the possible statistics and maps found in the Feinstat database. If you have not visited Feinstat or if you have not heard me or Jessica or Nadia give a talk before, this is Feinstat. Pick your favorite combinatorial object. It's probably in here, along with some maps to and from that set of objects and That set of objects and discrete statistics on those objects. So, as of the end of this last year, there were 24 maps from SN to itself and about 400 discrete statistics on those permutations. At the time of our original paper, the collection was just a little bit smaller though. So, we were only considering all possible combinations of 387 statistics. Of 387 statistics and 19 maps, which without SAGE would have been a very large task, but we did have SAGE. And we asked the programs to tell us, do we have a counterexample for something that is not homomethic, this map paired with the statistic for S4 or S5 or S6? Or do we have some hope? No counterexamples, there might be something. No counterexamples, there might be something to prove here. And we were able to narrow it down from all of those different combinations to about 117 potential homo messetes. And we were able to prove them all. So I have this list, and this list is mainly for comparison purposes. So we can see that a map called the Lemmer code rotation had the most homo messy. A complement and reverse map. Complement and reverse maps followed up in a very close second, with then a few other unique maps in last place. So after that project, we wrapped that up, we started moving on to the next thing, and then Nadia gave a talk where it was suggested that maybe we could extend our methods to cyclic sibbing. And then I had to Google what cyclic sibbing. So, if you're unfamiliar with it, like I was, Bruce Sagan has a really great survey that you can read of sick with sivving. Per Alexanderson also has a website that you can go to where he has compiled all of these sick with sivving phenomena instances in literature. But to put it simply, as simply as I can anyway, you consider a set, an action, and a polynomial. Polynomial. And if you take the order of that action, k, and a kth root of unity, then cyclic sieving occurs whenever you take the kth root of unity to the dth power, plug it into your polynomial, and you get back the number of fixed points from the dth power of your action. Again, very, very brief intro to cyclic sieving. And so for Exhibiting. And so for us, the reason this was suggested as some sort of natural extension of our work was because we could take our set SN, we could take all of our actions, our maps that we'd already looked at and the new ones, and then pair it with statistic generating functions. So step up from statistics to the generating functions. So, we have our new group, all five of us returning, and a new member, Ashley Adams, that some of you may know. She's going to be starting at, well, this week at North Dakota State. And this was a very lovely week working on this project in a lake house in Minnesota for a week. It was great. So, the numbers on this slide may have changed since I wrote them. I say we proved 53 instances of cyclic sitting in four conjectures. That might be down to two conjectures, depending on how the next few weeks go. But again, FindStat pairing up with Sage did so much work for us when you're considering 24 maps. Considering 24 maps paired with 400 statistic generating functions, ruling out with counterexamples everything that wouldn't allow us to see the cyclic sieving phenomenon. We also were able to track down seven instances that had been previously proven in the literature for SN. We also, as we were working on the problem, we did discover a few instances where we could prove cyclic sieving for. We could prove cyclic sieving for infinite subsets of values of n, say all even or all odd. We did not go back through and try to do that in a comprehensive way because that would have been a lot of work. And so here's a summary of our results. Again, as a kind of comparison to our previous paper. So you can see that the reverse and the complement have an option. Have an awful lot of examples of this phenomenon, whereas the Lemmer code rotation only has one. So if you're talking about similarities and differences between homo Messi and cyclic sieving, Lemmer code rotation is a great example of these two phenomena are not the same thing. Then we have all kinds of new maps that we did not study or discuss in the previous paper. Or discuss in the previous paper with just a few instances. And today, mainly, I'm going to talk about the various techniques that we used to then prove our results. We had evidence to support these, and then we needed to actually use combinatorics to prove them. Now, first technique was extremely useful for conjugation by the long cycle, which was the one map that nobody in the room was enthusiastic to work on. Nobody in the room was enthusiastic to work with. The orbit structures were going to be more challenging than something simple like the reverse. We were not looking forward to it. However, when you do literature searches, we were able to find previous examples of conjugation by the long cycle proven for a few of the statistics in our list. And so our main contribution then was to take the statistics that weren't found. To take these statistics that weren't found in the literature that Sage had suggested to us, show that they were equidistributed with the ones that were already proven, so that we now had a more comprehensive list of statistics paired with conjugation by the long cycle. So, equidistribution can do an awful lot of work if you already have results and you're staring at the lake and going, I