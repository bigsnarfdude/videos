Very much for the introduction. I will talk about the power of patches for training normalizing flow, and this is a joint work with a couple of people, namely Fabian Altegruger, Alex Denker, Paul Hageman, Antoine Houdar, Peter Maas, Claudia Riedenbach, and Gabrie Steidel. In this talk, I want to introduce two kinds of patch priors, namely patch normalizing flows and Wasserstein patch priors. For this, we first introduce normalizing flows, then we will see how we can do point estimates with Uh, do point estimates with patch normalizing flows and Wasserstein patch priors. And at the end, we will use also the Wasserstein patch priors for posterior sampling. So, we start with a short overview over normalizing flows. And the goal of a normalizing flow is to estimate a complicated data distribution Px here. And to this end, we consider a much simpler latent distribution Pz, which has the same dimension and is usually a standard Gaussian distribution. And it's usually a standard Gaussian distribution. And now our goal is to approximate the distribution px. And for this, we want to learn interferomet such that px is approximately given by the push-forward measure of t and this latent distribution pz. And then we call this diffeomorphism t the normalizing flow. And when we have found this diffeomorphism, we can do two things with it. First of all, we can sample from our First of all, we can sample from our approximation by just drawing a sample from the latent distribution and putting it into our normalizing flows. And second, we can evaluate the density of our approximation by representing the density of this push-forward measure here by the density of the latent distribution and the determinant of the Jacobian of the normalizing flow. And now we will approximate this diffeomorphism T by neural networks. And since this has By neural networks, and since it has to be a diffeomorphism, we need that the neural network is invertible. And yeah, this slide is actually about things where I don't want to talk about it, namely the architecture, how one can create the neural network such that it is by definition invertible. There are many approaches in the literature, but I don't want to go into details here, so I just put the references here. In order to train normalizing flows, we need a loss. Train normalizing flows, we need a loss function, and we base this loss function on a similarity measure for probability distributions. The similarity measure here is a Kulberg-Leibler divergence. The Kuhlberg-Leibler divergence of two probability measures, Px and Py is given by an expectation over the probability measure Px of the logarithm of this quotient of the densities here. And by using the rules of the logarithm, we can just split it in such a different. We can just split it in such a difference here. And the Kulberg-Leibler divergence is always greater or equal than zero. And it's equal to zero if and only if the two probability measures are equal. However, the Kuhlberg-Leibler divergence is not a metric because it's not symmetric. And therefore, we have two possibilities to create a loss function out of measuring the similarity of our approximation in the data distribution by just interchanging here the order of the arguments. And so we arrive at two different loss functions, and they have two different names in the literature. The first one, where we put the data distribution px, which we want to approximate in the first argument, is called the forward KL loss function. And if we compute then these terms, it's up to constant and equal to this term here. And the main ingredient here is that we have here an expectation of That we have here an expectation over the data distribution p and otherwise the only terms which we can compute. So, in order to train a normalizing flow with the forward KL loss function, we just need data from the distribution which we want to approximate. And we don't need to know the density. And vice versa for the backward KL, we can also compute this Kolberg-Leibler divergence up to some constant here. And here we have an expectation over the latent distribution, which is nice because we can generate Nice because we can generate as many samples from our latent distribution as we want. But here appears the density of the data distribution in the other terms. So, in this case, we don't need any data from our distribution which we want to approximate, but we need to know the density. So, it's a little bit like in the Markov chain Monte Carlo in that sense. Okay, now we want to use these normalizing flows in order to propose some patch priors. Patch priors. And to this end, we consider an inverse problem where we have some round truth X, which we put in an ill-push-forward operator, and then we apply some noise process in it in order to obtain some observation y. And here we use the Bayesian viewpoint on it, where x and y are random variables, and x follows some prior distribution px. And now we want to reconstruct the ground truth. Reconstruct the ground truth as the maximum a posteriori estimator, which is in some sense the best reconstruction which we can find, in the sense that it maximizes this density, this conditional density here. And using Bayes' theorem, we can just rewrite this as a sum of two terms here, namely the likelihood and the prior. And this leads then to our regularized functionals, which we always consider with data. always consider with data fidelity term and regularizer. And the goal, what we want to do now is that we want to learn the regularizer. And we assume that for learning the regularizer, we have given some example images from the ground truth, but we don't have given very many of them. We have just given very few of them. So one or two or five or something of this size. And since we don't have very many images, we Images, we reduce our attention to small patches because even those few images contain many patches such that we can neural networks with them. And finally, we adapt our methods to certain classes of images, which can be CT images or images from textures or material microstructures or something like that. In some sense, we will also treat natural images, but I put it a little bit in brackets and we will see later because we will treat them in a Because we will treat them in a special setting only. And now we want to create our regularizer. And in order to define our regularizer, we take our few example images and we extract all p times p patches from our example images, where p is relatively small, so we always use six times six patches in our numerics. And then we train a normalizing flow which approximates Which approximates the distribution of these patches from the training images. And in order to define the regularizer, we then take an image x, and from this image x, we extract all patches of them, and then we sum up the log likelihood of these patches over this, and there's missing a minus. We sum up the negative log likelihood of these patches under our learning. Matches under our learned normalizing flow here. And once we have defined this regularizer, we can reconstruct our x just by minimizing now our term consisting out of data fidelity term and regularizer. And we had this Bayesian interpretation in the beginning, and this Bayesian interpretation in the beginning said that the regularizer corresponds to some prior distribution by this formula here. This formula here. But in order that this really defines a probability distribution, this has to be a density function. So this term has to be integrable. And in fact, this is one property which we can show about our patch normalizing flows as long as our latent distribution is a standard normal distribution. So in this sense, the patch normalizing flow regularizer really defines a probability distribution on probability distribution on the space of images. So now we can consider some numerical results of the patch normalizing flow. We applied it here on CT images and we did it really in a naive way. So we took a data set, the load pub data set, and reconstructed, took from there the forward operator and the images. And here we have on the left the ground truth image. On the left, the ground truth image. Then we have here some reconstructions, which all don't need very much data for reconstructing that. And here on the right, we have one reconstruction, which is trained by an end-to-end neural network on really much data. So we had here 35,000 training images for this on the right. And I think in this example, one doesn't see too much difference. Maybe the patch normalizing flow recovers the structure a little bit better. The structure a little bit better than some of these comparisons here. But as we only tried it, trained a regularizer here, we can just use the same regularizer without retraining and apply it for a completely different inverse problem by just taking a limited angle CT regularizer. And here the differences are a little bit better visible that the structure here is recovered very well by the patch normalizing flow regularizer. And for many of the comparisons, it's not recovered. It's not recovered. And we can see that the patch normalizing flow regularizer is even close to the supervised trained method here. It's not exactly the same. It doesn't reach it exactly, but it's still closed. And yeah. Now I want to present one extension of the method from super resolution of natural images. So the problem of super resolution says, okay, we have some high resolution image here on the left and Image here on the left, and what we have given is some low resolution image, and we try to reconstruct the high resolution image from the low resolution image. And here we use some idea which appears in the literature as internal learning, and which basically says, okay, if we have a natural image, then the patch distribution is more or less invariant across the scales. So the low resolution image has approximately the same patch distribution as the high resolution image. As a high resolution image. So, in this case, we can just say, okay, we train our patch normalizing flow based on patches which come from the low resolution image. So, in this case, we don't need any high training image at all. But this is completely adapted to the special image there. So, if we now consider different reconstruction, we have to retrain our regularizer for any observation. And also, here we can consider And also, here we can consider some numerical results. And for these numerical results, here we have also considered some comparison methods with similar assumptions here, in particular, the original interning-learning method here. And again, in the images, one doesn't see that much a difference, but in the error measures, we see that the patch normalizing flow is really an improvement over the other methods. Over the other methods. Okay, now we come to the second regularizer, which we want to consider. And it's also here we adapt to a special problem, namely the super resolution of materials microstructures. And for this, we consider the super resolution problem where our forward operator is a concatenation of some blur and some downsampling. And here we assume also additive Gaussian noise. And additionally, And additionally, we assume that we have one reference image given from the same material. So we have given the low resolution image and one reference image which comes from the same material, but doesn't show the same area or something like that. And now we want to reconstruct the high resolution image. And the basic idea of the prior, which we will use now, is that images which come from the same material or the same texture for texture-like images have a similar pattern. Like images have a similar patch distribution. And to this, we construct the following regularizer by saying, okay, we take an image X and from an image X, we extract the empirical patch distribution. That means we take the image X, we extract all patches from our image X, and then we have many points in the R36, and then we take the empirical distribution, which just sums up these point measures. Up these point measures. And then the Wasserstein patch prior says: okay, we construct the regularizer which measures the Wasserstein 2 distance between the empirical patch distribution in our input image X and the one reference image which we have given now. And now then we can compute our reconstruction again as a minimizer of the data fidelity term and the regularizer. Data fidelity term and the regularizer here. And similar as before, we can show that this term, which corresponds to the prior distribution here, is again an integrable function. So this again really corresponds to a probability distribution on the space of images. And now I've just written here, we do gradient descent on this term here, which might not be that easy. And therefore, we have to invest. We have to invest some effort into it. And in order to compute the gradient of the regularizer here, we use the dual formulation of the Wasserstein distance, which introduces, which represented as a maximization problem of some dual variables psi here. And now one can show that the gradient of the regularizer, which is the gradient of this maximum of this function, which appears here on the right, is exactly the same as. Is exactly the same as the gradient of the function of the right with respect to x if we insert here the maximizer. So this leads to a two-step optimizer, two-step procedure in order to compute the gradient here. First of all, we compute the maximizer of this function h here with respect to the dual variables, and then we can do one gradient decent step of our whole functional where this is the gradient of the data fidelity. This is the gradient of the data fidelity term, and this is the gradient of the regularizer here. But this procedure here can be computationally very costly because we have for each step of the gradient to solve this min-max problem. And therefore, we say, okay, if we have now several low-resolution images available, then we train a neural network which says, okay, we take as an input. Okay, we take as an input our low-resolution observations, and as an output, we put out the map estimator for the specific input. And this corresponds to the loss function, which looks quite similar to our regularized problem, which we had in the beginning, where we have here in the beginning the data fidelity term and here the regularizer, and we just replace our reconstructions x now by the output of a neural network here. And also. And also, here I have some images how this looks in practice if we consider this, and for some textures in this case. And probably it's not that much visible from the back, but one can see that the Wasserstein patch prior retains always a little bit more structure than the other comparison methods. And we can also see that this regularizer, because it really Because it really penalizes differences in patch distribution, is very robust against some incorrect operators. So what I have done here is I inserted for the training a different operator, a slightly different operator than it was used for generating the low resolution images. And one can see that the Wasserstein patchboy reconstructions still get roughly the shape of it, but Roughly the shape of it, but many other methods break. I don't say that it's the only stable method against these perturbations, but it is kind of stable against it. Okay, now we want to consider one extension of this, namely, so far we always reconstructed one reconstruction for one observation. And what we want to do now is: well, we have still the same inverse problem, but Problem, but now we want not to create one reconstruction because we want also to measure uncertainties or something like that, but we want to sample from the whole posterior distribution and we want to create several high-resolution reconstructions here. And to this end, we make use of something which is called condition normalizing flows. And the basic idea is that we add the That we add the low resolution observation y here as an additional input to our normalizing flow. And then for each fixed y, we want that the push forward measure of t with respect to the latent distribution here results into the posterior distribution here. And in order to train this, we can use still one of the loss functions, the backward loss function, where we have here on the left. function where we have here on the left our approximation on the right the object which we want to approximate and since we want to do this for all observations y at the same time we adhere the expectation over all over all observations and this can then be reformulated in terms of this likelihood which is similar to the data fidelity term the prior distribution which we have the prior density which we have to know but which is Have to know, but which is related to the regularizer, and again, some determinant of the gradient of the normalizing flow. So, in this case, we don't need any labeled data, so it's an unsupervised, but we need to know the forward operator and we need to know the prior density. And now, the idea is that we take our regularizer and we insert the term which corresponds to the prior distribution here for the Distribution here for the regularizer here, and then we obtain a loss function which allows us to train such a normalizing flow and to reconstruct the posterior distribution. And also here, I have some numerical results. And on the left, we have the high-resolution image. Then we have here the low-resolution image. And here we have three different predictions. And if one looks at these edges. One looks at these edges here, one can still see that they are really different in the sense. And we can also plot the standard deviation, which gives the output that the reconstruction is uncertain on the boundary of the structures, which shouldn't be surprising at this point. We can also increase the magnification factors here of the super-resolution problem. And in this case, the differences become much more visible. And we see that there are really structural differences. So here, the structure. Really, structural differences. So, here the structure is open, here in the other two, it's closed. And similarly here in the bottom, it's sometimes open and sometimes closed. And of course, the uncertainty increases in this case. So I was obviously a little bit faster than I thought when I practiced the talk. So let me draw some conclusions. Patch briars can be used for learning with very Used for learning with very few images because even if we have only a few training images, we have many patches in the images and we can learn use our neural network and generative model machinery. And further, we can do map estimation by solving this variational problem with our regularizer. And in the case of the Basel patch prior, we have also done posterior sampling by the use of the backward KL where we didn't need any lattice data. Where we didn't need any latent data. And we applied it on several kinds of images, namely CT images, materials, microstructures, textures, and also, in some sense, natural images. There are some possible extensions of it. First of all, we could wonder if we can also do posterior sampling with the prior which I have presented in the beginning. And in fact, it's already done by some people, which weren't me, and they call it. And they called it a normalizing flow ULA. And as the name said, it's based on some Legeva sampling methods. And the second point is that the choice of patches is not the only possible choice to reduce the dimension of the images. And we could also try to use some deep features of this. And in fact, this is something what currently a master student in our group is doing and exploring if this works. Doing and exploring if this works. And finally, computing the Wasserstein distance with this minimax optimization procedure is computational costly. And one could think about to replace the Wasserstein distance by some other probability metric, which is maybe more scalable to large images or something like that. So now I'm at the end of my talk, and thank you for your attention. Are there questions? Yeah, thank you for this very interesting talk. So one, so you optimize through a neural network, right, when you do the reconstruction for the patch normalizing flow. What's your experience with that? Did you have problems with? That, uh, did you have problems with divergence or running out of the basically training space, stuff like that? Because that's something I always worry about. If I have some train some function and then I optimize the input, that's essentially the same process that you use for very close to the process that you use to create adversarial examples. And I'm always surprised when it doesn't work. And I'm always also surprised when it works because I have no idea when it works and when it does work. When it works and when it does work. Yeah. I mean, it's just, yeah, it's a mystery to me a bit. Well, I mean, there are some issues which we observe, namely that it can happen that we get stuck in local minima or something like that. But what doesn't happen is that we leave the region of the images. And I think I cannot say, I cannot give you a theoretical implanation of this because we still approximately. Explanation of this because we still approximate a neural network. But in this case, any pixel is covered by many pixels, by many patches. So if you have something like, if you optimize through the network, you wouldn't have to attack only the network once. You have to attack it at several positions. And this makes it maybe in this case a little bit more stable against such effects. Yeah, that's a very interesting point. Thank you. Thanks for your talk. I have a few questions. The first one is: I mean, you have the normalizing flow prior and the Wasserstein distance-based prior. Yes. What would you say? Which one is better? Depends what you are doing. The Spasserstein distance prior is restricted to a certain class of images or something like that, because you always need this similarity of these patch distributions between. This patch distributions between your reference image and your reconstruction, which can which is given for textures, okay, which is given for these material images, okay. But as soon as you start to do some natural images, and I take one reference image which shows a dog and one which shows a giraffe, then it's hard to assume that they have the same patch distribution because they show different structures. But isn't this just all about the training data? I mean, if you would train the normalizing flow just on. The normalizing flow just on a particular image class, then it's the same red. Yeah, but the difference is for the normalizing flow patch prior, you always say, okay, the density has to be large in this. There's no covering that any patch from the reference from the training images has to be somehow contained in your reconstruction. And this is not the case if you look at this faster shade patch prior, where you really say the patches. Prior where you really say the patch distributions have to be equal, so any patch which is in the reference image has to be in your reconstruction and vice versa. But this depends if you use the forward or backward KL, right? The mode collapse versus mode covering problem, right? Yes, it's one of the points, yes. Okay, and for this problem of estimating the gradient in the Wasserstein distance, could you use something like a primer dual algorithm here? I mean, this calls for primary dualist. Here, I mean, this calls for primal door, right, Maximin? Probably, probably you could, yeah. Uh, I haven't tried it so far, but yeah. And it's differentiable? Uh, there are some almost everywhere statement that it's differentiable, and yeah, so uh, if you have bad luck, then it's maybe not differentiable, and then yeah, so let's hope for good. So let's hope for good luck. Thanks. No? Okay, then you just no, no, we are not in a rush. I guess I kind of follow up on Tom's question. So, you know, so if I understand correctly, you're actually learning a prior here, including the constant, right? The normalizing constant, unlike, say, score-based models that learn the gradient of the log prior, and they never learn the constant. The log prior, and they never learned the constant. You're learning the constant too, I think. You have a fact, yes. I learned the prior distribution, and I have exactly exactly the density at the end. So, for learning score models, people always add noise. If your distribution lives on a manifold, right, it's very hard to learn this sparse distribution, but you don't seem to have that problem. You don't need to sort of add noise or smear out your distribution, right? You're using the original pattern. You're using the original patches directly, right? I mean, there's one big difference: namely, if you learn in the image space, in the space of the full images, the really hardest normalizing flows. But we are in dimension 36, which is much nicer than this really high-dimensional stuff. And learning these normalizing flows in the same way score models on these large images is probably not possible. Can you go back to the equation where you actually put together? Can you go back to the equation where you actually put together the where you use this ek as a regularizer, right? So you have to sum over the patches at some point. And I never saw the sum over the patches, but it probably was there and I just missed it for the reconstruction step. Yeah. So let's see here. Okay, I'll think about it more instead of taking everybody's time. Okay. If there are no more. If there are no more questions, let's thank Joanis again.