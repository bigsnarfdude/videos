System. The total lattice is known to be a completely integral system. And what I'm doing here is writing the total lattice in terms of an evolution of polygons, an evolution that, when written in, that is invariant under the group, and when written in terms of the invariance, will give you the Toda lattice. So this would be an equicentrifying realization of Toda. So essentially, a geometric realization is a rewriting of the equation, not in terms of Of the equation, not in terms of by assuming that the independent dependent variables are invariants in some geometry and trying to find the evolution of polygons in that geometry that would induce the integral system and the invariance. So, if you want, it's adding a group rather than taking it away. All right, so there are many authors that work in geometricalizations one way or another. One way or another, as part of the research or other reasons. And I put a partial list here. I'm sure I'm missing a very large number of them, probably some of the ones attending this conference. So my apologies to everyone who is not in that list and feels it has to be. I want to talk about work, current work with Antonio Sosimov. We're writing a paper. We're writing a paper, hopefully, it will be finished this year. And it's also based on work with Jim Ping-wan and Anna Calini. Both of them are in this conference. So let me define what the modularized space of twisted polygons is. So first, twisted polygon. What's a twisted polygon? I don't know if you can see everything. So I'm going to assume that I have a manifold which is a homogeneous space, DLD group H. GLD group H, the isotropic group of the origin. And we say a polygon is twisted. And can you see the entire? I can't see it, but you, oh, you can. Okay. It's twisted with a period n. If there exists an element of the group such that when you go around n vertices, you're back to the previous one up to the action of an element of the group. And this element of the group is the same. And this element of the group is the same for any n, right? So, think of a helix when you go around a period, you have a translation that moves your point up. So, is if you want a circle with a monodromy, with a translation when you go around. So, a twisted polygons is the equivalent in discrete geometry. When you go around a period of n vertices, you're going to apply a monodromy. G is called the monodromy of the Of the polygon of the polygon, you apply the monodromy, and essentially you have n capital N vertices and then their translation by their monodromy, on and on and on. So that would be a twisted polygon. And the moduli space is the space of all these polygons, again, up to the diagonal action of the group. And as I said before, the group is acting on your manifold, so then it's acting. So then it's acting on polygons, so it acts diagonally on all vertices. So moving frames are going to allow us to find natural coordinates for these spaces because moving frames will give us invariants. And the modern space is exactly that. It's a space of polygons up to the action of the group. And so it would be parametrized by invariance. So let me introduce the definition. So let me introduce the definition of right and left discrete moving frames. I know Peter gave some definition. I'm going to be sure that ours is clear. So a right discrete moving frame or a left one is an equivariant map. So this is the standard definition from n copies of the manifold to n copies of the group with respect to the diagonal axial on m and think on m as m to the n as holding all As m to the n as holding all the vertices of many different polygons. So you have the diagonal action there. And then in the group, you have the right inverse or the left diagonal action also in that group. So this is the very natural way to generalize the definition of moving frame to the discrete case. And of course, if it's left, then it's just a left action. So once you have a moving frame, you can And once you have a moving frame, you can define what we call the Serefran equations or more Akartan equations, which is essentially writing the moving frame at one vertex in terms of the moving frame at the previous vertex. And that matrix K, if it's left, it would be on the other side. This matrix K is called the Maury-Cartan form or the Mori-Cartan matrix. If I'm assuming. If I'm assuming that G is a subgroup of GLM. All right, so then there is a theory that says that under natural conditions, these matrices are going to define all of the invariants. They're going to give you a complete set of invariants that would generate any other invariant functionally. And then we can use those to define coordinates in the modularized space. Coordinates in the modernized space. All right, so to me, this was always very interesting because these equations always suggest a quotient in the following sense. If I had a different equation from k, the question is, can I take it to k by using some action so that these matrices k that give me the invariance would be a section of that quotient. Section of that quotient. And this is, and I'll give you an example right after this, but this is essentially a theorem. Oh, here is the example before I go to that theorem. Okay. On previous slide, could you show the previous slide? I can't hear you very well. I would like to ask a question once we. like to ask a question on the previous on your previous slide yes and this is maybe i just missed something but i think you are talking about diagonal action so on all points you you act by the same element so why then moving frame is a mapped to g n versus just g because i think you have moved am i understanding that on each point you add by the same group other than that The same cool weather map. Oh, oh, I see what you mean. Uh, right, because what the way to think about this is that you have a polygon, right? And then at each vertex, you have a moving frame, so to speak, or at each vertex, you have an element in the group. And this will be different for each N. So, you want all of this point to bring to the same prospection? Right. So, when you when I see. Right, so when when you when I say yeah, so when I say the action of the group is uh yeah, it's the action of the group, but the space itself, think about it as being the discretization of a continuous case, right? So you will have for every point t, you will have an element in the group, right? For every parameter x or whatever you call it, you will have an element in the group here for every n. Element in the group. Here, for every n, you will have an element in the group. So, if I give you the entire polygon, then you are going to have n elements in the group, one for each vertex. I think I need to think about this. And another question, question. So, when you talk about twisted vister 10, polygon twisted vister 10, it's a certain type of symmetry, right? It has a sort of, it is symmetric with respect to G, but it's a certain. With respect to G, but it's a certain type of CBD which is periodic. I'm sorry, I just can't understand you very well. I don't know. Um, the sound is from the room is fuzzy. Okay, I'll try again. So, if this is a when we talk about you talk about V-step polygon, yes, yes, this is a subset of symmetric polygon. I mean, each V-step polygon is a H is the polygon. This is it's not, it's not a symmetric polygon. What it means is that there is an element in the group. Think of a helix, right? So when you have a helix, you essentially have a circle, but when you complete the circle, you're going to apply a translation, right? And then you will turn around again and will apply a translation and so on. So it's essentially not periodic because you don't close it, but if you have one period and the monodromy. One period and the monodrome region, then you're going to have everything. So the twisted polygon is exactly that. You go N capital N vertices, and then you apply the monodromy, and then you again essentially take the previous N vertices and apply the monodromy to each one of them. And then you have the next N, and then you apply it again, and then you have the next N, and so on. All right. Okay. All right. So let me talk about the projective space because that's what I really wanted to talk about. So the projective space is a homogeneous space between the projective SLN and H, and H is again the isotropic group of the origin. And I'm assuming that it's non-degenerate. I'm going to tell you what non-degenerate means. You, what non-degenerate means. Non-degenerate, for example, this is the projective plane, it's three-dimensional. So, not the non-degenerate would mean that every three, so each vertex in the projective space is a line, right, through the origin. And non-degenerate will mean that three consecutive lines will have independent directions for every n. So, for every n, every three would be independent, and then you shift one. Be independent, and then you shift one again, it would be independent, and so on. No, yeah, this means that there is no uh flex point, yeah, exactly. So you're not going to have one that it would be in the same, yes. Okay, all right, so if I have a non-degenerate twisted polygon here, so again, I have a monotromy that is not showing there, what I'm going to do is I'm going to change a little bit how I visualize this geometry by doing the following. I'm going to choose points in this. I'm going to choose points in these lines that would create a volume of value one, right? But I'm going to do that for every n. So you can imagine that the next one comes and I need to choose the point so that the next volume is also one and the next one and the next one and the next one and so on. But when I turn around, I'm closing with the monodromy. So there is some non-trivial condition there that I need to satisfy. Are that I need to satisfy to be able to match them all, right? For every n. So that is a theorem that says that if the period n and the dimension m are co-prime, so here m is 3, so n has to be not a multiple of three. So if they are co-prime, then I can find that lift, what I call the lift to Rm, which is To Rm, which is unique with that determinant. So that the determinant of three consecutive ones is one for all n. So this is a theorem that it was proved actually, I'll tell you at the end by Tavaknikov of Sienko and Schwartz in a paper in 2010 at the Pentagram Map. They proved that this was unique. And the good thing about looking instead thing about looking instead of the u's i'm going to look at the v's is that now the projective action is simply the linear action of sln so now on vn i have the linear action of slm and of course it's not exactly the equicentral fine case that i showed you at the beginning because i have this condition that the determinant of m consecutive vertex is always one and that condition needs to be preserved Condition needs to be preserved. This is as a manifold of the one I showed you before. Okay, so with this definition, I can define a left-moving frame to be exactly what you see there is I would take all these vertices and I would put them as columns, Vn, Vn, plus one, and minus one. And that would be an element of the group. And you can show that this is a left-moving frame. In fact, a projected left-moving frame. Left more different. Catriona, can I have a question? Yes. So, in your metro, I don't see how N capital appears. How what? N capital. N capital, because this has to be true for every n, and n capital is the period where the monodromy kicks in, and you have to ensure that the shift to the next one is going to be fine. To be fine, and this will be true for every end, including you know, if you go over the period. All right, let's go. So, now what are the invariants? What I'm going to do is I'm going to take the next V, right? So, I have Vn, Vn plus M minus one. So, I'm going to take the M plus M. And because this is non-degenerate, all of these vertices are going to generate the entire space. Generate the entire space. So I should be able to write the m plus m in terms of all of them, right, as a linear combination. Now, the fact that the determinant is one implies that the last coefficient, the one of the n, is constant and is minus one to the n minus one. And then the others are the invariants. So the Serefren equation, the left one, you can see how it goes, right? Because Vn, Vn plus. Right, because Vn, Vn plus one, the n plus one is n plus one, n plus m. So you get all of this combination. The first part is just shifting to the next, and the last one will give you v n plus m and from a theorem. So this was already the fact that the a n's are coordinates for the moduli space was already proof, as I said, by Schwarz of Sin Puntavagniko. All right. All right, in fact, in this case, d to the n over h is a global representation of the moduli space, it's not a local one. All right. So as I said before, my question now is: how do I represent these invariants? How do I represent these matrices, these Moray-Cartan matrices, as a space? So I can say, As a space, so I can say this is the space of the modernized space. And the answer to this is to realize, and this is maybe would take a little while to see, but to realize that if I have any element in the group, I can take it to an element that looks like what you just see on the screen by some action. So the question was: what is that action? And because this is a linear. Action. And because this is a linear equation, the action is going to be the gauge action. And those of you who are familiar with linear equations would see that as a natural choice. So the theorem is that if any manifold is G over H, and this is not just for projected, this is for any manifold, the moduli space of non-degenerate twisted polygons in Mn, you can identify, in general, it would be a local. In general, it would be a local, it would be locally an open subset of the quotient. As I say, in the projective case, it is actually a global theorem where Hn acts on Gn via the right H action or the left one, depending on which identification you're using. All right, so then what you have here is that now suddenly the modularized space of polygon. space of polygons becomes a quotient g or at least an open set of the quotient g to the n over h to the n. So the next question is, are there Poisson structures or Hamiltonian structures and G to the n that I might be able to produce to this quotient? So for that to be true, this gauge action has to be a Poisson map of that. Poisson map of that structure. So the question is: are there Poisson structures on G to the N for which the gauge actions are Poisson? Because if I had that, then I can say, well, can I reduce it? And if I can reduce it, what do I get? And that's when things get fun. So that's the question. Is there Poisson structure in you to the end such that the right gauge in this case? Such that the right gauge in this case, or the left one, is a Poisson map, so that I can look into reducing it. And this was classified by Seminov Tianshansky back in 1985. So this is an old question that came from the theory of Poisson Li groups even earlier with Trimfell. And I'm going to describe that practice. All right, so we move on now to Poisson geometry on groups. Groups. So assume that G has an inner product. So for example, G is semi-simple, and I have a way to identify G with G star. And this has to be a joint invariant because I want to take it to the tangents by left or right identification. If I have a function, then I define the gradient the usual way one defines the gradient. I have the differential, I apply the differential to an element in the tangent. To an element in the tangent, and that gives me a gradient against the element in the tangent. So the only difference here is that I am in g to the n. I have n copies of g. So an element in the tangent would have n copies of a tangent. And then I will have, if you want, a vector with n copies, not n copies, it will have n elements in the cotangent, but I can identify. But I can identify one element in the tangent by the inner product. And this inner product, again, because it's adjoining variant, I can extend it to the tangent by left or right translation. They are identical, either one, because it's a joint invariant. All right. So depending on which identification I use, if RQ is right multiplication, LQ is left multiplication, then I. Multiplication, then I can define what is called the right or left gradient, which are now elements in the Lie algebra and not in the tangent. So the right and left gradients are two elements of the Lie algebra that are defined the way you would define it if you had five minutes to define it. All right. Now, to define possible structures in discrete cases, In discrete cases, I need to define what an R matrix is. And R matrices have different definitions. So I'm going to give you today two different ones just to make it more confusing. But the first one is an element in G tensor G is as applied to G star tensor G star. And you define R12 as you take the two elements that are Two elements that are multiplied, you put them in places one and two, and you add a one in the three, so it's a three tensor. And then you can define R1, three by putting A and B in the one and the three and a one in the two. And then R2, three, the same way, the one goes in first position, A and B goes two and the three. And now you take the Libra, and the Lee bracket is just a commutator, right? And you move. Commutator, right? And you multiply by as a tensor. So the first two, the first elements, second elements, third elements. And you get a three tensor here that has to be zero. If that is zero, then you have what is called a non-matrix. Okay, so the usual way to define our matrix is to have a gradation of the algebra. So in SLM that I'm going to discuss, the positive part is the upper triangular element. The upper triangular elements, the negative part is lower triangular elements. H0 is the diagonal, right? H0 needs to be commutative to generate these R metrics. And in the case of DLM, that's the R metrics. You would take a ij with the JI, you couple that, and then you have to add zero-order terms. And it's important to add these zero-order terms, otherwise this is not zero. So in that sense, it's not. So, in that sense, it's not what some construction uses. All right, so I have gradients and I have an R matrix and I'm going to call T the shift and this is Seminov's bracket. So, Seminov twisted Poisson bracket, that's how he called it, is R applied to the wedge product of the left gradients. You add, not subtract, R applied to the subtract are applied to the right products of the um to the to the wedge product of the right gradients and then you're going to subtract and add a shifted term in which the right gradient is shifted and the left is not and you say boh this is a mess but semenov showed that this was the one bracket the natural bracket that the poison map are gauge action and and in fact Action and in fact, the symplectic leaves are the gauge orbits. So, this is really the natural bracket that comes with gauge actions. All right. I'm sure you have a lot of questions, and I'm going to be skipping a lot of details, and I apologize for that. But I have a story to tell you, and I want to finish it in 45 minutes. Want to finish it in 45 minutes of email. All right. So now the question now, the next question is: can I reduce this? Remember that my moduli space was the quotient of g to the n by h n. This is in g to the n. I have that gauge action. And the question is, can I reduce it? And it's not immediate. In the continuous case, if you're familiar with all of these construction continuous. With all of these construction continuous cases, this is right away immediate that you can reduce, and in the discrete is not. And the reason is because there are two gradations going. So assume I have a group with a Lie algebra, and I have two gradations now. The first one defines the R matrix. So the first one has the G plus, the G minus, the commutative. You will have an R matrix like the GLM I showed. R metrics like the GLM I showed you. Each subgroup of GLM has its own R metrics, and some of them are more complicated than others. But you have an R matrix that is defined by this gradation. But I have a second gradation usually, or in my construction, we will have. And this will be a geometric gradation in the sense that M is homogeneous G over H, and H will be the positive part of the gradation, the non-negative part. Part of the gradation, the non-negative part of the gradation. So these are called parabolic manifolds. But if you don't know what that is, that's perfectly fine. The key thing is to think that we have two gradations, one defining R, the other one defining M, because it tells you which one is the algebra of the isotropic sub-group H. And these two gradations have to be compared. And these two gradations have to be compatible. They have to be coordinated. Otherwise, you will not be able to reuse. So we say that both gradations are compatible if the geometric gradation is contained in the other one. So G1 is in G plus, G minus 1 is in G minus. And this compatibility is needed to reduce. And the theorem says that if I have two such compatible gradations, then the twisted possible. Then the twisted Poisson structure I just sent you, I just showed you with R metrics associated to this first gradation is locally reducible to the modularized space. So now I have this Hamiltonian structure that is the final space of invariance, essentially. All right, so what is more interesting about this structure, apart from About this structure, apart from the fact that it's an invariant space and it's nice, and you see how we obtain things that we know what they are. What is interesting is that all the Hamiltonians have an easily identifiable geometric realization. And we go back to the original description I was giving. I can lift it to polygons. I can forget about the invariance and think about the polygons, right? Right. So, furthermore, any reduced Hamiltonian evolution with a local invariant Hamiltonian has a geometric realization that is also a local invariant evolution of polygons. In fact, you can algebraically find them. You can do that by hand in most cases, you know, and say, okay, this is the Hamiltonian evolution and the invariance, and I'm going to find the polygonal evolution that you can. Polygonal evolution, then you can, if especially the dimension is not too high, you can actually do it by hand and say this is it. I'm going to call that geometric realization X and F, and I put it in green, and I'm not using green very much because green is going to come back later. All right, in the projective case, what did you get? This is a paper with Jim Ping back in 2013 that said the reduced bracket, this reduced bracket I showed you is a Hamiltonian structure for discretizations of WN algebras. And these are generalizations of the business lattice for any dimension. So the Business lattice is in two dimensions. So these algebras, and I showed you the equations, those are the equations. They are They are Hamiltonian respected this one structure that we found. And we also found this other structure, which was totally bizarre, because it came from this right bracket. So it's the same R, right? But applied only to the right gradients, and that's it. This is by no means a Poisson tensor. This is not a Poisson bracket at all. But we proved that when you reduce it. That when you reduce it to the moduli space, it can actually be reduced and it gives you a Hamiltonian structure for the same integrable system for these discretizations of W and Adverse. And this was mystifying because we couldn't prove that these two things were compatible and they were pretty tough. So these structures are very complicated when you write them down. And there was no way on Earth. And there was no way on earth we were going to be able to prove that these were compatible, certainly not straightforward. So, here is the first problem that we managed to solve by using geometric realizations, which was a very hard problem just in the face of it, but when using geometric realizations became quite simple. So, the theorem, this was with Barna Carini in 2020. Anna Kalini in 2020, it says that both Retheus bracket can be lifted to a symplectic and a pre-symplectic form in the space of polygonal vector fields. So now instead of writing as a Poisson structure on Hamiltonians, I'm going to write it as a symplectic form on polygonal vector fields. And those polygonal vector fields are exactly the geometric realizations I showed you. So what we Showed you. So, what we showed is that the first bracket was just a symplectic form applied to the geometric realizations. And the second bracket, the weird one, was a presymplectic form. It has a kernel, two-dimensional kernel on the same vector page. And once we wrote them like this, we lifted it there, then so I. There, then so ah, yeah, I forgot to say that the geometric realization xf was the f Hamiltonian for omega 1, and omega 1 is on a space of vector fields, right? So the points are polygons. So when we say F Hamiltonian, it's Hamiltonian with respect to the polygons now, not the invariants, right? So it's different, but it's the Hamiltonian vector field for omega 1, but not for omega 1. Field for omega one, but not for omega two, of course. That's a very different. So, once we had that, then show it became quite simple to show that they are compatible and that they define a Hamiltonian pencil for these discrete Abundant algebras for the generalized bussiness lattices, and it could integrate them in the levial sense. In fact, the proof of compatibility was based on the fact that the Was based in the fact that d omega 2 was 0, which it was. In fact, omega 2 is an exact form, it's d over theta. So this was trivial. The proof of this theorem really was literally two lines once we had the relation to omega 1 and omega 2. So here is our first example of how geometric realizations, which is facilitated by moving frames. Facilitated by moving frames, help prove difficult problems in integral systems. Okay, so I'm going to shift completely gears now, and I'm going to start talking about pseudo-defince operators. And pseudo-difference operators are simply symbols. So these are symbols in which t is, again, the shift. So it's powers of shift, negative powers are infinity, and the coefficients are bi-infinite and. And the coefficients are bi-infinite and periodic sequences. So these are, you know, A has a lot of A's, but they are periodic. I forgot to say that when you have twisted polygons, the invariants are periodic, okay? So when you are in the modularized space, you have periodic elements, even though when you are in the polygons, you have a monadron. All right, so these are bi-infinite and periodic sequences. I'm going to denote this space. I'm going to denote this space by PDO, set of the difference operators. The non-negative parts are the difference operators, right? You throw away all the negatives, you just start with zero. And if the highest order time A to the M, M is the order of the operator is the highest term. If the highest order term is always non-zero, and that's a sequence, right? So you have a lot of non-zero terms. We say L is invertible, and we say that because you can invert them. We say that because you can invert them formally, right? It would give you another element of that space. So the space of invertible pseudo-difference operators is a Lie group, is an infinite dimensional Lie group with the standard operator product. So you just multiply them as operators, so you're going to shift the coefficients, right? All right. So, is Lie algebra just going to be PDOs because you won't have the inversibility of the higher element anymore, right? If you differentiate an element in the group. So, the Lie algebra is just a difference operator. So again, I have a Lie group and a Lie algebra, and I'm going to do the construction I did before, roughly speaking, with a different Poisson structure. So, I'm going to define the trace on the algebra, and the trace is the sum over a peak. Is the sum over a period of the A0 term, the one that doesn't have any shift? So I add all of the A0s over a period because it's periodic, it doesn't matter which period, your choice. And then I define, one can prove that the trace of the product is an adjoint invariant inner product on this algebra. So now I have an algebra, I have an invariant. I have an invariant inner product, which means that I can identify the dual with the algebra, as I did before. And I can use. So the next thing is to know what is a gradient, right? That's what I did before. How did you define the gradient of a function? How did you define a Poisson structure here? So given a function, I define the variational derivative. This is infinite dimensional, right? So it would be an element in the tangent of the group. In the tangent of the group, such that, sorry, if I have a curve starting an L and I differentiate it, then it's going to be the trace of the gradient with the tangent. And this is like before. Again, this trace has been taken, extended to all the tangent spaces, right? By left or right translation, your choice because it is join invariant, so it doesn't matter. Invariant, so it doesn't matter. So, again, as we see before, if I take the gradient and multiply on the left or on the right by the operator, I have an element in the algebra. So that would be the left and right gradient, if you want. And I'm going to define an R matrix in a very different way, in fact. So I'll define two maps, plus and minus, that is going to take the positive part or the negative part. Part or the negative part of the soda difference operator. So this is the standard. If you have seen this construction for differential operators, this is all very familiar to you. So the standard R metrics in so the difference operator is a skew symmetric operator that looks like that, a skew symmetric respect to the trace. It's just half of m plus minus m minus. And this is pretty standard and it satisfies the Jambax. And it satisfies the Jam-Baxter equation, which is what you need to define a Poisson structure. So the standard theory in Lie groups is going to tell you that there is a Poisson structure, and this time it's just the Scriani bracket, the one you probably all know, which is the inner product R applied to the right against the right, minus R applied to the left against the left, and it's quite simple. And the Hamiltonian vector field looks like that. And we are going to quickly come to that in a minute. All right. So the last step, and I promise I'm almost done, actually, I don't know what time it is. So there are two natural actions on this group, which is the left and right multiplication by scalar. Well, but not really scalars. They are sequence. It's not really scalars, they are sequences, right? But they are not operators, they're zero-order operators. So you could multiply on the right here, and then you will shift these scalars, or you could multiply on the left, and then you don't shift anything. You just multiply n by n. So this is multiplication of sequences, right? So these are just scalar, right? So the theorem that Isozimov proved. The theorem that Isozimov proved was that this structure that you see above is invariant under this action. And that using these actions, you can reduce the structure above to the operators, to difference operators, not so the difference operator, but difference operators where the highest term is minus one. Furthermore, you can use it to reduce further and make A. reduce further and make a zero minus one to the n minus one and you i hope that you see where i'm going so now you have an operator that looks very much like the projective operator i was talking about before or at least is kernel is related right okay so this is my last So, this is my last part. If I have this operator, then I have a twisted polygon because the coefficients are periodic. I have a twisted polygon defined by the kernel. So essentially, Vn plus M would be equal to the combination of the others. This twisted polygon is going to be non-degenerate because the coefficients. Because the coefficients are periodic, it's going to have a monodromy and it's going to define a projective polygon. Because the determinant you can show with choosing the right initial condition, the determinant will always be one, the m consecutive determinants. So the kernel matches my first picture to the second one. That's the connection point, right? So the question that I saw, Simov and I. So the question that I saw Simov and I were discussing is what is the relation between these structures that you have and I have? How are they connected? Now these structures look very simple before you reduce them, but once you reduce them, they are pretty hellish and very complicated to deal with. So the reductions actually, because you are restricting to a submanifold, it becomes really complicated. So it's very hard to deal with. So, it's very hard to deal directly with these structures. So, again, what we did is go to geometric realizations, use the moving frame to define invariance that define these invariant vector fields and compare them. So, I have a kernel, I have a projective polygon, I have the generating invariance, and the question is: do I have a geometric realization of A geometric realization of a Hamiltonian. The Hamiltonian vector is the one that I showed you before, right? This is the one on operators. Is there one for this isosimal structure? And this time is very easy to find, right? Because it's the kernel. So LBT is L T V plus L D T, right? L is the operator, you differentiate operator, you differentiate the kernel. And then you apply the vector field. You see here the vector. The vector field, you see here the vector field, you apply it to V, but V is in the kernel. So half of it goes away, right? And then you get the second half only, and then plus L applied to the hypothetical geometric realization, which would be Vt. And that has to be zero because it's in the kernel. So you can just solve and you get the vector field is this part up to the kernel of L. The kernel of L. But then you show that if you add anything in the kernel of L, then you will not preserve that A0 has to be minus 1 and minus 1. So this is unique, in fact, on the projective space. All right. So I have two geometric realizations, Xn that I had at the first half, and YF that I had at the second half. That I had at the second half, and the theorem is, of course, that they are identical. So they are identical vector fields. And so the two amitonic structures I just showed you are the same. And this is, for those who know the subject, this is dreamful soccolov, discrete, a discrete version of dream fluence alkolov. And that's it, I'm done. If anybody has questions, are there any questions for Gloria? Well, if not, thank you very much, Gloria, for a great talk. We have a question. Can you go back in your theorem of 2014? 2014? I don't know which one. It was more or less at the it was more or less at the not really the beginning but i'll go there i'll go back back back okay is it this one before right right right that one that one that okay yes uh no the one with the fact that there is the open set oh yes yes here you may you prove me yeah i know which one you mean this one okay that This one. Okay, that one, that one, that one. So it's open. What about the closure? Is the risky open? There is something in the closure? I don't know. That's a very good question. And I would like to know the answer. And you don't know if it is, I mean, it's just open. You don't know if it is dense. The problem is very often the way you prove this is to say, you know, I have these more part time matrices, right? And these are going to define a section of this quotient that that's how I know the quotient is the modularized pair, right? But sometimes to define those more economic instructions, you need some. You need some restrictions or some conditions, right? So that in the boundary of this open set would be the space where those conditions don't hold and my section is not global, so to speak. So in the case of the projective space, it could be a global section. So then there is no problem. And the question is directly the moderate space and done. But the way I prove this is by showing that the Mori-Cartan. That the more recordant matrices are a section. And the problem with that is that you have to have some conditions on the Mari-Cartan matrices in some cases. So it would be when that section cannot be extended globally. And it doesn't mean that it cannot be, yeah, it's a good question, and I don't know the answer yet. Okay, but I'd be happy to discuss. But I'll be happy to discuss it with you if you want. I mean, yeah, especially if you have a if you have an example, then it's always sort of fun to look at the example and see. I don't, I don't. I was asking to you. Okay, thank you very much, Gloria. Thank you very much, Gloria.