Up first here, Ansager Map Mashlove functional for SLE loop measures. Thank you. Thank you to the organizers. Yeah, so we'll be talking about Insider Matloop functionals for SLE loop measures. And this is a joint work with Lean One and AHS. And so as an outline, the first part will be telling you something about the first part of the title. So what are the entire macro functionals? Then we'll talk about SLE loop measures and eventually we'll combine. Lead loop measures and eventually we combine the two and see what happens. Okay, so something a very simple problem. But say you draw a curve, phi, and then you draw a neighborhood of this curve, so a tube around phi, and you ask yourself how likely is that a Brownian motion starting at the same starting point as the curve is going to stay inside this geo-field ground. Now, of course, as well. Now, of course, as the width goes to zero, this probability will go to zero. So what you end up doing is you normalize by how likely is the variant emotion is less than epsilon. And hopefully, this ratio here does have a limit. And this is what was proven by Anzager and Maple in the 50s. So they show that as epsilon goes to zero, then this quantity is approaching as an exponential. Has an exponential of up to a conference, you recover the initial energy of your trajectory. And their motivation was sort of, you can think of this quantity when epsilon is very small, if this probability is very close to one, p would be a good approximation of the brand emotion. So, what they had in mind was: can we approximate stochastic processes in some sense with something not random? And of course the limit here depends on how regular the curve is. So it depends on usually energy. Now, you can ask this question for different processes. There's nothing special here with the max norm. You can have two norm, whatever norm you want. What I want to do now is give you some more, some ways. Some ways of thinking about this functional theory besides the initial energy. Yeah, that's probably. Are you using your own laptop or their laptop? My own laptop. Right? No, it's working now. Okay, so let's jump back to something finite-dimensional. Something finite-dimensional, so take a Gaptian random variable, you have a nice density here, and you can think about the previous question, so the inside macro functional in the following way. So you're looking at how likely is that x is close to y, epsilon close to y, divided by how likely x is close to epsilon. From a geometric point of view, maybe the colors are not so clear, but this is like your tigging the ratio of the yellow area over the green. Of the yellow area over the green area, and you have absolute zero. So, what it turns out is that when you do this, after a constant, we recover the density of this term regression of the And this was interesting because this, as we'll see in a moment, is going to give you a way of thinking about this functional as some kind of density. Because what happens now is you do the same exact trait you did here, but in You did here, but in infinite dimensions. So, what you're doing is you're fixing this explicit metric measure space, which is nothing but the space of continuous functions starting at zero, the sum norm, and mu is the Wiener measure, so it's the law of Branian motion. So, once you have this setting for this specific metric measure space, you can review this probability here simply as. probability here simply as the ratio of the wiener measure of a ball of radius epsilon standard at phi divided by the wiener measure of a ball of radius epsilon standard at zero and what proved is that your limit is since i functional so in some sense people have been thinking about this functional as some kind of density in light of this connection here we can find Connection here with a finite dimensional case, analogy. And another way to think about this functional is sort of telling you how your measure, how the measure of balls changes as you're shifting, as you're changing the center. Because in some sense, what you have here is that you can think of this as you have a translation map, a map you're fixing V, and you have translations from CSURN. C sqrn to C sqrn. So you're taking a path gamma and you're shifting a variable S E. And as you do that, the Wiener measure, so you have non-translation invariant, but you're in infinite dimensions. So what this function is telling you is telling you when, for small values of epsilon, how the measure of the ball depends on etc. Then, the take-home of this slide is that once you have this way of thinking about the Unsafe-Mach loop, this problem is not just a probabilistic problem, because you can ask this question on any metric metric space. Take a measure, give your metric, you can talk about poles, and compare the sizes of different volumes of different poles. So, what we want to do now. So, what we want to do now is to sort of take this idea and apply it to a different metric metric space. So, instead of looking at the Wiener measure on the space of trajectories, we'll be looking at the SLV loop measure on the space of simple loops. So, how is this defined? So, I'm not going to go into details about SLE. SLE. Just some history, they were introduced in 1999 by Adolf Trump, where he was looking at some scaling limit of discrete models, which would be discrete models. You can think of them as a family of random curves on the plane. These are known as chordal SLE because they start at the origin and they go to infinity. And for each value of kappa, you get a different So, the curves that you get, they have different regularity. For example, when kappa is less than 4, they are simple trajectories. When kappa is greater equal than 8, you actually have space filling. So, kappa is telling you how regular these curves are. But instead of thinking of them as collection of random curves on the plane, I want to think of them as for each kappa, you have a family on the measure on the space of curves. Space of thirds. And a few years ago, so 2019, 2020, this construction was extended to loops. So in this talk, I don't have a nice simulation of a SLE loop. So just think of this and then close it. But this was constructed in 2019 by Dr. Jan for each, he has this extra emission here, Kappa has to be as we go before. And so, the measure I want to think about is this measure here, SLI loop measure, Niu kappa. It's a measure on the space of simple loops. And it satisfies a bunch of properties. It's infinite, sigma finite, and it's invariant under a conformal map of the Riemann sphere. And what we want to do, so what Illian and I proved, is I'm going to get into more detailed version of this statement. More digital version of this statement, but the idea is that you're fixing, fix a loop, a simple loop gamma, analytic simple loop gamma, and you ask yourself, look at the size with respect to mu kappa. You look at the size of loops which are in an epsilon neighborhood of gamma, and you divide that by the size of loops which are in an epsilon neighborhood of the spirit book. And what we showed is that as epsilon goes to zero, it converges to this explicit quantity here. This explicit quantity here, which up to a constant is the Loebner loop energy, which we'll also talk in a moment later, in a moment, which is known as, which was introduced by Billing and Staffin Rodev. Okay. So let's try to make this statement a bit clearer. Also, it's essential if you have time at all, it has to be analytic. Have time to tolerate. It has to be analytic. We strongly believe that the statement holds if the loaf is not analytic. You do not have a proof. I also tell you why, where do you need a loop that is analytic. We believe that this statement holds for any loop. What is nice is that if you have an analytic loop, then the Lokner energy is finite. But that should not be. There are some loops for which. There are some loops for which the energy is finite, but then they're not analytic. Okay, so what is this neighborhood? What do you mean by neighborhood? Problem. Something. Just realized that I forgot to mention. This has been faster than I thought. And I wanted to give you some details of the proof for the Bryan motion case to motivate the proof of this theorem, but we'll do them later altogether. Pulled together. Anyhow, so how do you define this loop? This neighborhood? So you have your simple loop gamma. So it's analytic. So you know that you can think of this loop gamma as the image under a conformal map, f of the unit circle, as one. So what you're doing, and f So, what you're doing, and f is defined, might not have to be defined on the whole space. Actually, if f is defined on the whole space, then this ratio would be one, because I didn't mention it, but the measure is polymorphic invariant. So that size one. So this is defined in a neighborhood of S1. Then what we're going to do, we're going to take a small annulus around S1, let's say with radius epsilon, and what you're going to do. And what you're going to do is you're going to take this neighborhood here, the neighborhood of S1 on the space of simple loops, is simply going to be the set of all simple loops which are contained here. So the bottom of this expression would be take the mu kappa, so the measure of the size of how many simple loops you have inside the state. Then how do you come up with the neighborhood gamma? Then, how do you come up with a neighbor of gamma? You just apply after this. So, you're going to simply take, so your neighbor of gamma are all the simple loops which are contained in the image of this tiny annulus here. So, it's important this is not, so this is the annulus. So, this width here is actually epsilon. This is not an annulus, because it's just taking the image from a map. I'm saying it because our original idea was for a moment we thought, oh, take gamma and then take the absolute neighborhood of gamma and look at simple spare. It doesn't work. So that's why. Which topology did you use? Very good question. And it was going to be here as the Alster topology in this case of simple oops. Actually, this is a wrong statement. It's not too far from being correct. It's not too far from being correct. No, no, no, no. No, okay. No. What I mean by this, so because we thought, okay, we have this very sort of like well-constructed neighborhood that really works for this specific case. Do we have enough of such a neighborhood? So, in the very first version of this paper, we claim that there were a basis for the astrophysic, and then we have to. Topology, and then we have to thank the referee. They realize they're not a basis because what we proved is that take any open set in the outsourced topology that is contained in a union of such sets here. But such sets are not open in the Azure topology. We did not realize that when we first wrote it, but still we have enough of such neighborhoods. And so basically, the statement here. So basically, the statement here translates to: can you show as epsilon goes to zero, does this finite have a limit or not? That's the open question. Also, one application that we had in mind of this thing was to look at some large deviation principle for the SLE to measure. And that goes back to all of these neighborhoods not being great to do a large deviation principle. So, a few words about the functional, this lunar loop energy. This loner loop energy. It's a function defined on the space of simple loops. But originally, this was introduced on the space just of trajectories. It was defined for portal SLE years ago, and then same time, around the same time, but three checker and then one. And then in 2021, Stefan and Eli improved it for extended to the space of simple loops. Simple looks. So I'm not going to tell you what it is, I'm going to tell you what it does. So it's going to be a functional that is going to be exactly zero if and only if gamma is a circle. So in some sense, what this functional is going to do is going to measure how far is gamma from being a circle. So the closer I L of gamma is to zero, the closer gamma is to be a circle. That's sort of, yeah. You can actually say something more, that is, not only is zero if That is not only zero if and only if it's a circle, but it's finite if and only if it's a quasi-circle, whatever that means. The image of a circle in our quasi-formal map. Anyhow, you're morally a circle. This thing is finite. Okay, few words about the proof, and that's where when I go back to the linear motion page, which is. Run in motion case, which I should have done earlier, but I forgot. So, in that case, what were you looking at? You were looking at the Wiener measure of the ball of radius epsilon standard P divided by the Wiener measure of the ball radius epsilon standard zero. So, one way to think about this in terms of using this map, you can write it as It as so it's the push forward of the Wiener measure under T phi applied to V epsilon zero divided by mu of V epsilon zero. Nothing crazy here. Why do you like this? Because the moment you write in this way, then you know what happens to the meaner measure when you're translating. Because I caught on Marketing theorem. Because they're current kind of marketing theorem, Gusano theorem, and they do tell you exactly what's the random negative derivative of this. This is explicit, and if you remember it, it's going to have something which depends on gamma and phi, but then it has a term which is exactly negative one-half the square root of the reduction. So this. So, the considerable functional in the Brighton motion case with the Vienna measure is coming from the Radoni Way theorem. So the moment we realize this, because there are many different ways you can prove the Instagram-Markov functional, what is the Instagram-Markov functional for a variant motion, that's one way. You can also use PE or other things. But the moment we realized this, we thought, okay, Daphne, it does. Daphne, it does, when you constructed this measure in the space of SQLs, it does tell you what's the random equating derivative when you apply some kind of formal map. So we had a randomly grading derivative. Great. Now, you see this term lambda star. There are going to be a bunch of different objects defined on the space of loops. So lambda star is what is known as the renormalized Bradium loop measure. Picture, as you have it. Picture, I do have it, it's small, so I'm gonna show it again. So these are not simple. These are make sure these do not look like simple looks. So what's the idea here? This is kind of looking at the following quantity. It wants to measure how many looks do you have that are attaching V1 and V2. Now, if you only do that, unfortunately, you have infinitely many such looks. Infinitely many such loads, or not. The measure of those loads is infinitely infinite. Because the idea is that you have two sets here, V1 and V2, and then you have this measure which is known as Branier loop measure, and you would like to measure how many loops you have that touch both V1 and V2. Now, unfortunately, you may have many large loops that are touching both of them. Touching both of them. They're like, we have too many such large walls. So that tells you that the Brandon loop measure of the set of wall loops touching V1 and V2, that's going to be negative. So the way you normalize this is, okay, let me not look at the measure of loop, not look at the set of loops touching V1 and V2, but I want to look at the loops touching V1 and V2 that are contained inside a big domain. Domain. So now this is going to be finite because now these two sets being disjoint tells you you cannot have too many small loops. And the loops being contained inside this red region here tells you you cannot have too many large loops. So that quantity is finite, which is exactly this quantity here. And as these get larger and larger, it goes out to infinity, but what red block color and Lower field. Laurie Field proved that this is a correct normalization. And it's kind of interesting that it does not depend on the stats to start with. It doesn't depend on D, like it's log R. So the quantity that Dauphin used to define the threat-on-equipped derivative is this object here. So more all it looks at how many loops you have touched in one of each loop. Unfortunately, it does not. We would like this quantity to go to the Loebner energy, but we cannot find any explicit relation between this object lambda star and Loebner energy. But Ealing proved a few years ago again that Loebner energy is related to another finite wave. This is known as Berner measure. A lot of measures on the space of books. What Werner measure does, I think it's a very nice way of thinking about it. Is on one hand, you have this Brandon loop measure, which is a measure on the space of loops. So they might intersect each other. And what this does is you simply associate your loop its outer boundary. And so you get something defined on unsimple loops. Or using Davin's work, this measure W is not. Measure is nothing but the measure mu kappa, so the measure that's because it's simple when kappa is entered. That's my way of just thinking about it. So what we had to do, our idea was the following. Well, there's this connection here. So if maybe we can prove that somehow lambda star is related to dots. Lambda star is related to W in some ways, then lambda star would be related to I L of the L. And that's pretty much the long part of the paper. The long proof is to prove that you do have this equality here. So that if you're looking at differences of W, so W, so this is the set of simple loops touching gamma and the component of A. Mine is the set of simple loops touching F. Simple looks touching the image of gamma and the image of this FA complement, that is equal to the same thing when you replace lambda star. The surprising part is that if you drop this condition here, so if you drop this difference here and this difference here, they're different. So what I mean is that their differences are equal, but if you look at them individually, they're not equal. And actually, they can be very far from each other, meaning that we have an example in which this point is zero. An example in which this point is zero and this point is infinite. So they can be very far, but somehow, as long as you take their differences, it works, and that was great because we only have differences here. So it actually worked kind of as a magic. And I'm just going to stop here with the somehow differences or analogies, I think, or the differences between the brain and motion case. Between the brain emotion case, which really goes back to the Cameroon Martin, Gersanov, Onsager Math Look, and then the whole idea was to, how can we pretty much translate the column on the left in the case of SLE loops, and that's how we did every single step. Thank you. Questions? Portal. Cordov case, what has the local entity factor? There is actually, I think, Elena has a student that is, we did not do the cordal case, I believe Elena has a student that is doing the cordal case, and if I'm not mistaken, you should get quickly statement. Instead of the Lovener loop energy, we just get the regular, I think, Lovener energy. Regular Lovener energy. Yeah, yeah, the Lovener energy, not the one, the one that the chordal. The one that the cordal lunar energy, I think. Oh, that was the one that was originally defined. Oh, just for ground emotion. No, it's the one that was defined for cordial SOE. Because this one here is not the one for cordal SOE. Sure. Yeah. And I actually have one comment that I just realized I wanted to make. There have been a bunch of results that show how the SLE SLE measure is related to globular energy. But pretty much, most of those results, they work as kappa goes to zero. Because usually you do get some kind of large regression principle as kappa goes to zero. And we're pretty surprised that these results hold for each fixed kappa. So it's really a connection between measure mu kappa and I L for fixed values of kappa. And what we And what we proved is we give upper and lower bounds on this quantity, and as X turn goes to zero, you know, magically you get this expression. But another thing we wanted to do, we wanted to use this inequality as estimates from above and below more to say something is calculus correct. And again, going back to low degrees and things like that. But yeah, this works for every fixed. Well I had a question. You said something about like this wanted to do some large deviations and you couldn't do it. What was the limiting? The limiting was this notion of the neighborhood. The problem was really that f F, you want to do this on the space of simple loops, right? So you would like for this to work even when you have a loop which is not analytic. And unfortunately, when the loop is not analytic, this trick doesn't work. So we tried for a few months to make it work because something you can do is you take your loop, which is not analytic, but it's simple, then you can approximate it with their own inputs. So they are analytic. Analytic simple loops. And so we were trying to do this kind of like construction of each analytic simple loop in the approximation and then take the limit, but it was we couldn't for it. Yeah, that was the main. So I think for the large evasion principle, one should really try to do this thing with maybe a better set of neighborhoods, I think. But yeah. Thank you, speaker. Thank you, Speaker. A three-minute break, get a cookie, some coffee, come back, and we'll do the last talk and get going.