How we establish computational hardness of problems by going back to kind of one of the classic problems where we study these things, which is a planted dense subgraph problem. So this is joint work with some collaborators, Fiona, Alex, and Dana. We did this actually while we were at the Science Institute about a Period. Okay, so the plan is the following. So I'm going to just introduce some of the classic problems for. Some of the classic problems for studying computational hardness, so planted wheat and planted dense subgraph. I'll give you kind of a toy example of the type of result that we show. Then I'll kind of give you the result, and if I have time, I'll talk a little bit about the proof. So the proof uses a class of algorithms called low-degree polynomials. And I think that, you know, this problem specifically highlights some interesting aspects of proofs used using these. Proofs use using these tools. Okay, so to get started, I'm just going to review like the classic sort of problem in this area, which is planted clique. So kind of the idea is planted clique, we have two parameters. You know, this is a graph problem, so we have in nodes. The nodes are connected with some background probability, let's say one-half. Another parameter is k. This is going to be like roughly the size of the planted cliques. Roughly the size of the planted clique. So the idea is you choose each node with probability k over n to be inside the clique, and then you completely connect the clique. So all the edges within the clique are completely connected. So here N is 16. This counts all the nodes, the gray and the red one. And then here, K is approximately 5, right? Since you pick each node with probability of K over N. So, one question. So, one question we could ask is: when can we actually recover this hidden structure? So, if I just give you the adjacency matrix of the graph, when are you able to actually return to me the five nodes that belong to this thread? And so, we understand this really well, right? So, what do we know, right? Well, the first thing we know is if the graph is just an Erdos-Renier graph with Is just an Erdograph with connectivity one-half, then we know roughly the size of the largest clique. So, you know, if we plant, if we don't plant anything, right, if we just have the Erdogan-Rigne graph, then the largest clique is going to be roughly the size 2 log n, right? So the first thing this kind of tells us, right, is that if we plant a clique that's smaller than, you know, A is smaller than 2 log N, we're not going to be able to find it, right, because it's going to be just, you know, totally overwhelmed by the background. Overwhelmed by the background noise, right? So, in some sense, this is the information theoretic limit. If k is less than to log in, we're not going to find a planet clique. The problem's impossible. Okay, so this suggests that, you know, K bigger than to log in, at least in theory, it's possible to recover the planted clique. And so, the question is, how might we recover the clique in this regime? So, kind of like the stupidest idea you have. Of, like, the stupidest idea you have, right, is like a brute force search, right? So, let's just kind of take all of the subgraphs of size, you know, K or over some range and look and see if they're fully planted. And then, you know, as soon as you find the clique, right, then you just determine that this is not going to clique, and this is going to work, right? In particular, when K is greater than. Is greater than two log n, this is successful. The issue is that searching over, for example, all n choose k subsets is computationally intractable when these numbers are large, right? So this gives us a method for finding the planted clique in this possible regime, but not like the polymer camo. So then the next question is: okay, so how might we do this in a way that's efficient? And of course, there's a number of methods. There's a number of methods, right? You could do something like degree counting. That's kind of the simplest one, right? So you just find the set of nodes with the highest total degree, right? You call this your bleak. And in fact, this works when K is bigger than root n log n. There's other ways, spectral methods, semi-definite programming. Kind of the short story is that all of them work at this K, like At this k, like root n log n. Alright, so what this suggests is that the landscape for this problem is kind of the following. We have a regime where it's impossible, if case too small, if our planted clique is too small, nothing we can do. But then there's this regime in the middle where something like brute force search works, but this is not computationally efficient. And so this hard regime, CARD regime is kind of interesting in the sense that we feel like we should be able to solve it, but we just don't know how to do it efficiently. And so this is what the community often calls a statistical computational gap when polynomial time algorithms fail at a strictly different level than our information here at Aquinas. So, okay, so there's some sense in which we can give rigorous evidence. Rigorous evidence to kind of root in here being the boundary between the hard and the easy machine. So I kind of motivated it by saying, Okay, well, let's do things like degree counting or spectral methods or semi-definite programming. They all seem to like succeed or fail at the same region. And so I determined that that was the kind of the gap, right, between, or the phase transition between the hard and easy. But the communities kind of come up with a structure. Kind of come up with a structure in which to provide rigorous evidence for these sorts of phase transitions between hard and easy problems. And it's, you know, it's rigorous in quotes, which means it's not rigorous, right? So kind of the idea is that we believe that there are classes of algorithms like low-degree polynomials, which we'll talk about today, that are optimal amongst polynomial time algorithms, right? And like approximate message passing is among those. This is why, like, Is among those. This is why, like, you know, in all of our problems, we're using them because we think they're the most efficient fast algorithms. And so, okay, so what I'm going to do today is look at low-degree polynomials for a slightly more intense version of this problem. It's kind of a specific example of a stochastic plot model. And, like, the idea with low-degree polynomials is: okay, we'll just look at all the possible We'll just look at all the possible, you know. I have a data matrix, right? That's my adjacency matrix. I'll look at all the possible polynomials that act on that data matrix and see if they can, you know, somehow tell me something interesting about the problem. So in this case, if they can find the planted clean. And we believe that this is like a class of algorithms that should solve the problem if one can, because it includes a lot of the things that we would naturally think of doing. The things that we would naturally think of doing. So it includes all these subgraph tests, like counting the number of edges or triangles or whatever kind of sidewall you want to count. It also includes spectral methods. In fact, like A and P is a low degree polynomial and vice versa. Okay, so the version of this problem we're going to study is the planted dense subgraph. So this is different from the Dense subgraph. So, this is different from the planted cleek only in the sense that the cleek was fully connected. Now, I'm just going to give this, you know, this cleat kind of a higher connectivity than the background. So, now I have four parameters, right? So, I still have N, my number of nodes, K, this is going to be the size of the planted structure, Q, a background connectivity from my Erdoganier, and then S is kind of like how I upweight the connectivity in the clique. Point the connectivity in the clique. So, in particular, just thinking about constructing this, I go through all my nodes, all my n nodes, I put them in the clique or not with probability k over n. And then, you know, if when I'm adding edges, right, if both of them are in the planted structure, they get an edge with probability q plus s. Otherwise, you get an edge with probability q. Right, so there are a number of algorithmic questions that people have. Algorithmic questions that people have studied for this problem. You know, there's, in particular, kind of the two most popular ones are detection, which just says, okay, if I give you the adjacency matrix, can you tell me that there's a dense planted structure or not? So this is a hypothesis testing problem. Is there a dense planted structure or no? And then, you know, kind of the harder one, which is recovery, I give you the adjacency matrix. Can you tell me which nodes belong to the planted structure? And so And so, okay, so in this talk, we're going to think mostly about hypothesis testing. So, I just wanted to go through quickly, you know, what this means. So, for this problem, kind of the challenge is the following. We want to say, you know, as N grows, for what parameters can we find some sort of test that, you know, can successfully distinguish between the null and the alternative? So, here the null is just, okay, nothing. Is just okay, nothing interesting is happening. This is an Erdos Vigner graph, and the alternative is like, okay, we have this planted structure, right? So I want to find a test such that the probability of type 1 error plus the probability of type 2 error goes to zero as the size of the graph grows. And in particular, the two parameters now are, you know, K, the size of the planted structure, right? If the planted structure is sufficiently small, we're not going to be able to find it. Sufficiently small, we're not going to be able to find it. And also a signal strength or a signal-to-noise ratio, which says, you know, what's the level of connectivity within the connected structure. And so in particular, we might be interested in two things, a test to distinguish and then also a test to distinguish in a way that's computationally efficient. Okay, and so we know the answer to this question. I mean, we being the community, right, people have studied this. Being the community, right? People have studied this for forever. So the phase transition diagrams kind of look like the following. So here on the x-axis, this is my signal strength. And so as we go to the right, it's decreasing signal strength. So I'm thinking of lambda. You know, this is the notation I use for this kind of signal-to-noise ratio. So this is like n to the negative beta, where beta is between 0 and 1. Is between 0 and 1, and this is on the x-axis. The y-axis is the size of the clique. As we go up the y-axis, the clique gets bigger, and k here is some polynomial in n. So in particular, it's like in alpha where alpha is between 0 and 1. So kind of in these graphs, right, this top left region is where things are easy because we have a big planted clique and a high signal stream. Okay, so lambda, you know, so beta equals zero is just lambda equals one. So this is the planted clique problem. So along this axis, this is the phase transition diagram we looked at for planted clique, which says that, you know, root n is where, you know, spectral method, these sorts of things fail. So when the size is above root n, the problem's easy. Something like degree counting works, right? When the size You know, the size of the clique is less than, rooted, it's hard. So now we want to grow or change the signal strength within the planted clique, right? And so this gives us the phase transition for the planted dense subgraph problem. So in particular, so what I've done here is plotted the phase transitions for both detection and recovery. And kind of the takeaway here, right? The takeaway here, right? So here are all the references that went into developing these phase transition diagrams. But kind of the takeaway that I want you to see is the following. So if we look at detection, which just says, can we test whether a dense subgraph is there or not? This is strictly easier than recovering the dense subgraph, right? So in particular, what we can see is that if I just What we can see is that if I just map the recovery one onto the detection one, there's all of the space where we can detect, but in fact, recovery is hard. So this is a problem where detecting the subgraph is strictly easier for some problems than it is to recover the subgraph. So, kind of the question that we wanted to look at in this work is: okay, so if we're in this regime. Work is okay. So, if we're in this regime where detection is easy, we can determine whether there's a structure there or not, but recovering the structure is hard. Our question was sort of like, can we learn anything about the subgraph that's there? So, Galen would call this an all-or-nothing kind of problem. And so, okay, so this is what we wanted to do. And so, what we decided to study was whether or not we could test on the number. Not we could test on the number of communities within the graph. So, in particular, the kinds of problems we wanted to look at are ones like the following. So, we have some null that says, okay, there's one community, the size is roughly k with connection probability here, we'll call it 2q, versus an alternative hypothesis that says there's two communities, each are roughly size k over 2 with some connection probability. Connection probability. So this just says, you know, you give me the adjacency graph. Can I tell you whether there's two dense communities or just one? And so, okay, so how might we test this? Okay, the first thing to do would be to think about just counting edges, but it turns out that I've set up the problem such that this is not going to work, right? So, you know, kind of the idea is these connection probabilities 2Q and 3Q are exactly such that the total edge. Exactly, such that the total edge count in both graphs is expected to be the same. On the other hand, counting triangles works here. So all you need to do is kind of some simple combinatorics. And you can look at, you know, if you look at these two distributions, P and Q, the expected number of triangles in both is some multiple of Q cubed, K cubed, and the variance of the number of triangles is on the order. The number of triangles is on the order of n cubed, q cubed. So, what this suggests, right, is that if kind of the difference in the means is larger than the standard deviation within each of the two probability distributions, then we're going to be able to distinguish. So, what this gives us is this sort of kind of success region here that says, you know, we're going to be successful if qk squared over n. If qk squared over n is growing. So it turns out that this success region that just stupid kind of triangle counting gives us is exactly the same barrier between the hard and the easy region as in recovery. So what this is suggesting is that in this region where detecting is easy but recovering is hard, figuring out the Know the number of communities is as hard as recovering them, right? So this seems to suggest that, you know, no, right, if it's hard to recover, then we can't actually learn any structure about the planted structures that are there. Right, so this says, you know, in the entire region where recovering that planted structure is hard, triangle counting is going to fail to tell us whether or not. To tell us whether or not there are one or two communities. And in fact, you know, kind of the main result that we give in this paper is that, you know, okay, if you go from triangle counting to considering any polynomial with degree less than log n, then it's going to fail at the exact same region. So what we say is, you know, the same sort of regime we were looking at for triangle counting is exactly the region. Is exactly the region where no low degree tests is going to succeed testing between one or two communities. So triangle counting is one example of a low degree polynomial, but what we show is that no, you can't do something more clever to distinguish. Okay, so this is just some framework for doing the planted dense subgraph problem with multiple communities, right? With multiple communities, right? The idea is more or less the same. You look at each node, you place it in a community or not with some probability. And then, you know, if two nodes are in the same community, they get a higher connectivity probability. And the questions we want to study are detection, recovery, and then the problem we're really interested in, which is counting. So, how many communities are present? So, it turns out that what we show is that. Turns out that what we show is that indeed counting communities is not easier than finding them. It has, at least for this computational hardness regime, it matches exactly that of recovery. Okay, so before I state our main results, I just wanted to kind of say a little bit about how we actually do this for all the low-degree polynomials. So, kind of the high-level idea is the following, right? idea is the following, right? So, you know, we have a graph G, right? This is a random structure that's either sampled from, let's say, like one community or two communities. Since this is a random structure, if we have, you know, if I'm thinking about some polynomial F, right, that works on the adjacency matrix, the F acting on G is now a distribution, right? This is a kind of, or it gives rise to a distribution that's in fact a random variable, right? Random variable, right? And so the situation is the following. So, you know, I have a polynomial f, a test f. F works at distinguishing between these two, you know, null and alternative hypothesis if we're in a situation like the following, which says that the mean, you know, acting of F acting on G from P is well separated relative to the mean acting of F acting on G. Of F acting on Q, right, where well separated means that I need kind of the difference in the means to be larger than the fluctuations within each of these distributions, right? So this is a good situation, right? This is a bad situation. If we see something like this, we're not going to be able to distinguish with my test F. So kind of, you know, in short, right, some degree D test F strongly separates. Strongly separates. There's also a notion of weak separation, so it's going to strongly separate if the difference in the means is growing relative to the fluctuations, so the maximum of the standard deviations. And so we'll say low-degree polynomials fail. This is a class of algorithms that we think are the most efficient algorithms. So if low-degree polynomials fail, then no degree of order login testing. Of, like, order login tests to text, and we believe that this is like the computational barrier for the problem. Okay, so these are the main results. You know, the notation gets a little bit hairy, but basically what I'm saying here is, okay, we can look at this framework and ask questions like, is there five communities or is there one community? And in short, what we see is basically what we saw before, the kind of the line. Before the kind of line where the phase transition occurs is this lambda squared k squared over n. You know, it's this guy right here. And so, you know, above, or sorry, below that line, no degree d test weekly separates. So weekly separates just means like, can you get kind of any information about the difference between the two? And above that line, there existed degree three tests that strongly separate, so it's kind of a normalized. Separate, so it's a bad kind of a normalized triangle problem. You don't have to, you know, you don't have to have one of these guys that's testing against one community. You can also test like four versus two. You know, this is, and you, you know, now you have some other terms popping up and these boundaries, but it's essentially sort of the same. I'll also mention that a similar problem is planted dense. Is planted dense submatrix, which says, like, okay, you have an IID Gaussian matrix, and then you choose, you know, kind of a community of size roughly K, and you elevate the mean within that community. The question is, can you detect who's in the community? And the whole story is more or less the same. So, interestingly, I think, like, you know, what is this, like, at some high level, what is this saying, right? This is saying that, like, if you do something like Sparse PCA, like, figuring out. PCA, like figuring out the appropriate numbers of principal components, right, is as hard in substance as actually recovering the principal components. If you're doing like K means clustering, determining what K should be is the same hardness as actually doing the clustering. So, you know, I think that it's not surprising. We kind of have a feeling that this is what's going on in these regimes where we see this statistical computational gap. It seems like, just at least with the algorithms we know now, learning. With the algorithms we know now, learning anything tends to be hard, right? This is the all-or-nothing kind of idea. So, I only have a few minutes left, so I'll say a little bit about the proof, just like technically, how do you do this, right? So, you know, I motivated with the pictures of the densities and the smiley faces, right? So, we say a degree D test strongly separates or weakly separates. Or weekly separates kind of by studying the difference or the difference in the means relative to the maximum of the standard deviations under the null and alternative. And it turns out that the object that you want to consider is this thing called the advantage. So it's basically just something that looks at, you know, you kind of are maximizing over all possible low-degree polynomials, the ratio of, you know, the mean. Ratio of the mean under the null, for example, versus the standard deviation under the alternative. And so, kind of the idea is if you can, if this doesn't blow up, then you get the strong separation. So, you need to show that this is like a O1 kind of quantity. The proof is like really straightforward. I won't go through it. It's basically a proof by contradiction. You can show if a test strongly separates in this way, then you. Separates in this way, then that advantage is going to fly off to infinity. So the kind of technically, how do you bound this advantage? Well, it turns out that it boils down to kind of bounding moments of polynomials. So it's a big combinatorial proof that, you know, honestly, combinatorics I find a bit challenging. You know, it's easy for me to make a mistake and then not really. Easy for me to make a mistake and then not realize it until, like, I don't know, I like wake up in the middle of the night and I'm like, you know. But, anyways, that's like the kind of the proof techniques that go into low-degree polynomials. I maybe have like one more minute, so I'll say something that I think is kind of interesting. So, there's this work by my co-author Alex Wine and Salil Schramm that was kind of trying to Was kind of trying to set a stage for doing these low-degree polynomials for actually studying the recovery problem. So, most often the low-degree polynomial framework is established for testing problems, right? Hypothesis testing problems. You kind of need like a, in fact, like a null distribution that's simple in some way, and you establish an orthogonal basis. This is what's going on behind the scenes, like in that null distribution. But they found a way to kind of But they found a way to kind of do the low-degree polynomials for recovery. So the interesting thing is, you know, the object that they bound there is what they call this correlation. And, you know, it's very similar to the advantage we were looking at. So our advantage has kind of terms depending on P and Q, my normal and my alternative, whereas in the recovery problem, there's only one distribution. Problem: There's only one distribution, so everything's q here. And kind of the ideas, you know, you use some symmetry in the problem, and the thing you want to balance or the thing you want to balance looks the same. The numerator is slightly different. So the numerator now has an x term that's some function of the signal, and kind of you try to learn that x is kind of what's going on. So in this case, it might be like an indicator about whether node one is in my community or not. Is in my community or not. And so by symmetry, it's like if you can, if computationally I can figure out whether node one is in my community, then I can just do that, you know, and times. And so I think kind of the, you know, what I wanted, the message I wanted to say here is that by just kind of comparing these two objects that you need to bound for establishing the hardness of the problem via these low-degree polynomials, it gives you a way to. It gives you a way to move back and forth between like the recovery problem and the hypothesis testing problem. So, sort of the interesting thing is like, all right, if I'm given a hypothesis testing problem, then what this shows, you need some normalization here. I'm like leaving out some of the details. But, you know, giving a hypothesis testing problem, this is going to be equivalent to recovering, like in terms of hardness, and to a recovery problem where the thing you want to recover is the law of likely. Want to recover is like the log likelihood ratio, or sorry, the likelihood ratio if it exists. And vice versa, you know, given like you know, the thing I want to recover is something like indicator of an event, indicator my node is in the clique, then this says that recovery is actually equivalent to testing the randomness and the full randomness and y versus the randomness and y conditional on the event A. Event A. So, what this says is that it could be that if we want to study computational hardness of recovery, which seems to be pretty hard for these methods and many others as well, then there might be an avenue to do this, which is setting up an equivalent kind of hypothesis testing problem. So, I think this is an interesting idea that's a bit half-baked, but here it is. The other thing is that, you know, I say all this, but I just gave you a hypothesis testing problem that's equivalent. I thought this is a testing problem that's equivalent to recovery, but it doesn't fall into this framework. So, like, you know, this suggested that I should be studying a different testing problem. But, you know, I think at some high level, kind of reducing recovery to a well-poised testing problem is an interesting avenue for future research for solving these problems. So I guess I'll stop there and take questions. 