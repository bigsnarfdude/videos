So, first of all, I'd like to thank the organizers for inviting me to give a talk and for their hard work on putting these things together. So, yes, I'll talk about the existence of optimal polynomial meshes on any multivariate convex domain, which is a joint work with Fang Die. So, first, let me define what are the optimal polynomial meshes. I'm just wondering if you can see the mouse pointer here. Yes. Yes? Okay, good. Yeah, so we just use the soup norm for continuous for functions continuous on compact sets. So we don't write L infinity or C, we just simply L infinity of c we just simply use the set as the like lower index of the norm to denote the sub norm and we use this notation this calligraphic P and D for real algebraic polynomials in d variables which have total degree at most m so we just note that it's easy to see because it's a total degree you're gonna have that the dimension of this space so this is space it's a linear space It's a linear space, linear finite-dimensional space, and its dimension is going to be of order n to the power d. So once, so the problem, the problem that we are dealing with is we want to try to estimate the norm, so here again, it's a maximum norm, supremum norm, of a polynomial of an element of this space, of a polynomial of degree n from above. From above by a constant times the norm on some discrete subset, on some substantially smaller subset. And the question is, how small can cardinality of this subset be? And it's natural to expect that, well, it's clear it cannot be smaller than the dimension of the space, but can we go within a constant factor of the? Factor of the dimension of the space. So, can we find such discrete subsets y n in omega, which can have cardinality a constant times n to the power d, so that we can control the norm of the polynomial on the all domain, on the all compact set omega through the norm on this discrete subset yn. So, here we want this constant c1 and c2 to depend on. C1 and C2 to depend only on our set omega. So this has various applications. And I just want to note that we do have, so this is a partial case of Martin Kiewich-Zygmund discretization inequality for the case when we discretize the L infinity norm. In the L infinity case, which L infinity in case, which is our case over here, you do not see the inequality in the other direction because it's automatic, because it's a maximum norm. And if you take the maximum norm over a bigger set, it's going to be bigger. So it's obvious that norm p omega is bigger equal than norm p y n. So there is no need to state the other inequality over here. We only worry about estimating the norm of p on the whole set from above using the normal. Set from above using the norm of p over this smaller discrete set. Now, I also want to say that the, and I will repeat it later again, that usually when we are proving this kind of inequalities in a rather constructive way, we would use some kind of polynomial inequalities, and those polynomial inequalities could be. Inequalities could be of independent interest. So, besides various applications, there is also an interest in the ingredients that one could use to come up with a proof of this kind of results. So, this is my goal over here is also talk a little bit about the ingredients that we had to use to establish this kind of inequality. So, I will begin with the simple examples just to get a little bit of a feel for. Bit of a feel for what we are doing over here. So if we take the one-dimensional case, we just take a segment, then there is the Pernstein-Markov inequality. So we can estimate the derivative of a polynomial in one variable at a point x. And there is this kind of factor that if you are very close to the boundary, the n squared will be smaller than this. Squared will be smaller than this. So you get Markov inequalities. So the derivative cannot exceed n squared times the norm of the polynomial. And if you are in the middle, like if x is about zero, with the midpoint of negative one, one, it's going to be of order n. And this is kind of the correct way to estimate the behavior of the derivative if you know that the norm of the polynomial is the uniform norm of the polynomial is fixed. Fixed and like you could look at Chebyshev polynomials and just move them around a little bit to get like lower bounds here. So to prove that this is essentially sharp. So there is this boundary effect here that close to the end points, because we only restrict the norm of the polynomial on the segment itself and we don't put any restrictions outside of the segment, we're going to have a more We're going to have more flexibility when we deal with polynomials. So, the polynomials may have larger derivatives, they may behave faster, and this forces us, if we want to discretize this norm, to put more points closer to the boundary. So an example of how we could construct this optimal meshes for the segment could be the following. You just take Chebyshev partition of some order and capital, which is a multi. Capital, which is a multiple, some integer multiple of little n, then you can show that this factor in front of norm p for x in an interval of this partition behaves like little of L divided by the length of the interval. So that's the behavior of this Bernstein-Markov factor. So if you choose L large enough, you would be able to show that for To show that for any interval of this Chebyshev partition, the largest value of the derivative multiplied by the length of the interval is going to be controlled by, say, half of the norm of the polynomial. And then you just select a point from each i to include into your yn. So this yn is going to have cardinality approximately. Have cardinality approximately L times N, and then you just apply mean value theorem. So you pick a point Z where the norm, the uniform norm of P is attained. That Z would have to be in some of the intervals of the partition. So you will be able to find a point Y from your mesh in that interval I. And then on this Z Y, you would apply the mean value theorem and you could. mean value theorem and you can just get the required. So if the mean value theorem would give you a transition to the derivative, you do know the good bond on the size of the derivative of this interval. And that way you will get the required inequality with C2 equal to 2. So that's how you would deal with the segments. So you could just take sufficiently like a good multiple of n, look at Chebyshev partition and just pick. Look at Chebyshev partition and just pick an arbitrary point from an interval, from each interval of the Chebyshev partition of constant higher order, and that will do the job for the segment. So that's an example. But of course, the more interesting part is what happens when we go to higher dimensions and how are we going to deal with this question when we get to higher dimensions. So, this is a brief history of So, this is a brief history of the problem, and it's not meant to be exhaustive, but just to illustrate some things that have been done. So, first of all, what's interesting is to mention that nearly optimal meshes. So, if you allow the cardinality not to be of order n to the power d, but to have this extra log factor, then nearly optimal meshes are going to exist for any compact domain in RD. And the proof of that uses factor. And the proof of that uses faceta points, which are quite unconstructive and hard to come up with. So, but that's going to be our, like, if you want to get rid of the word merely, we are fighting this log. Okay, so log is there, but it's all about getting rid of that logarithm to the power D. Now, that was the this problem was studied. This problem was studied quite systematically by Andrashkru, and he showed the existence of, in quite constructive way, the existence of optimal polynomial meshes on convex polytopes in Rd on C alpha star-like domains with a certain parameter alpha that depends on D. So if a boundary is sufficiently smooth, it works. If it's completely non-smooth, so kind of in line with the other problem. In line with the other problem that Andr√©s was discussing here, that was the situation, non-smooth situations, smooth situations were covered, and he made the conjecture that any convex body in Rd should possess optimal polynomial meshes. So here convex body, we just mean a convex compact set with non-empty interior. So essentially, D-dimensional convex compact in RD. So the So that was the conjecture. And in 2016, Pierre Zone has proved the existence of optimal polynomial measures on certain extensions of C2 domains, not necessarily convex. And then Andrus himself in 2019 proved the conjecture for the case d equal to 2. So for the Planck. Equal to two. So for the planar case, when we have domains in the plane, using certain new tangential Bernstein type inequalities, so it's quite expected, right? So like from what I said, that you do need to know how the polynomials can, how quickly they can change. So, and that one of the ways to proceed with that is to use the derivative, as we have seen in the one-dimensional case. Now, in Case. Now, in a little bit later, I was able to characterize the behavior of Christoffel functions for any convex body in the plane using just only geometric characteristics of the body. And that yielded a different alternative proof of the existence of optimal polynomial meshes in the plane. In the plane, using some interesting connection between Christoffel function and positive quadrature formulas, which was established in a paper by Boss and Vianello. So the main result that we managed, hopefully, if everything is correct, to achieve with Fang is that we confirm the conjecture. The conjecture for all dimensions. So for all d, at least three and d equals to two also works with the same technique. So for any convex body in Rd, we can find a sequence of discrete sets yn with in omega with cardinality at most n to the power d times a constant so that you can control the maximum norm of a polynomial of degree of total degree n on this. Total degree n on this convex body omega using the discrete, just the maximum at this discrete subset, which has cardinality n to the power d. Now, I just want to point out that we have this c of d and we'll discuss this later. We do have this constant that depends on d and doesn't depend on the actual geometry of the body omega. So it could be something like narrow and long, and it's still. Uh, and long, and it's still, uh, you don't have that the constant is going to blow up. So, the constant depends only on dimension, doesn't depend on the actual shape. As long as it's convex, everything is good. And we also, same technique works to do the epsilon version of this discretization result. So, you could replace this two with one plus epsilon for any positive epsilon, and then you'd have to let this constant. This constant C depends on epsilon as well. So now let's talk about the various ingredients that we used in the proof. So the very, very important, I would say simple, but very important result is the use of John's theorem. So John's theorem Uh describes it characterizes the ellipsoid of largest volume which is inscribed into a convex body. So the theorem provides certain properties of the contact points of this ellipsoid of largest volume that is inscribed into a convex body. And those properties imply that if you blow this up, if you take this largest inscribed ellipsoid and dilate, And dilate by a factor of t, the resulting ellipsoid will contain your body, will contain your convex body omega. So in a way, you can always squeeze any convex body between two ellipsoids, but because our problem is invariant under affine transforms, so the discretization, the set of polynomials of total degree at most n doesn't change if you make an affine transform. If you make an affine transform of the variables, we can just make that affine transform and assume that our convex body is always between the ball with the unit ball and the ball of radius D. So that's an assumption we can always make. It doesn't change the constants in our discretization inequality. So that's kind of the bottom, the bottom line, the key reason why. The key reason why we are able to get the C of D without dependence on omega. Now, there are other good things that come out of this. In particular, this kind of relation allows us to, instead of to work, to represent our domain as a union of certain like graph type domains, or what we call domains of special type. So we would just use. So we would just use some convenient set in D minus one dimensions. So it's like a cube, or you could use a ball. We just use cubes. So here, the illustration is two-dimensional. So we use a segment. And so on one side, it's going to be segment, and then there's going to be some convex function on the other side. And we'll just deal with such kind of yellow things. And this condition in Condition in particular allows to control the slope of this boundary piece. So we know that you cannot have too steep a slope because there is a ball inside and there is also a ball outside. So you cannot be too far. And if you were allowed to be too far, you'd be able to have a very high slope of the tangent line to this piece of the boundary. Piece of the boundary, but you cannot do that. And this turns out to be very important. So we could prove various things, like we could switch from dealing with general convex domains to the so-called domains of special type, and we can control the slopes here. It becomes much more convenient to work with. So that's, I would say, it's one of the very helpful ingredients that we ended up using and like keep. Ended up using. And like, if you need to work with convex domains, please consider this. If your problem is a fine invariant, even if it's not, it's very likely that it can really help you to normalize the position of your convex body and then go from there. So, and now there, now this is more important. So, while this is kind of expected, and this was using Of expected, and this was used already in various problems, approximation problems with the general convex bodies. Now, for this particular problem, it's one of the key elements that we ended up using. So this is the so-called Dubiner's metric on convex bodies. Now, there is a paper by Dubiner. A 1995 paper, which is like 75 pages long, which works with kind of relating local approximation error by polynomials with global approximation error. And there are probably some kind of, well, there are certainly some very powerful ideas in that paper, particularly this definition, this metric, which was key plus here. But many people. But many people complain that the paper is hard to read and hard to follow. So, while we do use some ideas from Dubiner's paper, I want to say that our exposition is completely independent of that of Dubiner. So, if we needed anything, we have reproved it independently. So, the idea is the following: you take two points, X and Y, in your X and Y in your omega and take a direction C. So now look at the projection of these points X and Y in this direction C and you can also find this A C, which is like the supporting line, the supporting line to our omega. There's going to be two of them. So you kind of take the one which has the smaller value of the projection onto this axis given by the direction C. Direction C. So if you denote by dx is this distance between the blue line, so the or it's going to be actually it's not it's lying in the two dimensions, but generally speaking, you look at the hyperplanes. So if you look at this distance between the blue line and the red one, so this is your dx, and your dy is between green and red. So it's over here. Take the difference of the square roots and Difference of the square roots and look for the direction, see where this difference is largest possible. This is what's going to give you this value rho of x, y, this distance between x and y. And it's designed in a way that, say, if you look at the one-dimensional example, if you take two points, so you could also, this also can be defined for the segment. So if here, what will happen is. So, if here, what will happen if you take arbitrary two points from a Chebyshev partition, this distance between them is going to be like a constant over n. So, this distance, it gets smaller towards the boundary, and it gets smaller, as we will see, in a very appropriate way for our problem. So we could use this distance to provide the construction of optimal measures. Of optimal measures. So, what we actually use more often in our paper is not this specific distance, but its version, which is related to the domains of special type. So, say for we introduce an equivalent metric. And so, here I describe how it works for two dimensions. So, if you take a special subdomain G, which is like the epigraph of F, so if you suppose. Epigraph of f. So, if suppose your domain is kind of essentially the epigraph, it's about your f, and you have two points. So, you have point x, which is now has two coordinates, x1, x2, and point y, y1, y2. So, rather than, so once you switch to this domains of special type formulation, there is no need to look for all possible directions. So, here C changes through the unit sphere. Through the unit sphere. Now, over here, what you could do, you always look at the distance along vertical directions. So inside the square root, you have simply x2 minus L of X1, where L is a supporting line to F. And the other root is going to be this Y2 minus L of Y1. So that's your nu dx, nu dy. And this is explicit. This is, you know, much easier. Is explicit. This is, you know, much easier. I think it's kind of easier to compute your one of the direction never changes. So your direction along which you compute this difference of root stays this root stays the same, but you vary over the supporting lines or like in the more general case, you'd have hyperplanes to your function over the domain of your function. So it's going to be the cube. So we show. Uh, so we show the equivalence. We show that uh this kind of distances uh are actually equivalent and it will be easier to work. Actually, the the the paper itself is quite technical. I'm focusing on ideas here and on nice pictures, so hopefully nice pictures, but there are a lot of technical details that are present. So it does take some work to to formally prove all the all the inequalities and relation. All the inequalities and relation and these equivalences between different metrics. So, that's that's the metric. So, now the construction of the mesh. So, how exactly do we construct? And I would like to say that this is a rather theoretically constructive proof. I think like if one would want to implement an algorithm, it's not going to be that easy, but still, this is this. I do believe it's a constructive proof that we have. That we have. So essentially, what we show is that you can take, in place of lambda n, in place of your optimal mesh, you can take C D over n, where C D is a sufficiently small constant that depends only on D. So you can take a C D over N covering set. So what you want is that any point of your domain be within distance C D over n. Distance C D over n, and the distance is this Dubiner distance of some point from lambda n. So if you think about like raw balls centered at the points of lambda n of radius C D over n, they should cover the whole domain omega. And also we want separation. We want that any two points be the distance at least eta over n, where eta is some parameter, which could be eta is some parameter which could be equal to C D over n or could be smaller. And in the case when C D is equal to eta, that's exactly just what you would call maximal C D over n separated set with respect to rho. And this maximal says, you know, like just hypothetically, the algorithm, how you construct one would be you just pick arbitrary point, construct this ball of radius C D over N, and look at the Do C D over N and look at the leftover, pick arbitrary point from the left over, construct another ball, and just keep doing this until you stop. And if you cannot select any other point, that means so any point is within the distance C D over N, so you got covering and you have separation by the design. But I think in practice, implementability of like how exactly you're going to track, you have a convex domain, how exactly. To track, you have a convex domain, how exactly are you going to track like which portion you removed? This may become you know problematic if you were to implement this on a computer somehow, because this is a very continuous data thing, like a subdomain of arbitrary convex set. And in computer implementation, you do want things which are more discrete. But anyways, that's not the goal here, but what I want to say. What I want to say is that this parameter eta, if you do take it smaller than Cd, that gives you more flexibility in the construction. So you don't have to have separation to be with the same radius as covering. So you can have separation with a smaller distance and covering with slightly bigger distance. Both distances should be the order of one over n, and then you are good. So any mesh, any mesh that satisfies this kind. Mesh that satisfies these conditions will do. Well, provided this constant is small enough. So that's the way we construct it. So, how do we actually prove this? So, there are two ingredients here. One ingredient is that we use covering condition and we actually establish the discretization inequality. And the discretization inequality follows immediately from covering and. Immediately from covering and the following theorem, which bounds how the polynomials can vary. So we don't establish a Bernstein-type inequality, we do not bound derivatives. What we do here, we bound the difference, q of x minus q of i, where x and y are arbitrary points in our convex body, and q is a polynomial of degree n. So the bound is like this, that this difference is at most constant. At most constant that depends on d times the degree times the row xy. So that's this result would immediately, rather immediately imply that once you have covering, you get this discretization. You get this discretization. So the way it's proved is that the proof is essentially two-dimensional, surprisingly. Two-dimensional, surprisingly. So, what you're going to do is you take this x and y, and you take, remember that our set omega is normalized, it's in a normalized position, it has unit ball inside, it has ball radius d outside. And you take the hyperplane through the origin and through these points x and y, and you would be able to reduce things to considering the two-dimensional version of raw, which is. Of raw, which is in that plane, and that two-dimensional in that section, and that two-dimensional version of rho is going to be properly bounded by the higher-dimensional version because of the fact that you can extend. So, like in this picture, say, if you take if L, if say, imagine that this is a section, imagine that this is the section of that it's not d. Of that it's not d equals to but say higher d, and you have and you just considered a section of your function. So if you take a supporting line L, this blue line L, to a restriction, to a restriction of your function to a segment, you will be able, if your function is convex, you will be able to extend this L to a supporting hyperplane to the Hyperplane to the surface that the higher dimensional F, not to the restriction of F, but to the whole F. So this follows from the classical convex geometry separation arguments. And that's the key here. That's the key how we can relate a two-dimensional version of the Dubiner metric with the higher-dimensional version on the whole omega. So we kind of can restrict ourselves only to working with. Ourself only to working with two-dimensional, so that's uh, that's where we kind of were able to get to that high-dimensionality because it turned out that it suffices to work in this section. Okay, so in order to work with the section, to do things in the section, we use just a minor modification of a lemma, of a two-dimensional lemma that I used in. Lemma that I used in that paper on Christoffel functions on the plane. And I wouldn't go into too much details here, but it's kind of hard to read and you don't need to. I'll just briefly try to explain what's going on here. The idea is again, we work, so black is our F and our domain is above F, epigraph of F. And what we want to do: And what we want to do is we want to find, so delta is a certain parameter which in a way describes how far we are from the boundary for at least one point. And what we want to do, we'll look at all parabolas. So you see this dashed red parabola, and then there is the solid red. So this are parabolas of the form like delta over 2 plus kx squared. We want this to We want this to be above our f on the interval negative 1, 1. And we want to choose the smallest possible leading coefficient k. So we kind of start with a steep and narrow parabola and kind of make it more shallow until it touches our F at some point. So what we do then, we take that point of contact, kind of it could be the end point as well, and it works either way. And then we Works either way. And then we take the supporting supporting line at that point q. And this inequality, the main outcome of the lemma is a certain inequality which relates the parameter like k is the leading coefficient of the parabola. And this alpha and beta are the parameters of this L, of the line L. So alpha is the slope, and beta is the distance from the origin to the intercept. It's like y-intercept. Y-intercept. So, in a way, we show that you can relate certain internal parameters of the convex body with certain external parameters. So, in a way, we try to inscribe the best possible parabola. And at the same time, we try to find two tangents. Two tangent lines which are from outside, and we want to squeeze them together well. So that's what this inequality does here. It kind of shows that there is a proper relationship between these parameters. So the two tangent lines, one is the zero, like constant zero is f here is no negative. So zero is the tangent supporting line to f. And we find this blue, this other line L. So we show certain L. So we show certain interplay between these parameters. So we then use this interplay to read out. So we from this two-dimensional version of rho, we make a readout. So this rho is the maximum over all possible tangent lines. And we know exactly which two. So there are many of them, but it's enough to read out the information, this maximum, from two specific. From two specific tangent lines. So we take or supporting lines. So these are the one is here, this blue one, we knew that it was the one that was tangent at that point. But the other one is given by the lemma. So we then use this readout, and that readout, that information, how large is that difference, how large is this difference of roots? Roots, uh, where is it? Are they two-dimensional? So, how large is this difference of roots for those two lines? That information is sufficient to bound the k from above. So, if you bring the root k in the other direction, you'll get a bound on k from above. So, you will know that this parabola is not too steep because the problem is when it's too steep, it kind of gets out of your domain, it can get out of your domain. You want it not to be. Domain, you want it not to be too steep, and you want to know how what's the value of that coefficient. So that's what's happening over here. Then we can just draw a path from a point X to the point Y, and we would use one vertical direction, and we would apply a one-dimensional Bursting inequality on this green vertical segment to get some bound for the blue point, and then you'd use. Blue point, and then you'd use a parabola. So you'd use a one-dimensional Bernstein inequality along this parabola, this red piece over here. And that way, you can use mean-value theorem here in combination with the Bernstein and you use the mean-value theorem along the parabola. When I say along the parabola, it's just we kind of take we form a one-dimensional polynomial by taking like p of x and parabola of x. X and parabola of X, like this composition, and we arrive at a one-dimensional polynomial, and then we apply Berstein inequality. So combining these two inequalities along segment, along parabola, you can get the required bound on this difference of Q's via a two-dimensional version, and then it extends to the higher-dimensional versions by that relation or with extension, extending the danger. Extension extending the tangent line to a tangent hyperplane, supporting hyperplane. So that's one component here. So once we know the Dubinear distance between X and Y, we can estimate how much can polynomials vary. And that gives us the required discretization and equality provided we have covering condition. But that doesn't tell us that how many points have we used. Many points have we used? We still need to establish that there's going to be a proper number of points in our mesh generated this way. And the way we do it, we use the separation condition. So we show that once we have a point set which satisfies this separation condition, then we can bound the number of these points in the mesh as a constant times n to the power d. So there's the second part that we need to do. And to Need to do, and to do that, we do it as follows: like, if this is not true, like, so we want to show that the cardinality of our y n, provided we have the separation condition, that this cardinality doesn't exceed the cardinality of space of polynomials of like some constant times n, sufficiently large constant times n. And that will imply what we need. And the idea is that if this is not true, if this is not true, Is not true. We would use certain fast-decreasing polynomials. So these are fast-decreasing polynomials, and you see there is certain, so they have value one at a given point x of the domain, and then they decay, they are non-negative, and they decay in a certain way. And this expression here again uses the Dubiner distance between x and z. So fast decreasing polynomials, they Fast decreasing polynomials, they've been introduced by Totik and Ivanov and studied by other people, but for the multivariate case as well, by Andras as well. So here, the structure of this fast decreasing polynomial really depends on this rho x, on this Dubinier metric. So it kind of this estimate, it properly accounts for the boundary. Accounts for the boundary of the domain. So, what happens if you see it's stated for polynomial of degree n, if you're dealing with polynomial of cc times n, then that constant c inside here will kind of make that polynomial decrease a little faster. And the idea is that if you take a large enough c, you will be able to get a contradiction with linear dependence. Linear dependence. So you want, you would show that if this inequality was not true, you'd have many more points that the dimension of your space. You'd have kind of too many needles and those because those polynomials are, those fast decreasing polynomials, they are kind of too much needly, like they have too much of spikes that they decay fast enough. You'd be able to show that you cannot. You'd be able to show that you cannot write down a non-trivial linear combination of them. And that would prove this desired inequality. But it's not just about this inequality. It's not just about the construction of these polynomials. You need some other ingredients. And this is our final ingredient that we used both for getting this contradiction. For getting this contradiction with linear dependence. And in the proof of the theorem itself, we needed the doubling property of this metric row. So we wanted to estimate the volume, the d-dimensional volume of a neighborhood of radius 2H using the volume of neighborhood of radius H. Neighborhood of radius H. This kind of property would also kind of tell you that you cannot have too many points. If you have separation, you can see how many points are close to your X when you try to expand. So it's important from various reasons, but that was really instrumental in this. Was really instrumental in this. We really needed it for this theorem itself and for the contradiction itself. So we needed the Doubling property to work with the details. So it's a very important property, Doubling property of this metric row. And otherwise, it's like once you have the Doubling property, the construction of this P is quite explicit. The construction of this fast-decreasing polynomial is quite explicit. Decreasing polynomials quite explicit. We use some Chebyshev polynomials and certain products of them until we get the desired estimate. And the Dublin property has actually quite a simple proof because if you look at this raw neighborhood of radius H centered at X, this thing is simply the intersection of the following. The intersection of the following strips. So once you fix the direction c, you would get like the set of y that satisfy from omega that satisfy this inequality would be in a strip. So your b rho, b rho, is just an intersection of infinitely many strips where you take c this direction over This direction over the unit sphere. So it suffices to simply show that if you take a single strip like this and you apply a homotech or like you dilate it by a factor of four with the center x, then that new strip, the strip, that new strip, the dilation, so here phi is the dilation with a factor of four, will contain the Will contain the strip that corresponds to 2H. And to prove this inclusion, you just deal with one-dimensional inequality. There are certain one-dimensional equality you need to establish. And then you just pass to intersections. So essentially, you just show that the phi of B rho xh contains b rho x2h. And this is why we get photo to the power d when you're estimating the volume. So that's, and the last thing I wanted. The last thing I wanted to discuss, and thank you for your attention. Okay, so we'd like to ask any questions. Yes. Andri, this is Danny. I wanted to ask: don't you use, don't you need the doubling property also for? Property also for the using the John's theorem because, after all, you want to make sure that you kind of can compare the inner ball with the outer ball. No, not really. You don't really need it's this doubling property doesn't require Jones' theorem. You can require, but don't you need it for what you do? No, no, for the Jones theorem, no, you don't. You don't need. No, you don't. You don't need the doubling property. It just comes by itself, you know. Like you can always make an affine transform, and it will contain a unit ball, will be inside a ball of RadioSD, and it has nothing to do with the metric row. Like this metric row is a different beast. Okay, thanks. Okay, so we have time maybe for one more question, not too long. Can you hear me? I hear. Okay. Can you hear me? Yes. Okay. Yeah. Thank you very much for the interesting talk. Can you say a bit more about the constant behind this estimate, N to the D times C of D? How bad is it? We didn't track it. It's a good question. It's probably quite bad. It's probably like. quite bad it's probably like d to the power d because the best in the best it would be exponential if everything was good it could be like uh four to the power d or something like that but i would my guess it's probably worse it's probably like d to the power d and for some specific examples of uh sets maybe there is it's possible to track it so well we didn't try but yeah i i i can look at i understand like yeah there Look at, I understand, like, yeah, that if you are coming from the point of view of like functional analysis, you'd be interested in independence of this constant on D, right? But at this stage, our main focus was, you know, just there was this problem and we just wanted to solve it. And that was our main focus. And we didn't really think of the constants, but it's a good question. Maybe last. Maybe last, last yeah, very last comment. If I give you a domain, maybe coming from practical problems, could you implement this approach, what you described? I mean, you gave a quite constructive. It's going to be hard. It would depend on how you... I don't see in my head a data structure for a cheese, so to say. So I don't see in my head a data structure which would be a convex demand. Would be a convex domain, and I would have to kind of eat out certain these neighborhoods with respect to this distance. I do have, we do have quite nice description, I think, of sets that are equivalent to these neighborhoods, but you would eat these neighborhoods and there would be something left. It would have to be determining if there is something left and we would have to pick at some point. It doesn't have to be random, but we would have to pick. Doesn't have to be random, but we would have to pick some point in that remaining neighborhoods. And I just right now I didn't think seriously about this, but it is constructive, I would say, in the sense that we usually say constructive in theoretical approximation theory. But in practical sense, there may still be some work to be done. But we can describe from our proof, we can describe reasonably well how the Reasonably well, how these neighborhoods with respect to distance raw, how they look. Okay. Okay, thank you very much. Yeah, thank you. Okay, thank you very much. I think we have exhausted the time, so we can thank the speaker again for this great result. Thank you.