So, hello, everyone. So, my talk is about deep learning and the use of deep learning in phylodynamics to estimate epidemiological parameters. So, here is a brief introduction to phylodynamics. The term was coined by Gwynfeld in Science in 2004, and here is And here is a wiki first definition of paradigmics. Parallynamics is defined as a study of how epidemiological, immunological, and evolutionary processes act and potentially interact to shape viral phylogenies. And when you look at the phylogeny of influenza the graph here, you immediately have the feeling that there is a strong selection and you're right. But you would like to know more about the model and the parameter of the model and Of the model and this kind of questions. So, first, phylodynamics apply to fast-evolving pathogens, mainly viruses. And the reason for that is that to make it short, there should be more mutations and transmissions. Because when you look at a branch here, for example, in that phylogeny, if we are able to detect that branch, this is because we have several mutations, meaning that we have more mutations than... meaning that we have more mutations than transmissions. Most approaches, at least the approaches I will discuss, they are based on three models. I will explain and describe a few of these models. These models are similar to the models that are used in mathematical epidemiology, for example similar to the famous SIR model, susceptible, infectious and remote. And importantly, with trees, And importantly, with trees, the likelihood is defined by complex set of ordinary differential equations. I will show you an example. And the goal is to both to select the best model and estimate the parameters of this model. For example, R0, which is the number of cases directly generated by one case. Here is the simplest possible model, which is the worstest model. Which is the worst S model with incomplete sampling. We have two states or three states. An individual can be infectious, then it can transmit at the rate beta, or it can recover at the rate gamma, and when it recovers, it can be sampled with sampling probability equal to s or not sampled with probability 1 minus s. R naught is R0 is equal to the ratio of beta divided by gamma. And this model is not identifiable, meaning that either the sampling probability or gamma or one over gamma must be given. And with such a model, we are able to simulate a complete tree, which basically describes an epidemics. But because that tip is not observed, what we add What we have as input of our method is not the complete tree, but the sample tree. We do not see that tips and we do not see that transmission. And this is what makes all the mathematical difficulties because we have to account for every very large number of unseen transmission, unseen tips, possibly infinite number. Here is a slightly more complex model, which is called burst desk with x. Which is called birth death with exposed class BDAI. Pretty much the same model, but before becoming infectious, the patient is exposed and there is a delay before becoming infectious, which is equal to one divided by epsilon, which is the rate, the becoming infectious rate. And typically, this is five days or one week with start culture. And here is a model. And here is a model we have more or less already discussed about, which is a model of super spreaders, where we have two types of individuals. Some are normal, they spread normally, and we have super spreaders that spread much more than the others. And basically, here we have two birth test models, this one for super spreaders, and this one for normal spreaders. And normal spreaders can transmit to normal spreaders can. Normal spreaders can transfer to normal spreaders or super spreaders, and this is the same for super spreaders. But the parameters gamma and S, they are the same for the two models. And this is a very common assumption to have super spreaders. For example, in HIV, but this assumption has been done also for Ebola, Altar Scop2, and other indices. But now R naught is defined as the ratio between ratio between these two transmission already developed by gamma and because of the trans because of these constraints we have a very simple expression for the superspreading ratio which would equal for example to beta of s giving to n divided by beta of n given to n and the fraction of superspreaders is also deduced from the from the parameters is equal to beta of s given to s divided by beta giving divided by beta given to S plus beta of S given to L. So the standard method to infer the parameters is based on the likelihood. This can be maximum, likelihood, or Bayesian approach. And for example, with the burst test model, we have a closed formula for the likelihood of a given tree. And we simply maximize. And we simply maximize the likelihood and will estimate the parameters. But for complex models like VDEI and VDSS, we do not have cross-form solutions. We have this is complex recursive systems of ordinary differential equations. I will show you the example of BDEI. Here are the equations that we have for a single merge. And basically, we have four ODE. And basically, we have four ODEs, for example, that one with starting condition here. And if you look at this carefully, I will not explain the detail, but there is a recognition there, meaning that the starting condition is based on the equation of the left child and right side of the load that is under computation. Meaning that it's very time consuming to compute the likelihood. This is that there are plenty of evidence. Is that there are plenty of numerical issues, and it turns out that it is a problem of convergence. I will show you numbers and examples. And because it is low, this cannot be used with large grids, basically larger than 500 tips. So then another approach, which is quite common, is ABC, approximate biasing computation. Approximate biasing computation. The real tree is encoded with summary statistics. We simulate using the model 10,000 trees within a prior. Obviously, the closer the simulated trees are from the real trees, the better will be the results. And then we have a registration step where, for example, using the Euclidean distance with the summary statistics, so we take the onboard closest trees and then we can And then we can simply use the median or the distribution of the parameter within these closest trees, or we can use an adjustment based, for example, on lasso or neural networks, and which slightly improves the results. This is like a tree. This is relatively fast, but we still have to generate these trees. And this is fairly accurate, at least for the Rate, at least for the examples that we have done. But this is dependent on the metric, which is very arbitrary, and of summary statistics. And here, in our experiments, here we have been using about unbelievable summary statistics, about bunch length, trigger balance, linear spotlight implots, etc., which we have taken from our previous publication with Emma Sori in 2016. So we tried in this way. So we tried in this work another approach. We used millions of trees with a large parameter range. And these trees are different tree size. For example, in the experiments I will show to you, we have trees between 200 and 500 tips. And yes, with a large parameter range. And these trees are encoded into a vector or a matrix. I will explain how. I will explain how in the next slides. And we use a neural network, and the neural network is trained to predict the parameter values. And then when we have a new tree, the tree is encoded using the same encoding system. And this is fed to the network, and we get the prediction from the network. So the time to learning is long because we have to generate all these trees and we have to learn. This tree, and we have to learn. Learning is not to learn, in fact, but generating the tree state types. And this is likely free, registration-free. This independent of the model, meaning that using another model, we can retrain another network and make the prediction. And there is no need for summary statistics. But obviously, all the difficulties is there. The very common difficulties in deep learning is how can we encode trees. Trees. And we tested many solutions. I will explain the solution the finally works. So, the first one, this is a two-step approach that we call complete, rejective, ladderized, vectorial, tree-encoding, the bit complex, CBLB. So, the first step is a kind of standardization. We perform lateralization, meaning that for every node here, we swap. Node here, we swap the order of the sub-trees to have the deepest sub-tree on the left, and we do that recursively in all the tree. And this standardized input tree. And why we had this idea, this came for influenza, because the influenza is clearly leader-shaped. And we had the feeling that giving the input tree this kind of shape should help in terms of epidemiology. We tried other standards. We tried other standardization, no one was better than that one. I could explain it later. And once the tree is laterized, we perform an in-order tree traversal. This means that we start from the tree root here and we go because it's in an in-order traversal, we go to these tips and then we record the depths or time to root of the tip, which is six. Then we go back to D. we go back to D, the depth is two, then to D, small D, the depth is three, and so on, etc., until we arrive there, and the depth here is three, and for example, here we have zero because this corresponds to the three roots. Then this vector is reorganized into a matrix, and then this matrix is completed with zero to account for the size, maximum size of the input tree in R. The input tree in our networks. Importantly, any tree can be reconstructed from this representation. I will explain that in the next slide. And very important as well, this representation is extremely compact because the number of entries in the vector is 2n minus 1, n is the number of tips. But with such a tree, we have 2n minus 2 branch lengths, meaning that with just one. Length, meaning that with just one additional entry, we have been able to encode the tree topology as well, plus the order of the quantities. And why we can reconstruct the tree? I will explain this in this figure. This is very simple. Here is an input tree slightly changed because this corresponds to the depth of age, which is equal to 6. Then we break that part. Bridge that path, then we construct this path here with length two and then an additional length of one for b and the same for big b and small c, etc. And once it is done, we just have to gather together horizontally all this path and we get the tree. Okay, then we use two types of architecture, neural network architecture, all implemented on Keras. Method on Keras, which is a very easy software for neural nets. And when we have an eye-level representation, we use summary statistics and a feed-forward neural network. The first layer corresponds to the summary statistics. We had this 98 summary statistics plus the sampling probability. And this is And this is the very standard neural network architecture, which is multi-layer cassetron. And this is a funnel shape, also called like that. And for each of this, each of the neurons, we have an EU activation function. And for the output, this depends on the task. For parameter in terms, we have linear activation and for most. Linear activation and for model selection, we have a soft map activation. So, for the CBLD architecture, which is the rough description and very combinatorial ad description of the tree, we use another architecture, which is a 1-deep convolutional neural network, where the input corresponds to the matrix I discussed before. And then we have And then we have a kind of feature extraction. And in this image, here we have six features corresponding to each of the column here. And we have a sliding windows. Here we have seven windows in the first layer. And each feature, for each feature, we have the same weight and same activation function that is applied to all the windows. And then we do that again, and at some point, And at some point, we have what we call max booting, meaning that we summarize this big picture map into a smaller one using a simple max. Again, convolutional layer. Again, now we have an average cooling and basically we use a program network, the same one I discussed before. And again, we have two types of output. Output corresponding to parameter inference or model size. What are the results with simulated data? With the first test model and the conditions were the same for all of it. We had a training set of 4 million trees. We have 10,000 trees corresponding to the validation set, which is used with auger 15. The test set was 100 trees, and the trees are. And the 2500 tips. And the results are here. Here, you see the results of this. Here, the results of the summary statistics with the feed power network. And here the results of the CBLE with the computational network. And vertically, you see the relative error, meaning that the lower the better. And here we have an average, this is the average error for the Android condition. Average error for the underworld conditions. And what you see here is that the results are very close to beast, slightly worse, but very similar. In fact, the difference are not significant. And this makes sense because in beast with the burst X model has a cross-form formula for the directives, meaning that in principle it's not possible to be better than this in that case. The time for inference is very short for the two approaches, meaning that this isn't working. Meaning the bit is involved in white. But with BTSS, it's extremely difficult. First, among the 100 test sets, 21 were problematic because for most of the cases, we do not observe convergence after three days of computation. And sometimes we have convergence toward locker optimal correspondence. So, the non-convergent correspond to the red points. And in case of non-convergence, we use the median of the prior for prediction. And the black geomets that you have here correspond to the fact that the predicted value has a relative error larger than one, larger than one or one other person. And but when it works, But when it works, this two is good. In fact, it has the same accuracy as our neural networks. Again, the two architecture and the two description of the trees, they obtain similar results. And the time for inference are extremely different because the neural network, they do the prediction much less than one second. And this here are the time for prediction of about three days. What about model selection? So, here the neural network is different. It produces a classifier, meaning that it was filled with one million of tweets generated on the BG model, one million on the BDI, and one million on the VDSS. And the accuracy is height, which is for the neural network is height, 91%. The 91 percent, and for this tool, we use the deviant information criterion. It's also called the AICM by people working in these two. In fact, it works where we just use the posterior along the load posterior along the chain. And when this converges, the accuracy is exactly the same as the accuracy. Exactly the same as the accuracy of the networks. But in 24% of the cases, this does not convey as meaning, but here it does not work. I have to say that the model selection is difficult because these models are nested. For example, if you assume that epsilon is very large, meaning that the time to become functions is very short, this model. Is very short, this model becomes equivalent to that one. So it's not really possible for some curves. So, what about confidence interval? We implemented a simple and very fast solution, which is close to the parametric bootstrap. But you remember that the parametric bootstrap is the following. You estimate parameter and then you resimulate data based on the parameter values you have estimated. Values were estimated. You use that to compute your competence interval. But our goal was to be faster. And then, during simulation, the simulation that we are used to train the network, we store the true and predicted parameter values. And given the values predicted by the neural network, we search for the thousand closest simulations among the 1 million simulations that were reported. This is very fast because we do not have to compare the three. Because we do not have to compare the trees, we only have to compare the tree size, the sampling probability, and the predictions. And using this thousand pairs of predicted and true values, we have confidence in the ones. We have to do some corrections, but it's a very easy method. So the computing time is negligible. And I have to say. And I have to say, it is not perfect. When you look at the coverage and the width of this interval, if you ask for a coverage of 95%, sometimes you have 90%, sometimes you have 99%. But this is very similar to the performance of B in terms of confidence interval. And the width of the interval is also similar. Now we Now we try to credit very large trees and first we try to build a special architecture for that but we realize that very large trees means very large number of connections in your networks and huge number of training trees. And then we use now a very different very simple solution. We learn from small tree, same learning already described, and we decompose the use tree into sub-epidemics. And we have a dynamic programming approach to do that. And this composition is Optima. That's the best cover. And once this is done, we predict the sub-epidemics using the network that we have that have listed. The method that we are detailing are 24 small tree, and we can make an average simply that the what is used here or it is fine. Was it also possible to have the prediction in different parts of the tree corresponding to different times or different places in the world? Or is it so possible to use different estimates? We have a kind of skyline estimation of the value of the farm itself. The value of the parameter in time. And here are the results. So, obviously, this cannot be applied. And here you have the comparison between three predictors. Here you have the value for what I call the small tree. In fact, this corresponds to trees that we have seen before having between 200 and 500 tips. And here you have the prediction using the summary statistics, the complete binary. Complete binary the CBIV destruction, and you see there is a very clear gain in terms of accuracy. The accuracy is much better with the last script as expected, and which is very reassuring on the approach, but I have to say that for BGAI, we have been able to find a way to efficiently, very efficiently and Very accurately compute the likelihood. And we know now that this result, which is in fact very good because this means the vertical error is 3%, can be even better when using a maximum likelihood approach. This is about twice better. And the paper about calculation is already summarized. So to summarize, Yeah, the paper has been published a few months ago in Nature Communications. There is a pipeline that can be used. You do not have to train your networks. Your network has been trained for you and you can just input your data and take estimates. And we have, for example, this pipeline, we provide this kind of graph corresponding to model adequacy using principal component analysis. Here you have the Here you have the cloud of all the simulated points and here we have the point in red. What surprised me at some point, I am coming from machine learning, so this was my original background. And I had in mind that summary statistics, because they come from poor intelligence, would be always better than artificial intelligence. In fact, the deep learners, they are surprisingly. They are surprisingly smart, and the results using the combinatorial representation and the summary statistics are pretty much the same. We applied this method to a data set for David Rasmusine, IO David. And the results, they are pretty consistent with the David found five years ago. DGSS is the best model. The infectious time is about 10 years. And the fraction of superspreaders is about 7.5%. But there is some confidence there are not larger than that. And the ratu is about 9%. So the results with Zootrides are good and very fast. It's possible to use this kind of approach, for example, to survey epidemics. Because once the number of Once the neural network has been trained in zero, and another interest of the approach is that if you have a new model, there is no need to do any mathematics. You can, or to compute the likelihood, for example, you can simulate your data, train a network, and using the CPLD architecture, there is no need to design summary statistics, and you test, you can test very quickly. Test very quickly if the model has some interest or not, depending on your data and your question. So that's it. I would like to thank you and my co-authors, maybe Jakob Rosnica and Anna Zukova, the first two authors, and all of you for listening and questions. Perfect. Thank you very much. We've got a few minutes for questions. We've got a few minutes for questions. I definitely have some of my own. I wonder if anybody in the group would like to ask the first question. Anybody online? Well, Mitt, let me ask just a question or two. So it was very interesting looking at the results when you were doing the analysis of very large trees by decomposing them into sub-epidemics. them into sub-epidemics. Would that same approach work if we had, I mean, for those models that you were interested in, you were looking at, they were linear models where the sub-epidemics are essentially independent of one another, I suppose. So would that same approach work if in the presence of nonlinearity? For example, susceptible depletion or other forms? Right, I don't know, because we have tested only these three models. Okay, fair enough. Fair enough. Okay, fair enough. Fair enough. And then I wonder if you could give us some insight. You know, the neural network architectures you displayed for the two different approaches, the two different deep learning approaches. Can you give us some insight into how you chose those architectures for the neural networks? This was done by Jakub. Yakub was a PhD student. By background, he was not a computer scientist. He used He was not a computer scientist, he used a Keras library. And we first started with a pit-power neural network. This worked well with summary statistics. It worked relatively well with CPAB as well. And then he had an idea to use the convolutional neural networks because these are intended to extract patterns, and we have the feeling that sub-trees could be very informative. And immediately, the results are. And immediately the results were better than the results with Pitpar networks. And now, because I understand better all the stuff, we are trying other architecture as well. And yeah, I believe it's possible to do better. And before we had the results for the very last reason with BDEI. For the very last tweets, this BDEI is doing these sophisticated likelihood calculations. Looking at small trees, we have the feeling that we are very close to the maximum likelihood, meaning that there is no need to search for another architecture. But now, with this huge trees, I know it's possible to do even better. And this is something that we are exploring at the moment. Fantastic. Meaning that all all the the stuff about neural networks is very empirical. Neural networks is very empirical. So we tested several architecture, several numbers of hidden layers, etc. And when we had good results, we stopped. You know, the point is that it takes fine. Sure, sure. No, no, I understand. Yeah, no, that makes a great deal of sense. So I hope you'll stick around after the next talk for some discussion during the break. But perhaps we should move on to the next speaker. So if you could stop sharing your screen, Olivier, and then. Stop sharing your screen, Olivier, and then we'll ask the next speaker who is.