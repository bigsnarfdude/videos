And this is the discriminant picture there. And in fact, we even centered it around the one with 20 subreclutions. That curve is degree 32 computed by George Boole. In the parameter space, in the parameter of the coefficients of the polynomials. Or cubics. Or cubics. That's in 20. I know how to compute that. It's not totally true. So. Totally true. Sorry. Alrighty, so now I think you are now a slideshow. Oh no. A British mathematician. I think that's fine. Okay, everybody's okay if I do this? You want to try it again? It was maybe. No, I don't want to try it again. I want to try it. Okay, so today I'm going to talk about monitoring equipment. So as I mentioned, so in graduate school, I learned about numerical. So, in graduate school, I learned about numerical algebraic geometry. In undergraduate, I learned Cox-Little O'Shea Grobner bases, and I felt the power of Grosner bases at my fingertips. I could compute things all the time, and then I tried to compute things with like 500 solutions, and it broke my heart. And then I learned about numerical algebraic geometry. I could compute things with thousands of solutions, and this was great, but couldn't go too far past the thousands of solutions with just start system/target system methods. Methods. Then, you know, the software has been developing over the last decade. And in 2016, there was this great software, Macaulay 2, that was developed for finding solutions via monodromy. And at that point, I was able to compute enumerative problems with hundreds of thousands of solutions, and I was happy again. And then, as a postdoc, this also reached its limit. So I switched over to Julia, to Julia homotopy continuation. It became very easy to compute things. It became very easy to compute things in parallel. This was great. I was able to get up to millions of solutions, but hundreds of millions of solutions was no go. And the reason it wasn't possible was not because of the timing anymore, it was because of the memory issues. And so we, in fact, actually pivoted on some problems at the Max Planck Institute because we did some initial calculations and realized we can't even store these solutions. This talk is a proposed first step toward a solution to that problem. Just to get a bit of an idea, where do problems come from that have finite but confidence in them as a solution? I'll show you. I'll show you an example. Yeah. If you'd like to go higher in the Schubert calculus, and you'll find. Or find problems. Okay. Enumerative problems. You've heard this today. How many geometrics, geometric objects do something to some fixed geometric objects? Something to some fixed geometric objects. There's one line through two points in the plane. Your geometric objects don't really have to be that geometric. They can just be complex numbers. There are two complex numbers that solve this equation. It can be as direct as that. That will be a running example. It's also my reminder to tell you that I'm always working with complex numbers. There are 27 lines on a cubic, as we know. There are 3264 conics, tangent to 5 conics in the plane. And if you soup this question, And if you soup this question up to three-dimensional space, you will find there is 666 million about quadrics tangent to nine quadrics in three-dimensional space. And this was exactly the question we came up with. People know the answer to this? Schubert knew the answer to this. Well, it was proven finally by Loxoff and Klein in a famous paper in the 70s. Okay, so let's just do a xanity check here. Podric surface, we're not going to do anything super clever. I'm just going to write it in terms of its coefficients. We have 10 complex coefficients. We need to store a solution instance to this problem. So I need to write down the parameters. I need to write down all the solutions. Nine parameters, that many solutions. And you may choose to represent complex numbers in various different ways. Row of Different ways. Row of block is the cost in bits to store block. For example, okay. So, what are we going to do? Great. So, storing 10 complex numbers is 10 times storing one complex number. And storing a complex number is really just storing two reals for the real and imaginary part of a complex number. Which we're talking about quadrics, so if you represent each quadratic, then all. Yeah, it's a vector of 10 complex numbers. Oh, I see. Okay. 10 complex numbers. Oh, I see. Okay. Yes, exactly. Yeah, so each of these, yeah, so this C10 is the parameter quadrics, this C10 is the solution labs. Absolutely. Great, so we get this, and using 64-bit floats for real numbers, this is 99 gigabits. Okay, and so this is on the order of the dimension of my space times the number of solutions I have. Today I'll give a representation which reduces. Today, I'll give a representation which reduces this to 14 kilobytes. So that's like a factor of 7 million. But it's arguable to what extent I've really solved the system, and we'll discuss the subtleties there. In general, this can be brought down to O and log. Okay, so whenever we're talking about solving things, I think we should be really critical about what we actually want. Want. You don't want to see 666 million vectors of complex numbers. Almost certainly you don't want this. But that's, I mean, the most naive thing you're looking for, maybe floating point approximations. Sometimes you do want this. You want to do some problem in engineering. Maybe there's only like eight solutions. You really want to see them all. Maybe you want to draw them, something like this. Okay. But a lot of times you don't. Maybe you want to count the solutions because you don't have a degree bound coming from use. Coming from use machinery or anything like that. Maybe you're Frank, you want to determine how many solutions can be real or positive or some other function of the solution set. Maybe you're Thomas, you want to compute the Galois group. Maybe you're doing some optimization problem. So you compute critical points. You only want the best critical points. You don't even care about the other millions of solutions. Or maybe one of those solutions is really interesting and you want to share this one with Share this one with a collaborator over here. And so the idea behind monodroming coordinates is that we're not going to encode the affine coordinates of solutions. We're going to encode how they were found. Okay, so essentially we're going to give a roadmap on how to get to all the solutions. This is my monodromy slide. This is solving a quadratic equation in one variable. This is nice because I can draw the real This is nice because I can draw the real parameter space and the discriminant that we know so well. It's p1 squared minus 4p2, and that's the parabola here. Now, above any generic parameters off the discriminant, I can solve the problem. Here's a picture of the complex plane, and here are two real solutions. Of course, if we're on the discriminant, we're going to get a solution with multiplicity 2. And indeed, if we try to move through these chambers, the real solutions are coming together. The real solutions are coming together and lifting apart as complex conjugates. Because this is a real picture. This discriminant is a real codimension one thing and a real dimension two thing, but over complex numbers, the discriminant does not disconnect the space. And you can do real paths, which avoid discriminant, so you can do monitoring. This dotted line, you're supposed to imagine as going down into some complex dimension. Dimension to get to here. And so, what happens is these two real solutions, as we go down, don't have to be in complex conjugate pairs anymore because we have complex coefficients. And we can move around, come over here, and then we can go above this little picture, and we can come back. And if you follow your favorite one of these two solutions, you'll see that it doesn't end up where it began. And so that's monochrome, as we've seen. This is maybe a coarser. This is maybe a coarser cartoon. And this is the coarsest cartoon that I'm going to be drawing today. Some parameter down here, some loop gamma downstairs. The fiber will be omega u, where u is the parameter. And the permutation that gets induced is sigma. That schema clear. Okay, and if we look at all the permutations we can cook up this way of the fiber over E. We can cook up this way of the fiber over u by taking loops based at u. I'm going to get some monodroming group, which right now I'm just going to call g sub u. This is well defined, but we don't need that for today. So that's just the monodromy group of the problem phase 8. So what is monodromy solving? Monodromy solving is the game you play if you forgot the quadratic equation, but you happen to know one of the solutions. But you happen to know one of the solutions. Okay, so if you know one of these solutions, let's say on this back page, you know this one, and you did a loop like this, you would actually discover the other one, thanks to the permutation. So this is a naive version. Here's the input: you need some enumerated problems, some base parameter, a solution to the system that you do know, and a stopping criterion. For now, let's just assume we know how many solutions there are. Let's just assume we know how many solutions there are and we'll stop when we get there. But there are other stopping criteria that I'll mention if I talk. The output will be numerical approximations for all your solutions. And here's the pseudocode for it. You have one of the solutions, so throw that in a bucket, call it big S. And then as long as you don't have all the solutions, start drawing loops, performing homotopy continuation along those loops. Apply to your start solution only for now. Your start solution only for now. And then you'll be able to see new solutions potentially. Might go to the same solution, but it might hurt. And if it's new, you can append that to the solution set and read. And then this is going to discover the solutions in some order. It's not going to be the same order every time, but maybe you fix a random seed. I'm going to call this the monodromy order. It's the order in which you find your solutions via monodromy. Later, we'll A monodromy. Later, we'll see a deterministic algorithm here. Okay, so this is the picture. A running example today will be a situation where we have 12 solutions like this. On a computer, we're just going to do a triangular path with two randomly chosen parameter values. And we'll shoot the seed around. Oh, we came back to the same one. That's fine. That happens. Maybe we try another one. Oh, and we got a new one. Great. On, we got a new one. How do you make sure that you will correct the discriminant? Did you get a non-trivial one? Yeah, so you don't. So the philosophy is that discriminants are complicated. So I showed a picture of this discriminant for the 27 lines. The way I often describe it is take a bowl of spaghetti and just dump it on the ground. Especially when you're looking at problems with millions of solutions, your branch points are going to have a lot more than the degree you expect. That's actually very accurate because. That's actually very accurate because typically these discriminants are the image of like a vector bundle over a smooth variety. And so you throw your spadium up in the air, it looks beautiful. So and remember, like this is very much an engineering problem in many ways, too. Because a lot of times we don't have the degree bound and things like this. Numerical methods have issues. So you just really walk around look and you create. And it works. And it works. You run this, and you have like a theory saying you get five million blah blah blah blah blah solutions, and you get that. So it's really no joke. This isn't a heuristic. These discriminatory local are very complicated, so a relatively naive loop will enclose several branches of it. That's right now. I mean, that's, I mean, it's just like it's sort of engineering probabilistic, but it's observationally that works. We're going to assume this all the way on X. All your issues will be just assumed. All your issues will be just assumed today. Popular stopping criterion, I already mentioned, you can stop when the number of solutions have been found. You can stop when you stop making progress. You've just done this a billion times, you haven't found any. Maybe that's the right answer. And then there are also things called trace tests. So if you have a particular type of enumerative problem, you can actually do a numerical verification step. It's not a certification step, but it is a verification step to check that you have found all the solutions. That you have found all the solutions. Okay. So, do you use some kind of heuristic when you're constructing these triangles? So, depending on the degree of the problem, you want to make smaller triangles or larger triangles? Or just you don't care? No. It seems to not matter that. So, you just randomly construct two points. Wow. How about the trace tests? Cheap. Cheap. The hard thing about the trace tests is being in a situation where you can use it. thing about the trace test is being in a situation where you can use one. The other hard thing about doing a trace test is checking if a floating point number is zero. That's why it's a heuristic and not a header. Exactly. Because if I were to just choose my points in a bowl of radius 10 to the power minus 23 or something like that, oh yeah, you're not going to get anything here, for sure. Yes. There's something. Yeah, so I think in Julia they're just doing like a normal distribution for each. Doing like a normal distribution for each of your parameters centered at zero with variance one. And this just keeps working. You know, like, I mean, people would look into this more if it didn't work. But it's fantastic. But now the monodromy assumptions, right? Because these things are hard. You can't talk about what a general enumerative problem looks like, but you still want to analyze these outputs. You still want to get an idea of what the complexity is in terms of time, where time is measured by paths in terms. Time is measured by paths in terms. That's a common measurement of complexity in numerical algebraic geometry. The first monodromy assumption is that the monodromy group is the full symmetric group. So Frank showed us some Schubert problems, and he was very excited when he had interesting Galois groups, and that's because most of them aren't interesting. Almost all of them aren't. There are some rigorous theorems about this. For example, if you take a linear section of an irreducible variety and you move the section around, And you move the section around, that's going to be full symmetric. So that's a really common situation. This is actually, it seems like a really strong assumption, it's like the least of the lies I'm about to assume. Okay, this one really happens often. This one, not so much. It's uniform sampling, that there is some way to produce elements of your monodromy group uniformly at vanity. It's a finite set, so there's a uniform distribution. The uniform distribution. And I'm just going to say that by choosing these random triangles in whatever point I'm doing, I'm inducing the uniform distribution. This is a lie. It's not that bad in practice. And there are some probabilistic theorems where as long as every permutation has some positive probability, then you can compose them and it'll mix and it'll become the uniform distribution, but nobody does this in practice. We're going to assume, though. In practice. We're going to assume, though, for today that we're sampling Unifair formally from the symmetric group. And then the last one is that homotopy continuation works. Okay, and it pretty much works. But if you're unsure or you're skeptical about it, there are certified homotopy continuation techniques. It'll just cost you. Is it implemented in homotopy continuation Jubilee? No, only after the fact certification of the solutions is currently. Solutions is currently implemented, and so you can't actually certify the monodromy correspondence. You might have a mutual path jump. I mean, we could several uniformly the questions, do you want to, right? If you want to have like a base of your full symmetry group, you could literally just take a random element there, ask them, okay, no, it's pressed sometimes in my generators, and you just run on the bed. Yeah, so for me, this evaluation over a call is homotopic continuation over a loop. Continuation over a loop. So I could ask GAP for a random element of the symmetric group. But then I would also require that GAP produce a loop which represents that permutation. And that's a no-go unless you want to get your hands on the discriminant, which you don't. Can't you just express that permutation in terms of your generators of your group and just one those? If I had a monodromy basis, I could, but I don't have a monodromy basis for problems of this size. Never mind. Yes. So, with, yeah, but you do eventually find enough loops that generate a symmetric group, and then you do have a basis. Yeah, you can do it usages. Yeah, but then I would already have all the solutions. But this is product of this. Yeah, so just for now, these are the assumptions we're going to make. And the great thing about this is now we've landed all of our analysis into the realm of statistics on this module. Of statistics on this module, probability on this module. For example, if we take the seed solution, we keep on shooting it around random loops, so we're getting a random permutation uniformly. It's the same thing as the coupon collector problem. There's like 100 baseball cards, and you get a random baseball card whenever you buy one. And the question is, how long is it going to take until you get a baseball card? It's exactly that problem. And so, this is this problem in probability in this metric. The symmetric expected time complexity is O of D log D. So here are three results about probability and symmetric group that I'd like you to either recall or learn about. So the first one is that the expected number of I cycles in a random permutation is 1 over I. In particular, the expected number of cycles in sigma being a random permutation. Being a random permutation is the dth harmonica. The next one is really fantastic. It's kind of what makes this monodromy story work for solving. The probability that two random permutations generate a transitive group, in other words, I choose two permutations and I can find all my solutions, approaches one as d goes to infinity. And it approaches it at this rate. And if you'd like a particular value, p1000 is over 99. P1000 is over 99.89% chance, and we're looking at millions. Two is enough, per se. Huh? Two is enough. Two is enough. We could call one, zero, and one, one. And then finally, this is a new one I learned. So if you take the expected largest cycle in a random permutation in the symmetric group, it is asymptotically proportional to D, and that number. And uh uh that number is about zero point six two four three ball well it's called the goal and getting a constant so that I'll show up in a moment okay so before uh so as soon as I described this monodrome solving algorithm you've probably already started thinking about ways to improve this like why would I only shoot the seed around why don't I use the same permutation over and over again assuming that we have two loops which generate a transitive subgroup here is a deterministic Subgroup, here is a deterministic algorithm for finding all the solutions. It's easier to walk through the algorithm than look at the lines, but for that, they're there. Here are 12 solutions, and here are two permutations. The first one is written in cycle notation across the middle, and the second one is endowed with arrows either on the top or the bottom. So the probability you gave is that if you pick two permutations. To permutations at random, then the subgroup that they generate is transitive. But that's not the same as if you have some underlying transitive group. I guess you always have SN. But is it not necessarily the probability of the two componentations that you can generate the full Sn. Correct? That's correct. Yeah, that's a much harder problem. A quarter of the time you'll be in the alternating groups, though. That's it. You pick two evens. And, okay, obviously. And I think that's a little bit different. Alright, so let's run through this. Hey, pretend you don't know anything until we say we know it. We started with S1. We have two permutations. I'm going to greedily use the first one. That's my really important permutation, sigma naught. I'm going to greedily use that until I know I'm not going to find anything new. Then I'm going to use sigma 1 when I have to, but I'm really going to prefer sigma naught. S1, okay, apply sigma naught. S2, great. Keep doing it until we get back to S1. Okay, I found S2. Until we get back to S1. Okay, I found S3 now, now I'm at S1. Now, any application of sigma naught is not going to earn any new things. So we're going to shoot S1 over by sigma 1. Get over to S3. Ah, but we already have that one, because we're keeping track of all of them in a bucket. So this is not going to find me a new cycle. Maybe this one will. So this is not the candidate finder of a new cycle. And indeed, it's new. Great. So now I'm going to apply. It's new. Great. So now I'm going to apply sigma naught again. Okay, I figure out this is a one cycle. Time to find a new cycle of sigma naught. I'll shoot S3. I'll find this cycle. I'll find everything in it. And then I keep on going this way until I find everything. Okay, so I'm greedily essentially working in the Cayley graph generated by these two things, preferring sigma dot over sigma one. And you're doing search for like a tree search too. A like a tree search, too. Great, yes, absolutely. I don't remember what the adjectives are, but it's like you're following a tree somewhere. Yes, so we're going to keep track. So, I'm going to introduce some definitions. So, the founders are the indices which find new cycles of sigma naught. The initial cycle representatives are the first elements found in each cycle of sigma naught. The last founder, so this gives us 9 times the last cycle, will be J star. We have cycle lengths. We have cycle lengths for sigma naught, and the number of cycles in sigma naught is five. But you have to keep checking whether you already knew a solution. So when you have 600 million, you have to know those 600 million? Yeah, yeah, you keep them in a bucket. 600 million squared. Yeah. Okay. Yeah, so I'm not keeping track of comparisons here. Okay. Okay. So yeah, so you're going to need to find every solution via sigma naught, except for the last one, you're not going to have to do sigma, or sorry, except for the first one, you don't need to find. And then J star times, you're using sigma 1. So this is how many path tracking procedures you need to do, or Oracle evaluations of the perturbations. And you need to be able to kind of go to the next solution that you found along the way to be. Along the way, to be your new candidate finder of a new cycle, and you also need to be able to determine whether or not something's failed. Those are the two kinds of subroutines that are important. Exactly. So the first idea for this project was: great, so we only need two permutation, call 1, 0, 1, 1, and now it can represent every element as a bit string. Element as a bit string. That seems like a good data representation. But these strings get really long, about as long as the longest permutation, because I'm going to get that many zeros at some point. So about 60% of the size of my solution set. And there's a ton of redundants in here. So as Frank already pointed out, you really want to represent this in a tree. Okay, so we'll call that a monodromy tree. So this is our root. And as we apply sigma naught, And as we apply sigma naught, we find new solutions. And then as we apply sigma 1, we find new cycles. And the founder indices are marked in green. And so the idea is to encode this tree in some way so that we can transverse it. And in the paper, which is And in the paper, which has been accepted at the ICMS conference next month, I did five ways to encode a monodromy tree. So, first of all, all the information is there as soon as you have oracles for the permutations and the CT. So, in some sense, that's a representation. And you can unpack it yourself using this monodrome software algorithm. It's not much help, but it's there. And for a lot of problems, having one solution is actually nice. If you've used homotopic. Nice. If you've used homotopic continuation before and tried to do monodromy solving, sometimes it complains that it can't find a start solution. And so it's not nothing to give this, for example, in a database. The next one is you can kept on having to check whether or not every index was a founder. If we keep track of those, we can skip a bunch of steps. Similarly, at the end of each of our cycles, we had to do a waste going back to the beginning to know that it was a complete cycle. Was a complete cycle, and so we could keep track of just the cycle sizes, and that would save space as well. And then finally, the one that kind of just made things the simplest was let's just keep track of an initial element in each cycle, if we want. And then interpolating between the usual monodrome we solve algorithm is you can store every alpha solution you find. So if alpha is one, you're just storing every So, if alpha is one, you're just storing every single tree. And then you're doing nothing but the classical situation. The initial cycle presentation was the 0, 1 thing that you talked about before, or what are the initial cycle presentations? This solution in its affine coordinates, this solution in its affine coordinates, this solution, this solution in that solid. So, here's type 1. I would just store the seed solution in blue in my picture. In blue, in my picture. Type 2, I'm also keeping track of these founders. Type 3, I'm also keeping track of the sizes. Type 4, initial elements of every cycle. Type 5, with alpha equals to every other element. So here, pick up one more. Yeah. So what if your monogamy is not generated by two circles? Yeah, this doesn't work. The assumption is that you have. Yeah, the the assumption is that you have permutations which uh yeah. Uh no, but you don't even need to generate the monoderm, you just need to generate transitism. But yeah, if you don't have that, you're out of luck. It's, I mean, you can adapt it, right? You can have instead of a binary tree, it's a ternary tree. But yeah, for cases that we're considering, this probability is pretty high. Okay, but this is, yeah, again, so this. Okay, but this is, yeah, again, so this doesn't solve the problem at all. You still have to solve the system and then compress it using one of these representations. So we haven't solved any of my problems at the beginning. Here is kind of how much it costs to store these several different types of encodings of the monodromy tree. And the time cost for unpacking them is still all linear and d, short of alpha being one, in which case it's trivial because you have all the solutions. So this is not so exciting. So, this is not so exciting so far that you can do this. So, looking at these, they're the same. Yeah, yeah, exactly. So, the first one is this d plus j star minus one. Here, we don't have to check every founder, but just the number of cycles, one founder for each cycle. Here, we already have initial elements, and so you can see exactly how many it takes. Um, but nonetheless, okay, fine, we go from 100 gigs to 14 kilobytes. Uh Kilobytes, and so the takeaway here is that if you're only storing every alpha, instead of doing n times d bits to store asymptotically, you're cutting that down by a factor of alpha. But importantly, this actually does give you an iterator for the solution set. So I could write down this 14 kilobytes and then talk to Frank about the thousandth solution. And the reason you can do that is because there's some next function, right? So if somebody gives me a monondromy tree, So, if somebody gives me a monodromy tree for the sake of simplicity, just type 4 here. I want, and I know that the 50th solution is this coordinate. How do I get the 51st? Well, all I got to check is whether or not it's the last thing in a cycle, in which case I need the next initial cycle representative, or I just apply something up. It's not that bad. So I have an iterative. And importantly, if I'm halfway through this process, Through this process, as long as I keep track of this data type along the way, I can check if something's new. Okay? So let's say I have just completed this cycle, S5, S6, 6, S7, S8. So if I just completed that, and the last thing that found a new cycle is S3, so my next step would be to try to use S4 to find a new solution. So let's apply sigma 1 to that. Now we'll go back. To that. Now we'll go back to S1. Well, I'll recognize that's new, not new, because I have it. Okay, lucky. What about if we moved over to S5 then? So S4 didn't find a new cycle. What about S5? This finds S2, but I'm not storing S2. So it's a priori not clear whether or not I found this one or not. But I know the cycle sizes of everything I've found so far. So I'll just throw it around sigma naught that many times and see if I ever hit the initial cycle size. See if I ever hit the initial cycle summation. And that is the main cost of producing this representation this way. It costs more to check if something's new because you're not keeping everything in a bucket. It requires at most, well, okay, if you're keeping every third solution, you only have to do this at most three times. But if you're not doing that, your worst case scenario is the largest cycle in your permutation. Can you remind us again what was the expected size of the cycle, the average? Size of the cycle, the average size of the cycle? Asymptotically, the expected size is about 0.6 times d, times d e. For 600 million, you expect the whole star. Yeah. So I wouldn't advise using this, actually. I would advise choosing alpha so it's sufficiently small. And what is sufficiently small in this problem? So that it's not. So if my largest cycle is 50. If my largest cycle is 50 and alpha is 100, it doesn't do anything. Sufficiently small is under my largest cycle. Yeah, so for the problem size of 600 million, you would choose 17 or would you choose 1,000? I would probably choose 10. That will cut down my memory usage by a factor of 10. I'm going from 100 gigs to 10 gigs. Yeah, I'd say it depends how much memory you have, right? You choose your computer, basically. How much memory do you want to use? Exactly. So that's what we're going to do. So that's what we're going to do. And because S1 through S12, it's too easy for our brains to remember what comes next. We're going to do this with fruit approaches. Okay, so let's see how your memory is. Okay, if you want to test your memory, just cover up the top part. That's to check that I'm not lying to you. We're going to do a few steps of this procedure so you can feel what it's like to be the computer, not remembering if you found that fruit before. So this is your data type that you're keeping track of. Apple is your You're keeping track of. Apple is your potential founder of a new cycle right now. The cycle, the point solution you're at is Apple. Your current cycle size is one. And because we want to count how many real solutions or how many foods are junk food, we're going to keep a tally of this. Since we know every time we find a new solution, we can actually accumulate this along the way. Okay, so we shoot apple along our first permutation. We got cookie. We're still not at apple, so we know it's new. That's definitely a junk food. That's definitely a junk food. We go again, it's banana. That one's kind of good for you, I guess. I'm not the best person to ask about this stuff, actually. And then we find apple again. Okay. So now we know that the first thing had size 3. And yeah, and now it's time to use Sigma 1. So we greedily applied Sigma 0 there. Now it's time to find a new cycle. Apple was our founder. As soon as we shot over via Sigma 1, we got banana. Have we seen banana before? Have we seen banana before? I don't know. Let's shoot it around at most three times because that's the largest cycle I found. So, is banana a cycle initial? No, no, because we found the apple. So, it was in that cycle. All right, that means I'm going to have to check cookie, which means I need to use the next function on apple. Next function gives me cookie. Now I can shoot cookie along sigma one. I get coffee. Is coffee an initial cycle? Cycle. No, I found coffee again. It's a one cycle. Right now, I just know that it's not new, I guess, and so I have to actually shoot it around. It's not the optimum algorithm in the last slide, but it'll give you the asymptotics. And then, yeah, you keep on going like this. Is Apple a new one? Well, okay, we get turkey leg, pizza, and that pizza. Pizza, and pizza. I raised junk food companies with that kind of pizza. Ah, yeah, yeah, but this is not the step where we have found new things. This is just the step where we determined if it was new. I see. So I'm keeping this as simple as possible. You can optimize this by using this information along the way, but I think it's going to be a lot more complicated. So now all we know is that apple is a new initial cycle. Now let's move through that and find them. Turkey leg, pizza, yeah, that's junk food. I agree. That's junk food, I agree. Candy, that's junk food, and et cetera. And I took a while copy and pasting these fruit emojis, so I got exhausted. This is the end. Five junk foods, that's the data type you get at the end. Keeping in mind that the founders are indices, whereas these are actually coordinates. Okay, so we have the usual solving complexity, and then the cost of determining if something's new, which we have to do. Something's new, which we have to do J star times as many as like the biggest cycle size at worst, or alpha. And then there's this incrementing cost, which is pretty negligible. It comes on the order of D times the minimum of the golden digman by D and alpha. So as long as you're choosing an alpha sufficiently small, what you've done is you've traded off a factor of alpha in the time space complexity of this algorithm. Time space complexity of this algorithm. You can save, you can use half the memory if you're willing to wait twice as long. You can use a third of the memory if you're willing to wait three times as long. That's the story here. So space is technically not such a hard obstacle anymore. So what would you do? Exactly as David suggested. You know how much memory is available. You have an idea, hopefully, of the size of the problem. Hopefully, of the size of the problem. So you know how much memory you need to save. Then you go up and you know how many, what to set alpha to, essentially. From the get-go, I need to know an estimate of the size of the problem. That would be good if you can choose the right alpha, otherwise it might run out of memory. You just start out with memorizing every second one, and then if you run out of memory, you just drop every fourth one. I imagine there are lots of ways. I imagine there are lots of ways to boost up this algorithm. As I mentioned, this is just like a first step to just theoretically say, look, you can solve the 600. One thing is, do you care if your solutions are in the quadratic convergence basins of the two solutions? Because when you're comparing them, you want to make sure that they're really true rather than close. Our monodromy assumptions say that I can evaluate these permutations exactly. That's your assumption. That's your assumption. Yeah. So I'm assuming that homotopy continuation is perfect. Or rather, you point out that's a different problem. It is a different problem. Yeah. So challenging the monodromy assumptions and trying to weaken them is a, that's one line of my research. This kind of includes on Smale's answer to how do you represent the solutions to the polynomial system. He says the answer is you give an approximation that lies in the quadratic convergence. That lies in the quadratic convergence basin to the true solution. And now we're improving on that by giving the first solution, the quadratic convergence basin, plus the monodility data. That's a huge memory saver. Interesting take-house. Yeah? Okay, thanks. Thanks for coming. Yeah. When do you run time? 15, okay. I can talk a little bit about trace tests. So, this is the classical trace test on the left. Classical trace tests on the left. I think it's one of my favorite results, I think. What you see here on the left is a quintic plane curve. And you're actually also seeing an enumerative problem. The enumerative problem has parameters which are lines, and the solutions are the intersections of those lines and this fixed quintue. So it's got degree five. And as I'm doing monitoring, I'm finding: okay, one, I found two, I found two. I'm finding, okay, one, I found two, I found three, I found four, I found five. I'm not finding anything more. Maybe I don't know the curve as degree five and I want to know whether or not I found all the solutions. What you do is something called a trace test. You take the coordinate-wise sum of all of your solutions, or average, if you want to draw it on a picture. And so there's like the center of mass of these five pink dots. And then you start to move your linear space in a parallel fashion. And And you continue to compute the coordinate-wise average or the coordinate-wise sum. And if you have found all the solutions, these averages will lie on Allah. And more importantly, if you have not found all the solutions, they will not lie on Allah. Does it allow you to locate? To locate a what? To the next. Oh, oh, yeah, because I'm only missing one. Sure, yes. But what if I was missing two? Maybe it'd be harder. But yeah. Yeah. You'd locate their setup. Okay, so I can just choose three slices like this. And generically, I'm going to witness linearity or nonlinearity correctly. But keep in mind, so the coordinate y sum is something I can accumulate. So I can accumulate this during my algorithm. Accumulate this during my algorithm. I can accumulate this sum, this sum, and this sum, and ask that the sum of these two is twice the sum of this one. That's what linear is. So I can keep track of this trace test along the way. I don't have to do it at the end every time I want to check whether or not I found anything. I just keep accumulating this, kind of put like a little midpoint in my parameter space, and accumulate it along the way. It's only going to cost one. And accumulate it along the way. It's only going to cost one more float. And so you can put the trace test in. So you really can use this to get a verified degree count, albeit not certified. So I guess here's now where it pays off that you always use this one sigma one the entire time. So you're. No, I didn't hear. So here's where it pays off that you only preferably use sigma one. Is that it? Sigma naught. Yeah. Of one, is that it? Similar naught. Similar naught, yeah. Because you because it is always the same. Yeah. Okay. What are your linear sections here in your polynomial system? What is the linear section? Yeah, okay. So the way numerical algebraic geometers represent positive dimensional solution sets is by slicing with enough hyperplanes until it's zero-dimensional. Because homotopic continuation works on zero-dimensional things. So, such an intersection is called a witness set. Set, okay, something like that. And so numerical algebraic geometers want to compute these things. And so my parameter space is... But I thought you started with a zero-dimensional set begin with. Where all models is the industry. Yeah, yeah, yeah. At the very beginning of the story, that's what was required. Now, what I'm saying is if somebody had a positive-dimensional solution set and you wanted to compute a witness set, then you are in this situation at the beginning of the slides. You have a zero-dimensional parameter. You have a zero-dimensional parameterized polynomial system. In particular, you have a very special one because here it is guaranteed that the monodromy group is a full-symmetric group. So you don't worry about at least one of your monodromy assumptions. And you also have a nice stopping criterion via the trace test. So, in my abstract, I promised some comment about certification. This is not fully. So, do you have to run something three times? Do you have to run something three times? I mean, here you have three slices, right? So does that multiply your run time by a factor? Three? Because, in order to. It depends on how you're measuring time, right? So in some, so not every path is equally hard to track over. So that's something I've dodged throughout this whole thing. And also, three paths in a row can be parameterized as one path. So the time complexity that I've written down. Time complexity that I've written down is kind of shaky to start with. But you're prime, you're going to go through your middle point. That's right. You can always make sure you stop there. If I'm tracking from here to here, I just have to stop halfway through my homotopic continuation. So that won't cost extra. But everybody should feel a little bit funny about maybe this being the measurement of time complexity. But unless you want to do some really serious analysis of condition numbers and stuff. Serious analysis of condition numbers and stuff, this is going to be how you get a first step at these compressing instance. Yeah. Why does this center of mass lie on a line? Because when you think of a bivariate polynomial in x and y, then the center of mass should just be one of the coefficients in y. Yeah, so if you want to convince yourself that this is true for plane curves, you should parametrize this family of lines. Of lines. And you should evaluate it at the implicit equation for the curve. And then you should remind yourself that the second to highest coefficient of a univariate polynomial is linear in the roots. And that is the proof of one direction. And the proof of the other direction is that the monodromic group is the whole symmetric group. So Thomas mentioned that Galois groups are very important for trace tests because you do not get the converse unless you have a lot of. Unless you have a full symmetric group. Or at least nobody, I think, has a proof without that fact. Yeah, it's an enjoyable exercise, and it doesn't take that long. We can talk after. Okay, I promised something about certification. This hasn't been fully fleshed out yet. But essentially, the idea is: so, I'll tell you what certification is very quickly. Certification is very quickly. When we're solving a polynomial system, here's a univariate cubic. There are true solutions here in blue, and the ones that we find in red, in reality, these points are right on top of each other visually, but for the sake of explaining this. And what we do is we construct an interval box around our approximate solution, and then we perform Newton's method on that interval box and bound the image of that by another interval. The image of that by another interval box, and we check whether or not the image lies inside the original. If that's the case, Newton's method is a contraction restricted to that box. And so therefore, by Bonach's fixed point theorem, there's a unique fixed point. And fixed points of Newton's methods are the ones. So you have a proof that there's a unique root in that box. That is a proof. That is a proof. It's only one-sided. If I do that for all of my approximate solutions, I haven't proven that there aren't more that I failed to find. One-sided, meaning you can be close to a root and yet not have a contraction. That's not what I meant, but that is true. What I meant was: if this is the picture I'm doing on my computer, if there is a natural solution over here, I don't know. So I'm only going to get a lower bound than the number of. Going to get a lower bound than the number of solutions to my system. The other issue you got to keep track of is like: if these boxes overlap and intersect, then how do you know that the fixed point isn't just in the intersection? You have two boxes, but maybe they correspond to the same solution. So you have to make sure these boxes are distinct. And this is, yeah, sorry, distract, absolutely. And this is the reason why the implementations require you to have all the solutions in memory at the same time. Require you to have all the solutions in memory at the same time. So you can make sure they're all district. A very easy way to simplify this is to just chop up your solution space. So you choose some random subsample, you get an idea of where the center of mass of your solutions are, you throw all the coordinate hyperplanes centered there, and now you have a bunch of different regions. And now you have a monodroming tree, so you can iterate through. So let's just pull everything in the positive quadrant there. Positive quadrant there. And we'll make sure those are distinct. And those are, of course, going to be distinct from everything in another quadrant. And so this is a way to essentially parallelize the certification procedure. All you have to do is bust it up into chunks first, which are destroyed. So, in summary, homotopic continuation is fast enough, or at least it's getting fast enough. Maybe I'm a little bit preemptive on this need for a memory. On this need for a memory time trade-off. Andrew Semesi gave a talk in 2007 saying that the problem now is storing the solutions, not computing them. Oh wow. Okay. Okay, great. So yeah, so definitely over 10 years. For over 10 years, it's been a real problem. Okay. Which is over 10. You don't want the solutions anyway, so stop asking. You don't even want. You don't even want them. What you want to know is information about it that you can almost always accumulate along the way. Monitoring coordinates is a new way to do this. It tells you how to find the solutions. They can be used for compression, so they're going to be great in, for example, a database of a numeric problems. They can be used as iterators for solution sets. They can be computed under memory restrictions too. It's not just you have to have this big memory to compute it and then you can compress it. You can actually create these under memory restrictions. You can actually create these under memory restrictions. You save a factor of alpha in memory, but it induces a factor of alpha in time complexity, thanks to having to recognize whether or not there's some solutions. And they can be used very easily with trace tests and allegedly certificate children. So that's my talk. Thank you very much for your attention. Any questions?