I would like to thank the organizers for the invitation. So far, it's been a nice list of talks. It's been hard to follow them all because I'm kind of just moved to Rio and also my wife is pregnant. So we sort of moved to a new apartment now. So I'm trying my best to. So, I'm trying my best to follow them all. And so, in this talk, I'm going to just give a bunch of examples of problems that are sort of a surprising to me that start learning about them in my PhD and I still work on them. It's basically a sort of a list of problems that I like. There are others out there. They are solved via some magic. Via some magic function construction. And so we will see problems ranging from geometry to number theory, and etc. Okay, so let's see. So can you see my slides? Yeah. So the generic version is this one. You sort of have like a symmetric convex body. It doesn't need to be symmetric, but sort of I have a Be symmetric, but sort of we have a symmetric convex body, and you have functions which are supported in this set. Okay, the Fourier transform is supported in this set, and this is the Fourier transform I'm going to use. Yes, sorry. Okay, back. Sorry. And sort of you have a functional that you want to minimize. You want to minimize among these functions. And so, this function is not usually linear. It could be like minimizing some LP norm. It could be some other complicated things. And so it's nice when it's linear. But usually the space you're trying to find out is some sort of a convex subset of your space. So that's that's the setup. Let's see a very simple example. For the first example, is this one? You have, say, a function f which is supported in a certain interval and which lies above a certain prescribed function. And in this case, it's the Gaussian. Okay. And so, and then you want to minimize its mass. Okay. But it can't be. Okay, but it can't be zero, but you have a trivial lower bound just by replacing by this Gaussian. But then you sort of ask that question. And such functions are very useful in number theory. For instance, in applications in bounding the zeta function or bounding zeros of the zeta function. And these sort of things in higher dimensions also can use. In higher dimensions, also can use to study this energy problems when you have the Gaussian potential, and etc. And the nice idea here is that if you know how to solve this problem for every lambda, okay, then you might be able to solve this problem for another function here. You'd like to replace this by. Function here. If you'd like to replace this by something else, say some function G. Well, if you can write that function g as sort of an integral in that parameter against a measure, then you could just integrate your optimal candidate, say, and get something. So that's the sort of a thing that shows up here. And that's actually what you have to use. And for instance, for applications, and so this will be like the completely monotone functions. And for applications, you will like to cover. You would like to cover to bound to get a function g, which is sort of like a power, like maybe an exponential, maybe a log. And is that last function here was used in a very nice paper of Sandra Rajan and Shandi to actually give explicit bounds for the zeta function. And currently, this is the best asymptotic bound known for the zeta function on the critical line. On the critical line, assuming RH. Of course, if you don't assume RH, then we only have very crude bounds, which are power light, and so you won't get this. And I think the best work there is quite well done. And so assuming our age, this is the best one. And sort of this constant here, this log two over two, is the number that comes from minimizing this function over here. Okay. This function over here. Okay. And so the strategy here is not to give actually what you have to do in all these sorts of problems to find this minimization problem embedded in the thing you want to study. It's more like giving a survey of the problem. So I won't get into details, okay, very often. And sort of the archetype inequality here is the following. The following. You have some sort of a sum, a weighted sum, say, of some function g that you want to estimate. You can also think of the sum as an integral, say, a continuous linear combination. And you want to bound this from above. That's the quantity you want to bound. But you don't know how to do that because that function g is kind of complicated. So, what you do is like you just create some sort of a filter. You put a function that is above it. Put a function that is above it and its band limit. And when you get to here, you sort of can apply some generalized Poisson summation argument to get to the frequency space plus some error. Sometimes the error is exactly zero, sometimes you have to estimate it, but in any case, and so, and on this right-hand side here, it's easy to estimate. So that's the sort of inequality thing you have here. And so for instance, a very typical example is say you have some function. Let's see. Yeah, so suppose you have like a set of points. You have like a set of points, a set of nodes, okay, um, on the line, say they are separated by delta, and you have this quadratic form right here, okay, that you sort of want to estimate its largest eigenvalue. So you want to like bound this quadratic form from above, so then you apply the archetype inequality that I just mentioned. You write this as an integral, so it would be integral of a Gaussian times some square of a trigonometric polynomial. Square of a trigonometric polynomial. And then you just replace this by a function that lies above. And then you open everything again. So you end up in here. And now, but this function is band limited and it's supported, say, this Fourier transform is supported in the interval, say, minus delta delta. So whenever you have two different points, this is zero. So you only have diagonal terms. So there you go. So there you go, a very simple way of estimating this quadratic form. And if you minimize this thing, you actually get the largest eigenvalue asymptotically. That is, if you send, if you allow, you have as many variables as you want and send, say, the number of psi n to infinity, the number you get here from this minimization problem is actually the largest, say, an estimate for the largest at the end. This is the sort of thing that sets up. Any questions so far? Okay. This is just repeating. And now you'll give some other examples of this sort of a problem. For instance, for other functions, this function here is sort of a weird because it's zero. It's zero. So if this is y and x, this is just zero, and then in the case, like an exponential. And it's sort of a knot in the completely monotone scenario. And so you have to solve it separately. And Graham and Waller use this thing to give a sharp form of the Vinaykahara-Talberian theorem. And the Vinay-Ikahara-Talberian theorem is. Carrot linear theorem is like a fundamental theorem to prove the prime number theorem. I mean, there are other ways of proving a prime number theory, but you can use a Talbert theorem. And in there, you like, you assume that the function has just a simple pole at some strip. And here, if you assume that it has a bunch of poles, you have to use their result. And for this other function here, which would be in a sense the most simple one, you can. In a sense, the most simple one you can think of. So you have a function which spans limited, lies above an indicator function of an interval and want to minimize its mass. Again, you can solve this problem. All these problems you can solve explicitly. And here was one of the first instances of this problem, actually. It was Selber, kind of, I think Selber was, Selber and Berlin were the ones that had this idea. This idea, and he used this to give a proof of the large sieve inequality. Again, the large sieve inequality, like a glorified version of this, was one of the main ingredients in this proof of large gaps between crimes and small gaps between crimes that were gave by James Maynard, which just won the Peach Medal. And so these sort of techniques are embedded in number theory a lot. A lot. You see these sort of optimization problems popping up in a lot of these number theory papers. And now I want to go to higher dimensions. So in the one dimension scenario, you sort of have a complete yes. You can always sort of solve these problems exactly. Even when you have weighted norms that you want to estimate. Weighted norms that you want to estimate, you can solve it by applying maybe some de Bruns theory. But when you go to higher dimensions, the scenario changes. And so how can you do those kind of things? Because we want explicit solution. And so it's hard to, you know, how to, yes, because the simple reason that band limited functions in higher dimensions are very hard to. Are very hard to deal with, at least from my experience. And so, a nice application is this Blashki-Sentelo inequality, which is if you have a convex body, say K, and you define its dual body, then you have some sort of uncertainty principle in both directions. I will explain also the reverse flash center-loo inequality in a moment. But if you try to maximize But if you try to maximize this quantity here, it turns out it's maximized exactly when you consider this body to be a unit ball. Okay, and there is a very nice argument that shows this in one line, and I will try to give it now. So define this quantity here, okay. You just minimize the L2 norm among functions. So now this is like band limited functions supported. Supported with the Hitler's form supported in K, which are L2. And you assume that at the origin is greater than or equal to 1. So this is sort of like a one-delta problem. It's bigger than one at the origin, but you don't ask anything outside the origin. And so this thing is easy to solve, of course. The solution is just given by one divided by the reproducing kernel of the space evaluated at zero. But, anyways, I will give it. But, anyways, I will give it here now. So, suppose you have such a function f, okay, and it's optimal for this problem here, this whole k. Again, I will give the solution. And the solution is unique. Model of multiplication by a complex constant of models one. Then, if you look at this other function here, okay, as a function of. Okay, as a function of y, you can show that it actually belongs to this space. So now it's we have transformed this apart in a ball in a ball with a particular radius, which would be this radius here, which is you consider the norm generated by the dual convex bottom and just compute the norm of this vector c input here. Okay. And for you just assume that c is a vector of l2 at zero. A vector of L2 L2 mark one, for instance, and this is just a one-line inequality. So, okay, which is equal to that, which is assumed to be optimal, you just rewrite that integral in a clever way using these averages, say, and you just apply the case that, well, each one of these guys is admissible for this problem here. So you just put a lower bound to that and it just. To that, and just then do the calculation. You put them on out because it's homogeneous, and then once you integrate this, it's a nice computation to do. It's easy to see, but it's equal to that. Okay, so you have a very simple inequality, and then what you do is that, well, what is Ho K? Well, Ho K is actually exactly what you want. Ho K is just one over K. And if you, for any convex body, if you just replace, you get the Bleschki-Sandov inequality. The Blaschki-Santolu inequality. And why is that? Because, as I said, this guy has to be the optimal because this guy is the reproducing kernel of the space. And its L2 norm is just the reproducing kernel evaluated at the original, the very simple technology. So this is a very nice argument to prove the Blash-Antelov inequality using these magic functions. Any questions here? Any questions here? Okay. And there is a reverse Blasch-Sentelo inequality, which is a theorem now, Bergen-Muilman inequality. And the conjecture, of course, is still open. It's a major problem, which is when you put this constant c equals four, and then Bergen and Millman. And then Bergen and Millman prove that there is some c that satisfies this. Okay, and if you put c equals four, it means that the minimum of this thing should be realized when k is the vital L infinity ball or the little L one point for that matter. And so, how can we prove such a thing? And there is a very nice paper of Nazarov in this thing. In this thing. And again, using magic function. It doesn't give the current best C, but it gives a pretty good one. So the proof of Nazarov involves tube spaces. Okay, what's a tube space? So let's assume that k equals 1 is symmetric for now. And the tube space will be the space of analytic functions here such that you have like L2 norm. That you have like L2 norm finite in the whole thing. Okay. And I think this is almost been limited in a way, because if you see it as the function of Iy, I think it means that it's weird to inform the case exponentially up to a certain range related with the norm associated with the dual convex body. But anyway, you can think of them as almost band limited in a way. And it has a reproducing kernel. It is a reproducing kernel, a qubit space. And this is a reproducing kernel. With this function j here. And what you can do is, this is a very simple estimate for a lower bound for this function j, which is the following. Well, if k is symmetric, then, well, for any y, this set here is contained. This set here is contained in k, so I can just say that j is greater or equal than this. Okay, and so then first I do a change of variables to put k here back. So I translate and multiply by two. That's why you get this two to the n. And that is why you get this. And then you apply a Jensen's inequality because exponential of minus whatever, it's convex. And then once you apply Jensen's inequality, Once you apply Anson's inequality, you get just the volume of k, which was one. This is one we assumed. And then once you take the supremum over all y's in k, this is exactly like taking the dual norm associated. The dual norm is associated with the dual of the convex set. So that's what you get. Okay. And if you put this back in the kernel, once you put zero, zero, that's what you get. So f of That's what you get. So f of zero, just by the reproducing kernel property, you have this because f of zero is just the integer of f against the reproducing kernel, and then you just apply some Cauchy-Schwartz inequality. So this is just Cauchy-Schwartz. And in here, from here to here, you just plug in the information you had from J in the integral, because, well, this would be just one. And then you just put it back, integrate, do the computation. Integrate, do the computation, and then you get this. Okay. And then now you have a problem to solve. Because once k is one, this guy is one, you just have to minimize, you just have to find a lower bound for the norm of k star, for the dual. And so you have this problem: minimize this quantity here. And if you minimize this quantity here, this would be a particular C you can put. C you can put, sorry, maximize this one. This would be a particular C you can put in there. So, okay, you want to know for every n, so there is an n play here, and you want to send n to infinity eventually. And so this is the hard part. And what Anazarov did was to apply Homonde's theory of the solution of the generalized Cauchy Heman. Cauchy-Hiemann equations to construct this function. Okay, so if you apply Homander's theorem, that very nice theorem he has in his famous book. And then you have also some power trick in the end, of course, because it's Mazatov and there will be a power trick in the N. And so you get pi squared over 16, which is not the current path, the current pass is to the Cooper bank, but you get a To the Cooper back, but you get a pretty good one. The nice thing is just the mere existence of a function just gives a C. Just the fact that the space is non-empty just gives one C. The problem is, you have to take care of the dimension because n is going to infinity. So you have to create a C, a function f that can deal with n going to infinity. Okay, any questions here? Any questions here? Okay. Maybe I'm going too fast. Or maybe not. The next problem is the packing problem. So another famous problem which gave the Fields Medal to Marina Byzotka just recently was that was not only her only achievement. Her only achievement. She had other very nice work, but this was the main part of her work, which is to study how you can put spheres in equation space that don't overlap so as to maximize the density. So if you have a collection of non-overlapping spheres of the same size, of course, then Then you can define the density just by like the upper density, say. So to be precise, that would be like the upper density. There are other ways of defining density. You could either take your box, move around, and take the maximum density you can possibly get, and then send it to infinity. There are other, you could, instead of getting this box, you can take a convex set. You can take a convex set and the same thing. In the end, the packing density of the space, which is when you take the supremum over all packings, give the same number. And so, suppose your packing is given by a lattice, say, which means suppose you have a lattice. And you can put balls of radii a half centered at each point. That means that the minimal distance between two points in the lattice is greater or equal than one. Otherwise, this wouldn't be attacking. And so the density is easy to compute because it's a periodic configuration. It would just be the volume of the volume considered divided by the volume of the fundamental region of the wave. Volume of the fundamental region of the valus. So, this is just to give an example how to compute these densities. And then there is a very nice problem, which is this is the linear programming problem found of con and alkeys. I'm putting them in this slightly different flavor here, which is the following. So, you take a number r, which can be one or infinity. Okay, and then you can see. Okay, and then you consider L1 functions which are band limit. Fourier transform support in this little book. And you assume two things. You assume that f is greater than equal to zero and f is equal to one at the origin. Okay. And then you also assume that the Fourier transform is less or equal than zero in this interval. zero in this interval. Okay, if f is, if r is infinity, that means that the Fourier transform is just less than or equal to zero for all x's, okay, greater than one. Okay, so you have these conditions, and then you can just apply a Poisson summation to get a bound, which is, well, f of zero is one. So again, that same archetype inequality, well, but that's just the summation over the Summation over the dual lattice, a Plyposum summation. That is just this sum. Okay, we know the minimal norm of this lattice. And so this is just bounded by this. And we know how to compute the density of P. So in the end, if you just move things around, you get that the density of P is less than equal to that. Okay. And so again, your problem. Again, your problem now is to minimize this function on the right, assuming all these conditions for f. So, again, the mere existence of a function gives a bound, but of course you want to deal with changing dimensions, and so you have to consider a family of functions, one for each dimension, and see what happens here. And you can extend these boundaries. And you can extend these bounds. You can show actually that the density of P for any packing, does not need to be given by lattice, is bounded by this also. And it's just a trick involving Poisson summation. First, you do, for instance, some periodic packings, which are just unions of translations of lattices, and then you approximate any packing by that. And in the end, you get the same. Okay, so this bound is actually. Same bound. Okay, so this bound is actually universal for any pet, you know, of any sort of form, this bound will hold. And then there is a very nice paper of Hohen and Zhao. I think I messed up the date here. Maybe I didn't. Which shows that they construct implicitly, I mean, you can write a formula for that function, but it's not going to be nice. But it's not going to be nice. So it's still the question if you can give a nice example of this F. But they construct an F with this radii here, with this R equals this very weird number here, one divided by sine of roughly 63 degrees divided by two. It gives at least 1.9. And they show they can do that in every dimension. Every dimension. Okay. And so that gives you this bound asymptotically, which is known to be the best current asymptotic bound for sphere packing and also matches the Kapachensky-Levenstein bound, which is built in a sort of a different way. They first build it a bound for codes in the sphere, and then they It's sort of a sort of a take that same it's sort of a take-a-pay packing in the space, putting on the sphere, apply the upper bound and get and get the upper bound for packing. Here is sort of a different sort of a, you start on the space and you only stay on the space. You don't go to the sphere whatsoever, which is nice. And still, I mean, you can think, well, there are. Uh, I mean, you can think, well, there the problem here doesn't depend on R, of course, and so you can change it, take any R. The issue is like, well, give me one, and then this function here, only with this guy, it's weird why it gives such a nice bound. There are other constructions with functions with r equals one, and I think it's a paper of Gorbachev, which gives with r equals one. It doesn't get to this value. It doesn't get to this value, it's slightly less, but it's also a nice one, which I think Point and Alk is also good in their paper. And then the case R equals infinity is sort of a case to explore because it's the case where you possibly can have equality here. And indeed, this happens in dimension 1824. Happens in dimension 1824. This was already expected numerically, and Vyazovska solved the problem, finding this magical F for R equals infinity and showing that the leach and the E8 and the Lich lattices are optimal. I think this is sort of a nice way of seeing this phase. If R is finite, we're sort of looking for a symptomic bound. If R is infinity, Bound in r is infinity, they sort of are looking for a specific dimensions. Um, and I think even when R if you put R equals infinity and try to apply Gozovska strategy in every dimension, you can build sort of a pretty good functions, but their synthetic upbound that they give is pretty terrible. It's not even close to this, and so. So that's the sort of a way here. Any questions before? Okay. And I think that's my last example. Again, I want to show some ongoing project that I'm doing right now. So consider this HN here. So Hn is just this convex set here. This convex set here. So for n equals one, you just have an interval of size at the interval minus one. If n equals two, you just have this hexagon here. And n equals three, we have this shape here, which I didn't know, but it's called cuboctahedrum. It's not a Cuboctahedrum is not a regular cuboctahedrum, if you wish, but it is something called cuboctahedrum. And so it's a nice name. So I have to include in my talk. And okay, so what we do with that, well, we want the following. We want to construct a function d, which is Fourier transform is to Pouder in that weird set. And by the way, this. And by the way, this set here can be viewed as: well, you go to Rn plus one, you take the little L1 norm and you slice it by the plane, which all the variables sum to zero. Okay, so you got this guy here. This is one way of seeing it as well. But in any case, you want the function f Fourier transform support in H. Transform supporting H and is greater or equal than the delta. What I mean is greater than equal than zero everywhere, and at the origin is greater or equal than one. And then you want to minimize this beast here. Okay. If we didn't have this guy here, that would be still a hard problem because it's the L. Because it's the L1 version, say, of this one delta problem. If we had L2 norms here, then that would be easy to solve, even with this guy. So if this guy was one, this is even, I don't know how to solve it. But now it's even worse. We have this weight here. And this here, for those who know, this is the Gaussian, this is the Gaussian, uh, this is the Gaussian unitary assembled uh Dyson matrix associated with the Gaussian unitary assembled model for random matrices. And so, this determinant here shows up not by any mistake because this vein comes from this n correlations results in number three for zeta functions and L functions. And so, why we want to do this. We want to do this thing because of the following. We have this result. This is sort of a numerical universality result, if you wish, which is if you assume the generalized Riemann hypothesis, which means if you assume that you're given a certain L function, forget what this function is. It's just a function which is usually entire and only have and have a zeros into the. And have a zeros in the region. The zeros in this region is called non-trivial zeros. And you want to show that all the zeros are aligned. Align real part equals a half. So the generalized Riemann hypothesis for this function just assumes that. Okay, so if you assume that, you can show that bounds for the multiplicity. Bounds for the multiplicity of the zeros. The zeros of multiplicity, at least n, these should be very scarce. There's actually, there shouldn't be any zeros of multiplicity n because it's conjectured that all the zeros are simple. Okay, but still with the Riemann hypothesis, we can't show that. So we still need downs. And you can show that this quantity here downs this. Okay, but so the percentage of zeros. Of zeros of this function having multiplicity, at least n, there should be none, but we can do that till it's bounded by this. And the only thing we can do is just to explore this problem numerically. Because as I said, I don't know any way of solving this problem exactly. So then we solve this problem using linear programming. But first, we thought of solving it using First, we thought of solving it using semi-definite programming, but it turns out there is a trick and you can simplify things. You only need a computer programming for that. And just so you know, this function here is sort of a Mellington transform of a model of form, say, and this modulo form is associated with something which is called an automorphic representation of the linear group over Q. Over Q of n by n matrix, m by n matrices. And so if m is one, these are just basically the Dirichlet functions, Dirichul√©, the famous Dirichuli L functions. For instance, the zeta function. Okay. And Dirichuli L functions, which just be like when you put here some character model Q, say. But if M is higher, then you have more complicated. You have more complicated stuff. Okay, our methods apply to any element. That's why such a call is sort of a universality result. And what we can show so far, and that would be my last thing, is this. So for n equals two, so yes, so if you put here zero of the multiplicity at least two, then you want to. Then you want to compute theta one. Okay, theta one is just the one-dimensional plane. And in this case, you can actually solve because it's one dimension. So we can actually compute the number and it gives this. And this is, I think it's like a half minus arc cosine of some artifact. It has an explicit point. And of course, this is only for m equals one. If you put m equals one. If you put other m's, then this number changes. And then for theta equals two, there is no way because for theta two, you have theta two is two dimensional. And again, once you go to higher dimensions, I don't know how to solve this problem. Maybe there is some hope for solving this explicitly, but when you go to three or four dimensions, it's pretty much impossible. Impossible. And then we have to do appeal to numerics. But what's the nice thing is that we can show such a small values that if you count the zeros of multiplicity at least four, say, they stand only for like less than one percent, less than a half percent of the zeros, which was sort of a surprising. It seems like this is like reducing by a factor of 10 each time. So this would be even like 0.0 something. Even like 0.0 something. But the problem here is that this computation here took a month and we realized there was a bug and we are still running it until we find another bug. And so these problems can be very hard, even computationally. So we had to stop here. We can't go to build here, I don't know, we need the help of Google or something. Google, or something. And so we have to stop at some point. And well, and that's all I wanted to present today. Thank you. Okay, thank you very much. Any questions? Yes? Do you hear me? Do you hear me? Yes. Yes. It's of Dmitry Gorbachev. Thank you for a very nice talk. Let me give one remark concerning my result. You mentioned R is equal one. Its result refers to Siegel paper of nineteen thirty five year. 35 here as I remember. But in context of Levinstein Kabatyansky, bound R is equal to BC function zero, something like this, as I remember. But the solution of my problem, yes, gives less estimation. So now it's not so important, but as I know, people. Yes, I know people believe, yes, that Kabadianskin bound can be improved solving this problem. Yes, I know. No. Yes, what I know is that there is a conjecture that this number here, that via this approach, the best you can do is to put a number which is like 0.601 and this number. And this number it comes after some normalization. It's like pi, like there is a form of this, like pi divided by something square root or something. And this comes from this physics paper. Like there is an intersection with conformal field theory here. And these physics guys, they had better numerics. And it turns out that you can do these numerics and they. You can do these numerics and they can like sort of extrapolate what would be this number in the end. And it's pretty close to this number here, which has some significance. But of course, you know, a recent paper of Khon and the conjecture. Yeah. It can be improved, as I remember. Probably I'm wrong. Yes, yes, that that's the paper, yeah. Yeah, yeah. That's the paper. Yeah, yeah. And yes, this is the result of Strom from Victorian Burger. I remember on the sphere, yes, because it's interrelated. Okay, so it's interesting. He has to continue to solve this problem, in my opinion. Thank you. Thank you. Very natural. Thank you. Very magical. Thank you. Okay, more questions or remarks? If not, let's thank Philippe and all speakers of morning session. So we resume at three.