Do a live code demo. I'm not as brave as Ben to run it off the internet. But if you are interested in running this as I go through, you can go to meltzer.ca/slash burrs22. And on here, it's got a lot of information and references, but in particular, there's this bold link to run it in your browser, and it uses a similar thing. Put that link up again? Meltzer.ca slash burrs. You could do it on your phone, but the UI basically takes 90% of it. 90% of it. Yeah, and thanks to Ben and all his collaborators because all this low-level work is obviously absolutely crucial to anything high-level like this. So it's great, all the work that you're doing. So just to remind you of where we were in the story yesterday, so we were looking at p-recursive and d-finite functions and there was this motivation about the shape of biomembranes that led us to ask, can we prove when things are exactly positive and what can we say about the asymptotics of p recursive sequences? Asymptotics of irreversible sequences. So, skipping ahead a little bit, we have these ways of encoding them in Sage using OA algebras, either the shift recurrence, or either the recurrence using shift algebra, or definite equation using this differential Weyl algebra. And we were able to derive asymptotics and things like that, but we had some problems. So, one of the problems was determining. So, one of the problems was determining which singularities were actually singularities. So, we could find these coefficients, but sometimes it would say that the leading coefficient is like 0 to 300 decimal places. So morally, we know it's 0, but we'd like to prove that. And the other thing is that the dominant asymptotics we got, we got them up to coefficients that we could numerically approximate to great accuracy, but we didn't know them exactly. So now I want to go back to this question of how can we How can we figure out what the dominant singularities actually are for some kinds of problems like this? In particular, if you remember at the very end, we had this lattice path problem. So you're looking at walks in the first quadrant that take north, south, east, and southwest steps. And if you try to use the techniques we talked about yesterday to find asymptotics, it will tell you it's 3 to the n times something that looks very close to 0, plus something that looks very close to 0, plus something that looks very close to 0, et cetera. Very close to zero, et cetera. So you might start to think: okay, maybe this isn't actually 3 to the n. Maybe this grows like something less than 3 to the n. Maybe the singularity of z is equal to a third is not really a singularity. So now we're going to move on and use a different representation, and we're going to move on to this theory of analyticals and several variables that you've heard about several times from Robin, from Terrence. So the framework now, we don't have a univariate function. We now start with a multivariate A multivariate series, so devariate series. And I use the multi-index notation. So if I have a bold vector to another bold vector, that means you multiply out the one in the bottom to the powers at the top. And you can think of this as specifying sort of a multi-dimensional array of coefficients, and we're interested in asymptotics. So for me, what I'm going to do, I'm going to focus on a univariate sub-sequence. So you're going to fix a direction of vectors, and you look at all the terms where the exponents are multiples of this direction. Exponents are multiples of this direction. So the most common case is the main diagonal direction, so the 1,1 direction. So you take your series, you expand it as a bivariate power series in this case, and then you look at the constant term, so that's x to the 0, y to the 0, then the linear term, well, the term that's x to the 1, y to the 1, the term that's x squared, y squared, and so on. So all the multiples of your direction, 1, 1. Now, in this talk, I'm going to focus on the case when we fix r, and r has a natural number coefficients. R has a natural number coefficient, so this coefficient is always defined. You can also think about varying R and looking at asymptotics, and you get some uniform behavior, and you can prove limit theorems and all kinds of beautiful things. But in this case, I'm just going to fix R. It's usually going to be the main diagonal. And I'm going to say, how can I get asymptotics? So there's this nesting of different types of generating functions that I think Robin talked about briefly in his talk. So we can go from rational generating functions. I guess polynomials are really the simplest if you think about asymptotic. Are really the simplest if you think about asymptotic behavior, which is zeros eventually. Then, rational functions, algebraic functions, definite functions. There's a notion of dealgebraic functions, which are those that satisfy polynomial differential equations instead of just linear differential equations like definite functions do. And if you look at rational or algebraic functions, so there are some technicalities involved in cancellations and things like that, but in most cases, and in cases that come up from combinatorial contexts, it's essentially automatic to go from It's essentially automatic to go from a rational function or a minimal polynomial of an algebraic function and give asymptotics. For d algebraic functions, it's actually known to be undecidable to find asymptotics in general. And definite functions, we can kind of do it up to this connection problem that I talked about yesterday. So we know how to find asymptotics here. We know it's not possible here. And it's kind of unclear exactly how far we can go here, if we can prove everything, or if we always have to worry about this connection problem. So one natural thing to do is to look at a class of functions between L. To look at a class of functions between algebraic and d finite. So, if you take this multivariate setup and you look at multivariate rational functions, and you look at all the diagonals of those, let's just say the main diagonal, so the all-once diagonal, if you look at those sequences, it turns out they lie between the algebraic functions and d-finite functions. So this is how I got interested in the field of analytic combinatorics and several variables. Actually, through these lattice path enumeration problems, like the one I showed before, there were some conjectures on asymptotics and trying to figure out how to get asymptotics and get around the connection. How to get asymptotics and get around the connection problem for these is how I got into this area. I think when Robin and Mark Wilson started looking at this, they were interested more in thinking of actual multivariate generating functions. And then you can also get results related to that. But for me, I personally saw them as a data structure for univariate sequences and using that as an alternate data structure to some of the other ones listed here. However, you think of them, the goal will be to, so for us we'll now focus on rational functions. So take a rational generating function with a power series expansion. Generating function with a power series expansion and try and find asymptotics of its main diagonal. So, we're going to assume it converges in a neighborhood of the origin. There is a theory that works for Laurent expansions, so you don't have to have necessarily a power series expansion, but I'm just going to focus on power series expansions here. Now, the singularities, if you assume that g and h are co-prime polynomials, then the singularities are given by the zero set of the denominator. Now, this is why the multivariate theory is so much harder than the univariate. Multivariate theory is so much harder than univariate theories because now you have an infinite set of singularities that form an algebraic variety. And so there's an infinite number of them, which means it's harder to know which determine asymptotics. But you also have geometry, so they can self-intersect and get things that are very pathological. So I'm going to show some software for how you deal with these things, but I'm going to stick to the simpler cases, like when your set of singularities forms a manifold, for instance. So these are things that happen generically, like if you randomly picked a G, and an H. Now, of course, examples. Now, of course, examples that come from real combinatorial problems are not necessarily generic. So, just because something holds with probability one on random functions doesn't mean it holds a probability one on combinatorial examples. But I would say that maybe 50% of the examples I've seen in the literature, 75%, follow the assumptions that we need to run the software that I'm showing you today. So, we have our set of singularities, just like in the univariate case, the singularities that are closest to the origin are important. So, in the univariate case, we call these dominant. In the univariate case, we call these dominant singularities, and they're the ones that give you dominant asymptotic behavior. In the multivariate setting, we call these minimal points. Now, one of the crazy things about the multivariate setting is that minimal points might not actually be the points that determine the asymptotics. So in one variable case, you start by integrating around the origin, and you expand your circle outwards, and you have some singularities, and eventually you have to cross through the singularities and get stuck there. It's really hard to picture, but in at least two complex dimensions, the way the dimensions work out, because The way the dimensions work out, because you have four real dimensions and two complex dimensions, you can actually deform around the minimal points without going through them. And so, what you would maybe naively expect is that minimal points, since they're so close to dominant singularities, these are the ones that always give you the asymptotic behavior. But that's actually not the case. There's another generalization, which in the worst cases takes a little bit to define, so I'm not trying to define it in general. But there's other types of points that really are the ones that always sort of determine what happens. Always sort of determine what happens with the asymptotes. But the minimal points are important. Now we're in the multivariate setting, so by closest to the origin, I mean coordinate-wise, close. So there's nothing coordinate-wise with smaller modules. And the way that we find asymptotics, just like in the univariate case, we start by setting up some kind of Cauchy integral formula for our sequence. So here now you integrate over a product of circles. So you say modulus of X is really small, modulus of Y is really small, modulus of Z is really small. And just like in the Univariate case, the idea is you. And just like in the Univariate case, the idea is you manipulate this domain of integration. What usually happens in our analysis is you compute some residues to reduce to a lower dimensional integral, and then you apply the saddle point method after that. Again, this is the software demonstration. I don't want to get too much into the theory. But I'm just going to state what the main result is and then show you the software. So the simplest case, which is the case that I'm going to talk about how to use software for, is when your denominator and the partial derivatives don't simultaneously vanish. So you get Simultaneously vanish. So you can think of this as the smooth, simple pole case. So you don't have a multiplicity to your poles, and your singular set forms a manifold. And in this case, I'm going to define a new set of points called critical points. And these are really the ones that determine asymptotics. Now, in general, if you have self-intersections and things like that, it gets more complicated to define. But under these assumptions, which again, I would say hold for at least 50% of the results in the literature, you can just take this. You can just take this set of polynomial equations as the definition of critical points. So, critical points are the points that satisfy these equations, where these are partial derivatives here. So, this is the partial derivative with respect to the first variable, this is the partial derivative with respect to the last variable. And you see it depends on the direction. You're missing, there's only one term instead of subtracting two of those equations. Yeah, those aren't all zero. Oh yeah, sorry. They should be yeah, they should be pairwise equal except H. Thank you, yes. equal itself. Thank you, yes. This should be equal to zero and then these are just equal to each other. Thank you. Sorry about that. So you're a singularity and then all these quantities are equal to each other. And so morally you can think of critical points as being points where if you restrict to your singular set you have saddle points. So you can find asymptotics using the saddle point method. And minimal points are those that are closest to the origin. So these are what we can take our coaching. Origin. So these are what we can take our Cauchy integral that we start by integrating close to the origin and move them close to without worrying about deforming around singularities and stuff like that. So when you have minimal critical points, you're usually in a good situation. You can get close to the singular set, and then you can do some kind of computation with residues and things like this to get something on there where you can apply the saddle point method. So this is one of the main theorems of ACSV. So take that polynomial system I showed. Suppose you have one solution that has non-zero coordinates and it's minimal. So there's Coordinates, and it's minimal. So there's nothing coordinate-wise closer to the origin. Suppose that this partial derivative doesn't vanish, and you construct a certain matrix, which I'm not going to describe, but it's explicit in terms of the partial derivatives of this H polynomial. And assume that matrix you construct is non-singular. Then you can find an asymptotic expansion for the nth term of the diagonal. So in this case, I'm in direction r, but usually I'm going to just restrict it to the main diagonal. So you get an exponential. To the main diagonal. So you get an exponential part, which is depending on the singularity, so it's the location of the singularity, as everyone is familiar with in the univariate case. You get a factor of n here, and then you get an expansion with coefficients that you can compute explicitly. So you can actually get a series in 1 over n with more and more accuracy. If you have a finite number of minimal critical points that have the same coordinate-wise moduli, so they're all basically the same distance from the origin, then you compute the contribution of each. Then you compute the contribution of each and add them up, just like when you have multiple dominant singularities in the univariate case. And as I said, you can also show things about the error when your direction varies and get limit theorems, but I don't want to talk too much about that right now. So just a quick example, if we take this binomial coefficients here, if you look in the main direction, main diagonal direction, 1, 1, those polynomial equations just become these two right here, which you can solve and get 1 12, 1 1/2. You can solve and get one half, one half. Now, the hardest step is usually proving that you have a minimal point. In this case, it's quite easy because if x and y add to 1, you can't have the modulus of x and the modulus of y both less than 1 half. You add two things with modulus less than 1 half, you can't get 1. So in this case, it's very easy to prove that it's minimal. In general, it can be very hard, though. That's always typically the hardest step. Anyways, the assumptions of our main theorem are satisfied, so you can immediately get asymptotics that many are familiar with, that we can put this into Benz package, you would tell it to us in high accuracy. And tell it to us some high accuracy in millisecond or something like that. We can also then, if you change your direction, the only thing that changes is here in this equation, so you change your location of your singularity and then. As I said, the hardest part of finding asymptotics is usually verifying that you have minimal points. So generically, the set of critical point equations, you have d polynomial equations and d variables, so you should have a finite set of critical points usually. And you want to check which of these are minimal. And you want to check which of these are minimal. And this is a very expensive operation because it involves moduli of coordinates. So it's kind of a real algebraic geometry problem. Now, this is much easier when you know that your power series has only non-negative coefficients, or you're actually allowed to have a finite number of negative coefficients. So you have something that's kind of like Pringstein's theorem in the univariate case. There's a generalization to the multivariate case. And just like Pringstein's theorem helps you find the dominant singularities, you just have to look along the positive real line. This property of being combinatorial helps you a lot. Property of being combinatorial helps you a lot improving minimality in multiple variables. So, in general, you have to worry about the coefficient sizes that appear and stuff like that, but abstracting all that away, roughly the difference in complexity is something like 2 to the 4d, where d is the number of variables, to 2 to the 12d, if you don't know your combinatorial. So this is expensive, but you can still do it on many examples. This is like very infeasible, even for really, I'll show an example of some software we have to try and attempt this, but it can really still only work with low dimension. Dimension. As part of my PhD thesis, working with Bruno Salvi, we gave some kind of symbolic numeric algorithms to do this using this idea of what's called the chronic representation, and we gave a preliminary Maple implementation. But our implementation was not very rigorous. By not very rigorous, I mean you decide some accuracy that you need to compute things to, and what we did is we said, okay, we can need this accuracy, we'll just double the accuracy and tell me if it'll work to double the accuracy, and it can maybe do stupid things, but it probably won't do like twice the amount of stupid things of the accuracy of the issue. The amount of stupid things in the accuracy information, which is not the best way to do it, I think. So, a couple summers ago, Robin and Marty Mishna, Mark Wilson, and Yelvrishnikov organized this math research community through the AMS, and we had different projects. And one of them was making good software implementation of this. So Ben was one of the participants, and of course he's a Sage guru, so he was very helpful. So him, Elaine Wong, and Jesse Selover started working on a Sage package for this with rigorous numerics using interval arithmetic and things like this. Using interval arithmetic and things like this. So we worked on it a bit and then it kind of sat for a while. And then last summer, I had an undergraduate at Waterloo, Emu Luo, who took it over and really did a huge amount of work over the summer to make it where it's basically, well, it is publicly available now, and we should have some documentation, so it should be pretty much complete in a few weeks. All the assumptions that you need to verify asymptotics are proven by the package. So it only applies, as I said, maybe 50% of examples or 75% of examples, but if it doesn't work, it Percent of examples, but if it doesn't work, it will tell you that it fails. So you can trust what it returns. Except it requires you to know that you have this non-negative coefficients. So that's one thing. It's actually unknown even for univary rational functions if you can decide this. So we can't, again, do this. But assuming you know that, then you can run the package and if it outputs the asymptotics, you can trust it. So I know it's sorry, it's been a long time, I haven't actually shown any software in this software presentation, but here's our In this presentation. But here's our package. Now it's, Ben got it worked on the PyPy system. So you can actually install it if you have Sage with one command from your command line. Just say sage pip install sage-acsv and it will install it for you. So you should import it into your document. And what you do is, okay, here I'm making three variables. n is actually not needed for this part. I'm going to use n to talk about asymptotics on the next slide. But all you need to do is import your. Slide, but all you need to do is import your variables x and y. Say I have a numerator 1, denominator 1 minus xy, and I can run this code and I'll use the show formula equals true command. And it will tell me that here are the asymptotics. Now, I admit we have to actually fix this because this square root 2 is part of this constant, so we should have a numeric single number there. But it gives you a numeric approximation of the asymptotics. And you might think it's a little weird, because it says you get this when u takes this value. When u takes this value. So, what does it mean? What is u and why does it come up like this? Well, the reason is that in general, your asymptotics will depend on general algebraic numbers. And so you can't always just return exactly what the coefficients are, because they'll just be, you know, a degree six algebraic number. You can't store it exactly. So we return asymptotics in a symbolic numeric data structure. So we return, in general, three polynomials, A, B, and C with integer coefficients, and another polynomial, P with integer coefficients. Polynomial p with integer coefficients and a list of roots of p. And so you get asymptotics by taking this exact symbolic expression in terms of integer polynomials and evaluating them at the roots that it tells you. So in this case, if we want to get the actual asymptotics, we can find these A, B, C, P, and U from the last slide, and then just use that formula, and it tells us it's 4 to the n over root pi n, which is what we expect. You can also change the direction so we can. You can also change the direction, so we can put in the 2,1 direction, and you'll see here's the asymptotics. Of course, you can put in like whatever you want, 21, 13. Okay, if you wanted to know that kind of weird sequence of binomial coefficients, there are the asymptotics. Here's another example that's very common to use. So Aperi's proof on the irrationality of zeta 3 uses the exponential growth of a particular sequence. It's this binomial sum sequence, and you can write any such binomial sum sequence as the main diagonal of a rational function. Some sequence as the main diagonal of a rational function. So if you want to find asymptotics of this, you put it in your function, you get your parameters. There's another flag you can set, show points. So it tells us that there's a minimal critical point at roughly this location. It stores it exactly, but it tells you just an approximation if you want to know roughly where it is. In this case, you'll see that asymptotics are obtained from evaluating this quadratic polynomial at this root. This root. So we can do that, and you get a numeric approximation. But in this case, since p is quadratic, we can actually solve it in radicals exactly. So we know that u is this number. Again, it returns u as an interval, or really, I guess, a rectangle in the complex plane, that contains exactly one root of the polynomial p. And we do things like smell-alpha certification so we know that it's close enough to be. So, we know that it's close enough to be a unique root, and you can refine it using Newton iteration and things like this. And so, if you look at the roots of P, it has two roots. One of them is negative 3,000, the other is negative 63, and we know this to however many decimal places this is. This is the number of decimal places it needed to prove that it was minimal. So you can see that when you do these queries, it actually requires a decent amount of accuracy. But knowing, of course, that this is the root that we need. The root that we need, then we can just use Sage's manipulations with algebraic quantities to get the asymptotics exactly. Here is the example I showed last time, yesterday, this lattice walks in the quarter plane. So we guessed a differential equation of count asymptotics. It turns out that one of the ways you get that differential equation is by representing the sequence as the main diagonal of this rational function. So there are two singularities. So there are two singularities. If you remember, back to that example yesterday, there were sort of, we had an asymptotic expansion. There was something 4 to the n and something negative 4 to the n, and that corresponds to these two points. If you multiply these points, you get 1 over 4. If you multiply these points, you get negative 1 over 4. So these correspond to those two singularities. These are both minimal points. They're returned here. But actually, only one of them will have the first term in this asymptotic expansion not vanishing. So the asymptotic is actually only determined by the 4 to the n point. And we can print it. Point, and we can print it out here, and it's 4 times 4 to the n over pi n. If you remember yesterday, I computed this constant to like a thousand decimal places. It was like 1.27 something, something, something. That's actually 4 over pi. So before we could show it exactly using a differential equation, but now we can figure it out exactly. And because we can figure it out exactly, we can actually resolve the connection problem for these types of sequences and generating functions. And generating functions. Something else I want to point out is that the way the algorithm works is it relies on what's called a separating linear form. So it introduces this new variable u, which is an integer linear combination of your old variables. And what it usually does is it picks u as a random integer linear combination. So with high probability, it's separating, which means that it takes distinct values on all the solutions of the critical point equations. So if you run the code multiple times, it might give you different, it will probably give you different. It might give you different, it will probably give you different p and different u, but of course those will represent the same asymptotics in the end. So if you run it multiple times, it will give you a different output representing the same asymptotics. You can also specify the linear form, either if you want to make it not randomized, or if you know a small linear form, like if you have small integer coefficients in front, in this case one and one, that usually gives you a smaller coefficients in p. So it can be a little bit more efficient if you specify your own linear form. Specify your own linear form. This is, you know, there's nice documentation in Sage. So if you install this or you run this in the cloud, you can use the usual sage help commands like help, and it will tell you how it goes, give you some examples and stuff like that. Yeah, just to say, if you have a non-combinatorial function, then the computations, as I mentioned, are much more expensive. So I'm not going to go through all the details, but there's another package we have, which is written in Julia and uses homotopy containations. In Julia, and uses homotopy continuation, so a different method of a numeric method for completely numeric, not symbolically numeric, for solving polynomial systems. And this is in general not 100% rigorous because these homotopy methods can technically miss solutions, although they very rarely do. But you can use this to test for You can use this to test even if you don't know that your function is combinatorial. So if we take 1 over 1 minus x minus y, of course, we do know that's combinatorial. It's a generating function for binomial coefficients. But you could run, so we have commands here that test for minimality without assuming combinatoriality. So if you assume that you're combinatorial, it's faster than other cases. This case, it's so small that there's like compilation time and stuff. So normally this would be much faster, assuming it's combinatorial. Assuming it's combinatorial. I'm not going to talk too much about this. If you're interested, you can look at this reference or ask me about it. But just to say that if you're really desperate and you have a function and it's not combinatorial, there are some things you can try. But again, it's difference between 2 to the 4D and 2 to the 12D. So it's not a great situation. But if you're in, say, 3 or 4 variables at a decently low degree, there is some software you can use. It might not be 100% rigorous, but it can help you detect these. Rigorous, but it can help you detect these points that determine your asymptotes. If you're interested in this material, then there's lots of places to learn about them. So, there's a book that came out last year, Invitation to Analytic Combinatorics, which focuses on basically what I've been talking about over these two days. So, effective methods for analytic combinatorics, starting with univariate things like p-recursions and finite functions, and then going to the multivariate theory. With Robin and Mark Wilson, we're working on a second edition of their first book. Of their first book. It's probably, even the sections that aren't new have been almost completely rewritten. So I'd say probably 75% of the text from the first edition is completely rewritten, at least, in order to make it more understandable. And with the benefit of time, it's not only now more understandable, but it's also more general. So it has more powerful results and it's actually easier to understand at the same time. So I've been working on it for several years, and hopefully the manuscript will be available, the completed manuscript will be done in January or so. We'll be done in January or so. Our next deadline is January 31st. Hopefully, we make it. Of 2023. Actually, 2021, of 2021. Okay, and yeah, so we, you know, I know a lot of people, so you could, yeah, so if you're interested in these books, this is out now, this will be coming out soon, or the first edition is available. You know, if if stratified Morse theory or those kinds of words scare you, then you can look at this book. Then you can look at this book. If they excite you, then you can look at this book. Or either way, I think you should look at both books, right? Look at the computation and the theory. And I also want to say that people get scared when they hear things like Charlotte Morse theory, but we're also working very hard to make it more accessible. So again, I have some students that are working on things like visualizations for how these flows go, that Morse theory tell you the gradient flows and things like that. So we're using this time level theory, but I think it should also be in a pretty accessible introduction to s the way that some of these things Just the way that some of these things. Okay, so that's all for now. Thank you again. Questions? So you use this sort of arbitrary hyperplane thing to get a polynomial P that variable. So, and that, I guess, you. And that, I guess you you can show what that is, right? When you when you u is an algebraic number, and it's always the same algebraic number, right? What do you mean always the same algebraic number? So if you if you u is defined by a linear form, which as you say is like basically projecting onto a random hyperplane. And so if you fix the linear form, or if you fix the hyperplane, you will always be the same. But so really, it varies. How does you then appear in the solution? I couldn't tell, you know. I couldn't tell, um, you know, I know what I expect the the solution to be, like, where are the minimal points and powers. But then, you know, you you printouts and you see this U. So did it also say what you do with you in final formula? So basically the way the chronic representation works is it takes a zero-dimensional algebraic variety, so an algebraic variety with just a finite number of points, and it parametrizes them by u, which is this linear form. Which is this linear form. So, because it parameterizes by u, you can represent all your old variables in terms of u. And so then when you have this asymptotic formula, like the product of the variables, or one over the product of the variables that appear in the asymptotics, you just write each of your variables as a polynomial in u. Really, we actually use rational functions because you can get lower coefficient size, which makes deciding the things you need easier. But whatever. So basically, you need one more variable, one more equation, cyber plane, and then when you And then when you eliminate, you end up getting, you know, eliminating you last. So you have U there, and then everything else is defined in terms of U. And in terms of, I mean, normally Gribner bases, the elimination orders will define one of them in terms of u, and then one of them in terms of that one and u, but you just end up with the same thing. Yeah, so if u is separating, then and you have to take the radical and stuff like that, then you'll just have everything else, every variable we specified in terms of u. Every variable will be specified in terms of u. Yeah. So this is why we introduce u and take this, if you take a generic enough, you know, random enough linear combination, if it's make a separated linear form, then those get everything indirectly in terms of u. And it tells you if it fails to separate, or it just reads that. Yeah, it tells you that if it fails to separate. Again, there's a range that you're guaranteed to have a separating linear form, and we, I think, take double the range or something like that. And so it takes random coefficients in that range and tests if it's separating, which it should be with high probability. To test if it's separating, which it should be with high probability. And if it's not, it just tells you it's not, you run it again, it should be separated.