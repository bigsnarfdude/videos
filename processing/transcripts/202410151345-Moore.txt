So, yeah, so my title is some geometry of some Martin Schultz regular algebra of dimension four. So, when I say, I guess first, let me go to the next one. So, we're going to start with the setup. We're going to just explain what I mean by geometry and then define the algebras we're going to consider. I'm not going to spend any time explaining kind of where these algebras are coming from, but But Ellen Kirkman will talk about that some in her talk on Friday. So forgive me for just giving you some generators and relations, but I think I hope to convince you that the geometry of these algebras is interesting. Okay, and also along the way, I'm going to kind of demonstrate some of the software that I used in order to do the computations. So I've written some I've written some packages for computations for graded algebras, mostly in Macaulay 2. So, Macaulay 2 is a primarily for commutative algebra, which was kind of what I was raised as, I guess I should say, a commutative algebraist. But it now has some non-commutative capability. And so I wanted to just demonstrate what it's capable of doing. So anyway, so let's get started. Anyway, so let's get started. So, the setup is kind of standard. We have an algebraically closed field. I'll have a vector space, finite-dimensional vector space on some generators xi. The EI will be the dual basis of that vector space. And A is going to just be the tensor algebra, should be parenthesis, sorry, tensor algebra on V mod I, or just, and we're going to look at quadratic algebras. Okay, so, and so whenever I say geometry, what I mean by that is the point. What I mean by that is the point and line modules. Okay, that's what we're going to focus on. Of course, one could consider geometry much more broadly, but we're just going to focus on kind of these basic objects. And so when I say a point module of an algebra A, I just mean a module whose Hilbert series is one over one minus T, and then line modules are one over one minus T squared. Okay, so and And in this, there's a theorem of going all the way back to, or done by Art and Tate and Vandenberg that essentially says that if you have such an algebra, if you have gamma is the zeros that kind of come from the multilinearizations of the defining relations, meaning you just assign one set of variables to the first variable in a quadratic binomial. A quadratic binomial, a quadratic term, and then another set of variables. So the second one, you look at that in PV dual cross-pv dual. You look at the e is the image of the projection map onto the first component. If this gamma is the graph of a scheme automorphism from E to E, then the set of points of A are in bijection with E, and E is the point scheme. We call E the point scheme. We call it E, the point scheme. So you can, there's this scheme that parametrizes all of the point modules of A. And we're going to regard tau as being part of the data, part of the geometry data associated to A. So that's kind of the point side. So E is the image of pi one. That's right. That's right. So, okay, so and the condition of being the graph of an automorphism of a scheme automorphism is satisfied. Automorphism is satisfied for a large class of AS regular algebras of dimension four. So, and the examples I'm going to show you, I said, as the title suggests, are dimension four. So we're going to be in a setting where we can look at the point scheme. So, okay, and this is a theorem due to Shelton and Van Cliff. Okay, so, and I wanted to go about, I wanted to just run through how to actually compute this thing. Run through how to actually compute this thing. So it's a very, you know, you can, so the way to compute this is you first kind of express the relations as a matrix M times kind of a column vector of the x's. So you have some six by four matrix. And then you take the four by four minors of m viewed as elements of the of the symmetric algebra on v, those turn out to define the scheme, the point scheme. And you can compute what the And you can compute what the automorphism is given this data by, so you take a point, right? That point makes all the four by four minors vanish, so that matrix is rank deficient. And so you just take the kernel of the matrix that you get when you plug in that point. Okay, so we'll do an example here in a bit. So then let's, I just want to tell you how to do the line scheme as well. So this is also due to Shelton and Vancliffe, but in a different paper. Shelton and Vancliffe, but in a different paper. But if you have a quadratic, noetherian, GK-Cohn-Macaulay algebra, then there's a projective scheme whose close points parametrize lines of A. And so how do you do that? It's a little bit more involved, but not too bad. But you take the Kazoole dual of the algebra A, and then you express the relations of the Kazoole dual in terms of this matrix M hat. So here you have 10. So here you have 10, right? There's 10 relations of the dual, of the Kazool dual. And then you take kind of two sets of four variables, u's and v's, and you plug in u to the left, or you plug in u to m on the left and then v to m on the right, and then kind of horizontally concatenate these matrices. And so that's going to give you a 10 by eight, a 10 by eight matrix. And so those, the eight by eight minors, the 45 eight by eight. By eight minors, the 458 by eight minors of this matrix are, you can express them in terms of these, in terms of these little smaller determinants. But you're kind of on the dual side of the Grassmannian. And so you apply the orthogonality relation. And that will give you, sorry, this should say Plucker coordinates. Sorry. You'll get 45 quartic polynomials in the Plucker coordinates. And then you throw in the Plucker relation. The Plucker relation, and those relations coming from those eight by eight minors expressed in terms of the Mij, together with the Pooker relation, define the line scheme of a four-dimensional AS regular, or I guess four-dimensional algebra as described before. So, and just more much more concretely, you know, if you have, you know, a two by four matrix, you know, that's corresponding to two linear forms, you can ask. To two linear forms, you can ask whether or not that can, how does that correspond to a line module? You just check the conditions on the minors that we're going to get and see if it holds. So we'll see examples of that in just a bit. Okay, so that's just a brief recap of like how you, or a reminder perhaps of how to actually compute the point and line schemes. And so we can, now I want to talk about our algebras. Our algebras. So the first algebra that we're going to talk about is this algebra R. So this is, I believe, if I had to guess Ellen's slides, this is probably not the presentation that she would give anyway. But so the one that, maybe I'll say one say. So where these algebras are coming from is from a representation theory question about trying to find dual reflection groups. Okay. And so, but the presentation. But the presentation that you kind of write down isn't as amenable to computation. However, if you do a change of variable, you get this kind of double or extension presentation. So this is the one we're going to use for our calculation. And just to demonstrate kind of what you do for the line scheme, you know, this is what the M matrix, sorry, not line scheme, the point scheme, excuse me. This is the matrix that you would get, right? You can see that x1, x2 plus x2, x1 is a relation. And so there, this. One is a relation, and so there, this, you know, if I multiply this together, you can see that's where that relation is coming from, and you get the other five relations similarly. Um, so I just wanted to show you kind of what this looked like in Macaulay too. So let me see if I can do that. We will see. Okay, so can everyone see that okay? That large enough? Okay, great. So here are, this is how you define. This is how you define a tensor algebra. So it kind of looks like a tensor algebra, but there's a funny little bar there, unfortunately. This bar here. That's just because less than is kind of already taken for other things. And so the parser doesn't really understand that otherwise. But you define your ideal. I'm just telling it to computer Grovener basis out to degree 10 to make sure that it doesn't. I can't remember if this one's a finite Grovener basis or not, but I want to stop. I want to stop. And so there's our tensor algebra. So to do the, let me actually define these maps. So you can, here's where this is, it'll compute this matrix for you, and you can compute the minors. So that's the, these are the defining equations of the point scheme of this algebra, which you can, of course, do in one command that we have here, this point scheme of Rx. So, and the nice thing about Macaulay 2 is it's pretty, you know, it's. too is it's it's pretty, you know, it's quite good at commutative algebra questions, right? And so this is a, you know, we're in commutative land looking at these different schemes. And so we can ask for the primary decomposition of this algebra or the defining ideal and it spits it out for us. And you can see this junk down here. That's just the component at the irrelevant ideal. So just projectively, you just throw that away. And so, but we can see that the point scheme. The point scheme, you know, it's six components which have projected, you know, projectively their dimension one. They're all degree one, so we'll see that in just a second. But it's kind of obvious from the equations, they're all degree one sub schemes. And at least the code tells us that they're all, except for this last one, which is again the irrelevant ideal, they're all prime. Okay, so let me just go ahead and come back, pop back over here. Pop back over here. And so I want to describe what the components of the point scheme look like. And they're described in terms of these hyperplanes. So if we consider these hyperplanes, now these are just going to be in the projective space over V dual, but they're not going to actually correspond to points themselves, but their intersections. themselves, but their intersections will. So we have EIJ is the intersection of two of these hyperplanes. And there's four points that are in the triple intersections, and they're these four. And kind of we've numbered them in such a way that if you just have some way of or some permutation of one, two, three, four, that this intersection holds. And the result is that we kind of have described what the Have described what the point scheme is of these algebras and what the automorphism is. And you can see, as the code suggests, it's an equidimensional scheme of dimension one and it's reduced. Okay, so the calculation is not too bad, but it's kind of it's nice to have some software to help verify or have know what to prove in some sense. So that's the So that's the as I said, the components of the point scheme are these six one-dimensional components. And oh, go ahead, question? Yeah. That's a good question. I don't know. I'm not sure. Quantum super cell is what he asked. I'm not sure. Because, yeah, it's there. Yeah, I'm not sure. That's a good question. Yeah. Yeah. So, and then, so I'm not going to go through the derivation of this, right? But this is what I meant by the kind of wider matrix with two things concatenated horizontally. You have the U part and the V part. The trick, of course, is that you have to express the 8x8 minors in terms of those UI VJ minus UJVI things. But you can use an elimination. But you can use an elimination order, for example, to help you do that, which Macaulay too is great at. And so doing that, you get that this is these are the components of the line scheme of R. So I have some code to do that as well, but I think I want to make sure we get to the stuff on S and T, the other two algebras. So you can notice a few things. So each component kind of has, I mean, I guess. Component kind of has, I mean, I guess these don't have the Plucker relation explicitly, but they follow from these linear relations or the linear conditions. So they all really have the, they're all contained in that Grossmanian. But these are all copies of P2, and then these are three copies of P1 cross P1. And you can actually describe, kind of describe these in terms of the hyperplanes from before, also, in the sense that the pre-image of the pre-image of the pre-image of the first four components are just those hyperplanes that we had before. And then the pre-image of the last three, those P1 cross P1s, are joins of kind of pairs of those components of the point scheme. Okay, so they're not like a, kind of like a secant variety, but I have two separate varieties, right? So, okay, so, and we also have some results that kind of completely describe like which points. That kind of completely describe like which points lie on which lines and which sorry, we have which points lie on which lines and which points are contained in a given line, et cetera. So that's neat. So we have our kind of, but I'm more excited about the second algebra. So these are these other algebras S and T. And again, the origin of these algebras is kind of coming from representation theory. So, but yeah, go ahead. Yeah. Seven components of the line scheme. That's right. Four of them are P2. Yeah. And three are P1 cross P1. That's right. That's right. Okay. So this one is, so this is, these are these algebra. So you can, so S and T here, if you take minus for If you take minus for both of these relations, you get s and then plus you get relations for t. So the reason for these two different algebras is that we investigated whether you could put parameters into all these and get an algebra that was still AS regular again. And it turns out that even with like your parameters throughout here, you only get two possible types of algebras. So go ahead and question. So go ahead. Go ahead, question. The previous one? Yeah, uh-huh. Well, they're mapped into, so this is the, into the Grossmanian. Not sure how they're. I think these are, I believe these are smooth, but I have to. Smooth, but I have to think these are smooth. But I should double-check that, but I'm not sure. I can double-check that after. Okay, so wrong way. Nope, wrong way. Here we go. Okay. So yeah, so that's why there's only kind of two cases here, the plus and the minus. And let me go back to my Macaulay 2 here. Okay, so the so here's. So here's S, and I'll just jump straight to the point scheme here. So there's the formula. The thing I wanted to mention here or see is what the point scheme of this looks like. And so you can see that these are all just points, right? And so this is, we get 20 different points. And you might see this kind of weird thing here, this 10754. So this is an artifact of the fact that Macaulay 2 is much better over. Fact that Macaulay 2 is much better over finite fields than, say, over Q or even an extension of Q. And so you choose a prime, at least for this example, the work, you choose a prime such that it contains an eighth root of unity. And that 10, 7, 54 is like I. And there's another one we'll see in some other weird numbers that kind of play the role of an eighth root of unity. So anyway, so the point scheme is these components, and there's 20 of them. Okay, so 20, 20 points. Okay, so 20 points, and then the line scheme is. Let me run this one here too. This one takes a little bit longer, but it is mainly just to print this gnarly last component that, again, is kind of corresponding to the irrelevant thing. So there's for these, there's the line scheme of S is 10 quadrics. Okay, so and so let me come back over here. Let me kind of just explain what those look like. Just explain what those look like. Okay, so as I said, the point scheme of both S and T are 20 distinct points. And they're also, their line schemes are one-dimensional. So it's a general result that kind of your generic AS regular algebra of dimension four should have a point scheme with 20 distinct points, and the line scheme should be one-dimensional, okay, generically. And so these are kind of And so these are kind of, you can consider these as examples of generic AS regular algebras, I should say, of dimension four. And so, what do their point schemes actually look like? So they're described in terms of these 16 points together with the standard basis vectors, and they're indexed based on the second and third coordinate here. So pjk is i to the j, i to the k, and then q j k is i to the j, and then a root of eighth root of unity times i to the k. And um And so the point schemes are those 20 closed points, so the 16 coming from PJK and QJK, and then the four standard, kind of the standard coordinates in P3. And what does the automorphism do? Well, it fixes E1 and E2, interchanges E3 and E4, and then it sends, at least the one on S, sends PJK to this P, so a different one. To this p, so a different one, um, which, if you kind of go back and chase through, um, you know, what points you have, there's um, there's a couple of points that are exchanged among those groups, and then the rest of them are, I think there's two orbits of size two, and then, um, let's see, three of size four, and then the on the T side, there's uh four other orbits of size four among the q's. So the and then the line scheme, as I said, is. And then the line scheme, as I said, is 10 different quadrics. And they all look like this. So this alpha is either one for S or I for T. So they all kind of have very similar behavior. You might ask, like, oh, well, you know, why are they so similar? Are they, is S or is T just a twist of S? We, I mean, it's definitely not a twist under an automorphism, but we don't know if there might be. Automorphism, but we don't know if there might be some interesting twisting system which twists T to S to T. So we're not sure about that yet, but it seems like that is, yeah, we're just not sure. So anyway, so what is the, what can you say about like kind of the incidence relations? So how are the points and lines kind of mixed together? So it turns out that every point is inside inside or Inside inside or contained in precisely three lines of s. And by the way, I'm kind of speaking loosely, right? What does it mean for a point to be contained in a line? It just means that that point module is a quotient of the line module, right? So what lines are you on? So for the E1 through E4, you're just on the lines joining the other E's. And then you can write down exactly which, given a PJK, which other points you're connected to. Okay, so. You're connected to. Okay, so kind of a really nice description. And what's really interesting, I think, is that the lines that join the points are exactly the lines that are in the intersection of these components. So in particular, if you're a line that is not in the intersection of any of these components, you don't contain any points at all. Okay, so the only lines that actually contain points are those that lie in the intersection of the different components. Oops, of the different components that are here. So that's, we thought that was pretty interesting. And then I've stated this in terms of S with the P's, but the same exact theorem is true for T and the Q's. So the same indexing works. Okay, so, right, so I want to describe what the components of LS and LT are and kind of like give a give a more Give a more, I don't know, a description of the different components that you can use to investigate further. So, let's just, I'm going to demonstrate it with an example. So, let's take this first component. So, this is M12, M3, 4, M1, 3 minus M2, 4. And this is why I subjected you to how to construct these line schemes earlier is because I wanted to, you know, that you to know that if I take a line, right, how do I know that this line is in this component? Is in this component, well, it means that the one, two minor of this thing has to vanish, the three, four minor has to vanish, and then one, three minus two, four, right? The one, three minor has to be equal to the two, four minor. And so you look at this system, and so the, you know, the rank is at least two for any point in P3, and you get it's equal to two if and only if this quadric is satisfied. A quadric is satisfied. So we call that the quadric associated to C. And so, you know, given a quadric, you have two rulings on that quadric. And so we'll call the ruling R1 and R2. And it turns out that if you take the first ruling and kind of take the pre-image of that, or sorry, that ruling corresponds to the pre-image of that component under the map. Component under the map from P5 into or from the Grossmanian into P5. So you can actually, like the, going back to our, going back to the component, like the, the line modules that are on this component are precisely the ones that are defined by these, by linear forms of this type. Okay, so we can kind of completely describe what the line modules are on that component. But you might ask, Component. But you might ask, like, well, why is that ruling good in this one? I mean, I mean, you know that, like, if I take the two different rulings on a quadratic there, they don't intersect, right? And so what do the modules that correspond to these linear forms correspond to? And so we don't really understand that yet, but it seems that they're giving fat points of multiplicity two for the algebra. So, which I think is pretty interesting. But we don't have a proof of that, so I didn't want to announce that to a at least not for all. That to a at least not for all the components. So I didn't want to state that just yet. So, but these description of the point, the component, the elements in that component is useful for trying to do other calculations. And so there's actually another ruling that you can associate to this type of thing. And this is due to a paper of Smith and Vandenberg. So to demonstrate this one, I'm going to take a different component. Demonstrate this one, I'm going to take a different component. I'm going to take component nine. So this looks like this, where I'm going, and I'll be on S, I guess. So alpha is one. And so it turns out that, so yeah, the quadric that we wrote down has this. And so this has this ruling. So now we have a complete description of like what all the line modules that correspond to this component look like. We can easily write it down. But also on top of that, you can That you can, it turns out that you can, that every element in that, every line module in that component is annihilated by a unique up-to-scalar quadratic element in the algebra. Okay, so everything in C9 is annihilated by this element. Okay, so there's some quadratic element in the algebra that you can associate, in fact, to every component. Okay, so. Okay, so for this particular component, the element happens to be central. So, but you have this, you can consider the quotient. And so what Smith and Vandenberg do is they talk about like, given a maximum Cohen-Macaulay module over this quotient, you can look at the Hom space of a shifted copy of M into A. So this turns out to be a two-dimensional vector space. And so you take, you know. And so you take, you know, so this is just a cop, this is just P1. And then you look at the co-kernel of all of those maps, and they turn out to all be line modules. Okay, and so this gives another notion of ruling. And so you can ask, like, what, you know, what is this? Are the two rulings the same? And they are. Okay, so the ruling kind of coming that we kind of obtained by just choosing this quadric associated to the to the to that component and then this. To that component. And then this one is kind of more given by the coming from the arithmetic and the algebra, they give the same ruling. And so in all of those examples, so for each, as I said, for each one of these components, go back to the components again. So for every one of these components, there's a unique up-to-scalar quadratic element that annihilates everything in that component. Some of them are normal. Some of them are normal and some of them are not. Okay, but for all the ones that are normal, you could use their machine to get a ruling of line modules and or I guess a family of line modules. And they all correspond to the ruling that you would write down for that particular component. But there are some that are not normal. Okay, so actually, I think now I wanted to do a Macaulay 2 demo. Oh, I just wanted to show you what those. Oh, I just wanted to show you what those look like. So let's see here. So we're going to run, let's go down here. So I'm just going to skip to the end and let run all this. So this is just some code that actually does these calculations for us. Okay, so let's do this. And then, so here are the different annihilators of each component. Annihilators of each component. So I just use the rulings to choose generic elements on each component, choose enough of them to where it uniquely identifies the annihilator. And the one I was showing you was this one right here, was nine. Okay, and so this column tells you that, you know, it asks, is it normal? So you can see that a couple of the components are normal, or kind of have normal annihilators, but a lot of them don't. And so what's going on with the ones that don't? And this, I think, is really interesting. And this, I think, is really interesting. I hurt myself here. So, as I said, one checks that these F's are normal, but the other FI's, and I didn't show you what the T's look like, but none of the T, none of the annihilators of any of the components of T are normal. Okay. And so, however, you get some really interesting relationships among the ones that aren't. So, for example, you get a relationship like this. So, you can push F1 past the variables of S1, but you pick up. The variables of S1, but you pick up an F4 on the other side. And then you can push F4 past the variables of S and then pick up an F1 on the other side. So it's kind of like a sick, I don't know what to call these things yet, but it's like a normal pair, right? So it's more than just saying that like the left ideal generated by the F1s is also a right ideal. It's like it's a little more structure there. And then for T, you get kind of a cycle of order four, which is interesting. Okay. But Okay, um, but uh, but these um these kind of elements uh give rise to some uh to twisting systems that I don't think have really been studied in the literature as of yet. There's something similar to this defined by a paper in Tran and Van Cliff, but these are some of the conditions that they are assuming aren't satisfied by the maps that you get out of these. Um, and then also the note. also uh the uh the notion of non-commutative matrix factorization due to mori in uyama you can i think you can kind of um kind of generalize that idea to instead of having the same element every time you can kind of like um on the uh say for this first example on all the odd ones you could have f1s and all the even ones you could have f4 um or uh for you know the ones zero mod three have g have g1 one mod four sorry i said mod three zero mod four have g1s one mod four have g four or something mod four have G four or something like that. So you could define some kind of interesting notion of matrix factorization that includes different elements at different stages in the complex. So anyway, so that's, I think, about all I wanted to say and I'm out of time. So I'll stop there. Thank you very much.