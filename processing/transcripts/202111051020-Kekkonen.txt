Thank you, and thanks for the invitation. So, yes, I kind of sneakly added the with Gaussian product priors to the title. Okay, so I will start with introducing what is the problem that I consider. So, this follows very much the setting that the record was talking on Monday. That the record was talking on Monday, and so we will have we will consider one PDE, which in this case is time dependent, and then I will look into the case that does it actually, does the Bayesian solution, is this consistent, does it make any sense on the limit? Okay, so let's assume here that I just said we have an open-pounded sense, and as I say, this problem is And as I say, this problem is time-dependent. But so let me see. Okay, so the problem we consider is a kind of a heat equation with additional cooling term F. So the F here is the unknown that we would like to recover. And of course, we will be using then a Bayesian inversion to recover F. And the question will be that, well, when we put a Gaussian prior on F. We put a Gaussian prior on F, does the posturing make sense? So, does it concentrate around the true F that we think that actually generated this measurement? And yeah, just to mention that we assume here that we have some boundary measurement G, and we have some initial measurement U0, and these are assumed to be known. So, basically, if you think that you have your set O, which is the kind of where the X takes values. Of where the x takes values, and you then have added the time, so you get this kind of a q which is sort of like a time cylinder. So it the o and the set o stays the same regardless of the time. Okay, so if we assume that the boundary and the initial values are sufficiently regular and positive, and I say here that they need to satisfy some natural consistency. To satisfy some natural consistent conditions, this just means that because obviously they meet on the where the time starts and when your boundary ends or starts where your boundary is. So they need to meet smoothly enough there. So obviously they need to be the same value, but also they need some extra smoothness on that boundary. Well, in that case, also, if we assume that f is positive, then we know from the PD theory that there exists a A theory that there exists a unique positive solution UF to the forward problem. And this, what I mean by this C21 is simply that the solution has two time, sorry, two space derivatives and one time derivative. Okay, so we know that the solution exists, but then of course it would be sort of nice to have a little bit extra information about the solution. Well, one way we can write the solution down Write the solution down is using a so-called Feynman-Park formula, which is printed here. So let's take an Xs, which is a deep dimensional Brownian motion, and we started in some point X, which is in that set O. So what does this thing is? It basically wanders around in your space, so in this kind of a cylinder queue, and in some point it exits. Some point it exits the space. And we assume that this exit time is finite. So, of course, when we consider the solutions you have, which depends on the x and the time t, there are two possibilities. Either the t is smaller than the tau, so the exit time, which means that you are kind of considering this solution before your Brownian model. Solution before your Brownian motion has had time to exit the set that you are in, it wanders in. And in that case, this solution is kind of has effect from the initial value u0, because this is the brony motion has not yet exited from the q. Well, other option is, of course, that the t is larger than the exit time. So this means that you're considering the solution. means that you're considering the solution at time which is after your Brownian motion exited from the queue. And in that case we get this part where we consider the boundary value g and here we can also see that so remember that f is the unknown that we would like to recover. So well the solution here is we have some part of the u zero but then we have the exponents and integral of the f. And an integral of the f. So, this is definitely a non-linear universe problem. Okay. Now, so we would like to then use the, well, we want to recover the f of some measurements. And of course, there are two ways you can consider the measurements arising. So, one is that you would assume that you That you observe the UF, so the solution, the PDE, with some additional noise. But in real life, you don't generally have measurement devices that could actually give you a full function f. So I'll consider a case where we assume that we only observe the measurement, well, the UF in certain points. And these points are, this is again, These points are, this is again kind of in the similar setting that Ricard was explaining on Monday. These are chosen randomly. And remember again that this problem is time and space dependent. So I sample uniformly from a uniform distribution in the queue. So both the X and the time t are sampled randomly from the uniform distribution. Okay, so then the statistical question. So does this solution make any sense? And what does this mean? What I do by that, does it make any sense? Is that is it consistent? So if I make a measurement, in this case, that I make some finite amount of measurements, and when I make more and more and more measurements, does my posture distribution start to kind of concentrate around the true F0 that generated this measure? That generated this measurement. Because if it doesn't, then of course, this is not a very good solution. And of course, also the question is: on what speed does the deposit distribution concentrate around F0? And here are just some results from the recent years for the non-linear inverse problems. These are exactly the same papers you have seen several times during this week. And I'll be talking about the first one. first one so where we where I will be using specifically Gaussian process briars and the reason for this is because these are well they generally are used in in practice so of course it it it's an important question to ask do they produce consistent posterior measures okay so I'll then be moving to the rescaled Gaussian priors so of course we need to assume that So, of course, we need to assume that the unknown lives in a certain type of a set or a certain space. And I assume here that it is in some solar space H alpha. I say it is positive. And I also fixed that in the boundary it's one. Well, this is, it doesn't need to be one, it could be some set function. But for the simplicity, let's assume that it is exactly one on the boundary. One on the boundary. And additionally, I also assume that it kind of arrives to the boundary smoothly enough. So there is no kind of weird kinks on the way there. So this is a type of assumption that kind of makes sense. There is no reason to assume that my F would be, for example, zero specifically on the boundary. But this is a little bit problematic because I want to use Gaussian process priors for the F, but they come. But they kind of naturally live in H alpha C's, and also meaning that they are compactly supported in the set O. But as I said, I don't have any reason to assume that this would be true. So there is no reason to assume why my F would go to zero when it gets closer to the boundary. Well, the solution for this is using a regular link function, as were introduced by Rugged Nickel, Sarvan Gier and Sven W. Nickel, Sarvan Gir, and Swan 1. And what these do is they kind of give you a nice convenient bi-check reparametrization of the F. So they split it into the, well, in this case, capital F and the link function. And once I do this, I then now consider a new solution map. Well, this is exactly my old solution map, but now instead of having the F here, I just Having the f here, I just have written the f using the link function and the capital F. And why this is very convenient is because using these regular link functions, we can actually write the original F alpha that I defined here for the F's as being kind of a composition of considering capital F's, which now actually live in these suball spaces. These suball spaces H alpha, but have a compact support. And then, of course, I bring them to the small f using this link function. So now, instead of using the Gaussian process prior for the original F, I can actually use the Gaussian process prior for the capital F, and this kind of naturally makes sense. Okay, so what is the Bayesian approach in this case then? So let's say, I say, we have the capital F, which is now we know that it's in some sobular space H alpha and has a compact support. And so we can choose some borer probability measure pi, which is supported in continuous functions and with certain Kamer-Martin space H, which I'll Common modern space age, which I'll choose later. Okay, so in that case, we then, after we have chosen the prior, we make a measurement. So in my case, again, remember that we have the measurement yn, which are actually measurements, but we also need to condition it on the points where those measurements were made, because these were also random. So in that case, we get the posture distribution. We get the posture distribution, and here the kind of the important part to notice is that we have the log likelihood function, which is given by this thing here, which is again, this is just basically the data fidelity term, which you would see, for example, if you are doing regularization. So, formally, when we do assume the Gaussian prior, we can write the posterior distribution in the following form. Form. And here now it's again clear to see that if you would be taking the negative logarithm of this, we actually get just kind of a regularized solution. But because I assume also that my prior is Gaussian, that means that the posture distribution will also be Gaussian. So the map estimator, so the mode, and the mean of the posture, so the conditional mean estimator actually. Conditional mean estimator actually coincide. So we can kind of be talking either of the map or conditional mean estimator in this case. Okay, so then what kind of a prior should I choose? So I assume here that we have some alpha. So this is remember where the where kind of will assume that my Will assume that my truth will belong, so H alpha. So alpha needs to be greater than some beta plus dimension over two, and beta here is greater than two. So this comes basically from how many space derivatives you have. And then we assume that the H, so my Kamer-Martin space is a Hilbert space, which is continuously embedded into this space Hα C. And then I'll take some kind of a base prior, pi comma, tilde, which is a centered quotient probability measure on the continuous functions, which is supported on a separable measurable linear subspace of C beta. So this is again, so it is at least two times differentiable functions. And then we assume that the reproducing kernel field. That the reproducing kernel Gilbert space of my base prior equals exactly this H, which was embedded into the H alpha. And where the rescale comes, we will scale this prior on the amount of how much data I have and what the exact scaling is here. And just to note that the H n. The h n. It has the basically the norm is given by the norm of the f in h, but it's again just scaled by the same factor as for the prime. Okay, so this is kind of a very abstract sounding. So let's have an example. What exactly does this mean? Does it make any sense choosing this kind of prior? So we can consider, for example, Whittle Matten process prior, where we have Where we have the x again in the z O and smoothness alpha minus damage over two. Now, if we assume that the f0 is in h alpha and it's supported in some compact set k, then we can take some cutoff function so that well it has one value one on the set k and then goes to zero. Then we can take a Then we can take a base prior where we basically consider this little matter process prior, but now so that it's cut off using the cutoff function. In that case, it's supported in a separable linear subspace C beta1, which is where the C beta1 just needs to be slightly larger than the beta, but still smaller than alpha minus dimension over two. So basically what this means is that then This means is that then assuming that f0 is in h amounts pretty much to the standard assumption that f0 is in h alpha and is supported in some strict subset k of the original set O. And remember that this F0 was the kind of, it's not the original F, so this was created in the way that we can assume that it is compactly supported in. Compactly supported in O. Okay, so this brings us to the first result. So, posture contraction around the true potential F0. So, we now have created the prior, but the prior was for the capital F. So, I'll use a push forward for the actual potential F. And in that case, you can actually show. And in that case, you can actually show that, yes, the posture mass will indeed concentrate around the true F0 with a certain speed. And the certain speed is given here. Okay, so of course, this requires a couple of other estimates to get here. So the main kind of The main kind of contribute or the main things that we require are, as mentioned a couple of times earlier, is well, first we start with the forward estimate. So if we know that the F1 and F2 are quite close to each other, does it mean that also then the GF1 and GF2 are quite close to each other? And in this case, so remember that the G is a So remember that the G is a smoothing operator and also kind of a very non-linear operator. So since we are considering the distance of Gf1 and Gf2 in L2, it means that here on this side, we actually need to be considering the distance of F1 and F2 in some rougher space. And in this case, the space that is required to throw this is the dual space of H2. Space of H2. And again, the 2 here comes from the fact that we were considering two space derivatives. And one thing to remind is that F here is not time dependent. So we have this cooling term, which we assume that it's obviously space dependent, but it does not change with respect to the time. So if you had a time-dependent F, the whole thing would come even more complicated. And the one other thing is that here, The one other thing is that here you would need to be considering spaces where you take into account both space and time. Okay, so when we have the forward estimate, we can use this to prove that we have the kind of a similar theorem to this, but here considering actually the UF or the GF. And the speed would be exactly the same. Other thing, Other thing that we can use, other thing we get from using the forward estimate is that the posture actually concentrates into some poles in the C pizza. So this means that I was looking for the F0, which lives in some H alpha, and I can show that the posterior mass will be concentrated in some post in C beta. Some both in C beta. So, but notice that the beta here, so the z beta is slightly rougher space, slightly larger space than the h alpha. And the other thing, of course, since this is an inverse problem that we need to prove, is a stability estimate. So in this case, so what we need to show is that the distance between F and F zero in L2 can be bounded above by Above by the distance between gf and gf0, but again now we need to consider the zilbert spaces where we have both space and time derivative. So then, well, we have shown that we have the contraction around the truth, but what we often consider is, of course, that we want to use some point estimate. Use some point estimator, and since we have a quausion prior and quotient noise, the using either conditional mean on the maximum purity estimator will give you the same estimator. But so we can also show that the postural mean contracts or conjuncturates close to the F0 with the exact same speed that we got earlier for the contraction. For the contraction. And using the link function, we can also show that well, we don't need to only consider the mean for the capital F, but we can also consider, well, how fast do we actually go to the actual F0 using the link function? So we have now shown that, yes, the posture contracts around the truth, and we can get the kind of the same speed. Can get kind of the same speed also for the mean. But the question is then: that is this good speed? So could I do better? Could I have the estimators that actually converge even faster than this? So far, this is just a kind of a bound for it. So this brings us to the lower bound estimates. So actually, we can So, actually, we can show that for this problem, we can calculate the minimax lower bound. So, this is basically the speed you can, the best speed you can get for the worst possible true F that could actually generate this measurement. And just one thing to point out: that this is the lower bound that holds for any estimate for F. So, not just this Gaussian estimator, not just Just this Gaussian estimator, not just kind of using these techniques, but any estimator that arises from these measurements, it can't kind of converge faster than this in the minimax sense. So if we now look at this and compare to the previous one, someone might notice that this is actually not the same speed. And this is true. So the above. The above minimax rates can be also written as delta power alpha divided by 2 plus alpha, which is not exactly what I had earlier, because earlier I had beta divided by 2 plus beta. So what I had earlier, the speed would be optimal if I could replace the beta by alpha. But this is something we definitely couldn't do because we We definitely couldn't do because we specifically assumed that alpha is greater than beta plus dimension two. And so, why does this happen? So, this is the kind of step 1b that I pointed out that what we needed to prove the term earlier. So, we are interested in recovering F0 that is in H alpha. So, it has alpha weak derivative. Alpha weak derivatives. But what I showed earlier was that the posterior mass concentrates in C beta balls. And the beta here is smaller than alpha-monster blue. So it's unclear if this is something that comes from the proof. So it's unclear, could we actually prove that the concentration happens in the ij alpha? But also another question is that But also, another question is: do we want the minimum speed? Because even in the simple kind of linear cases, we know that if we consider kind of smooth enough priors that produce the minimax convergence rate, if you then look at the coverage of the credible sets, it's actually zero. So basically, the credible sets, for example, Basically, the credible sets, for example, for the priors that would produce the minimax speed, they are not very good for, for example, for uncertainty quantification. But I try to kind of overcome this problem by considering truncated Gaussian priors, because there we can show that the posterior mass actually then concentrates in some H-alpha poles. So these truncated Gaussian priors. So these truncated Gaussian priors will be based on kind of the same base prior, the kind of the infinite dimensional prior that before, but they will be finite sums and obviously that the sum will be growing the more and more measurement I make. But this truncation means that even though I use these rougher priors, the truncation kind of gives me additional smoothing by cutting off of the highest frequencies of the sun. Of the sum. So, this is also the reason why we can actually get the minimax rate by using the kind of same base prior. Okay. So here's just how we kind of create the prior. So basically, I will be using WG valids. I make an extra assumption. So, what we assume here is that now the F0 is in Is that now the F0 is in H alpha, but now it has a compact support, so I fix some K that it needs to be supported in. And of course, now I have fixed, so it's non-zero in some K and zero everywhere else. So I don't need to consider those wavelets which are kind of outside the set K full. So the truncated Cauchy priors are created as a sum. And so for any fixed j, this is of course a finite sum. And the j is chosen in the way that the more and more measurements we have, the larger and larger the j gets. And in that case, we can then consider We can then consider this kind of new base prior. But the one thing to notice is that we now have a HJ, so this space is now independent J which are spanned by the wavelets, the topic wavelets here. And so these are all finite dimensional spaces, which and the dimension grows the more and more. Dimension grows the more and more J grows. And I will be considering basically exactly the same prior as before, but now my base prior is this truncated Gaussian prior instead of the infinite dimensional prior. And the scaling here is exactly the same as it was before. So in Oh, sorry. Okay, so in that case, we can actually show that now if we look at the again the conditional mean estimator, so the mean of the posterior, in that case, we actually get the exact minimax convergence rate. So for the infinite dimensional, it is a bit unclear if the prior is too rough to get the minimax convergence rate, but this can kind of be. kind of be helped by considering instead of considering the full prior by considering a kind of a finite sums where the amount of summing goes up the more and more measurements we get. So just to summarize, so we consider Bayesian approach for solving a non-linear parabolic inverse problem of recovering the absorption term or the cooling term F in a heat equation. Germ F in a heat equation. We showed that a minimum slower bound on the rate estimation for the F. The one thing actually I don't think I mentioned is that the rates we achieve for the forward problem is minimox optimal. But if we use these infinite-dimensional continuous Gaussian priors, the The rate for the inverse problem, so the contraction rate for the inverse problem is not generally minimax optimal. The smoother and smoother you take your prior, the kind of closer and closer you get. But for a general smoothness, it's not the minimax optimal, right? So as I said, it's not sure if that's necessarily what you always want. Necessarily, what you always want. So, the problem there was that we want to recover F0, which has a certain smoothness, which is the H alpha smoothness. But what I can show is the mass concentrates in C beta balls, where the beta has to be smaller than alpha minus dimension over two. But this problem of not getting the minimax rate can be recovered actually instead of considering the full infinite dimensional Gaussian prior. Infinite dimensional Gaussian prior, considering the truncated and rescaled Gaussian priors, and then we can attain the minimax convergence route. Okay, thank you.