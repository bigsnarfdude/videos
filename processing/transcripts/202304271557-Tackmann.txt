Do you make toys from the observed counterpart and subtract the asymmetric? If you have some, if you have an asymmetric? No, no. So maybe let me start somewhere. Yeah, yeah. So the toys are generated according to your observed counts using the Poisson statistics. And then you subtract the observed counts so that you basically don't consider the data fluctuations. And then you inject the asymmetric data set and then you put a fit. Okay. Okay. I would have thought that if you have something like maximum data scene, couldn't you just use that model to generate always? No, no, because here you are trying to do data model compatibility. So you want to have somehow the properties of the data, but you don't want to have the fluctuation part of the data. Okay, the other one is maybe easier. On the right you've got this quantity WL1x3 mean over the twice. So quite often we use something, I don't know if it's the same thing, but we use basically the difference between the negative twice dot likelihood for a particular model and subtract the so-called saturated version, so where you replace heavy. Expectation of the data. So that's then usually gives you something that looks like a tight code. But this looks like something different. So you just expect the minimum over all the tools. Yes, that's correct. I don't think I've seen it before. Or is it just a convenient way to shift the x-max or something? No, yeah, I mean, it follows the chi-squared distribution here. But I don't know if there is a reference, but this is something. There is a reference, but this is somehow yeah. I don't remember saying that. So this is somehow a method that I have not seen in the CMS and Atlas analysis, but that we use in order to make toilets from data has fluctuations, which we do. Can you explain this a bit more, like what is happening here? I mean, it's only interesting, but what is going on? You mean expand the flow? Can you explain for an expert what is happening here? Yeah, so we are evaluating the minus rule of likelihood for each toy, and then we toy and then we get so this is the minus two log likelihood but with the the mi uh the the minimum value that we got. So it's not minimized over toy you're not finding the toy data set that is like as some of the minimum value, but you're for every toy you're minimizing. Right, correct. So I think it's in its notation. So it's a minimization over the parameters. We do something similar on TTK. Maybe it's just experimenting in Japan, but we do something similar. Yeah, maybe that's the interesting tool. Curiosity, although it's trivial, can you find since it's uh I find uh broadly all the right uh of uh exceptional deduct deductible uh value. Didactical uh value. Uh I wanted to ask you if if you didn't use software it's uh somewhere. Uh which one? This one? One to the right. Yeah, sorry, I should put the source. This is Wikipedia. That's the default. So yeah, this basically shows you it's not uh it's not uh um precise, it's just wrong. I mean the feed doesn't seem to be paid to anything very like very this only shows you the well. This only shows you the what we call the three contributions and um yeah this basically encapsulates the CKM matrix if you're you know yeah but if you look at this likely most likely map every work has a decay that has some probability representation. What exactly is the colour telling you? Because I was reading this as meaning that 99% of the time that T goes to D, 95% of the time that C goes to S, but also a little bit C goes to D. So where does the D go to? It has a very long life. Okay, I I hope I also have a long life, but I don't sit so I start looking at me. I think it's not it's not it's not it's not okay to answer that question captures the light I mean it captures the the CKM element strength that you have yeah but there's unitarity so the fee must go somewhere I mean it goes so it's an absolute no no because it's not normalized yeah all right I was gonna yeah I saw yeah, yeah. So flavor physics is one of these industries which is very intensive in terms of averaging and combining experiments, and there's even dedicated groups to this. I'm just wondering if you could say something about how the treatment of systematic errors is done in such a way so as to facilitate that type of work that we screen on. Yes, actually this is my favorite topic. I I have been on both experiments and I think we are really in the unique way on both experiments. On both experiments, to really make combinations and to think about treatments of. For example, I mean, you talked about correlated systematics, by which I understand that to mean that there's a nuisance parameter that's at least partially common to two different measurements. So if you wanted to do a combined analysis and some multiplied likelihoods, we'd have to identify that Moose's parameters equal to one Moose's parameter. So, are games like that played in the heavy flavor averaging every year? So, I don't know really heavy flavor averaging group. So, I can't really talk for them, but if I was the heavy flavor averaging group, yes. But I think for this to really happen is we would need to have representatives from both experiments and really discuss because one of us are doing relative measurements and one of us are doing. Measurements and one of us are doing absolute measurements. What we maybe mean by the nuisance parameters is not exactly the same. Just because they have the same preference. Exactly. And I think this really stems from the fact that one of us are doing relative and one of us are doing absolute. But I mean, I am very much a warrior for the public likelihoods and putting them out, and I personally would be very interested. And I personally would be very interested in making combinations. I mean, maybe most of you are familiar with the fact that there are or there were some anomalies, and actually both experiments are able to measure the same thing. So we are really, we are coming from two different detectors with different things. And the earlier we agree on how we define things and what is correlated in terms of the nuisance parameters, the better. Parameters, the better. I mean, yeah, that's the way how we can get size sigma. I mean, if we both work together, hopefully. Is it related to this? Because otherwise, I would follow up with a very small question. Do you know there are some, since you mentioned the anomalies, there are some fits done by independent groups trying to do, I don't know, these interpretations. Do you know if they ever run into problems exactly because of this? Or is it important how they quality or not quality absolutely from the different experiments? I I don't know, that's a pretty good question. I mean so far so far on the reported animals there is only I mean LHCb has only the measurements. So BEL2 or Bell is not yet competitive in that area at all. So we are not in something that might help at a pair of messages. I just like with the kind of two ways of model building, like the simulation base. But to me like in my mind it was always some some of flavor physics was extremely comfortable with the functional forms that they use and somehow you know the idea of like Chevy Shift learning always press the bolt and so on. And I always found it kind of impressive that you can do the unbidden version because you somehow know it functional. I wanted to ask so to what extent Ask, so to what extent is there a discussion about the right function? Like the discussion we had in this workshop, you know, should it be like this or whatever, some spline? So, to what extent are these discussions happening, or is it just that you know? I mean, there are this, so once you are doing the unbind uh model, um there is a discussion on what functions to use. I don't think uh necessarily it's maybe um Necessarily, it's maybe uh yeah, I don't want to say something that's wrong, but for in the rarity case, um I mean, if you have peaks like this, you know, if you have some errors on the background, I mean it's not such a big deal. I mean, if you're trying to write, but of course, if you have a smaller peak, there the discussions become a little bit more prominent. So, it really depends where in your search or measurement. Where in your search or measurement? So I was kind of asking what is like a typical story look like. So if you look at this plot, I guess there's like a small Chebyshev thing on the left. So what is like the modeling story? Do people just look at the data and they're saying, okay, let's do this combination. It looks kind of like a Chebyshev? Yeah, yeah. So I mean, usually you have a double crystal ball that parametrizes usually your signal because this is also a physical interpretation. A physical interpretation. This catches your tails that are coming due to the resolution effects. So there is a lot of physics thinking about it. And I mean, there were papers written, you know, to capture the parametric forms that are based on the physics reasoning. So this is why you have color generation. Then, when it comes to the backgrounds, you have, so this is an LHCb plot, this is L2, LHCb. To LHCB. So usually you have a misidentified background. So this usually is coming from some control sample. And there the shapes are, you can use KDE estimates or other things for that. And then you have combinatorial background, which you combine three particles and they look like you're being and this is usually Yeah, like your B, and this is usually exponential. But then again, you can start to play it with other functions. But yeah, for the signal, it's usually physics motivated. And also for the peaking backgrounds. Yeah, I mean, this reminds me also this discussion with the backlash. Feilest. Where we had this double beta decay, and we got the question over the spectrum, like file events, spectral events, and what to do in that case. And you have also to analyze a very rare channel, that's where you start this time from that. You might have just two events in the whole spectrum. What are you doing in that case? Yeah, so for the rare decays, I mean, yeah, so this was what I was trying to also. Yes, this was what I was trying to also mention in my last slide in terms of the smoothness. So, what one does usually is we just say, okay, we allow more backgrounding, so we don't try to optimize for the signal background ratio, but we really, I mean, we prefer in that way to have a fit stability, so we are not doing the best measurement we can, but we at least don't get a big uh error on the shapes of the functions. Of the functions. But actually, I have a small backup slide. This is a big problem actually in the data dark matter searches that we also do at VEL2. Here the problem is that you have a signal that has a basically it's like a delta function really. And so here you are really sensitive to single events and then you're trying to interpret this as an R. And then you're trying to interpret this as dark matter. Yeah, you. Here, I mean, this is a I don't want to go too far away, but here when we are looking for dark matter, we are usually looking for displaced particles, and most of our bees are produced in the inner part of the detector, and this is also where all the background sits, right? So, here we don't have so much background, but then we are having these kind of problems. problems. So so yeah, uh sometimes uh having very little background is actually awful. Yeah, so I I don't know, this is uh somehow a discussion that we also have in in where I work because there are people who are working on dark matter and there is a discussion whether to yeah whether to have more background or whether to have zero background and yeah you have to think about these things. No background is a nice problem to have zero background Zero background is a nice problem for that. I think very low background is a bad at least in what I have seen so far. Maybe I can ask one more question. It's a bit difficult one for me myself. I mean, you see the profile frame colours, right? I mean, we had in CMS Angular Analysis P5'. Yeah, she mentioned P5' on our Angular slide. Yeah, she did. Yeah, and I think I looked up some previous some NHCD papers and so on, and then I read that you know you measure simultaneously the signal speed with background and then you measure this simultaneously in the five-dimensional angular phase space and then you single out one party of interest each time. We can easily one of these five different parameters and the rest of your potlining. And then I think you are wondering about the television of this, right? That might not be so well known. I think you made often the statement that this is not very normal. But was never studied. You question me? What I remember of the P5 prime analysis in CMS was because that was where LACP had seen the anomaly. Seemed an anomaly. In the end, CMS did not have enough statistics to distinguish between the standard model value and the anomalous value that LACB was claiming. So at that point, my interest was lessened a little bit. But up to that point, they were using so-called Feldman cousins because to me it was exactly the case was designed for you. You had some allowed region. For you, you had some allowed region in parameter space that was a triangle, the interior of a triangle. And with a small amount of data, the so-called best-fit value was perilously close to the boundary of being inside a triangle. So the only way to guarantee coverage by construction is converting the likelihood ratio test. But in fact, I think LICB had used that. That. Later, LA CD remembered at ViceDat Flavor, they had stopped it of using something else once they have all the data. Once you're safely inside the physical triangle, your MOOC starts to work. But I don't remember the specific question you're asking. No, I mean, I wanted to actually ask you, of course, but it's a very important topic but this I mean this using this profiling for This using this profiling for the resource parameter but also parameter interest and for custom construction will say it's important. Yeah, I don't I can't because it's not I don't have an expertise. I'm not familiar with the debt measurement in to that detail, but I know exactly what steps. So back to this thing about the disadvantages of having a low statistics. Of having a low statistics or low background. What Enzo showed with the higher-order asthmatics was in a completely different context, but actually that technology could be used. Yeah, I think it could be used here, right? Yeah, so I'm given that you have some. Yes, yes, I realized actually that this would be very useful and one could use it here. It's the type of technology that's had some attention in particle physics, but not enough really. that that very often we're looking in regions of phase space where the expected number of background events is one or less and and so it would be very useful I think to have ways of pushing asymptotic like formulas down into that region. Yes definitely. Yes. I'm really interested in the background like because it's kind of similar to my experiment, the buckle of xenon under a mountain. So do you understand the nature of your background? Yes, I removed it from the plot. I removed it from the plot for now. Yes, but there are, these are some processes. Yeah, some of them are picking and they are actually really hard to... I don't know maybe what exactly you are asking. Right. So essentially you don't have a good background model between them. No, no, not in this case. Right, but even in it's a bit out of fashion now, but I shouldn't say this public fee, but even there are yelling methods. But by you know there are yelling methods, right, in which you don't have a nice background model but by by the very fact that you can you see empty spaces in your observable you can put some upper limits on the parameter space. That's what Xenon used to do like ages ago. Until we developed a nice background model, then we used the likelihood ratio test and stick. And even if it's not in the Wills region, right, you just spend a bit of money and then you toy MC out your critical threshold. Your critical threshold. That's why we do it, and it seems alright. So, how is it? How can you remember? Can you repeat how it is called? Which one? The yellow? If you have no background model, you can use the yellow optimal interval. Okay. Yeah, I want to just follow up to Glenn. So I've been wondering ever s so since this talk, so why are we using these uh high order asymptotics? So can you say a bit about this history? There has been some attention part of the Some attention part of the. It seems like we could just build this into our fitting tools. I mean, what we've been using for a long time are things like profile likelihood ratio statistics, which we think should asymptotically follow a cross-current distribution or something to data. And then what we see is for very small data samples they type. And so then what do you do? And what we've learned from people like Alessandra and Anthony is that one of the things that you can do is just to find One of the things that you can do is just to find a new statistic that is scaled and shifted in such a way that its sampling distribution is closer to the asymptotic form. And it seems kind of naive to my ears, but it turns out to really work. And then there are much more complicated techniques as well that you find in the future. No, yeah, so I guess I'm asking why are we not kind of making this a default. I mean, so people are kind of abusing asymptotics a lot. I think people A lot of the thing, like, people like S and don't use it just fast. And so it seems like we could submit something to Roofit or whatever statistics tool to kind of make this default and we use it. I think we should investigate. Actually, Anthony did a similar study for one of the earlier advanced challenges and saw people have looked at this at some level. Somehow it just hasn't taken hold and gotten into the community's tradition. But there is more weapon involved. It's not like Working in bulk. It's not like you can just an extra thing. You have to throw some toys. Yeah, no, I mean, it's a little bit more extraordinary. Absolutely, yeah, no, and some of the, so you've got higher-order asymptotic methods that involve simulation, and you've got other higher-order asymptotic methods that can be done with paper and pencil. They're hard. I guess my bonus, so it's just not like the toolbox of possible cross-checks. So people know there's either asymptotics or kind of toys. Kind of toys. And so, you know, like the conveners, they're helpful. They're like, okay, do like a toy-based thing. And so nobody's ever asked, do like the second-order. I think this would be an interesting thing to start, change the mindset. Already the background shape, and but we've discussed also in this week with two methods adopted by Atlas on CMS: the specific profile, and one is the sports view signal. Finally, I mean, what do you do? Do you do just some variation of the functional shape, or something more? Yeah, so I think for now we are doing, as I said, variational functional of the shapes which may be all as the. Okay, maybe just since you've been in both. I mean, since you've been in both experiments, LHCP better, you can maybe tell us about differences about the system like frequency. Beyond this what you discussed, that they are using slightly different observables. But is there, or is there something interesting we discuss all the time between Atlas and CMS, but maybe what we should do or do later? Is there something that there's a fundamental difference in the In terms of the the types of the systematic, as I said, this is ones you have relative and you have absolute these are the differences. I would also say that in generally, I think for the rare decay searches, LHCB still uses unbind maximum microbits. Much more, Bell 2 doesn't do it. But also, this comes back to the argument of how good the resolution is. How good the resolution is. Bell two can also do inclusive kind of measurements, and then the resolution is not good, or you cannot basically describe it by the functional forms. I don't know what I think there is not much difference apart from the fact that there are much many more bint fits in Bell 2 and more functional forms at LHCP. Just one question from curiosity on what you showed with the decomposition in the eigenvectors for the uncertainty. Instead, you pick three, you must have had some criterion to decide on this. I was just wondering, how did you decide on three versus four or fewer? I think, okay, so this was not so this so here we had the matrix for the follow-to-focus. For the full coherence, and on the right for only the three leading. And okay, the criterion was that we see that it captures most of the correlations. There was not a measure, I think. Okay, just look, just look, yeah, that is that this was. We are for this measurement, we are statistically limited, right? So at some point this will become a really important Can I ask a question just out of curiosity? When you have these very narrow scene distinctions, how do you model, like the width is considered text, or the progressive lid that converge from delta Virus? So for instance for this dark matter where you have very so this is modelled usually by a Cauchy function. And the weight states like And the weight states, like you just assume it about 0.00 sample? Yes, I do allow it to change with the sample. Like it gets narrower the more sample, the more observations you get? I think it's fixed by the detector function. But I think this is modelled really by Cauchy function. Any other questions, comments? And maybe a general comment that is kind of a bit out of scope. So one of the things that we always show is that we have like the simul or like when this template morphing idea, we always kind of show, okay, we have like the Newton's parameter space, and in principle we could evaluate it everywhere in the like your three methods kind of assume, okay, you could principle evaluate. Kind of assume, okay, you could principle evaluate also the jet energy scale at 0.745 or something like this. And so, somehow, practically, in our actual reconstruction simulation frameworks, like in our simulator, we cannot actually set the completely random value of somehow. The structure of the way we write our software mandates this on-axis simulation. So, wonder whether or not that's limiting us. Good. Right, so we are I can only set jet energy scale up or down. I can't set it to 0.7. But I mean, like in the I mean like in the tooling that we have, I mean principally, but the tooling is I don't know that maybe we should change it to you mean, yeah, how you really this is uh very Atlas and CMS uh related, yeah, because in uh at uh Bell 2 weekend. Because at Belto we can actually at Beltu we can do this. Software is written in such a way that you can do this. Sounds like a U problem. Any other question or final? At the end we can come We're at the end. We can come to the end of this session. Thanks again, Tony.