We had this exciting workshop. So, I'm going to talk about our recent work in which we benchmarked several methods for inferring hidden variables in molecular QTL mapping. So, this is the work done by my PhD student, Heather Joe. I will acknowledge her in the end. So, first, what is the QTL analysis? So, I think given that we have a lot of omics data, so now the Of omics data, so now the traditional quantitative trait locus analysis has gone to this molecular phenotype level. So, here every phenotype is a molecular feature, such as a genes expression. So, this is what we use for gene expression, QTL. And then the molecular phenotype can be a genes alternative splicing. Also, it could be the genes 3' ETR, alternative polyadenonation phenotypes, and so on. Denonation phenotypes and so on. So there are many, many QTLs because of the data availability. And so the association is done between one molecular phenotype and one SNP. And the SNP is encoded as 0, 1, and 2. So the goal is to find how can we explain the variation in a molecular phenyl type using a SNP nearby. So this is called a cisQTL. So in the left one, So, in the left one, we want to say, oh, this is a QTL because we eqtl because we see association. On the right, we don't see an association. So, that will be the finding. And why do we need to incorporate hidden variables in this association analysis? The reason is that people believe there are some technical or biological variables that may confound the association if you don't consider them. Let's say if you have 200. Let's say if you have 200 individuals from, say, the G-Tech data, you simply run the association between the gene and the SNP. You may find strong association in many cases. But people may wonder whether the association is driven by some factors we don't know. So that's the motivation of doing hidden variable inference. And the variables are inferred from the individual by gene expression matrix. So this is the this is This is the basically figure from GTEC, in which they use peer factors inferred from the gene by individual by gene expression matrix and include a number of peer factors as covariates in a linear model where the Y is the genes expression, the X is the SNP of interest, and a bunch of other factors to adjust, including the peer factors. So the goal is peer factors. So the goal is to improve the power of QTL identification. And this is the original peer factor citation. And empirically, they found that this did work. So by including peer factors, there are more EQTLs found. And if a gene is having more than one significant EQTL, we call this gene E-genes. So this is reporting the E-gene number. And we see that. Number and we see that for different tissue types when we run this association because the gene expression or genes expression differs from tissue to tissue, we could see that different numbers of e-genes were found. And in particular, I want to note that an interesting thing that has been done in G-Tech is that the number of peer factors is determined by maximizing, to maximize the number of E genes. So to me, this is not something that natural to think about. That's natural to think about because we're using the results to decide the number of factors to include. But that will not be the focus of my talk today. For the talk today, I will focus on comparing peer with some other methods that will infer hidden variables from the individual by gene expression matrix. So we were interested in this question because my student Heather, when she learned that G-Tech used P factor, she carefully studied the PR factor. Carefully studied the PR factor paper and then she found some related methods that can do the same job. So, PR factor is the most popular method used for QTL mapping. And because it's perceived, it has some perceived advantages, which is that the performance of pure factor in reporting the original paper is that it does not deteriorate as the number of inferred covariates increase. So, you can see from here, right, there's You can see from here, right? There seems to be a roughly monotone relationship with the discovery e-genes, even though we have some saturation part, so we stop here. But you don't have that fluctuation that much. So that's a peer factor used in GTAC. And earlier, there was a method called surrogate variable analysis SVA by Jeff Fleek and John Story. So this is one of the first method that infers hidden variables and then Hidden variables and then use it in the gene differential expression analysis to gain better power, even though it wasn't designed for the EQTL, but still the idea is similar. We want to learn some hidden factors from the individual by gene matrix. And finally, there's a newer method called hidden covariable prior. We knew this method because our colleagues at UCLA used this method instead of peer for their hidden variable inference in their QT analysis. Inference in their QT analysis. So that's why we also considered this into our comparison. And reading those papers, we found that there are commonalities that they all have PCA underlying their methodology. So then the question we had would be, which method is the best? Does Peer have the perceived advantages? How would they compare with PCA? So that's the starting point of this benchmark analysis. So, this is the result. Actually, we look at several criteria, including this computational speed, the performance. We have several criteria based on our simulation, how interpretable the method is, and how easy it is to choose the number of peer factors, and also the software, including stability or easy to use. Surprisingly, we found that PCA is the winner in almost Is the winner in almost every aspect. In particular, we were very surprised to see that in our very different simulation designs, I'll introduce the next slide, two simulation designs we considered, PCA has the top performance. And so here is the paragraph that shows the input required by each method. So basically, the variable interest is the X, and in our QTL case, it's the SNP, and non-covariance. A non-covariate, it is what covariates we already observe and want to include into our model. K is the number of hidden variables and also priors or some tuning parameters. So as we can see that except PCA, the K will be applied in the last stage, right? Once you do the PCA, you could decide the number of PCs to include. So you don't need to input K to run PCA, but for some other methods, you will need to But for some other methods, you will need those additional inputs to run it. And also, for the output, we could output the inferred covariates. So for PCA, this will be the PC scores. And for peer, you can additionally output residuals. So this is another point we will touch because in the practice of using peer factors, some people use the inferred covariance, some people use the residuals, and there's no consistency, no consensus. Consistence, no consensus here. So we consider two simulation designs. To be fair for peer factor, the first simulation design follows the original paper, even though we couldn't find the code, but we try to follow the procedure as much as possible. And notably, here the genotype data is simulated. And also, here, the primary source of gene expression variation are due to trans-regulatory. Are due to trans-regulatory effects and the number of SNPs is small. So we've highlighted those three aspects. And also because it's similarly genotype, there's no LD, no correlation, and every gene has a large minor allele frequency. So the second simulation design is from a recent popular paper, Susie from Matthew Stevens group. So we follow the simulation design in this paper. Design in this paper. So we have two scenarios, and we didn't design the simulation ourselves because we were relatively new to the field. So we just followed the original paper and another high-impact paper to do the simulation. So here are the results. So the left two panels shows the runtime, computational runtime. So here we have the minutes as the scale and every method is listed on the right. Lefted listed on the right, so we can see the first one, number three, is PCA. For PCA, we also consider using the PC scores or the residuals, two cases. So three and four are PC. And so these orange box plots are peer factors. So one thing that's very obvious is that peer factors are the most computationally intensive ones. So they run for the longest time. And so another thing. And so another thing we consider is the AUPRC, an area under the precision recall curve for calling EQTLs. So we can see that here, the higher, the better. And ideal case means that because it's simulation, we have the ground truth of the hidden covariates that are not observed in data, but we have the ground truth in simulation. So the ideal case shows what's the best we could do in terms of AUPRC. And we can see. And UPRC. And we can see that PC gave the best result closest to the ideal. And the second column shows if we don't do hidden variable inference, how much do we lose? And it has the lowest bar, right? So every method is doing better than the unadjusted case. But among them all, surprisingly, PCA has the best performance in terms of AUPRC. And another thing we want to give the message. We want to get the message to the field is that the peer factors, if they are not used carefully, or if you don't run the peer package carefully, you may turn out to have very highly correlated peer factors. And this is the thing we don't like. So basically, we obtained this data from our collaborator, Dr. Wei Li's lab at UC Irvine, because they had a high-impact paper published in Nature Genetics, where they proposed the idea. they propose the idea three prime alternative polyadenylation is another molecular phenotype qtl so they're not looking at gene expression but they're looking at another aspect of gene transcription and and see how it's affected by the snip so specifically 3' APA means that you may have a long isoform and a short isoform they differ in the 3' end where the polyate tail attaches but you could Attaches, but you could quantify this from the RNA-seq data and then do the analysis. That's the basic idea. So we surprisingly found that looking at the peer factors they use in this paper, all the peer factors are perfectly correlated, meaning that they actually capture the same signal. And this is definitely not what we would expect. The reason is because when they generate pure factors from their three prime three prime APA matrix, they are just using the APA values, which are ratios between zero and one. That's probably the reason why it doesn't satisfy the probabilistic model using peer. And if we do center and scaling, we will see that top peer factors are still highly correlated. If we just do the inverse normal transformation on every gene, we see that the top peer factors are We see that the top pure factors are less correlated, but the later ones are still highly correlated. If we do inverse normal transformation for each sample across genes, because this is still something people may do in practice and we have seen, you see this high correlation among pure factors are still there. So, what does this imply? Here we can show that, so this x-axis shows different tissues, and the y-axis shows the number. And the y-axis shows the number of peer factors included in the G-Tech study for every tissue. So, G-Tech used the sample size for that tissue to determine the number of peer factors. And we see that, interestingly, if you look at how many clusters are there of peer factors. So, by clusters, we define every cluster to include highly correlated peer factors. And we see that the cluster number can actually Number can actually be one for most of the tissues if you don't do any transformation. And the situation is better if you do center and scaling or inverse normal transformation with feature. But at least this basic shows that how you use peer factor package, what transformation you perform on your data can change the results significantly. So on the real data, G-Tech data, we show that if we do both PC. That if we do both PCA and peer factor for the EQTL and the SQTL, these are the two analysis done in GTAC. We see that for most tissues, the correlation between peer factors and PCs, this matrix is very close to a diagonal matrix. So there is almost one-to-one correspondence there. So the correlations are consistently high for EQTL. And for the SQTL, And for the SQTL, we may have some divergence for the not so top factors. And I think another reason is probably for SQTL, the original unsplicing molecular phenotype values are ratios between 0 and 1. So very similar case to the previous slide where the model assumption doesn't satisfy it that well. I think that's probably the reason there's some divergence between pure factor and PCs, but still the top ones are. But still, the top ones agree very well. So, to summarize here, we just try to draw this diagram to say how conceptually these methods are related. So, pure factor can be considered as a Bayesian version of the factor model or factor analysis. So, we listed the classical methods on the left and the newer methods on the right. So, and we know that the factor analysis has a special Factor analysis has a special case which can be considered the probabilistic PCA. So, this was proposed in the 2002 paper so that we offer a probabilistic interpretation of the PCA algorithm, which has been there for many years. So, and we can show that PCA algorithm is a limiting case for probabilistic PCA. Okay, so these are related in this way. And the surrogate variable analysis is a Variable analysis is a pure algorithm. So, in its iterations, every iteration will do PCA. And finally, this HCP method, which we also included in our comparison, its loss function is closely related to PCA. So that's why we had this idea that maybe we should include PCA in our benchmarking, even though it's not a specific method designed for QTL analysis. Okay, so finally, we want to say. Okay, so finally we want to say that because people have done a lot of analysis or provides a lot of techniques for selecting number of PCs, we feel that that's an advantage of PCA. So therefore, we use two ways to decide the number of PCs. So we didn't use the strategy GTAC used for selecting the number of peer factors, that is to maximize the number of ages. We didn't use that strategy. We didn't use that strategy. We purely use this traditional approach. We look at the percentage of variance explained by each PC and how it decreases as the number of PC goes. So we use the elbow method, which is the one that's most popular, and also the BE method that has provided some theoretical justification. So we consider these two ways for deciding the number of PCs. And for the peer effect, PCs and for the peer factors, we just used whatever GTAC used. So we could see that using both strategies for PC, the number of PCs are almost always smaller than the number of peer factors using G-Tech. But if you look at the number of E-genes found, the difference is not that big. They have very similar E-genes, especially when we use the BE method to choose the number of PCs. Method to choose the number of PCs, then the number of E genes as a result is very similar to what we get from the peer factors used in GTAC. But we have to know that there are more peer factors that were included than the number of pieces we included. So the take-home message here is that we use simulation studies and real data analysis to show that PCA is orders of magnitude faster, it has better performance. Faster, it has better performance in our simulation studies, and it's much easier to interpret and use. And on real data, in most cases, it gives very similar results as the G-Tech peer factor results. And in addition, we want to argue that PCE has another conceptual advantage. So, because SVA, PA, and HCP are defined as hidden variable inference, or some people call it factor discovery method. Well, on the other hand, Discovery method. Well, on the other hand, PCA is both a dimension reduction method, right? We could reduce the number of dimensions in our data and a factory discovery method. So this is analogous to another practice in the genetics community, that is, for the individual by genotype matrix, people do PCA and extract the genotype PCs as covariates to imply that they correspond to some population. Correspond to some population structures. So, therefore, there are people may have a question: right, why would you do PCA for the individual by genotype matrix, but you don't do PCA, but you use peer factor on the individual by gene expression matrix? What explains the difference? This is another reason why we did the study in the beginning. And so we think PCA, because it's also a dimensional reduction method, and also every TC can be considered as a linear combination. Considered as a linear combination of the original features, dimensions, the genes, right? So we think it has better interpretability and it solves the conundrum that inferical variants such as peer factors are often difficult to interpret using technical or biological variables. If we cannot explain, then it's hard to justify what do they do, why we included them, right? Not beyond saying that, oh, we get better power for discovering e-genes. for discovering e-genes, then what the conceptual, what conceptually are we doing? I feel like explaining the hidden vectors is important. So my student Heather has an R package that has her implementation of PCA for QTL and also very detailed tutorial, including every result I went through today. So I want to say that our paper is already online and in genome biology and because genome biology has this transparent Genomics has this transparent peer review. So, I think I can share this quote from our reviewer's report in their first round of submission, which is very interesting to me. The reviewer says, these results may come as a surprise to some, given the nearly uncontestable status that Pierre has achieved within the community, but sadly, they reflect the fact that computational biology methods can rise to fame almost by accident rather than by sound statistical arguments. I feel like this is also. Arguments. I feel like this is also a very important message for my own method development that we must consider classical methods as our baseline for benchmarking, even though we are tackling a new biological problem. So I want to acknowledge my PhD student, Heather Joe, because this whole project idea was her conception. And she wanted to do this simply because she's a super careful person and she really want to understand the method she will use. Then, the method she will use. That is the peer factor in this case. And she wants to understand every detail. And if not, she wants to do a benchmark. Even though this may not be the most common scenario in our academia, given the funding, computation, and also publishing pressure, I have to say. But I think Heather, she is an outlier and she's still doing this traditional style of research. And she really wants to be careful about her details. You can see that from her tutorial on our package. That from her tutorial on our package. So, I still want to say this is something I admire, and I want to encourage my students to be careful. So, and also our collaborators, Dr. Wei Li and his postdocs, because they brought this QTL problem to our attention. And they're very generous to share their own nature genetics papers, peer factors, so we could discover this high correlation problem in their analysis. And so, they're our co-authors. And also, of course, Co-authors, and also, of course, the funding agencies that support my research. Thank you. Thank you, Jesse. And I see there's a question in the chat box from John. Do you think similar conclusions would hold for other multivariate data types? For example, decomposing multiple phenotypes, AMR of a bacterial species, gene transcript from a. I think so. I think so. So I feel like for other multivariate non- Like for other multivariate analysis, we also need to be careful about our method choice, right? So, I think it's very tricky because some methods are very complex. So it's not easy to say which one is better conceptually. So that's why I feel like benchmarking, especially when we do simulation, we consider more designs would be very important. Also common, I think the rise of methods definitely involves Definitely involves leak, but also how methods are sold in the original paper, how well easy work and software for sure. I think the software quality plays a very significant role because most of the users are not software developers. If a method doesn't run in the software package, people will give up. You cannot expect your users to fix the code, fix the bug. And also, how fast it runs is also important. So that's why in this paper, we did say that the fast runtime. The fast runtime and also the stability of PCA because for sure it's such a classical method, right? It must be stable. So I think these are very big advantages. Thank you. Great. Thank you, Jing Yi. Any questions here in Mahaka? Jing Yi? No, nothing here. Yeah. It looks like there's a question in the chat. Yep. Yeah, yeah, I see his question. Yeah, could auto-encoder-based methods address the non-linearities in manipulating data utilized? That's a very good question. It's basically essentially we here all the methods, actually, they are linear. They are trying to find a linear combination of genes, basically. And for auto-encoder-based methods, they could find non-linear combinations. But I think the challenge here is the But I think the challenge here is the simulation design, because currently all the simulation designs are based on linear models. So I feel like, how can we justify whether a simulation design is reasonable or not? That's another challenge. So we kind of avoided this controversy. So we follow the famous designs in the literature. We didn't invent a new design. But I feel like that's kind of, you know, a loop, right? What is considered reasonable simulation? And you need. Reasonable simulation, and you need a simulation to provide you the ground truth for benchmarking. So, I think we still have a long way to go. Yeah, thanks. Thank you. Thank you for the question. Great. Thank you, Jingy. Any more questions in the chat or in person here? Okay, it looks like no one else has any more questions, so we'll go to the coffee break. So we'll go to the coffee break. Yep, and we'll stay online as well for a breakout room, I guess. Thank you.