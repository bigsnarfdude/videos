And we just considered two examples of how to answer that question. For simple types of functions, there are simple answers to this question. And then we could also ask, how does this model depend on this variable? I think of these as two separate questions. And even here within this, there's a variety of questions depending on how we interpret the word depend. And that's an important subtlety, which we'll talk more about momentarily. All right, so. Alright, so why should we care about these kinds of questions? Well, explaining models could have impacts in the real world. Like, if we're trying to assess if an algorithm is discriminatory and we use some explanation method, like a partial dependence plot, to understand how that model depends on a feature, in this case, a sensitive feature, if that explanation method gives us a bad answer, then we can reach the wrong conclusion. Reach the wrong conclusion. We could have a model which is actually discriminatory, and our explanation method tells us it's not. So we go on about our business and we don't fix the model. So this is a problem. We have flaws in our explanation tools. It have real-world impacts. In a scientific setting, it might not be something about discrimination. It might be something about a hypothesis about the world. And our explanation method might just lead us down wrong paths where we're then studying really. We've been studying really flawed hypotheses for a long time, not realizing that we're wasting our time. So, these are some citation search result counts to show some idea of the scale of the use of some of these methods that I'm talking about here. So, PDP, partial dependence plot, I'll say a little more about it soon. SHAP, SHAPLY, additive feature values. These are two kind of recently popular methods. Recently popular methods. And on the left graph, I have papers that cite either the original PDP paper or the original SHAP paper. And then I search for certain keywords and count how many papers have that keyword and cite that method. So for example, fairness or legal or ethical papers that use these words and then cite Schappen values. There's thousands of them, 4,000, 6,000 that That papers that are talking about fairness and legal things, and they're using PDPs or they're using chaplain values, or they're at least citing them. Over here, we have a PubMed, the database of like medical research. And so you know that these articles are about health and medical research, and how many papers are put on PubMed that also match keywords like SHAP for Shapley Value. For Shackley value. So, this is a huge increase. Like, just very recent as well, there's a huge explosion in papers using Shackley values. Okay, so let's talk a little bit more about partial dependence plots. They were first described by Jerome Friedman. This is a highly cited paper. The way you compute a partial dependence plot is you create, you Is you have a plot where your horizontal axis is the feature that you're trying to understand, like xj. We want to know how does our model depend on xj. So we make a grid of xj values, and at each point in that grid, we plug in features into the function f. So we're at a certain grid point for xj, but this function f depends on lots of other features. So where do we get the values for all of the other features? So, where do we get the values for all the other features from Yule's answer? Our daily exists. So, data. We have a data set. We have values of all the other features from our data set. We're varying xj arbitrarily to create the plot. We're getting the values for all the other features from our data. So you might now ask: if Xj is being varied arbitrarily to create this plot, but all of the other features are coming from data, what's the relation between them? And the answer is: when you're computing PDPs, The answer is: when you're computing PDPs, no relation. Assume they're independent. So you're varying xj, and you're assuming that all of the other features are independent of xj. You just plug them into F, you get a value, and you plot that value. So this is a method I should say that is very nice for describing the dependence of a function of your model on xj if this assumption is true. If this assumption is true. So, in the special case when our original function is additive, PDPs are exactly the plots we want to see this dependence of the model on that feature. So, just to clarify, is it they plug in the average values of those other features? Good question. You plug in every value throughout your whole data set and then you average those. So you marginalize over all of the other features across your data set. All of the other features across your data set. And if you don't marginalize, you could, if you have a data set with n observations, you could also plot n individual curves. And that's another plot called an ICE plot, individual conditional expectation. Okay. So I'll spend a little time on this slide to explain some things, because this is where I'm going to talk about PDP, ICE, and then our new method that we're proposing. That we're proposing. And so the first thing to know about this is: I have the partial dependence plots and the ice plots that I just mentioned on the right panel. And this is from like some other software library, not our implementation. So we have a data set here, which we generated so we know what the truth is. And our data has three variables: one outcome, and two predictors. Predictors. Then I'm going to create a plot to show the dependence of the model on one of the predictors. So our story, the motivating story here is we're thinking of this as something about school funding. And our variables here are something like parental income. Oh, we're actually sorry, the outcome is not school funding. The outcome, this is a simulation, so it's not real data. The outcome is starting salary. Salary and the predictors are parental income and school funding. So the idea is: parental income influences the school's funding, and both of those things influence starting salary when students graduate. So we're going to ask how does the predicted starting salary depend on parental income and create these different plots to answer that question. And the PDP and ICE plot is on the right-hand side. So the thing that I find So, the thing that I find unsatisfying about PDPs and ICE is, as I mentioned earlier, when we're going along the horizontal axis, varying the plot variable, we're treating it as though it's independent of all the other variables. And that assumption might not be true. It might be a very bad assumption. So, what could we do instead? And my way of answering this is: let's Answering this is: let's assume a causal model. Let's have a causal model somehow that tells us how the other variables change when we vary the plot variable. So we have this model in this case. When I change p, I'm going along the horizontal axis varying p. This model tells me f changes as well. So instead of treating f as independent and getting this curve here in what we call a total dependence plot, we allow Total dependence plot. We allow F to vary according to this model. And then now, with both of those varying, we see how the model's output changes. And that gives us the blue curve. Okay, so why are there three different panels here? These are just different predictive models fit to the data. So this is the original data. So this is like the true model. Oracle told us the real functional form. Here we fit a linear model to that data. Here we fit Fit a linear model to that data. Here we fit a random forest to that data. So in these different cases, we have a different f hat that we're trying to explain, and we have different curves that are explaining explanations of f hat. So one thing to notice here, the right two panels are both plots for a random forest, F hat, and our plot called a natural direction dependence plot is the orange curve. Is the orange curve, which looks exactly like the partial dependence plot. And it's not a coincidence. So I haven't told you what the natural direct dependence curve is. If you're familiar with causal mediation analysis, for example, you may have heard the term before. The natural direct dependence is represented visually in this graph here by the multicolored arrow here. This is measuring the causal effect of P on S cat while holding. On S hat while holding the other predictors constant. So intuitively, it makes sense that it's the same as the PDP. The interesting thing about this for us is this establishes a causal interpretation of PDPs, which was not in any of the original work about PDPs, and it's something that people have speculated about before. So we have like an answer to say: yes, PDPs do have a particular causal interpretation, it's a kind of a limited one. It's a kind of a limited one. Okay. So, yeah, there's a theorem. The PDP, also, the ice curves are just the things that are averaged to get the PDP. Those are shown in a natural direction dependence plot. So I mentioned earlier things like Shapley values. There are also other potential, there are other kinds of model explanation plot methods. There's one called the Accumulated Local Effects Plot, or ALE. So here's a comparison. So, here's a comparison for the random forest model with a few of these other types of plotting methods. And the high-level takeaway here for me is that aside from our total dependence plot, all of the other methods are showing an attenuated effect. So, the effect of parental income on predicted salary is kind of being flattened away by all these other methods, either because of marginalization, like the PDP. Like the PDP, or because of conditioning, like the local methods like ALBU. In both cases, in all of these cases, they're not using causal structure, which we are using for the blue curve here. Okay, so I think the first question I've said here, we have this way of creating a new plot, but it assumes that we have a model here, which we're calling an explanatory causal model. That's a big assumption. That's a big assumption. I'll be the first to admit that's a huge assumption. How do we get an explanatory causal model? And if we have it, why would we even be fitting functions to predict S hat and so on? So one thing is that our explanatory causal model just has to tell us information about predictors. It doesn't have to actually include the outcome variable in it for us to generate these plots. So we don't, we're not limited to the setting where we are. Limited to the setting where we already know everything, including full knowledge about S, and we wouldn't even be fitting these predictive models in the first place. The other thing is, we have lots of different ways of trying to get explanatory causal models. For example, we might have domain knowledge that we could use. People are out there doing experiments in some settings and accumulating knowledge. And whatever knowledge there is that exists, we could incorporate it into our plotting method somehow. So, this is just an example where there's a pretty Where there's a pretty well-validated data set, where people have done a lot of experiments to try to figure out which edges should be in this graph. And so we take what they say the graph should be, and we use that. We fit some predictive models on that data. We use that graph while generating these plots. The purpose of this example is mainly to show, you know, it's not limited to the simple three-variable type examples that I started with. You have more complex. That I started with. We have more complex data sets. And also, there's a difference in this example between the total dependence and the natural direct dependence. So how this prediction depends on this variable, the answer to that question depends on how we interpret the word depends. So we really have to be more precise. Have to be more precise in asking this question. We really can't just say, How does it depend on this? We have to really say, in a total dependence causal sense, or in a direct dependence, causal sense, or perhaps along some other indirect pathway. Another way we could get an explanatory causal model is to learn it from data using causal structural learning algorithms, which I don't work on, so I'm grateful to people who do, that they're putting That they're putting methods out there that make something like this potentially useful. So here's an example where we use the PC algorithm to get an explanatory causal model, and then we use that to generate some plots. So, if you know about causal structural learning, you might also know that that is hard, and the output of the algorithm might not be a single graph. Be a single graph. It might have some ambiguity built into it. So you might have some uncertainty about what explanatory causal model to use. And here, I think, we can get to the strength of visual model explanations because when you're looking at visuals, you're judging them qualitatively. So we can do things like create an envelope plot for a set of explanatory causal models and have an intuitive visual sense of. Intuitive visual sense of the relationship now that includes our uncertainty about which explanatory causal model is generating the explanation. And again, for me, the nice thing about visuals is I think they give us a little more wiggle room to work with. If I just want to know is the qualitative conclusion the same, then I have a little bit more space breathing room around my uncertainty about the causal model, the explanatory causal model. Explanatory causal model. Alright, now I want to tell you a little bit about the limitations of all of these different methods, including CDPs. I'll start with our method of causal dependence plots. The big limitation here is that if you have a bad explanatory causal model, you get bad explanations. So here, the top row is showing the true dependences. The bottom row is showing a case where we use a bad explanatory causal model to generate the plots. And they're obviously wrong. They're very different from the top row. Is from the top row. So that's a big limitation for our approach. Any causal, any non-causal model explanation method has limitations as well. I'll start with the ones that are sort of my opinions about non-causal methods. But then really, what we also saw in the examples here are they're producing explanations that tend to show attenuated effects or flattened effects, like the dependence that they show. The dependence that they show tends to be weaker than, say, a total dependence. And this can be really impactful. So, for example, in a fairness audit, they might only show something like direct discrimination, which might be a very narrow and limited sense of discrimination that might not really capture anything about how much people are being impacted. And in particular, if our model F does not take S, a sensitive feature as an input. A sensitive feature as an input, then the automatic usage of a method like PDP or SHAP will show no dependence at all. Like, these methods don't show dependence of a model's output on any feature that is not an actual formal input into the model. With causal models, the explanatory causal model can be a bridge from features we care about to inputs of the predictive model. All of these. All of these model explanation, model agnostic explanation methods have the potential to give you different explanations for the same model. So that's kind of weird. We don't like ambiguity. If you have one predictive model and you try out PDPs and SHAP and you get things that look very different from each other, I don't know what to tell you about that. I don't know which one to trust or which one's good. So that's not nice. Another super important thing that we have to remember is that. That we have to remember is that a good explanation of the predictive model might not say anything about the real world, because the predictive model and the real world might function very differently from each other. This holds for our method as well. So the causal interpretation of PDPs, the theorem that I said earlier, that's the natural direct effect of the predictor on the predicted output, not necessarily the real output. Why? So that's a Real output Y. So that's a big, big potential gap there. In particular, I also like graphs for this purpose because we can represent that fact visually by always drawing our predicted, our explanation, our predicted thing, as some other thing in the graph aside from the true outcome Y. So I like this visual representation. To me, this is a very honest way of telling people: look, the real world is doing something. We have this predictive model, and I'm explaining this predictive model. Predictive model, and I'm explaining this predictive model, and it might not have any relevance to the real world. Alright, so just wrapping up here, there's more stuff that I want to do here. Make some software that people can use easily. That's probably the biggest hurdle for anyone using this. And end with another quote about Yule. So, and this, I think, is something that's probably, we'll probably hear again and again in this workshop. Measurement does not necessarily. Workshop. Measurement does not necessarily mean progress. Failing the possibility of measuring that which you desire means you may measure something else and forget the difference, or you may ignore things because they cannot be measured. So again, like right from the beginning of the era of using multiple regression for the first cases of studying social science and multiple regression, this message is often forgotten, but it has nothing specifically. It has nothing specific really to CDPs, but I just threw it in here because I would emphasize mindless all about it. Okay, so that's what fixed. Okay, to put questions from chat. Clarification questions. So when you're doing a predictive model on attack, so is it that you're you're using the observed outcomes also as predictive models so why is that the reason of edge from y? Why is there that reason of H from Y to this S1? Yeah, I'm treating F hat as something that's fixed, and I don't actually, so this is a little strange, in the model agnostic setting, I don't actually know that the data set that I'm looking at is the same as the data set that was used to fit F. So it's possible, you know, but I'm thinking about applications like external. Like external audits. So F is just something that's given to me, and it might not actually depend on this Y in my data that I'm using now. But I think it's a good potential extension of this to consider that case when you do allow that. One clarification I had is clarification I had is do you just need the DAG or do you need the structural equations yeah you need the structural okay so you need to assume some structural equations in order to do that makes sense yeah because we're doing we're we have these these plots are all path specific type plots so yeah we need the structural equations thanks yeah so um I guess one of the good things about like partial dependence plots is that they mean something I mean whether you think that they are like whether you like the meaning or not Are, like, whether you like the meaning or not, it's like something that is like, you know, computable and something. Okay, so what happens? No, no, no, no, because listen, so what if your model is totally wrong, right? Like, is there any guarantee that that's a meaningful quantity, even just like in adding up numbers? So this one is totally wrong. Yeah. Well, there's still no guarantee. Nothing. Yeah, of course. Definitely not. Well, before you say that, I think you could say, like, for instance, if you are measuring. For instance, if you are measuring treatment effects, even if you don't have unconfoundedness, you're still measuring some kind of non-parametric readjusted covariance. If you think you can do something here, so it has meaning in the sense that I like reminding myself how mathematical reasoning works, this hypothesis conclusion. So it does have meaning in the sense that if data is generated from this model, then this plot shows how f-hat depends on. shows how F hat depends on this. So it does have meaning. So this could be wrong for the setting that you're studying, but the meaning would be if you ever found another setting where this graph did accurately describe the data in that case, then you'd have... Now, this is not just a joke. You could use this, actually, for, say, stress testing model for distribution shift. You could hypothesize a data generation process. A data generation process, which is not real. It's not true. It doesn't match how things work now. But you may be asking: what if this happens? We want to stress test our model for this case that we came up with. You can specify this case using a model like this and then ask what the model performs like in that setting. Are there other questions, by the way, or is Daniel lost? Last. Alright, got a daily loss. I just want to follow up on a previous point. It doesn't seem to me clear that you need the structural equation models if you do non-parametric estimation. Ultimately, you're estimating some past-specific effect. So if you have a graph and you check that the thing is identified, you can call these semi-parametric, non-parametric ways of estimating past-specific effects. That doesn't rely on knowing the structure of the things. Yes. So I think, for example, I mean, I showed. Example, I mean, I showed like I showed some real data, I showed this where there's just a graph. So, what we did was we estimated the structural equations. Given the graph, we estimated the structural equations. Assuming things are linear. Machine learning. Yeah, so non-parametrics. But I feel like that that is more prone to error, like that approach is more prone to error than estimating that same path-specific effect given the graph using like stuff bronze. Like stuff Rozzi and Kosi K and others have like worked on. Because if you're wrong about the structural equations, even ones that may not be directly relevant to you, it can trickle down to be both about the effect. Whereas there at least you have some control over what your bias is about. Yeah. So I've been thinking about this in this CS conference modular research way of minimum idea. Sure, sure. But that means I want it to be used by, you know, anyone who has their way of estimating structural equations, they can now do this and they might also have a different way of visualizing uncertainty and they can also create plots that show the uncertainty as well. And it kind of is a related point, really quick, like you mentioned that the interpretation of the partial dependence plots is this like natural, natural This like natural, natural direct effect thing. And there's this idea in Call's inference that mediation analysis is harder than just getting total effects. You need stronger assumptions for estimating a natural direct effect. Then you would if you're just estimating an average treatment effect. And so if the most common method is interpreted in terms of this natural direct effect quantity, it's like there's these strong assumptions under the hood about what's like. Under the hood about what's like with those things. Whereas you might, a benefit of your approach is that if you only care about total effects, then your graph kind of exists under, is achievable under some like weaker assumptions. Yeah, and I think generally you're right that probably the total effect will be something that you can get in more cases without as many assumptions and so on. But that the reason we're able to get natural direct effects seemingly easy with PDP. Direct effects seemingly easy with PDPs is because they are the natural direct effect of this artificial thing. So it's like the reason why counterfactual contrastal explanations are so easy to do is because the thing you're explaining is this superficial layer on top of the world that no one actually hears about. Thank you, Josh. 