And since this is the first talk, I'm pretty flexible about what I talk about. I mean, I have a plan, but first of all, I know it's too long and it has many tangents. So if you have any questions or if you'd like to bring in your perspective, then you should feel free to interject. That's one of the benefits of a chalk talk. So the title as I supplied it was this. And I'm not lying quite a bit, so first of all, I'm only going to give one proof, and I'm going to talk more than anything about two settings, and the proof is only going to be in one of those two settings. And I should say that the main paper I'm going to present today, which will be, you know, depending on how things go, like between 10 and 25 minutes of the talk, that's a single author paper. Talk. That's a single author paper showing up in cold, but I should say that I have a spiritual co-author for pretty much everything I'm going to do for the next, it's going to feel like 30 years or so. I don't know if any of you have had such a relationship, but basically I had this co-author, 2HE, that at some point it would be like if we didn't speak for a month, we'd come back and we have proved all the same theorems. It became really weird after. It became really weird after a while. So, yeah, so the first thing I want to talk about is something called what I call the weak implicit bias. And I've actually never used this term in a paper, though I started using it in talks. And this is what we'll talk about today. And then I'm going to talk about something called the strong implicit bias. Originally, I was very ambitious, and I wanted to talk about both But I don't think there's time, and also the second one, it's not on archive yet, so I'm not quite ready, but it was submitted, so we'll see how that goes. So, let me tell you what I mean by both of these. So, by this, I mean there exists, so here's, I'm going to say like an implication, I'm going to say, suppose there exists a solution with low tester. With low test error and low norm, then this implies that SGD finds solutions with comparable norm and test error. Okay, so there's no uniqueness statement here, just saying There is no uniqueness statement here, just saying if there exists, there can exist many, then SGD finds something which has a comparable Norman tester. It could find a completely different solution. It could be unrelated to this one. And so this is just saying there exists a comparator, and the proof, of course, used the competitor. But again, I'm not saying we converge to this thing. And I urge you to think about this theorem. Do you really need to converge to that thing? If you're getting a good test there and good norm, did you really care? So I'll contrast, and so I'm going to give a bunch of examples in just a moment to make this more crisp. In just a moment, I'll make this more crisp. But by contrast, this is what I think most people think of as the implicit bias, which is GD converges to some sort of notion of an optimal. And I'll make this more crisp too, but usually these characterized things like minimum. But usually, these is characterized by things like minimum norm solutions and max margin solutions. And I was also a little bit careful by differentiating between GD and SGD. These proofs, they basically always need GD or something even more evil, like gradient flow, and this one or Washerstein flow. And this one I use SGD. No, I mean I say evil because it has to do an infinite, I'm counting an infinite number of computational steps per time step. So what is that algorithm doing? Doing. But, okay, so let me give you some examples. So, my first and favorite example of the first category is the Novikov perceptron proof. I think this proof is highly undervalued. I think this is one of the strongest proofs we have for many reasons. I think many treatments of this proof post-62 have made it weaker than it actually is. So, just to refresh you on what this theorem says, it says if there exists this unit norm vector that's able to get positive. Get positive classification on the entire sequence that you observe. This thing doesn't have even statistics in it, right? It just has a, it just runs on a data sequence. Then it implies that SGD on the ReLU loss. So the loss is literally ReLU applied to W transpose XIYI. And it's a specific ReLU, you have to choose the one subgradient at zero, otherwise the algorithm breaks. Otherwise, the algorithm breaks. So it says she on the Reali loss makes less than or equal to 1 over gamma squared errors. So this theorem for me is amazing for so many reasons. But going back to what I was saying about this... Sorry, when you mean error is like how close you are to the true, to this vector U? No, it's misclassifications. So it's in the spirit that I wrote above: misclassifications. So it's in the spirit I wrote here above where not only is there no guarantee you're converging to you, you are not converging to u. Because the way this thing, I mean this is a non-smooth function, right? This thing just stops when it stops making mistakes. You stop rotating your vector. So you might stop completely unrelated to u. This is not converging to you. But who cares? Why do you care? Did you care? I don't care. So, and the other nice thing is that you can choose different solutions with different gammas and get different theorems. Different theories. Okay, so this is one example. Another example, so then the examples from here are my own and basically Ohad's. So and yeah, Ohad, he very politely told me that he will not be here. But I was going to plug him anyway, so because my cold paper title copies his paper title, but so in 20 So, in 2018, Zhu and I proved that you get basically the morally same theorem for the logistic laws. So, let me use my space a little better. Let me move it over here. So, this theorem is that if you do SGD on logistic, and I should just say it now with, wait, let's say clock. Where's the closest clock? Okay, I'll follow that. So, SG on the logistic loss. And when I say SG on logistic loss, I mean no. So, when I say I just can't histigolos me, no projections, no constraints, no regularization. Then, and under this same condition, then it gets test error. I'll just say 1 over t and wait, sorry, let me just get my, sorry, I just got flushed because I don't think I was going to go. Helpful because I don't think I was going to go into this. I'll just say exactly how I did. Yeah, so this one it needs log over gamma squared steps for one over epsilon gamma squared steps for For test error epsilon. And the norm you get at the end of the day is log t over gamma. Okay, so my handwriting got kind of bad because I didn't write down the theorem and I forgot it for a second. But yes, let me just say it again in this context that if there exists this separator, then SG in this many steps gets tested epsilon and it gets this norm at the end of the day. And it gets this norm at the end of the day. No constraints, no projection. It gets norm log t over gamma. And gamma is going to be the step size of your L G D? No, this is constant step size L G D, so step size is 1. 1. Step size 1. Yeah, let me just... Okay, now that I remember the theorem, let me write it down correctly because I think it's. Event for Novikov. Hmm? Event for Novikov. Yeah, Novikov also is one. This proof is basically the natural copy and paste of the Novikov proof. So S G E. So SGE on logistic with the step size of 1, 1 over epsilon gamma squared iterations, no projections, no constraints, and it gets that test error, so with high probability, test error, epsilon, and norm log t per gamma. Log t over gamma. Okay, and I think it's clear this is not a classical theorem. If you open like a Nestrov book or something, it tells you your norm is going like routine, you can't see anything else. It's not going like routine, it's going like log2 over epsilon, or log2 over gamma. You can check everyone in the experiment. I'll have you run great extent on like a deep error, but it also goes like log2. And test error measured with the logistic loss. You can actually prove it, but it's not proved in this paper with logistic loss. It's proved in the paper I'm going to mention today. But this one is actually the misclassification loss. Is actually the misclassification loss. So it's a direct copy-paste of the Navikar where it goes through directly. Because how does Navokov proof work? If you look at the end, the mistake bound is actually the derivative of the reality loss. The derivative of the logistic is the sigmoid. The sigmoid is an upper bound on the 0-1 loss. It's the copy paste. So the true thing that this group is controlling is the sigmoid loss, but that upper bound is the 01 loss. You're still having the condition that there exists a realizable. Yeah, there exists such a U, yeah. Okay, so that's what we've. Okay, so that's what we proved. And then one other thing we proved that I wanted to mention because I think it fits the spirit of this workshop. And I should say that for the theorem I'm about to give, we have also done actually one year ago the non-realizable generalization, so you can ask me about it afterwards. So I'm going to tell you a shallow neural network theorem that says for anything Shallow neural network theorem that says for anything that a shallow neural network can separate. So neural networks are universal approximators, so the only kinds of things that can't separate are things where the conditional probability is not exactly 0, 1. We do have a non-servable theorem, but I have nothing to help that. So I thought this theorem would be interesting for sort of this workshop, where one of the things I read in the description of the workshop is that we want to be able to analyze settings where the optimization complexity and the statistical complexity are matched. Complexity are matched. So, for this one, here's what we can prove. So, if the data is separable in an NTK sense, you take the features that the NTK gives you and look at linear predictors over those, and you look at the gamma in that space, then what you can prove is that with one over n squared steps and And your width needs to be gamma to the eighth in the denominator. And in the numerator, you just have polylog of the stuff that people are used to having, like t, one over epsilon, one over delta. Okay, so again, there's no dependence on the number of training points or number of steps. So this is going to be the width of the network. Then you similarly get tester. Less than epsilon. And this one also has a norm control hysterical space. This is SGD, this is SGD on the logistic and a shallow network under this sort of what I call the NTK margin assumption. And you prove that in the same number of SGD steps and a network whose width is at least polynomial in this margin condition and polylog and all the things people usually need to be polynomial in. People usually need to be polynomial in, and you get test area less than epsilon. Okay, go ahead. Are those tools are basically mistake bounds, essentially? Or do you need anything stochastic in bounds? Yeah, well, no, so I do, so these are bounds on the form of the final thing is it says that if you look at all the iterations, because I do use stochasticity. So there is a mistake-bound version in the papers that we proved, but just because of machine learning settings, we did then apply a martingale concentration to it and get a Concentration to it and get a concentration. And the theorem we get from it is that if you look at probability of a prediction, so this is the quantity that's less than epsilon. Or this is the predictor at time i. It's just like, uh, yeah. And the 2019 result, first steps and the same error, but with the more recent result, you've got stronger conditions on the results. Yeah, so I didn't even write it out because I'm just disorganized. it out because I'm just disorganized. But for this one, this was a linear, this is for linear predictors, and the assumption is a linear separability condition. This one is for shallow ReLU networks and the separability condition is a sh separability under the infinite with NTK. Yeah, so I just didn't even write it because, you know. So the separability is with the NTK column? What is the separability NK? The separability definition is equivalent to infinite with NTK. I think it's a good idea. I didn't think so. So I'm a little bit of an outsider. I don't have any idea how the proof of the Nautikov62 goes. Is it possible to give that idea? Yeah, I mean, so the key thing of the proof is that I would say that the key, so if you're familiar with, so the potential function that we, and I can give this in just a few sentences, so the potential function we usually use in We usually use in something like a typical mirror-decenter SGD analysis. We take this, we expand the square, and we end up with a bound that looks like: if this is the step size, and this is the objective function, so we get this less than 2 eta times t times, and this is some comparator, typically the optimum or something else, plus this term. And here's the comparator. So in the Novikov perceptron proof, The Novikov perceptron proof, you actually just set the output to zero and you instantiate this with zero and you use some clever manipulation, but there's a key thing in the perceptron proof which we use, which is not immediate from this, which is, so I'm claiming that you can get the Nabokov theorem by doing some manipulation on this and the following key property. If you look at the norm of Wt, so I said this is zero, so that makes me look at this. If I write out the representation of W. If I write out the representation of Wt, so Wt is the sum of gradients up until time t. And the gradient according to the ReLU, according to the ReLU loss, is this. Did I make a mistake at that time or not? And now I use the definition of, variational definition of the norm. I can lower bound this by interproducting it with that predictor. With that predictor. And what you end up getting is that you get u times this, which is at least gamma, times the number of mistakes. So you get gamma times the cardinality of the set of i such that mistake at time i. So you get this. You up or down the right-hand side, you get the number of mistakes again, and once you deal with the square and you rebalance the sides, you get the number of mistakes less than equal to 1 over gamma squared. So it's like a classic mirror. So it's like a classical marrow descent proof, except there's a key step. And when we do our logistic proofs, this key step is used extremely heavily for the logistic case. This is actually why I said aggressively that this proof is undervalued, because typically when I see people, they sort of don't use this proof to his power. Oh, it also holds for gradient descent. It holds for gradient descent, yeah. Yeah, what you do is you replace this with the half. Yeah, you, so this is some. With a half, yeah, you so this is sum over time, which is still the representation of the gradient, but now this is a this is an empirical average of the trading scent. Yeah, this proof, so so yeah, in this paper, we we also did we did gradient descent and SG, and it was the same proof for both. Do you get any advantage by doing gradient descent or SGD? Yeah, so we um so we do actually in this paper we do SG and GD. Let me try to summarize it in the most efficient way possible. So, we actually do get So, we actually do get a strange improvement for SGD. Our SGD analysis is tight with lower bounds on sample complexity. So, there's a lower bound using parity for sample complexity of kernel methods, and we get a matching, that matches our sample complexity upper bound. And also, our width is good. For the GD, so here's the, I won't get too far on this tangent. We have a 1 over root t. So, here, this is like a 1 over t, right? Because this is 1 over epsilon. Because this is 1 over epsilon sample complexity. So we translated it to a statistical problem. It'd be like 1 over root n. So in our paper, we only gave a 1 over root n. You can do the local Rademacher analysis and get 1 over N. Zoe refused. He said it's too disgusting, too many universal constants. He refuses to write it. So we left it at 1 over. If you do the local Rademacher, there's still a gap in the dependence of gamma between the SGD and the GD. I went crazy about this. And the GD. I went crazy about this and I wrote out the entire nuts and false versions of the proof. I couldn't figure out what's wrong. So, yes, we do have a gap between GD and SGD, and I'm justify it. The SG gets better, but only by constants that 10 people on Earth, including me in Ukraine. No question. No? Yeah, so I thought that this theorem would be exciting for us because, like I said, all the complexities are rooted in this gamma here. The width we need, is Î¸. The width we need? Is delta in the polyglot? Oh, so the delta is the confidence parameter. So it's a very technical comment, but so that means that in the final bound, we get a log 1 over delta when we apply concentration. Many of the NTK proofs that we had as prior work get a 1 over delta because something doesn't concentrate properly, and they have to use Markov instead of a Martingale. So it was actually kind of nice that we were able to do it like this. But yeah, I mean, this proof was based on this proof, and then this proof, well, I have to say, so this proof was mostly me, this one was mostly Zoe. He did a couple extremely brilliant steps. So I think this one was, you know, this one's definitely a copypiece of that, but this one was more clever. There's some clever things you have to do. Okay, I also just wanted to plug a couple other papers. One is One is one thing I mentioned when I did this is that I'm very emphatic about using this term. It's used in Novikov, and I use it in my papers. And also Varun, I think he's going to talk about it. So he uses that term with his papers. So Varun et al. I think 20. And I'll also say that Ohad has a very nice paper about radiant descent if you're overfits on separable data. And that's And that's um you know, he's not here so I can make the joke that I have to use for everybody's first name so you don't get confused. I'm talking about RSA or something. I think everybody knows it's dad. Okay, it's not going to be on my session. Okay, so that's weak implicit bias. This is what I was planning to talk about today. And we're still good on time. I can still show you the theorem. I do want to just give some pointers for what I call the strong implicit bias, so this is more clear. Because none of these proofs am I proving. Because none of these proofs am I proving that I converge to the maximum solution. And by the way, because proving that you converge to the maximum solution for deep networks is such a terrifying problem for me, I usually spend my effort trying to prove that this sort of stuff is proved for deep networks just because it gives you more slop. You don't have to prove it converts exactly to some object. And then you have to write this like for there's all this quotient you have to do for deep networks. There's all these non-unique rotations. Networks. There's all these non-unique rotate, right? Rotate the way it's like scale one and deep scale. You have to quotient all this stuff out within your curve. So I like having this thing where you don't really have to converge to a unique object. But I want to give some plugs here for what I mean here. So the oldest results I know of this type are actually quite recent, and I'm confused by this. Maybe Syria knows the earlier ones for the squared loss, but in January. Squared loss, but in Zhang and Yu in 05, they proved in appendix D, without caring about, in a paper about boosting, that coordinate descent converges to, for them, the minimum norm solution. So write it as this, max over predictors whose L1 norm is less than or equal to 1, min over examples u transpose xi, yx. So u converge to this with Corinth descent on an exponential log. This is with Courtney Descent on an exponential loss. That's in their appendix D of their paper. That was about rate, so it was asymptotic. I proved a rate. So I proved a rate which is roughly 1 over root t rate using actually the log semester functions. And I didn't think when I wrote the proof, I thought it was clear from the prior work, so I don't. Clear from the prior work, so I don't really know. Then the result, I think, pretty much is the result that everybody knows in this literature. So I'll only list some of the authors. Well, actually, I'll just stop at Siri if this is enough authors. The 10, I think? The first one, 17. So this is that gradient? That's maybe Maximum's grid. That's maybe maximum more state, but whatever. No, that's not my paper. But anyway, so this converged, that gradient descent converges to now the L2 norm solution. Just to slide by the all the three results are like some variation of logistic plus exponential. Some variation of logistic loss, exponential/slash logistic loss. Yeah, good point. I should clarify. Yeah, so this is a vote before this on quadrant descent for regression loss. It's earlier than this. Yes, but it's very weird, is it? It's one of the Lars papers. Oh. Tipshirani's papers. But that means it's not running gradient descent. I mean it's running Lars, which is some crazy thing. No, it has an epsilon Fortnite descent version. If you remember the If you remember there is the Lasso, Lars, Fallbook, Sting, and all those papers, there's a paper which kind of summarizes the things in 2004, just a little bit. But it's not clean in a sense that the coordinate descent converges to minimum M1 nomsification only holds under certain conditions. Like not the path, Lasso path, and so on. That's fine. I'm happy to give them credit, especially since it's hard to come up with all these results, I think. You really have to. You really have to look for the result to know that that is a result. Well, this one, as far as this one's not even written as a theorem, it's just like a sketch. I don't even remember if it's correct or complete. And when I did this, I didn't even know about this. Somebody told me, like, during review or something. I think the same thing with Gypschinese. Yeah, so all of these then, yeah, so these are logistic or exponential laws. And I should say there is a difference between these, and it's very disturbing. Very disturbing differences between these two. For most of the results, we can prove the same thing, but the proof is harder for logistic, which is incorrect because logistic is used in practice. I also just want to mention that so Zoo and I have an A-team version using this proof technique. So this proof technique doesn't actually care about the norm. So we did the same thing and Same thing. And Syria and Jason Lee and Nati and who else was on this phone? Daniel Zuj. Oh, he was okay. I don't think I don't know what it is. Yeah, so then they used this same technique, and I just have one comment I want to make about this. So between this and then following works by Zoe and I, and then also Nati was with us on one of these papers. And also, Nati was with us on one of these papers. I just have one comment that I think is a little surprising. So, you notice a one over root t here. And let's say you don't care about the logistic laws, let's say, you don't care about the exponential loss, you don't care about classification, you don't care about SGG, you don't care about GD, you don't care about anything I've said so far. Maybe you care about something Nestorov said at some point, though. We're solving a non-smooth optimization problem, right? So, the generic black box oracle lower bound complexity for the Oracle lower bound complexity for this problem is one over root t, right? You open Nestorov or you open Namrovsky UDIN, it tells you one over root T is the best way to get these types of non-smooth problems, right? Right? Good? For loss minimization. Yeah, if you run SGD on just this loss, you just plug in SGD, open Nebraska Uden, somehow, I can't read that book, but if you can read it, you get whatever root T. So now my question for you is using these log sum X techniques. These log sum x techniques, do you believe 1 over root t is the best convergence rate we can get for this objective? I know you know because you've seen our talks, but what doesn't mean guess is the fastest rate you can get for this problem. And by the way, to be clear, I'm talking about measured in this objective function. I don't mean measured in terms of the logistic loss. I mean I'm measured in terms of this non-smooth objective. So run gradient percentile logistic loss, and then measure your error convergence rate in terms of this objective function. Objective function. So, do you think we can beat this? You can guess that my answer is yes, we could beat it. But my question is by how much? Want to have a guess? What's the fastest rate? Linear electrons. So, actually, I don't know. I don't know. You're sure that this is the best rate within the physics class? Well, okay, so first of all, this one of the route I proved for coordinate descent is actually tight. So, I was then shocked when Ziwe, one summer, came to me and was able to get this rate. Was able to get this rate. I don't know how we did it. I mean, I can read the proof. It's shorter than my proofs, but I don't know how we did it. So you got this rate. You were creating also a step size. This strip size is actually increasing. But then Nati saw me give a talk, a very incoherent talk, and Nati said, you violated like 10 lower bounds I know about. So Nati and I got talking, and then Zoo and I were like, we don't even know if one over T's optional. Nati's like, what are you even talking about? Then Zoe proved this. He proved his convergence rate. And by the way, this is real. I've implemented the algorithm. He wrote the theorem, I wrote the code, and this thing is actually absurd if you're running the practice. So the reason I'm telling you this story isn't to brag about Zoe. The story is actually that even if you don't care about any of the stuff I said, in terms of running time, the fastest solver I know for this non-smooth optimization problem is to translate it into a smooth, a non-trivial smooth optimization problem. A non-trivial smooth optimization problem. It's not just like, you know, like an infemal convolution trick or something. So non-trivial smooth optimization. So quick question. So why do you want to consider like optimizing over manifold? Because it's like you're optimizing on the sphere with the maximum of the over into the well you you could actually uh interpret what the logs and x group is doing as doing that. You can do it as doing a sphere index. I don't know if it's better to have if it is possible to have better rate of conversions for that, but maybe by considering region design manifold, maybe you can have like a clean proof of. Like I said, the proof internally in some sense is actually projecting the dynamics on the sphere. Okay. But and I can make this precise, by the way. If you consider, so in many of these, we're using the sum x. We're using this. So to make this look This. So, to make this look like the margin function, you put a log in front. But if you really want to make it look like the margin function, you should actually normalize it by this. And this thing starts to look kind of, how do you optimize in terms of w is this convex anymore? But there's a translation into something called the perspective function. If you know convex analysis, something called a perspective function, you split the norm in this thing, and now this is like the projection on the sphere. You don't understand what I said, Dorothy. But yeah, I make something like on the sphere. Yeah, dynamics I like on the sphere. And separately, Zoe proved directional convergence of homogeneous networks in this completely terrifying proof, and that proof projects all the dynamics onto the sphere explicitly. All right, thank you so. Yeah, the projection of the sphere, and that's how Lehnik did his two two-layer implicit bias proofs that I meant proofs. Sorry, I was going to mention them, I forgot. But it's one of these things where you can have intuition for a lot of things, but then what actually ends up making the proof nice is often very different. You know what I mean? Very different. You know what I mean? Like, I ask you to prove Hufing's inequality. You're like, yeah, I flip a coin, coming somewhere from Routine. You're actually proving Huffington's inequality. Like, what's the proof, right? First of all, log some excuse, maybe you believe that, but then there's like the lemma part, which is just some calculus trick or something. Okay, any other questions? I like these questions. I don't understand this one over t squared at all. I can tell you how it works. Yeah, I can tell you how it works. Okay, I mean, does it tell you about a non-smooth operation? Tell you about smooth optimization problems. Yes, it does. Okay. By the way, this, so I'm on this paper, and Naki's on this paper. This is 100% Zoe G. This is, I take zero credit for this because this is just disturbing. So the way this thing works, the way he proved this, is by the duality between this and a certain conjugate function. So it's not actually the exact conjugate. Using a bunch of brain power, you can actually derive a different tool, which is A different dual, which is something kind of like the SVM dual. It looks kind of like just a two-norm where m is a matrix of the data, and q are some dual variables that are the gradients of this thing. So this is like a dual potential associated with this. So he got the one over t rate by analyzing mirror descent on this thing. So instead of analyzing gradient descent on this, you can analyze mirror descent on this, and they're actually iterate for iterate equal. I'm not going to say why, because it's time, but reader for iterate equal. So okay, that's Equal. So, okay, that's how this proof worked. He looked at this, he said I can write it as mirror descent with entropy potential on this, and he got this one over T rate via tons of brain power. It's basically all I can say, because I knew that these were the same, but I was not able to prove this. Again, we went from here to here. Then he asked himself, This thing is actually well-behaved, right? This is like smooth, it's bounded, right? The rating of this lives in the probability simplex. This is a smooth, bounded optimization problem. This is a cookie-cutter. Optimization on this. This is a cookie cutter, it's a cookie cutter classical optimization. So then he ran Nesteroff on this. He ran L1 Nesteroff on this, which got this rate in the dual, and then he translated it into an algorithm in the primal. And you can use this translation technique for any dual and primal objective. So you get a relationship between the sense of being a momentum method in the primal, and it's a momentum method in the primal. So you were talking about grainistic. So you were talking about grandiose central lungs, except for the last rate, which is this. I cheated, yeah, sorry. So this rate is a momentum method, but it's not the standard momentum method. It's still defined in terms of prior iterates, but it's a different weighting, and it's a weighting given by the Nestroff weighting. Sorry, it's momentum in this, and it's vanilla, cookie cutter, nestroff, and this. By the way, if you're curious about this, I should also say while I'm giving credit to Zoe. Also, say while I'm giving credit to Tzue, I don't think this was known, but it's actually possible to handle Mayor Descent and Nestrof in one proof. They're actually the same algorithm. I didn't know this. I don't think anyone else did, but yeah. So in our paper, we have a lemma where we give commercial rate for both, and it says lemma. And that makes all these connections clear. Is there a paper by Alan Jewel? Uh yes. So um uh So that paper is very similar to the paper of prior work by Paul Tsang. So this idea of them being equal is actually due to Paul Tsang. The Paul Tsang paper is very good. Which is the bottom of which paper? Sorry? The exact reference? For Paul Tsang? Does anybody know the title? Okay, I can look up your piece. Would you write down the name of the author? I think you can just hold Sam. Yeah. I think you can just halt Sang, yeah. I mean, um his web page is hosted on Dimitri Corpsekis' webpage channel due to very sad reasons. So on the linear convergence update? I don't know. No, probably not. But this, I should say that, to my understanding, I asked Lee-Vandenberg about this in the Boyden vanberge book. It sounds like this is the paper that got everyone to understand Naskarov method for the first time. Everyone to understand Nasarov method for the first time. This is a very important thing for. So, in five minutes, I can actually tell you the proof technique I want to tell you about. I won't show you the full proof, but I'll show you the key idea. Let me just show you this proof technique, and then we'll stop. The key idea is actually just one, like one word. All the proofs I've told you about so far were very clever. All the ones by Dewey were clever. I didn't mention, but this paper by But this paper by Surya Nadi that you said is not yours, used a different proof technique. So, all the proofs I told you about, let me just finish this idea. All these proofs I mentioned were very, they had clever things in them, non-standard things. But now I'm going to tell you about something I think is much simpler and more general that can prove all these results and also a bunch of good ones. I wanted to clarify for the record that the two papers you mentioned was primarily driven by Daniel Subley. There are other papers by me, but not the stupid. It's very polite of you, but you know. Yeah, I'm one of the unimportant authors in the video. And Moore Maxson is the other important person in that picture. They're all very important in my heart. So let me just tell you this proof technique. So let me just tell you some limitations so that I can erase with the thing I'll tell you. Erase with the thing I'll tell you, and then we'll stop. And if you're curious about the examples, this is a quote for this paper for this here. The version online is a disaster, but I will make it better. But so some of the assumptions we had so far were that we were off games in logistic. No longer true. I can handle square loss. I always used IAD examples in this. It's not true. I can handle Markov data in what I'm about to tell you. It doesn't have to be prediction. It doesn't have to be prediction. There's this fixed-point method called td. Those of you familiar about it, it's like a square to loss, kind of, but it's Markov data. All the prior work I'd seen for it was not in high probability, only in expectation, needed projections. I could remove all of those. Let me tell you the proof technique. And it's still, I think, much simpler than all of these. So what is the difficulty in all the proofs I was talking about? One of the key difficulties in the proof is that the iterates go off to infinity. Iterates go off to infinity. Okay, that's especially difficult for stochastic proof. We try to apply Martingale, any controlled variation of the Martingale when things going off to infinity. A lot of the proofs I told you, we had to do something very clever to get the Maringale to work. So I wanted something stupid so that I could use it. And so here's the proof technique. And it's TD is a fixed point method. It's popular in reinforcement learning. And when I looked, I needed it for something, and when I looked at literature, I needed it for something, and when I looked at the literature, I couldn't apply it. I just couldn't use it because it didn't satisfy any, needed too many assumptions. So, yeah, any assumption you know from the literature I can draw pretty much. So, here's the proof technique. And so, again, the theorem is going to be a general theorem of the type, given a nice comparator and some loss function that, yeah, I won't cryptically, I'll just say they have to be quadratically bounded. They can't be faster than it quadratic tails, but it holds. Than a quadratic tails, but it holds. So if there exists a good comparator, then we get something at least as good as it with good test error and high probability. So here's the proof technique. Okay, we start with our iterative sequence WI. And this iterate sequence, we don't know how to control it because it might be unbounded. You know, like I said, the martingale doesn't work. I want to get high probability. It sounds like a mess. So how do I control it? I'm just going to totally cheat. So this thing is defined in terms of my actual probability. Defined in terms of some, my actual probability space is these xy pairs I observe over time. These random variables live in the probability space defined by these examples. So here's what I'm going to do. I'm going to say, don't know how to analyze this. I'm instead of going to analyze projected iterates. Oops, this section. I'm going to analyze projected iterates where Vi is equal to, sorry, V dot zero is equal to Sorry, v du 0 is equal to w 0. So they start at the same place. And thereafter, vi plus 1, I'll just write it as a gradient method, but it holds for the other stuff. So it's a gradient method. Yeah, I'll just use it and write it as a gradient method. Okay, so it's a gradient method. The key idea, though, is that these are going to be the same. So I'm going to couple these, I couple ballistically couple these. So there So they're coupled in a strong sense. So I run gradient descent on unregularized, unconstrained gradient descent on this data to prove my, but to prove the theorem, I mathematically, oh, I say it's mad at me. Just to keep sure. Okay, well, I'll stop soon because I don't, yeah, it's kind of rude. So I run mathematically VI and I use projections for this. Because I use projections for this, I can use all the I can use all the cheap theorems. So I use projections, and I get a theorem that looks like this. It says VIT minus my good comparative squared, less than or equal to this average loss term that you saw me write before. But everything is in terms of the projected sequence. So then plus. So then plus V0 minus W bar squared plus, then we have this deviations term. And the game you play here is because this is just a mathematical construct, it's not an algorithm you care about running, we get to choose the projection radius so that the left-hand side, which is the norm, is less than or equal to, strictly less than the right-hand side. So you pick the projection radius so that this quantity is less than this. Is less than this. In other words, projection is never used. So you can pick the projection radius if the projection is never used. And because these were coupled, then you need an inductive argument to say under the good conditional, under the good event that made this concentration work, then these two are actually going to be equal by induction. So it sounds like magic, but the proof is really. Like magic, but the proof is really short and versatile. And I was able to use it for. Let me just finish this discussion and we can. Yeah, so again, the proof technique is can't control this. Make up a fake sequence for use projections. I didn't tell you what we projected onto yet. Use theorems from boundedness. Just take your text to the theorem. In fact, you could view this paper as being self-defeating. I said, just to prove a theorem for unabounded things, take your favorite theorem from bounded stuff, probabilistically couple it. stuff, probabilistically couple it, and then just copy and paste the term over. Yeah, you don't actually have to do new work for unbounded settings. Maybe actually I should just stop there and you want to leave for coffee, usually for coffee or you can ask me questions. I don't know if you want to hit the recording button, just in case people want to ask questions. Oh, yeah, I'll stop recording so it's on. Ask questions. Oh, yeah, I'll stop recording so it's all good.