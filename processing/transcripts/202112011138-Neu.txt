So the climbing is kind of nice on the star. Okay, so so I'm gonna I'm gonna try to make this quick. Uh so this is uh this is something that uh that we've been working with we've been working with Galor for like a couple months. It's been taking forever. Uh we've been trying many different ideas and it slowly started to work and then I decided okay let's finish this and the best way to do that is by committing to give a talk about it. So I so at least at least that part was already useful for me. Hopefully it's going to be insightful for you as well. Okay so this one is about Okay, so this one is about deriving generalization bounds for simplified learning. It's this very basic setup that we all know and love. So we have a data set of NIID points drawn from some distribution mu and we have a hypothesis class that's denoted by the squarely w. These sets Z and W are assumed to be very general, they are just measurable spaces, but yeah, if But yeah, if you want to think about these W's as just the set of neural network weights or neural network architectures, whatever. And we have a loss function that maps pairs of W and Z to real numbers. And what you're interested in is studying the properties of learning algorithms that map data sets to hypothesis. So in particular, we're going to denote by Wn the output of the algorithm run on the data set Sm. On the data set SM. And what you're curious about is deriving generalization error bounds. So we want to find a way to bound the gap between the training error, which is the first term in this average, and the population risk of the output that we get. So this is a very standard setup. And the starting point for this work, so there are many, many ways of deriving generalization error bounds. One thing that is recently becoming That is recently becoming popular is something that's called the information theoretic generalization maths for some reason. So, this has been first proposed by Dan Russo and James Zhu in 2016 and further elaborated on by Shuen Roginsky in 2017. So, this is a very general statement that says that for any learning algorithm A, I guess there's a small detail that's missing, I'm going to fix that later. So, for any learning algorithm, Later. So, for any learning algorithm A that uses loss functions that are bounded, the generalization error on the expectation can be bounded in terms of the mutual information between the input and the output of the learning algorithm. So I write this here as the KL divergence between the joint distribution of W and Sn and the product distribution between the two. So it depends on this quantity, the mutual information between the input and the output, and it also goes down at the rate. And it also goes down at the rate of 1 over root 10. So can you say what Wn and Sn are again? Right, so Wn is the output of the learning algorithm run on the data set Sn. Oh, okay. All right, and then this KLWN is really just the mutual information between the two, which roughly measures the number of bits that the learning algorithm leaks into its output WN. So if my algorithm is doing nothing, then this is going to be zero. And if my algorithm is using a lot of bits from the input, then this is going to be large. Yeah? Large. Yeah. You said L is sigma sub Gaussian. Sigma is one in the end. Yeah, yeah, right, right. Okay, so it's if L is sigma sub Gaussian, then there should be like a sigma squared under that squared root as well, right? So there's a sigma that multiplies everything. Okay, so this is very general. This is very nice. I sort of doubt that this is information theoretic in any meaningful sense, because it just has a mutual information term in there. And basically, this whole work started out with me being grumpy about it. Like, why is this called information theory? There's nothing information theoretical about here. And we are just asking this question: okay, what is special about this KL divergence here? So in particular, I'm thinking of this KL divergence term in there as just some general convex function of the joint distribution. And I think that it's very easy to derive this particular bound from the perspective of convex analysis. And this work is about trying to understand, well, this perspective. About trying to understand well this perspective on this particular generalization are amount and trying to understand how much can we generalize it. How much can we copy on this? Okay, so I have this beautiful landscape to make you not feel so bad about being here and now outside. So the question that you're asking here is that, okay, can we just replace this ka divergence with some other function h and get a bound that looks very similar to what you've seen before? What are the conditions on h and that? On h and that enable us to get like rates of 1 over root n, and how do these constants look like? So, of course, that constant should depend on the data distribution, it should depend on the loss function, it should depend on properties of the algorithm, and this is what we are trying to understand here. And our starting point is really convex analysis, because that is really the framework that I like to think when it comes to pretty much anything. Alright, so this is the question that we're. Alright, so this is the question that we are asking, and I think we have some interesting results. So, by the way, yeah, so this is really not published or anything, it's under development. We have some theorems, we are not really sure if it's useful or anything. I just wanted to show you because it's cool. All right. Okay, so basically, most of this work is about just messing around with notation, right? So, once you find the right notation, then it all starts to make sense. So, let me introduce uh uh my notation. Introduce my notation. Okay, so we're going to consider a set delta, which is going to be the set of all joint distributions over W and S, right? All joint distributions over input and output sets. And we put the constraint on delta that S, the data set, should be distributed according to the real actual distribution, according to IID samples. And some important special choices here are going to be obviously the joint distribution. Obviously, the joint distribution that we care about, the particular one of WnSn, right? So, when Wn is really the output of the learning algorithm, run on Sn. And another special case that is going to be interesting for the analysis is going to be this PnR, which is going to be the product distribution between the two. So basically, you need to think of that as a joint distribution of Wn and some Sn prime ghost sample that is independent of S. Okay, so we are also going to consider So we are also going to consider a set that is dual to this, which is f, the set of measurable functions over s and w. And with this, we can introduce this dual pairing, this kind of Banach space notation. We're going to denote the expectation of Fws with Ws drawn from P by P times F. It's a bilinear function. Both P and F live in Banach spaces, which are dual to each other. To each other, right? So, this is going to be very, very useful for most of our results. Okay, so I suppose this is clear, right? So, like W and S are drawn from P, right? And this P times F is the expectation. And specifically, we want to use this notation to act on these functions that live in f, right? We're going to care about the centered loss, right? We're going to take the loss function and then we're going to center it. The loss function, and then we're going to center it by the expected loss evaluated on a randomly chosen independent data point z prime. And then we're going to take the average of these losses over the data set. So this centered average loss, this takes as input a w and a data set, and it is simply going to evaluate this centered loss over the data set and average it, right? So this is clearly a function from w and s to r that is uh that is metro, as long as the loss function is. That is metropolis, as long as the loss function is metro. And you can already see that our notation, like introducing this notation, is paying off because this allows us to write the expected generalization error in this very simple linear form, right? So it's really just Pn times L barn. I guess this is easy to see. Okay, so I'm sorry, more notation. This is the last slide on notation. So these are going to be also useful concepts for state. Concepts for stating and developing our results. So we're going to consider a convex function h that maps elements of delta to real numbers. And it's convex in the usual sense, right? That if you take mixtures of p and p prime, then you evaluate h on this, and this is going to be upper bounded by the mixture of the function values. This standard convexity. And another object that is going to be super important is going to be the finishal conjugate. Important is going to be the finished conjugate of H, or Legend Fanschel transform of H, which is written as H star of F, evaluated for any f of f measurable function, as this. So you just need to look at P times F minus Hf and then just look for the probability distribution that maximizes this. So this is easily seen to be a convex function because it's a maximum of linear functions. And one immediate consequence of this definition. Immediate consequence of this definition is that if you take any p and f in the appropriate spaces and then we consider p times f, then this is going to be always upper bounded by hp plus h star of f. So this is, huh? There's a type on the definition of the definition, right? That's hf should be hp. Oh yeah, oh yeah, sorry, sorry, yeah. This should be h of p, otherwise it doesn't make any sense. Okay, so I think that you know we're. Okay, so I think that you know where this is going. So we're going to use this Fenshal-Young inequality on the generalization error, on Pn times L barn, and it immediately gives us this bound, which is, you know, may or may not be meaningful so far. So this says the generalization error bound, and the generalization error times any parameter eta is upper bounded by H of Pn, by our convex function, and its conjugate. And of course, And of course, the question is: okay, when can we show that the right-hand side goes down at a rate of 1 over root n for the appropriate choice of eta? And in particular, it's easy to see that this is going to be implied if we can show that this h star goes down at the rate of eta squared over n. So the first thought that you can have is that, okay, h star is a convex function, we can use the essence inequality. If you do that, then you get nothing, right? Because then this second. Nothing, right? Because then this second term is not going to go down at any rate, right? Then you only get the constant upper bound, then you're basically done from that from the start. So we want to understand conditions under which this can be guaranteed. So let me give you an example and let me prove this information theoretic bound using this technique, which I think is the simplest technique to prove it. So if we consider H of P as the KL divergence. Of P as the KL divergence between P and the product distribution, then a standard track is that the conjugate of this is just the log moment generating function of f. This is the standard Danskner-Paradigan formula. I guess everybody knows it under different names, but this is a standard formula. So if you apply this to our function eta ln, then we're going to see that conditioned on Wn, right? If you look at that expectation conditioned on W. If you look at that expectation condition on Wn, then we see that that whole thing is really just the exponential moment of a sub-Gaussian random variable with IID random variables, and the sub-Gaussian constant of IID random variables goes down at the rate of 1 over root n, and as a result, you get an upper bound very easily on this that is of the order eta squared over n with a constant that is the subgaussian constant of the of the losses. Okay, so this is very simple. This is very simple. So, if we just plug this into the bound that we had, then we immediately recover the standard information theoretic bound. This is the exact same thing, but you showed it before. Yeah, this is the exact same thing. And then a proof like so fast, yeah. Okay, so let me give you another example, which I'm not sure like how well known this is, but this is, I think, also obvious given this notation. So, if you consider, instead of the KA divergence, we consider the chi-squared divergence. divergence we consider the chi-square divergence between P and the product distribution. So the chi-square divergence is just a weighted L2 distance whose conjugate turns out to be the variance under the product distribution. Then again we have this nice property that the variance of IID ranking variables goes down at the rate of 1 over n, right? And then we see that the conjugate can be bounded as eta squared times the variance over n. Over m and again, this gives a generalization error bound that is in terms of a variance and the chi-squared divergence between the joint distribution and the product distribution. So this is like super simple, just a few lines. I'm not sure if I've ever seen this written down in a paper, so I'm not sure if this is new or no, but well, anyhow. Now this compared to the previous bound is incomparable or right, right, right. Okay, so this one does not require sub-vell sentient, right? So this does not Requires some Gaussian theory, right? So this does not need the losses to be bounded. All that you need is that their variance is bounded, which is actually like a huge upgrade. But we are paying for it because the chi-square divergence is an upper bound than a K-L divergence. So this is always larger, that factor. And it can be actually a lot, lot bigger. So there's definitely a trade-off here that I'm going to talk a little bit more about later. Because I I think that maybe the general results are gonna be like a little bit more suggestive as to what is going on. What is going on? Okay, so how do we pick H? Okay, so here is our construction that we found to work. It's going to include some other notation, I'm sorry about that, but this is how it is going to look like. Our choice of HP is going to take the following form. It is going to be the expectation of some crazy function. So let me walk you through that. So this notation, this kappa P of S, this is This notation is kappa p of s, this is the probability kernel from s to w under the joint distribution pws, right? So this is really just a conditional distribution of w given s equals lowercase s, right? Under p, right? So this kappa p is actually a linear function of p, which is neat. And then what we're gonna do is that we're gonna take this kappa p of s for a random s, and then we're gonna chuck it into a convex function lowercase h. lowercase h, which is going to be a convex function acting on distributions over w. So this is no longer a joint distribution, this is a distribution over w. H is going to act on these distributions. Okay, and then our h, then our capital H is going to be, we're going to just take this little h, evaluating it on this on this random measure, right? And then taking an expectation obvious. So actually all f divergences are of this form. Are of this form, which can be seen after a little bit of thinking. But for now, I'm just going to leave it at this. And I'm going to impose one more condition on H, which is that this lowercase H not only needs to be convex, but also strongly convex. So that this convexity condition is going to hold in a stronger sense, that H is going to have some non-trivial curvature as well. So this rules out H's that are like totally flat. Okay, so this is like one possible definition of strong convexity. It says that, well, this is going to be satisfied in a powerful strong sense. And importantly, the strong complexity has some parameters. So alpha measures, well, like the strength of the complexity. So as if alpha is zero, then of course we are talking about flat functions, and the larger alpha is, the more curved. And the larger alpha is, the more curvy our function is. And also, strong complexity is also measured in terms of some norm. And this norm, the choice of this norm, is also going to show up in the guarantees. Okay, so let me state finally the bound under these conditions. So if this is our family of H that we are considering, then the generalization error on expectation can be upper bounded for any algorithm again by the following. By the following expression, it is going to have the square root of h pn, right? Like h evaluated at the true joint distribution between wn and sn. And we are going to have 1 over alpha term, right? So the more strong the convex we are, the better the guarantee is going to become. We still have a rate of 1 over root n. And what's interesting is that the dual norm of L is also going to show up in this bound, right? So in particular, what So, in particular, what we have is that we evaluate the loss function at some randomly chosen data point z. And then we're going to look at the dual norm of this. Well, the dual norm is the dual to the one that we have strong convexity in. And then we're going to take an expectation of all of this. So in some sense, this uh this norm, well, measures a notion of regularity of the loss function. Of the loss function. And the dual norm is defined in the usual way. Okay, so again, just getting back to those examples that I talked about. So if you're talking about the KL divergence, then we have strong convexity with respect to the total variation distance with a constant one, and then the dual norm is the infinity norm. And then we almost recover from this argument the original one, the original band, except that, well, Except that, well, we cannot do sub-Gaussian thing. We need to have bounded losses, but for bounded losses, we do recover the original thing. Also, if you consider the chi-squared divergence, that is strongly convex with respect to this weighted 2-norm. Well, I mean, the chi-square divergence is a weighted L2 norm itself, which is strongly convex with respect to itself. And as a result, the dual norm that shows up there is again. That shows up there is again just the variance of the loss function. Okay, so these are just the basic examples. So we cover all of that and hopefully more. Do you want on definition? You had a norm on those measures, or like what norm are you using in the soup? Right, yeah, yeah. So the norm is the one that we use in the definition of this strong convexity. And this needs Convexity and this needs to be a norm over distributions over W or at least over that set. It needs to be a norm. And when it's, so for KL, the norm is L1? Yeah, so the KL divergence is strongly convex, so it satisfies this inequality with the total variation. This is actually equivalent to. Total variation. This is actually equivalent to being Scarcina call it very, yeah. Yeah, in that case. Is there a jewel to the South Piacian norm? Yeah, I wonder. Yeah, so I'm full of these questions. I'm gonna get to these in the end, right? So yeah, we had this very general statement, right, with all these norms. I showed you these examples. Well, you know, this already shows that what kind of works. But in order to make this interesting, well, we need to find some more. This is interesting. Well, we need to find some more H's. I mean, even though I recover the DKL, it may be that it's also convex with respect to the dual to the sub-Gaussian document. Yeah, that's impossible. Yeah, so we have some other candidates as well. Like, for example, if you take the square Wasserstein distance, that may be strongly convex with respect to some norm and some dual norm. Yeah, I have a bulleted list of these interesting objects at the end. These interesting objects at the end. But let me talk about the proof because I think that is like the really cool part. This is where we really unleash all the convex analysis machinery. And what I really like about it is that I come from online learning, and there's like a hidden online learning proof in the analysis of this. Or at least this is the way that I think about it. Of course. Yeah, of course. It has to do with my operating, of course. Of course. So okay, so let me give you the proof idea for like uh under like a slightly stronger condition that H capital H itself is strongly convex in P. Because in the theorem our lowercase H was strongly convex and the conditional distribution, but the argument is a little bit simpler for H itself strongly convex. Okay, so I'm going to define this potential function, which looks an awful lot like the Fenscho conjugate that I've been thinking about with the difference that this is pretty. With the difference that this supreme is going to be taken over some special set that I designed for the analysis. I'm going to specify that on the next slide. Okay, so I'm going to define this potential function, phi of eta ln bar, using this final conjugate-like thing. You can see that this is a convex function of ln bar. And what we're going to use is that it's also a smooth function. Okay, so I'm going to define this additional definition, which I'm going to generalize this L and bar. I'm going to generalize this Ln bar to Li bar, which is just going to be the partial sum of rewards, or a partial sum of centered losses. So I'm summing not all the way to n, but to i, but I'm still normalizing it 1 over n. And with this notation, I'm going to just decompose this potential in this telescopic form. So just write it as like this trivial telescopic sum using that phi at 0 is 0. Okay, so like uh nothing uh nothing uh really shocking here. Uh but here is when I'm gonna start using convexity a lot and strong convexity and strong smoothness duality in particular. So what I know is that if my function h is strongly convex, right, so that it has curvature, then a standard fact from convex analysis is that its conjugate, this function over here, is strongly smooth. Here is strongly smooth in the sense that it doesn't blow up too fast, right? So the curvier my function H is, the smoother the dual of that function is, so that it doesn't change too fast. And this is going to help us bound the increments between the potentials. So in particular, this is going to, right, so I use both convexity and smoothness of phi, but I guess mostly, you know, mostly smoothness, right? Mostly smoothness, right? So, in particular, this is going to imply that those difference terms are going to be upper bounded in terms of, okay, so this is like a first-order Taylor approximation, right? And what this band says is that, well, we are not too far from the Taylor approximation. We don't go too far above that. Okay? So if you just squint at this a little bit, then you can realize that, you know. And you can realize that, you know, just again some more like juggling with the notation and everything, you know, see that this second term is actually like this eta squared times dual norm of L bar of the center loss over n squared, right? Because the n squared is there because of this normalization constant that we have in there, right? And then we're going to sum this up for all the m's, and that is going to give us the eta squared over m. And then it really remains to understand what is happening with this. What is happening with this, with this first term, with this first and third term? And really, most of the work in the proof goes into showing that this is actually zero under the appropriate choice of delta and h. And this is where the choice of h is going to be important for us. Okay, so let me explain this construction of delta because I think it's kind of funky and maybe insightful as well. Maybe insightful as well. Okay, so in order to define this delta n, we're going to define a set of ghost samples. You just need to think of this as like a test set, right? It's just an independent copy of the original sample. And then for all i, we're going to define a mixed bag of data points that has the first i data points as the training data points, and the rest are padded out with the test points. And then for all of these mixed bags, All these mixed packs, we're gonna run the algorithm on this mixed back to get wi, and then we're gonna use pi to denote the joint distribution of this wi, those trained at this mixed back, and the training data. So you can see that pn is really like the joint distribution, and p0 is the product distribution. So this is somehow, you know, moving from the product distribution to the joint distribution as I increase. To the joint distribution as i increases. And what we're going to do is that we're going to define our delta i as the convex whole of all of these p's. So basically, we just consider n of these data points and then we just construct these p's and then consider their convex whole. So, this way, so this allows us, so this is the delta n that showed up in this definition. So, you basically went from taking the supremum over all measures to just taking a supreme overall measure. Do just take an asseprem over this n-dimensional object, which really makes thinking about this whole analysis a little bit easier. Okay, so this is how we construct our delta n, and then exploiting the properties of this and also of the function h, we managed to show that that linear term is zero. Okay, so again, a little bit more convex analysis. Vax analysis. So, first step is to look at this function, which, right, okay, so this is phi, right? Just phi is the max of this, right? And a standard traction convex analysis is that if you take a function of this form that is linear in L and convex, or like concave in P, then the derivative of the maximum with respect to L is really just the argument. Respect to L, is really just the arcmax. So this is called Dunskin's theorem, or this is like an application of Dunskin's theorem. And then, crucially, using the definition of h and these deltas, we show that this maximizer, that this gradient, this has to be somewhere within delta i minus 1. And that is because basically, for all the p i's that have index greater than i minus 1, so for all the p's that have So for all the p's that have large indices, these products are all constant, right? So it doesn't really matter which p i I pick, as long as it's smaller than, as long as it's larger than i minus 1. And also, the other fact, which is how we use the construction of h, with this lowercase h and all that thing, is to show that this h is just monotonically, well, so it's non-decreasing as I increase the index. Increasing as I increase the index. So as I get more and more data points from the training data, my H is going to increase. So as I walk from the product distribution towards the joint distribution, I'm just increasing. So this H really truly measures the distance between these things. Well, the actual argument is slightly bit more complicated, but this is the gist of it. Okay, so now it really So now it really remains to see that if we consider any p tilde that is in delta i minus 1, so importantly this p tilde and this w tilde that is in here, this is an algorithm that has only seen training data points, the first i minus 1 training data points. And it has never seen the ith one, right? So for this reason, this expectation, which is like the difference, which is like the linear term. Difference, which is like the linear term that you want to ban. So, for this reason, this expectation is zero, right? Because this w tilde is independent from zi, right? So, this has the same distribution as zi prime, and for this reason it's zero. Right, okay, so I suppose that's that's uh, yeah, well, it's done right, yeah, so I just have some concluding comments, so I guess. Some concluding comments. So I guess like really the insight that, well, I gained personally from this is that yes, you can go beyond this information theoretic techniques for proving these generalization bounds. You can use convex analysis and you can get some nice results. There are some interesting trade-offs implied by the raw form of these theorems that we have. So one thing that you can immediately see that the more strongly convex Immediately see that the more strongly convex your function is, this more strongly convex h is. So as alpha increases, then your function gets more and more curvy, and as a result, its function value also increases. So h over alpha is the one that showed up in the bound. It's non-trivial how to pick this trade-off correctly. And of course, what's more interesting is the choice of the dual norm for L. These dual norms can capture different types of rates. Norms can capture different types of regularities about the loss function. Like boundedness or bounded variance. What I'm mostly interested in is norms that measure the smoothness of W particular. So that is sort of the direction that I want to take this into. So of course you can show that all sorts of F divergences satisfy the conditions that we have as long as F is strongly comics. These are not so exciting because these are strong economics with respect to the total variation norm and Respect to the total variation norm, and we need to find some more exciting norms to get some real interesting results. Another interesting choice, which is something that I already used in a cold paper this year, would be to use the smooth KL divergence. So instead of the KL divergence between W and the marginal distribution W prime, you just add some Gaussian noise to W and then take the KL divergence between them. This has to do with the With the Wasserstein distance. And eventually, that is sort of where I want to go with this: to show that the Wasserstein distance satisfies these conditions with a dual norm that captures some info properties of the loss function. And another question is how do you do high probability bounce from this? Well, I have some ideas regarding that because if you look at the first step in this proof, this Step in this proof, this potential decomposition. This looks an awful lot like a Dew Martingale. So there is definitely a Martingale decomposition that should be possible here to get some high-product amounts. But that is a thing that is absolutely not finished yet. Okay, so that's all I had to say. Thank you very much for sticking. 