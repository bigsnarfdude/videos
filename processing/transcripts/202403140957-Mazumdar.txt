And I think you can see my screen as well. Okay, thanks a lot to the organizers for inviting me to the workshop. It has an amazing name. I hope I work in this area. This area. So, the work that I'm going to present is a joint one with Daniel Su from Columbia. It is about the sample complexity of parameter estimation in logistic regression. And feel free to interrupt me during the talk. I don't know how smooth that is going to be, but yeah, just don't you don't have to hold your question till the end. So, we do have. So, we do have a data generation model for classification, and this is something that you may have seen somewhere earlier. So, we have this data. You might be able to see my cursor over here. This x1, y1, x2, y2 up to xn, yn, and samples of data that are coming IID to us. We assume x is generated from some standard normal process, and y is. And y is a binary random variable. It takes two values: plus one and minus one. Plus one happens with probability one over one plus exponential minus x transpose theta, and minus one with probability one minus that, which happens to be this. Now, given n samples of such of data generated in that way, we would like to estimate the parameter theta, which is a d-dimensional. Theta, which is a d-dimensional real vector. So we have this input and we want to output this theta hat. Why would we want to look at something like this? One reason is that this is a very standard model for non-separable data and classification. It's extremely well studied, logistic regression. And secondly, this is a very special case of generalized linear model because if you look at the average value of the response given Value of the response given the covariates, it's some function which is called the link function or the inverse of the link function of x transpose theta. So it's some linear model and then there is just some non-linearity on top of this. So these are like extremely well studied in statistics. So for this problem, we do have a standard estimator, which is the maximum likelihood estimator. It has a very Likelihood estimator. It has a very nice form. If you write this out, it turns out to be minimizing this loss function, which is also called the logistic loss. It is log of 1 plus exponential minus yi, which are the responses, times xi transpose, which are the covariates times theta. So you minimize this over the domain of theta. So there are many, again, this is extremely well studied. Uh, again, this is extremely well studied. So, if you have a large enough number of samples, then a lot of things are known from classical statistics. For something like from the textbook, you know that this ML estimator, theta ML, it's actually a Gaussian random, like it in distribution with large enough number of samples. It is a D-dimensional Gaussian random variable whose mean is the original data generating. Is the original data generating parameter theta, and its covariance matrix is this inverse of the n-sample feature information matrix. So now we can try to estimate some dependence on of the parameter error on the number of samples and dimensions from this expression. You see, since this is a d-dimensional problem and this is n-sample feature information, we can, barring some constant, we can. And barring some constant, we can think that this variance to have power d over n. Therefore, the estimation error between theta maximum likelihood and the original data generating theta is somewhat like, again, like neglecting constants. This is something like square root of d over l. So, in a hand-wavy way, this gives some expression for sample complexity in terms of dA and epsilon, which is d over. Epsilon, which is d over epsilon squared, right? So, if you have Euclidean parameter error epsilon and it's a d-dimensional logistic regression problem, then results like this are available where you will see that the sample complexity scales as t over epsilon squared. Very good. We can try to make these things more rigorous. But there is something surprising here, because if you look at this model, y is plus one with this probability and is plus one with this probability and minus one with this probability. And you let the norm of theta to go to infinity. So this is, so just assume theta is very large. If theta is very large, then if x transpose theta is positive, then exponential minus theta. So this value is going to be very small. And therefore, it is going to be plus one with probability one, depending on what is the sign. Depending on what is the sine of x transpose theta, is if the sine of x transpose theta is plus one, then y is plus one with probability one when theta goes to infinity. So at theta equals the norm of theta being equal to infinity, this basically reduce to this separable data case where y equals to sine of x transpose theta. And again, this is called, then if we have samples. Then, if we have samples from this model, that's also extremely well studied, all the half spaces or linear threshold functions. It's known from standard learning theory textbook that D over epsilon samples are actually sufficient to recover this within parameter error epsilon. So, usually people look at prediction error in this learning theory textbook, but you can convert for the case of half spaces with Gaussian co-group. Of half spaces with Gaussian covariates, the prediction error to the parameter error. So now, so this is again very standard. I'm not going to go over how you arrive at this, except for the fact that if you're really choosing your x to be Gaussians, they're sort of uniform in all directions. So you can really just try to do a simple argument where you want to construct a covering of the unit sphere. Covering of the unit sphere. By the way, you see that here the norm of theta is not preserved at all. You are no way going to recover the norm of theta. But we are not interested in it right now. If this epsilon is just an error in direction, then we can just construct a covering of the unit sphere and basically show that any two of them, two points from that covering are going to be separated by one of the Gaussian measurements that. By one of the Gaussian measurements that you are doing, the lower bound is also straightforward. It follows from the fact that if you sort of try to segregate a D-dimensional space with n hyperplanes, the max number of parts that you are going to get is approximately n 20. Okay, so this upper and lower bound sort of matches barring maybe some constants. So you have d over epsilon. Okay, so now this gives us a mismatch because from Are a mismatch because from classical statistical theory, we just saw that. Um, is there a question? No, sorry, okay. So, yeah, from classical statistical theory, we saw that for the case of small, when the norm of theta is small, we should expect something like d over epsilon squared to be the scaling of sample complexity. Whereas when norm of theta is large, we are getting d over epsilon. So, therefore, a natural question. So, therefore, a natural question is to look at what is the role of norm of theta in the sample complexity. So, we can have a new data generation model now where we are sort of taking out the norm of theta as a new parameter called beta. And we restrict our parameter theta. Now, we call that theta star to be in the unit sphere. So, the data generation model is exactly like before, x are generated from this. Before x are generated from the standard normal process, and yi is plus one and minus one with the same thing, but theta, the norm of theta star is now one, and you have this new parameter called beta in front, right, which is playing the role of the norm of theta. And we are interested in looking at the scaling of sample complexity with respect to beta. So, there is a name for beta in the literature, it's called the In the literature, it's called the inverse temperature, and you can see why. Because usually, whenever you have this statistical physics model, you have this X transpose theta star divided by some capital T. So this is the inverse of that capital T, it's inverse temperature. Our parameter error is whatever estimator we are getting, theta hat minus theta star, Euclidean norm of that. So, what we want to do is to mandate that theta, this parameter error theta. Theta, this parameter error theta star minus theta hat to be upper bounded by some epsilon, either in expectation or with high probability. And we want to compute the sample complexity, the optimal sample complexity n star as a function of b epsilon and beta. Yeah, and look at the scaling. And hopefully, we'll be able to resolve between the we will find the smooth transition to large beta to small, sorry, large beta to small beta. Okay. Small bit. Okay. Great. So that's our question. And this is our result. We can show, like, I don't know if, so we were not expecting this at the beginning. We thought that there will be one change point. It turns out that there are two change points. So this n star d epsilon beta, this is given by, this is equal to d over beta square epsilon square when beta is small enough. That means in the high temperature regime. You can also think of this as the Also, think of this as the extremely noisy case. The SNR is really small. So, beta is sort of playing the role of SNR here. It is D over beta epsilon squared when beta is between 1 and 1 over epsilon. This is the moderate temperature case. And when beta is over 1 over epsilon, so it's beyond the limit of what is the parameter error we are allowing, then that's the really low temperature regime, and it falls. Temperature regime and it falls back to be d over epsilon. Now, if you look at this corner points at beta equals to one, beta equals to one over epsilon, you will be able to see that there is actually like there is a transition which is sort of smooth. So for example, in the moderate temperature regime, if you replace beta to be one over epsilon, you get d over epsilon. So, and after some value of beta, it really doesn't change, the scaling remains the same. So, that means when you go to When you go to low enough temperature or low enough noise, it is not changing. Okay, so that's the main result. In the rest of the time that I have, I'll try to give you some idea on how to prove this result. Okay, so what is one standard way of recovering this? We can go back to the maximum likelihood estimator, which is minimizing this logistic loss. Minimizing this logistic loss. And we can make some simple observations, which is if you look at the expectation of this logistic loss over the data, it has a nice form. So for example, if you take expectation over the data that you have, L of theta for any arbitrary theta and L of theta star, so the difference between this is also called the excess loss in some literature. Then this is. Then, this is expectation over the covariates. You just, it is some KL divergence between two Bernoulli random variables that has this form. So, and we can get pretty good approximation of KL divergence because there is like you can write it, write out the Bregman divergence form for KL divergence and do some calculation involving this Gaussian integrals. And in particular, what you will arrive at. Particular, what you will arrive at are pretty good upper and lower bound on this term, on this excess loss term. Okay, so this actually gives us some idea of lower bounds on sample complexity, because since we have derived some upper bound on KL divergence, it doesn't have to be theta and theta star, it could be some theta and theta, um, theta prime for any two arbitrary theta and theta primes. Then theta primes, then we have for two different data generation theta and theta prime, we have an upper bound on the KL divergence. And we can plug that in with some generalized version of Thanos inequality, which can be found, you know, for example, on N42 or like some, you know, some Bayesian version of Thanos in this, like in some other papers. So you can just plug that the upper bound on KL divergence with this pretty standard. With this pretty standard tricks, and you arrive at the lower bound on sample complexity. So, this actually solves our lower bound problem in the moderate and high-temperature resign. We don't get the, we get a sub-optimal lower bound using Thano. I mean, maybe that is because we have not tried like with all guns, like in the in the low temperature design. But the low temperature design, the lower bound is not that difficult. It follows from the It follows from the lines of the zero temperature case. When beta is equal to infinity, I just said that you can, you know, how many different partitions can you form of the high-dimensional space with this many hyperplanes. So just by using that type of counting arguments, you can get a lower bound. So yes. Arya, can you hear me? Hey, Yuri here. Hey, yes, I can hear you. I can hear you. Can you say what is the generalized version of Fano? I just never heard of this. I think I wrote a book on Fano. So I'm just curious, what is that? So, well, for example, you can take your paper with the Arimoto Channel Converse paper, right? With Vartu, and you had a couple of different versions of it. So it's by generalizing, it's some sort of new construct. You know, you come up. construct, you know, you come up with a bunch of theta, theta, like a bunch of different thetas, and you just to which which can be for which are all within distance, say epsilon from some central theta. And since you have an upper bound on KL divergence, you can just plug that in in this fashion in this multi-hypothesis testing lower bound from Fano, right? So yeah, so any of those. So, yeah, so any of those are going to be this is nothing fancy. So, you just want to come up with a large number of theta that are. So, it's like you take a two-epsilon ball and then come up with a number of theta. Each of them are epsilon distance apart. That's exactly the standard Fano method. That's the, yeah. So, that's the only, like, yeah, it's sort of a local funnel. Yeah, that's the standard funnel. Okay, okay, cool. Yeah, nothing, nothing you can say about it. Yeah, nothing fancy about it. Yes. Right. Okay. So the lower bound is sort of simple. So what we were stuck for quite some time was the upper bound question. But we do have this really nice scale divergence lower bound on the excess loss. So we can try some of the learn standard learning theoretic approach, right? So we can try this logistic loss minimization and we And we know that this excess loss. So, for by the way, this is the KL divergence. We have really good lower bound on this thing. But this theta ML is also a random variable because it is sort of found from minimizing this, minimizing from the data. So, we can take one other expectation over theta ml. It turns out that from like if you just write. That from like, if you just write out a couple of lines, this is upper bounded by this, you know, they call it supremum of this empirical process, where you need really strong, you know, approximation, sorry, concentration inequalities over here. And we can try to use those type of concentration inequalities. Turns out that you get by using the fact that the logistic loss is actually Lipschitz. So, you do some symmetrization over here, and then there is contraction because it is lip shapes. You ended up getting a suboptimal dependence on theta. So, this very standard learning. The reason for that is we cannot have really good approximations on the moments of the logistic laws, right? So, all these are, so this concentration inequalities are usually calculated using some version of one. Usually calculated using some version of Bernstein inequality where you need to calculate all the moments. But if it is the logistic function and we are trying to find the Gaussian moments of this logistic function, it turns out that we don't have or we couldn't find good approximations for that. And we ended up getting a suboptimal dependence. There might be one other reason why we are getting the suboptimal dependence is because this supremum is actually very loose. Is actually very loose. So, again, in standard learning theory, like you come up with this notions of Pradhimakar complexity and so on. Whereas all we needed to have this supremum was around the theta star, right? So they have these notions of local Radhimakar complexity. However, we were not able to calculate that well in this case, and therefore we sort of failed to get the optimal dependence that we promised in a couple of years. Dependence that we promised in a couple of slides back. Okay, so we couldn't make the learning theoretic approach work. One other approach that we tried is an optimization approach where we use the fact that the maximum likelihood estimator over here is actually a convex function. So, like this logistic loss is convex. So, using the convexity argument, and then we can try to have tight bounds on the Taylor expansion of the gradient of the loss. Expansion of the gradient of the loss function. So, there are some papers analyzing logistic regression using this, like self-concordant analysis of logistic regression Bach and then Ostrovowski and Bach few years back. We tried that approach and it ended up giving us a sub-optimal dependence on beta. We didn't really get what we've again promised a few slides back. So, at that point, we So at that point, we given up on logistic laws, which is something I'm going to pose as an open question. Like, this maximum likelihood estimator in practice works fine, but we didn't really, we were not able to analyze it to get that optimal dependence, optimal scaling of sample complexity that I proposed three or four slides back. So, what did we try after that? We said, oh, okay, why don't we? We said, oh, okay, why don't we? It's a classification problem. Why don't we look at the 0, 1 loss? Whenever we are talking about 0, so you see, we are giving up on the convexity of the logistic loss. So this becomes much difficult computationally. However, there are very strong generalization bounds from 1970s even on zero-one loss. So it's again exactly this excess loss over here, but in terms of this metric. So we can relate this. So, we can relate this excess loss. Previously, this was the logistic function. We were able to relate it to scale divergence for all beta. It turns out that this can also be related to the parameter error theta minus theta prime, but only when beta is greater than 1 over epsilon. And this actually solves our low temperature case or very high beta case. So, there we get back this d over epsilon bound. Back this d over epsilon bound just by analyzing the 0, 1 loss. But it is not computationally efficient, except for the exactly beta equals to infinity point, where it becomes solving a linear program. Okay, so now we are left with the moderate temperature and the high temperature regime. So in the high temperature regime, what works is this linear estimator or the it's called the temperature. Or the it's called linear estimator by Plan and Rashnin. It was called the average estimator by Sarvedio. These are all work from in the decade of 2000s. So since like we had to have a little tweak, since we are restricting our theta to be unit norm, our linear estimator is just argmin over all unit norm theta, then just this average of yi xi transpose theta. Of yi xi transpose theta. So this is simply this thing normalized, so projected back to the unit sphere. So you just do this average thing, then normalize back. Again, we can have this type of excess loss argument. Moment of this functions are much more manageable. It's some form of Gaussian. Therefore, we can use the Bernstein equality and so on. So these are all done in some of Plan and Rushmin's paper. And this we can just Paper, and this we can just pick up their analysis. It solves the high temperature case. Okay, so you see, so when we have the linear estimator, we have now resolved the high temperature case. With the zero, one loss, we have resolved the low temperature case. The only thing that we are left with for quite some time was the moderate temperature regime. By the way, how much time do I have? Five minutes or so. So, minus like a couple of minutes. To minus, like a couple of minutes, maybe. Okay. Sorry, I couldn't hear it, but I'll just take a couple more minutes. So, in the remaining regime of moderate temperature, there is actually a key observation that were made in a paper by Kuchelmester and Van De Geer very recently. And this is the observation that the logistic loss that you have can be divided. Have that can be divided into two parts. One part is bounded, so you know, and the other part is this thing that we call the Releu loss. So, what is this? So, whenever this y x transpose theta, if this is, so just to see that this is correct, so when this is actually greater than zero, this second term is zero. And in the first term, since this is greater than zero, just have the absolute value of x transpose theta, which is positive anyway. Transpose theta, which is positive anyway. So that's true. So when this is less than zero, then this is negative. So this indicator function will just be one. And you have the absolute value of beta x transpose theta. You can write it as log of exponential beta x transpose theta and take it inside of this log and you will get back this. So now this first term over here in this expansion is bounded. This expansion is bounded. So the logistic log loss were not bounded. So that was a problem in calculating, in computing all these concentrations. But now we have the unbounded part coming as the Rayleigh loss. So what is the Relev loss? The Rayleigh function is just the max of zero and some of this. It's a very simple thing. So what we have is, so we are now looking at this loss function where we take minus. where we take minus of yi xi transpose theta. This is exactly this part. So whenever y x transpose theta is less than zero, that means we have a mismatch. We did a wrong prediction. We take the absolute value of x transpose theta. Otherwise, we don't add up anything. So this is the Releu loss. The excess Releu loss, it turns out that the expectation over data, this bounded part. Over data, this bounded part, this is not going to be dependent on what theta is. So it just cancels out, and the excess value loss is exactly equal to the excess logistic loss. But for this, we can control this term very well with the KL divergence lower bound that we found on the excess logistic loss. On the other hand, moment of the Releu loss, if you look at the Releu function, this is very similar to the moment of the To the moment of this linear function, right? It's except there is a max of zero and this, but that can be handled. So this actually gives us very tight upper bound on the concentration of that supremum. And we also have pretty good lower bound on the excess loss via KL divergence because we can connect this readily to the logistic law. So that solves our moderate temperature region. So in summary, we Summary, we get this sample complexity scaling by using three different estimators in three different regimes. And we think these are actually good approximations of the logistic laws in this three different regime. And the logistic laws should have been analyzable and that can could have given us, you know, one estimator could have given us all these three things. But we were just unable to do that. So I'm just leaving that as an open question. Leaving that as an open question. There is also a question of computational tractability and if there is any gap over here, because even if you are using logistic laws, it is unit. So it's a non-convex projection that you have to do because theta is, you want to restrict theta to be unit norm. So we believe there is no gap, but there might be some computational statistical gap over here. That's all. Thank you. 