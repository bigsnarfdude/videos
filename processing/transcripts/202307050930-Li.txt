The real cell types feature. So it will further reduce the performance of those tools to identify those real cell types. But before we discuss our method, we first want to answer a question. So do we need a dedicated tool to identify and discover the novel cell types? So if, I mean, those tools, although they are not developed specifically for the task, and they can still do it, right? So in their tools, So, in their tours, they can still give a table, a label, say that this cell type is undefined. So, if their performance is good, then we don't need to develop a dedicated troll. So, we perform some experiment, and this experiment is in silico. We use some, and this data set is actually not very difficult. We just do the intro data set evaluation. So, we randomly select one, two, three, or four cells. Three or four cell types, and we mask their cell type and we try to create this kind of undefined cell type manually. And we check those tools performance. So those evaluation criteria are actually quite easy to understand. So the first one is first positive rate at 95. Second one is your RSA. Last one is AOPRC. So the first one, first positive rate, is essentially the lower, the better. For the second and the third one, it's the higher. For the second and the third one, it's the higher the better. So, we can say if we consider only consider the EOPR say, so most of the tools, if the task is simple and we just have one type of undefined cell type, the performance is quite okay. However, if we increase the number of undefined cell types on our cell types, the performance drops a lot. And if we consider AURLC or the first positive rate, so in fact, their performance So in fact, their performance of identifying novel cell types are not very good enough. So the main reason is not those tools are not good. So those tools are still very nice tools, but they are designed, not designed for this specific task, and thus they are not good enough to identify the normal cell types. So that's why we want to develop such a tool. And here is our pipeline. So essentially, we are using the neural networks to ask the backbone for this IC. As the backbone for this SE neural model. So we have the training stage. So during this training stage, it's essentially the same as the supervised neural networks. But during the query stage, we have some sophisticated analysis. That is, we will use some specific pre-processing to the output of the neural network. And then we are further put into the neural network to get some score. And this score serves as a thresholder. As the thresholder, if this thresholder is, if the scope is higher than the thresholder, then we are considered as a novel cell, and then we will do some further downstream annotation and analysis. For the analysis, I guess I'm very familiar. I'm not going to discuss in detail. Let me discuss how we check the query stage. So, for the training stage, it's essentially the same. We use neural networks for the feature extraction. We use SoftMax to calculate the cell type probability, and then we use. The cell type probability and the way you cross-entrophyler back propagation and so on. So I'm not going to go into detail here, but I'm going to discuss this query stage. So, well, in other words, the novel real cell type detection. So essentially, we have a very critical step here. That is the barcoder pre-processing. So you can consider this back processing as digitalize or binaryize the output of the neural networker. So we essentially So we essentially use this to enlarge the margin between the typical cells and the novel cells. And then with this backcoding, we further get some output from your network. Eventually, we will combine the backcoding with the output from your network to get some scoring function. So we define this scoring function using four different modes. So if the score is higher than a certain threshold, It's higher than a certain thresholder, we are defined as typical. If it's not, then it's novel. So currently, the threshold we do is based on the statistics from the data. And in the future, we're thinking of probably this threshold can also be learned. So this is essentially the pipeline. In fact, not very difficult and it's quite straightforward, but the performance is quite interesting. Quite interesting. So, here is the performance compared to the previous methods. So, on this introduction evaluation, we found that we can improve the performance greatly. So, for this first party rate, we reduce the significantly, while for the AOLC and the AOPRC, we also improve the performance over the previous methods significantly. But of course, Um, but of course, um, we said that the real cell types are very important, so we perform some dedicated performance evaluation on the real cell types. So, we select the top C-normal most real cell types, say that one, two, three, four, the most real cell types for testing, and the other ones so as the typical cell types. We found that we can outperform the second best method as a learn by around 15% AOC on average. And here, And here are the illustrations. So, due to the limited time, I won't go through it in detail. And also, here are further some UMAP variation and calculated AOL say. So, we can say that regarding the variation, our kind of prediction is very close to the ground truth. For the other tools, because they are not dedicated for this task, so their prediction sometimes can seem to be random, but this SALE. Can seem to be random, but this SALEN is indeed better than the other methods. Yeah, um, and we further want to check. So, uh, the previous experiment is kind of in silicon and is considered as symbol. So, in the real case, we probably want to do some cross-dataset annotation. So, probably we just have one reference dataset and we have some new data set coming. We want to do that some annotation. So, we perform such experiment. So, we use the CellSIC data set as. The CellSIC data set as a reference, and we annotated like five different other data sets. We also used this 10x Chrome V3 as a reference and annotated the other five different data sets. We found that on this cross-dataset annotation, we can also outperform the other methods by at least 7%. So, here are also some viralization and the AOLC completion. We also want to check our methods. And we also want to check how the method is scalable or not. So, here is the running time comparison. So, here we didn't perform the very large data set yet, but in fact, we run our data sets. We run our method on the meaning scale, for which I'm going to introduce later. So, essentially, because this is a neural network-based method, we just have the training. So, essentially, the running time is quite stable, and we are directly handled the raw input. We will directly handle the raw inputs and we don't need to some additional transformation. So it's scalable. So, with this scalable tool, we do some larger scale experiments. So, we want to annotate the million scale data set. So, we applied our tool on the 1.5 million COVID data set. And we use the health case sales as the training set. The query set is essentially the COVID asset. So, we also identified 10 novel. We also identified 10 novel cell types and some viralization like this. So, yeah, due to the limit time, I'm going to go through it very quickly, almost the end. So, as Novel can also discover some new or obviously missed cell types, because we also trended on this 1.5 million copy data set and tested on another copy data set, and we found that 23 cell types that didn't appear in this 1.5 million data sets. We think there might be some miss. There might be some misannotated cell types. Yeah, so this work was done by most of my PhD students and also some RA and the collaborators from Georgia Tech and the NTO. So in my group, okay, so I believe this is kind of a seminar for collaboration. In my group, I'm essentially developing those computational tools, especially machine learning tools, to solve the PR problems in computational biology and human health. In computational biology and human health. Yeah, so that's it. This is a very brief introduction to our ongoing talk. Very come if you have any questions, suggestions, or potential collaborations. Yeah. Okay, thank you. Hi, next word. So on the real data set, where would the ground fields come from? Okay, so for the real data set, we have we essentially use the The we essentially use the already annotated data set, and we mask some of them as the, you know, the we like kind of in cyclic amplified data sets. And for this COVID, the two COVID data set, we essentially apply it and identify those normal cell types. And then we do the human annotation by ourselves. Yeah. I have one question. Can we consider this in a view of comparative analysis? Like we have one group of diseased people, one group of healthy people. In healthy people, we already clustered and ask a question if there's a normal cell type in diseased people. Is this work? Yes, yes, it's possible. So that's essentially what we have done here. But because the COVID asset it ourselves is already well annotated, so we didn't. Ourself is already well annotated, so we didn't discover a lot, but still we can identify 10 of cell types. Yeah, it's possible to do that. So, for this experiment, we use the health case cells as a reference. Yeah. So, how do we adjust about the batch practice? If there's one new things, it could be because of the batch, and also all of that damage consideration of that. Okay, okay. That's a very good question. That's a very good question. So, although I didn't put the slides here, we find that our model is quite robust to the batch effect question. It's mainly because, so when we trend on different sorts of data, so you know, batch effect is a very difficult question to answer. So, I think we more to run experiment to answer this question. I do have some some Some data and some experimental results for which I can share with you. But I would say sometimes you can consider the batch effect as noise. Sometimes they're real signal. So it really depends how you define the batch effect. Yeah, this is a very good question and it's very tricky to answer. Yeah, I'll just say that. All right, so let's move on. If you have other questions, please ask Zach and also in the speaker interview if you'll want to. Also, the speakers, if you want to provide some references, please post it in the channel as well. So, our next part is by Mia Li about geographic modeling of single-cell transplantomic on the cell type and disease sub-type evolution of bulk.