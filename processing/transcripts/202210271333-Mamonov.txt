And I after a little bit over seven years. But so the stuff I'm going to present today, it's kind of related to what you have seen. Leliana presented yesterday, and it's joint work, of course, with her and with Justin Garnier and Jorn Simmerlin. So, what we are going to do here, I will discuss waveform inversion with reduced order modeling, so a slightly different So a slightly different spin on the kind of approach that she presented, and I will try to emphasize the differences between what she talked about yesterday and the particular version of this technique that I'm showing here. Okay, so the idea, this study, you can say it was motivated by seismic exploration where you send some acoustic or elastic waves into the subsurface, they reflect from Subsurface they reflect from various features and they travel back to an array of receivers where you record that data and you try to infer the subsurface structure from the data that you record. Okay, the framework that we built here to address this problem is based on reduced order models, in particularly the data-driven reduced order models. So we sort of separate our inverse problem of S. Our inverse problem of estimating the acoustic velocity from the measured data into two steps. So, in the first step, we will construct in a data-driven way the reduced order model. And I will talk in more details about which operator we want the reduced model of. And then, as the second step, we want to extract the actual acoustic velocity from that reduced order model, which we employ. Model which we employ to come up with an objective which is better than the conventional FWI data misfit objective. So, in that sense, this kind of approach is similar to say what Bjorn was showing, that you try to modify the conventional non-linear least squares misfit objective of FWI, which is known to have all kinds of multiple local minima and Multiple local minima, and which makes convergence pretty hard with any kind of localized gradient-based optimization algorithm. Okay, so the setting will be very similar again to what Liliana presented. I will have an array of M sources and receivers, so all my devices, all my measurement devices can both emit and receive. Although, again, this condition can be relaxed a little bit, but I won't go into technical details how to do that. Into technical details how to do that. So, I will have my acoustic wave equation with the sound speed C here. The right-hand side here are the sources. So, this is the wavelet that we emit. These are the source locations. And the data that we measure has this form here. So, the waveform inversion problem or velocity estimation problem that we are trying to address here is knowing this measured theta m, we want Measured data m, we want to recover the acoustic velocity c that sits in here. Okay? And last in the in Liliana's presentation, she spent quite a bit of time to show you how you can convert the problem from this form where you have the source as a forcing term to the problem where you have the source in the initial condition. So here I skip all those technical details and I will work with the hyperbolic system. With the hyperbolic system in this particular form. Okay, so now you see that I don't have any sourcing term anymore. And one extra thing I do to simplify the presentation, so instead of dealing with operators in the continuum, I will deal just with matrices. So for that reason, I discretize my PDE on some fine grid. This is purely for derivational purposes. Of course, you can do everything with continuum operators. Do everything with continuum operators. Okay, so my PDE operator, my spatial operator, becomes a large matrix. And as I said, what we can do is we can move our source term to the initial condition. So this B here, it will encode everything about this term, the source wavelet, those emitter functions and so on. So all of that will go into the source vector B. Into the source vector B. Okay, since my sensors can both emit and receive, my receiver matrix will also be the same, will also be this matrix B. B is a tall skinny matrix, so capital N is the number of spatial degrees of freedom, and M is the number of sources and receivers. Okay, so this hyperbolic system, of course, we can easily solve like that, just with a cosine. Okay, so. Cosine, okay, so and then since both my source vector and receiver are B, or I should say matrix, are B, then we have this following data model. Okay, so we simply think of this cosine here as the Green's function of this hyperbolic system, and then we hit it on the left and on the right by our source and receiver vector, which is the same in this case. Okay? And again, Okay, and again, as Liliana showed you yesterday, you can relate this data model to the original data that you measured, that I showed in the previous slide. Okay, so now our job is given this data, given this data model, to construct a review storter model. Now, of course, you never measure your data, you never measure your data as a You never measure your data as a function of continuum time, right? You always sample it somehow discretely in time. And this is one of the crucial aspects of constructing the reduced order model. Because if we sample the data discretely in time, so the number of discrete time samples and the number m of sources and receivers will in the end drive the size of the reduced model, right? So of course the amount of data that you measure. I should. I should mention here that this tau should be chosen according to the frequency content of the wavelet that you emit. So it should be about the Nyquist limit, maybe a little bit oversampled, but not much. So maybe two points, maybe three points per wavelength, but not five, something like that. And this is due to some simple linear algebraic considerations. Linear algebraic considerations of stability of the procedure that I'm about to show you. Okay, so if you sample your data discretely in time like that, this also induces the sampling of time domain snapshots of solutions in the whole domain. Okay, so these use, remember are wave fields, so I wrote them here. So these use are wave fields in the whole domain. Okay, and so when you sample them at the precisely the same time. them at the precisely the same times as you data, you get those what we call solution snapshots that are given, of course, in terms of the Green's function by this formula over here. So the reduced order model that we want to obtain will be the projection of the operator A. Okay, so I didn't tell you this operator A here is the discretized version of the symmetrized wave equation operator. Again, same as the one Same as the one that Liliana showed you yesterday. Okay, so we will be building the reduced order model of this wave equation operator A by projecting it onto the space spanned by those snapshots for the first n times and discrete times at which we sample our data. Of course, we have to come up with a data-driven way of doing that because we don't really know the snapshots and the whole data. The snapshots in the whole domain, and we don't know the operator in the whole domain. So, somehow we need to compute this projection here. If you assume that the columns of V form an orthonormal basis for this projection space, you have to compute this product without knowing either A's or V's. Okay, so that's a bit difficult. And actually, I should mention here that here lies a difference between what Liliana showed you yesterday and what I'm doing here. So, if you remember, Liliana. So, if you remember, Liliana was projecting what we call the propagator, which is like the Greens function but without this k here. So, just cosine tau square root of n. However, we found out that if you want to estimate the velocity, that it is better to work with projection of this guy A. And I will mention when I show you the numerics in which sense it is better. Okay, so we want to project this guy onto the subspace spanned by the wave fields in the whole domain without knowing neither the operator A nor the snapshots uk in the domain. So how can we do that? Well, we can do it by playing the following game. So even though we cannot get the snapshots and the operator, we can get what we call the mass matrix, which is the matrix of a pair. Matrix of pairwise inner products of the snapshots. And we can get also the stiffness matrix. But again, in contrast to what Liliana did, this is going to be the stiffness matrix for the operator A. So to get the mass matrix, we use a simple trick with this trigonometric identity that says that the product of cosines is just the average of a cosine of a sum of arguments and cosine of difference of arguments. Why do we care about? Why do we care about cosines? Well, because our Green's function is a cosine. So, if you want to compute now those inner products, you simply plug in the expression for the wave field that has this cosine Green's function, use this trigonometric identity, and then you get the blocks. So, both the mass matrix and stiffness matrix are block matrices because remember we have M sources and M receivers, M receivers, so these are. So these are little m by little m blocks of the mass matrix, and those can be obtained easily from the data using this cosine trig and this formula over here. Likewise, we can also get with a very similar trigger the stiffness matrix. The only difference here is that to get the stiffness matrix, what we need is in addition to the measured data, which we call d. Data, which we call dec, we also needed second derivative in time. And this is actually not hard to get. You may think that it's numerically differentiating. Measured data is unstable. However, if you know the frequency content of your data, which is of course driven by the frequency content of your source wavelet, then you can regularize this differentiation simply by a filter that throws. That throws away all the high-frequency components that shouldn't be there, and then this process is actually very stable. So there is no problem in obtaining time derivatives of your data. And so then, using those, you can play the very same trick that we considered here in the previous slide with this trigonometric identity formula. And you can also get the blocks of the stiffness matrix using. matrix using instead of now instead of using the data itself you use second time derivatives of the data. Okay, so this is sort of the data driven part. We obtained the mass matrix and the stiffness matrix and the last step that we need to take to compute the actual reduced order model, the projection, is the following. So we can remember that when we define the projection, we wrote it down in this form here. Okay, so we took In this form here, okay, so we took our operator that we want them to obtain the reduced model of and we project it on the space spanned by the counts of V, where this V contains the arthanormal basis for this projection subspace. So we can view the process of obtaining that arthanormal basis, at least in principle, since we don't know the use, but you can obtain them, for example, using the Obtain them, for example, using the block QR or Gram-Schmidt factorization of this matrix U, right? So then this gives you the matrix with orthonormal basis V that you want, or that you will never be able to compute, but that you need, right? And then the block upper triangular matrix R. And if you simply take the square of this expression, you see that on the left-hand side you will get precisely the mass matrix, on the right-hand side. To the mass matrix. On the right-hand side, you will get R transpose times R. R is block upper triangular. So this is nothing else than a block-Chalysky factorization of the mass matrix, the trick that you have seen already in the Liana's talk. And then what you can do is, once you do this Chalysky factorization, because remember the mass matrix you know from the data, right? So we have a formula, we have a formula that we derive for the mass matrix. So the mass matrix you know from the data, you take the Chenesk. You know, from the data, you take the Chalewski factorization and then you take this formula for the projection away. You plug in V that you get from the block QR or from block Gramschmidt, right? And then you rearrange the terms so that this term in the middle becomes a stiffness matrix, which again you know from the data, and the R's you know from the data because they are just the Blockchainsky factors of the mass matrix. Okay, so what we have achieved so far, we have computed the projection. We have computed the projection, which is V transpose A V without knowing A and without knowing V, just from the knowledge of the data that we measure. So that's the first part of our approach, where we construct the reduced model in a data-driven way. And now the game that we need to play is, well, how do we extract the acoustic velocity that we want to endure for from this reduced model A? Okay, so here. Okay, so here comes into play the idea that we have seen more than once throughout this workshop already: is to replace the conventional approach, which is the full waveform inversion, which is non-linear minimization of the least squares misfit of the measured data and the model data over some space of admissible models. Okay, so we know that this by now. This by now, we know that this functional is not very nice in the sense that it has lots of local minima, especially if your source and hence your data lacks low frequency content. So if you can go all the way to zero frequency, then of course you get a problem that is almost convex. But if you are probing your media, If you are probing your medium with band-limited sources, this presents a problem. Okay, so what we want to do is instead of minimizing this guy, we want to replace it with a new objective that utilizes the reduced order model that we have just constructed. Okay, so this will simply be a misfit of two matrices. One will be the reduced model that we constructed using the formulas that I showed you previously. The formulas that I showed you previously that we constructed from the measured data and the reduced model that we can construct for any current model C from some admissible sets, from some admissible set. So the question is, of course, well, why is this guy better than that one? Unfortunately, I don't have a proof, but I have pictures. So pictures are good or good enough. All right, so let's consider the first thing before I do any inversion. The first thing before I do any inversion, I want to do a study of the topography of the objective function, of this objective function that I have just constructed, and compare it to the topography of the objective function, this one, which is the convention of FWI. Okay, so the example that I will do here is as follows. I have the following model. So I have a model with two velocities. This is, say, water 1500 meters per second. This is rock, 3000. Per second, this is rock 3000 meters per second. There is a slanted interface between these two media, and I will do, of course, I cannot plot the objective topography in some high-dimensional space, so I will do a slice of that objective topography through a space parameterized by two parameters. One parameter will be the depth of this interface, and the other parameter will be the contrast. Be the contrast, okay? So the true allocation of that interface is shown here, and the true contrast is two, right? Because water is half the sound speed of this rock. Okay, so and then for each point in this two parameter space, I want to plot the objective, the classical one, the conventional, this one, and the one that we proposed. And here are the plots. So here I Are the plots? So here I have in the vertical axis that corresponds to interface position, and the horizontal axis corresponds to contrast between these two media. Okay, and so, well, I hope the pictures speak for themselves. This is the objective of the conventional FWI objective topography. This is the topography of the objective that you get by computing the misfit of reduction. By computing the misfit of reduced order models. Okay, if you ask me which one I prefer to minimize with gradient descent, I would definitely choose this one. Okay, by the way, so the circle here shows the true parameter values. And one reason that I like this plot is that it shows you the cycle skipping effect. The cycle skipping effect corresponds to this horizontal lines, okay? Because this horizontal lines correspond to the corresponding corn Because these horizontal lines correspond to shifting this interface up or down. And that's precisely where you get local minima with the conventional FWI approach. So hopefully this numerical study convinces you that this approach is superior. So now I will just show a couple of numerical experiments of actually performing inversion with our approach. Inversion with our approach and comparing it to FWUI to conventional one as well. So this will be my source, and I actually use the same source for this example as well. Of course, I took it band limited so as to make FWI life harder, right? If you the further you take your cutoff frequency away from Frequency away from zero, the harder it will be for FWI, of course. So, here I'm using the central frequency of six hertz and the bandwidth of four. So, most of the power of my source sits between two and ten hertz. Okay, so I will do Gauss-Newton iterations with adaptive technical regularization to do. Kind of regularization to do both FWI inversion and inversion with our approach. And so I also use some causality in the construction of the ROM of A to improve the convergence of our approach. And yeah, and I will do two examples. I will do what is called a common behavior model, and I will do Camembert model, and I will do a section of Marmousy model. And for Marmousy, I also add some noise to the data. Okay, so let's start with the Camembert model. Camenberg means it's like a single inclusion in the shape of a cheese. Okay, and this is, it may look easy to do, but when we submitted this thing to geophysics, they actually were more expressed by the common bird than by Marmousi. So, who knows? Okay. Now, Okay now conventionally in geophysics they do it in a slightly they do it in a slightly easier way. They use transmission or sometimes even both transmission and reflection data. Here in all the examples I only do reflection data. So my source receiver array is always on top here. Okay so the contrast is not that high so it's 3000 meters per second, 3 kilometers per second, background 4 kilometers. 3 kilometers a second background, 4 kilometers a second disinclusion. But the difficulty of this example is that the reflections that you get from the bottom, they are very faint, okay? And so FWI has a hard time resolving the bottom of this cheese wedge. So you can think about it as a sort of idealized soul-body inclusion problem. And if you have Okay? And uh i if you have ever seen uh the conventional FWI applied to inverting a solid body, this is precisely what you would expect. So you will get the top and the bottom it it has a very hard time of recovering. Okay, so here I will do I think 60 Gauss-Newton iterations. So the convergence for this one especially at the very end can be slow and I will show the estimate that we get with our method and the estimate that we get with FWI and you can see that And you can see that while our approach, so after 60 iterations, I stopped here. So our approach was able to fill in that delicious cheese almost entirely, and FWI only got the top portion. It tried to put some reflector here, but I mean, it's a pathetic attempt, I think. Okay. Anyway, but at least it showed that it tried. I failed, but I tried. Okay, so this is the common bear example, and of course, since I spent at least some portion of my career working in geophysics, I have to show you a chunk of a section of a Marmousy model. Okay, so I cut off like a five and a quarter by three kilometers section of a Marmousy model just to save computational effort. So I took the part where most of the action. Where most of the action is going on. Okay, my initial guess for both FWI and our approach will be a 1D gradient in depth, which is over here. I have an array of, I think, 30 sensors at the top. Again, everything is in the reflection mode. Although at the end, I will add some more data to just to show you that you can inject more data and get a refined more refined. More defined reconstruction. So, on the left here, I show just a couple iterations with our approach, on the right with FWI. And basically, by iteration 18, FWI gets stuck, and you can see all kinds of artifacts. So, here, here, here, well, I mean, a lot of artifacts. So, I mean, again, it tried to do its best, but since I was evil enough to deny all. To deny low frequency data to it, of course, it couldn't do better with the frequency content that I had. Okay, and so again, so this is to compare the true one, the initial guess, what we get after 18 Gauss-Newt iterations, and as I said, if we inject more data, same frequency content, by the way, just a little bit more, I think I doubled the number of sensors here, you can do a couple more iterations using this model as an. More iterations using this model as an initial guess and get the reconstruction that you see here, which I think is pretty good. If we compare, if we take a look at the vertical logs, vertical slices of that velocity, you can see that we're doing a pretty good job, except maybe for the deeper parts. But this is because I stopped measuring here, so my wave just barely went over there and then was able to reach the receiver. But you see, in the The receiver, but you see in the shallow part, I think we're doing a pretty good job recovering those velocity across those different vertical slices. Okay, so to summarize, we introduced an approach, used other model framework for doing full waveform inversion. It is essential here that I worked in time domain because we were able to. Because we were able to use different properties of time-domain solutions. In particular, we were able to use causality of time-domain solution snapshots, right? Because this Gram-Schmidt process, it's a process that only looks back, right, and it never looks forward. So, in a sense, this Gram-Schmidt articleization preserves the causal nature of the time-domain snapshots of the wave equation. Okay, so it's a so this Charisky factorization. So, this Chalewski factorization is a linear algebraic analog of a causal orthogonalization process. Okay, so we saw that this approach is much better behavior, especially when you have band-limited sources. So, the topography of optimization objective that we get with reduced order model is close to convex, or it looks more convex. And with FWI, of course, we. And with FWI, of course, we have cyclones keeping and all things like that. There exists a robust version of this algorithm that deals with noisy data, although it's a little bit difficult technically to construct, so I'm not doing the explanation over here. And of course, we want to extend to both pictorial problems like electromagnetics and elasticity, and maybe transfer some of these techniques from time domain to frequency domain because we did similar. Because we did similar kinds of approaches in the frequency domain, but so far we only did it for diffusive inverse problems like parabolic and such, and we didn't do it for waves yet. So this work is soon to be published in geophysics after like four revisions or something. And then there is a bunch of papers that describe these reduced order models in slightly other settings. Slightly other settings, okay. Slightly different settings. Alright, so thank you so much for your attention. Any questions for our speakers? So I assume this will depend on how long we record a signal, right? So