Supposed to be used for rate setting, but they are. And it's not written down anywhere. And only through observational work can you figure out that this is exactly how it's used. Except foster parents and caseworkers have figured out that there's some relationship between the caseworker going to a foster home and writing down something on an iPad or a piece of paper and then the rate changing two weeks later. So they figured it out. They don't know that an algorithm exists, by the way. Foster parents do not know that an algorithm exists, but they've figured this out. So we live in this like really, really messy world. Caseworkers Work. Caseworkers believe that their expertise can help, but they're also legally mandated to use the outcomes of algorithms, so they think that their expertise is being kind of undermined. And that's not nice, right? And of course, they're aware of biases. They know very well that they live in a very biased world, yet the state government makes them do it. This is real, right? So then, of course, we kind of generated this theory about a theoretical framework. This is like sociological theory, it's not math theory. Geological theory, it's not math theory. The idea is that how are decisions actually made? Because we have to connect the process of algorithmic decision-making from a socio-technical angle with the mathematical decision-making theoretic framework that many of you in this room are developing. That connection is very key. You can develop all the algorithms that you want, but people are going to use them, right? And people are not going to use them in the ways that you think they are. So, you know. So, you know, I'm not going to go into this in too much detail, but there's a bunch of different criteria factors that kind of go into these algorithms. But the important bit is that there's three main criteria. One is human discretion. Choosing what to do, or most importantly, what not to do is really important for algorithmic decision-making. And of course, that is tempered by bureaucratic processes that are written down and that they have to follow. Because things like child welfare, there's a lot of like. Like child welfare, there's a lot of very strict kind of rules that they have to follow. So, of course, with all these key outcomes, talked about this a little bit. We realize that algorithms are currently used. Those algorithms are biased and we live in a very messy world that is this weird intersection of work practices and AI outcomes and legal and bureaucratic processes, right? This weird nexus. So, the question that then arises is: you know. Is, you know, how else could we potentially think about this work, right? So I started off this talk by kind of coming across as a critic of child welfare systems and algorithms, et cetera. I actually think that algorithms, there might be some really, really interesting positive outcome use cases for them. I'm not like a let's burn everything down hashtag default children. I'm not that person. I don't know what you say wrong with that either. No, I don't think there's nothing wrong with that either at all. At all. But the thing is, when you start kind of connecting back to this theoretical world, you realize the data that's used, again, is data that's available and quantifiable. But then, when you start getting into the details, you realize that that data is approximately 15% of the overall data that's in child weather, right? So there's a lot of dark matter, if you will, in the child welfare data space. And that dark matter is narrative data, which is a Is narrative data, which is approximately 85% of all data. Things that are written down. And in child welfare, in a lot of welfare cases, case for, you know, they have to write detailed, almost ethnographic narratives about each case over a period of time. So the question is, that is very useful and interesting data. So what could we learn about child welfare systems if we looked at that data, if we looked at narrative data, that's not easily available and that is also not easily quantified, right? Quantify, right? So, this is another important point I want to point out. A lot of the discussions that we've had in the past couple of days have kind of rested on this idea that there's a person making a decision. Except, this is how algorithmic decisions are made in child bed for about a single case. These are the actual numbers of people. This is an actual use case, and these are the different roles. It's a group decision-making kind of environment, collaborative decision-making, right, that actually happens. Right, that actually happens about a child in case. And so, if we live in this world, you know, we kind of have to do some initial analysis. We want to see if this kind of works. So, very naively, we were like, okay, we've got large amounts of unstructured narrative data. Let's do some topic modeling. Because at the end of the day, I know it's an LLP, you know. So, let's do some topic modeling. So, we do some topic modeling. Most importantly, we did not interpret the. Most importantly, we did not interpret the results of the topic modeling. So, topic modeling results in topics and large amounts of text associated with the topics. We did not do the interpretation. In the best practices of participatory design in machine learning, we did what's called a member check, which is essentially the case workers and social workers interpreted the data. Because what do I know about child design? I'm just like a scientist who's coming in to look at the system. They know how to interpret the data. So, they interpret the data, and you know, we came up with like a whole And we came up with a whole bunch of different topics. I want to point out one important thing over here, which is there's a lot of invisible labor and work around algorithms that are not recognized by the algorithm. The algorithms can't really deal with invisible work, labor, and practices. And so this presents kind of like a conundrum for those of us who are trying to develop algorithms in these kind of cases. There's invisible labor. How on earth are we supposed to? How on earth are we supposed to know that this is actually a thing that's not written down somewhere? It's not written down. It's not part of the practices, official practices, but it's become part of the official practices. So these things affect these kind of decision points, events that happen in the life of a case, affect the case. So then what we did was, we did, this is a very typical way of breaking up cases in child welfare. This comes from social work theory, where you kind of break up kind of like Where you kind of break up kind of like high, low, medium needs based on that number of interactions that they have with the system. So we kind of broke up this data. And then we wanted to look at something really interesting. We wanted to see if from the topics we're able to notice during the lifespan of a case where these important events happen. And what we note is that actually we can to a certain degree. We can to a certain degree. Can to a certain degree look at the life of a case and then look at certain types of risk factors. This comes from great work done by Maria Antoniac. Okay, all right. All right, so you can, so we were looking at something like parental assessments. This is a very important criteria. This is an important thing. But we note that through the text analysis, we can actually figure out that this happens third of a way through a case and towards the end point of a case. Through a case and towards the end point of a case, which is backed up by qualitative action interviews that we have done. So it is possible to do these types of things. Another thing is home safety. Something is done at the beginning, something is done at the end. This comes from the textual data, right? So when we do all of these things, one of the things that we really, really want to know is, you know, where's the narrative power in all of this, right? So we're able to do something called a computational power analysis through the text data, and it kind of depends on this kind of theory. And it kind of depends on this kind of theory from Martin Sapp that kind of looks at the words that are used to kind of denote where things are kind of going. And you can kind of get some really interesting inferences from all this stuff. So for instance, this group two medium needs. These are people, these are children who are placed in short-term foster care, right? And you see that through the text, you can kind of see that actually it's the foster parent who has the most amount of kind of like power. Of kind of like power, they're not structural power, there's narrative power, right? So, things that happen on a day-to-day basis, and this is actually backed up by theory because caseworkers must maintain good relationships with short-term foster parents. There's a very, very good reason why. If this is a long-term foster parent, there's a different kind of relationship. If it's in-home placement, there's a different type of relationship, right? So, you can infer all of these kind of really interesting patterns from our models. And so, you know, I kind of repeat all. And so, you know, I kind of repeat all that. What I want to really talk about is how do we think about this kind of risk in childhood? Kind of go back full circle in the last couple of minutes. The problem is, through the prior series of work, we have established that risk is very problematic to predict. And we also know that the life of a case is a very temporal, very dynamic thing. So, we wanted to use this same narrative data to do some understanding of risk factors in child. Risk factors in child welfare over a period of time. And so, when we do that, we do a better, a less naive form of topic modeling, it's more semi-structured kind of topic modeling. And it's called a core X model, where you basically have some anchor modes. It's kind of semi-structured. And when you do that, you realize that risk can actually be decomposed into four kind of underlying components. Risk doesn't mean the same thing. Same thing. And this is again, this comes from another prior theory. And essentially, risk has many different elements to it. So there's like a risk factor, which is like a negative thing, but then there's something called a more positive thing, protective, procedural, and systemic factors. And these are all components of risk. So treating risk as a singular construct is actually a very, very bad idea. Yet in the public sector, everything is good, right? So this is kind of like the big takeaway that these kind of complexities need. That these kinds of complexities need to be understood when we're trying to develop these kinds of models because what you think you're predicting is actually a lot more complicated than just like we're assuming that your outcome construct is a single thing. It is not stratic and of course it's both temporally and situated through processes. When processes, policies, legal bureaucratic processes change, this idea of risk is also going to change. So, however, we start to develop However, we start to develop our kind of theoretical models to do these types of more sophisticated algorithms down the line, we need to be thinking about these kinds of ideas, right? And then finally, we did one last kind of analysis where we're trying to compare risk assessments that are used as predictors versus the case narratives to see if any of those different kinds of categories have any predictive power. And no, they don't. None of them actually really have any predictive power. Actually, really have any predictive power, but narrative analysis helps in contextualizing what we may need to do in the future. So, that's really the world that we live in. This is my understanding of risk. It's in child welfare, but I think that it is generalizable across areas like criminal justice, like education, public health, so on and so forth. So, I'm happy to talk about all of this, or I don't know if we're making for lunch, but yeah, thank you very much. So we have a couple of minutes for each talk. Yeah, thanks for the really interesting talk. I hear you. I think, yeah, that keystone seem really promising for incorporating context. And we did some work on this in Allegheny County, but kind of got stopped at the point of looking at like racial biases and how caseworkers talk about families from different races. I'm wondering if you saw that in Wisconsin as well, or how much, how concerned you are about those headings? Absolutely. Caseworkers do talk about. Absolutely. Caseworkers do talk about caseworkers have their own biases. Caseworker human beings. They talk about families and children from urban Milwaukee in a different way than rural Wisconsin. One population is poorer and more black, and the other population is also poor but white. So yes, absolutely. Did that show up in your narrative analysis? No, because not in this paper, but yes, we've done, yeah, yeah, it's because I have read. Because I have read your paper, and it's you know, we have similar results, so maybe we have to publish it somewhere else. So, I enjoyed the contrast you made between interviews and ethnographic observation, because I think that's a very important distinction. It's easy to miss. And I'm wondering what differences you noticed, or how those two designs paired together, give you something that neither one gave you individually. Yeah, absolutely. So, interviews are self-reports, right? They're self-analysis. Interviews are self-reports, right? There's self-understanding, and people are just reporting what they think happened, and they come with a certain amount of biases, right? But when you do ethnography, you actually observe them in the workplace and you actually see what they do. And so we're able to tease out differences between what they said they did and what they're actually doing. And then so then that kind of, we followed up and we were like, you are doing this thing around rate setting. You're actually setting the rate based on the outcome of the algorithm when you specifically told us in the interview that that is not allowed. Interview, but that is not allowed, right? Yes. Yeah. Question: I really, yeah, really love this line of work. So, my question is: do you think there's a way forward to incorporate this understanding of these multiple factors into algorithmic position making? Or is your perspective that we should do away with risk prediction because it's not well defined anyway and replace that with like this proof decision making that the This group decision making that doesn't like doesn't deal with any numbers? Or is there, you know, where do you fall on that? Yeah, that's a really excellent question. I think risk prediction as it stands today in agencies in the public sector, it's a mess. It's an absolute mess, right? But could we, I think the promising thing for scientific inquiry is could we be taking these insights, right? So essentially, we know, we recognize, this audience recognizes, that we deal a lot of the time. That we deal a lot of the times with data that is quantifiable, that's available to us, right? But there's a lot of, again, dark matter out there. So, how do we integrate that? I think we're kind of coalescing around very similar kind of ideas. And then, what is the outcome? Like, maybe we could live in a computational world, that's not a computational world, that's not a risk prediction, right? Maybe we don't need to be predicting risk because the mission of child welfare is not minimizing risk for a child. Minimizing risk for a child, that is not the stated mission. The mission is improving positive outcomes. So, can we have something that's more geared towards that? Because right now, what happens is it's not really minimizing risk to the child that even they're doing. Everything that they're doing is minimizing risk to the system. And that comes from old, like a lot of sociologists have looked at this over a period of time, that any time that there's like bureaucracies, you're gonna kind of try to make yourself more stable and protect yourself more slowly. Most of the others maybe an opportunity to still have scores that's more centered around action. Or, you know, very aspirationally, could we live in a computational world that doesn't assign a score to a child? Like computation, but not necessarily score. Why do we always imagine a computational world the outcome is like some kind of score? Right? Right, right. I'm super fascinated by this work. I'm super fascinated by this work. It's really interesting. I have two questions. One is how, you know, I'm curious how you got this sort of access. And I'm also curious what the response of the people you were doing the ethnography on was to your sort of theorization of risk assessment in the system. Like how did they, when it was fed back to them, how did they respond? Yeah. So the way we got access is we were not very privileged people. So nobody came to us and said, please study us at all. We did something that's very old school that perhaps the sociologists That perhaps the sociologists in the room will know from Goffman, Irving Goffman, who was a sociologist. We did something that's called, we started by doing something that's called an ambush interview, which is incredibly, it's very hard to do. And I'll tell you exactly how it happened. We lived in downtown Milwaukee. Wisconsin Department of Children and Families is in downtown Milwaukee. And the building is accessible public. But the offices are not accessible public. So we went and we stood outside. We went and we stood outside. We already stood outside, and then some people came out and we said, Excuse me, will you talk to us? And most of them said, No, get lost, don't bother me, right? Like what we do when we're walking on the street and someone is coming to us to ask us to donate money, you know, like that. It's like that. But then after a month of not getting access to anyone, we very fortituously bumped into the director of the business analytics division of GCF.