Okay, so it did his uh so today I will talk about a couple of books that we did recently with my collaborator Rachel Wood from ET Austin, Hayden Schaffer from CMU, my other colleague Ram Ho from Datong Sea University, and my two Stephen Asha and Nicolas. So overview of the books, there's the theoretical part and the application part. In terms of theory, it is a supervised learning problem. It is a supervised leveling problem when we are given a piece of like M measurement that containing both the input and output. And then the input here could be in a very high-dimensional setting. And now we want to learn the unknown function there. Most of the time it's non-linear that make also contain the noise. Where the thing that we want to emphasize here is in terms of the input data, we don't know the distribution of the data. And in terms of the theoretical perspective, our approach here is we want to have some data expression. Want to have some kind of fast representation of approximation with a neural network-like structure so that we can inherit some kind of like best approximation in a neural network, display more or less by sharing network. And then it also provides a typical recursion guarantee saying that given just input and output, put in terms of our model, then we can have some kind of generation about how good it is with the two solutions. And the key idea behind it is the three information that we need. Three informations that we need. The first one is we use random feature expansion as our spectral basis. The second one is using sparse promoting models where we see a lot in the talk using either L1 or N0 pennyization. And interval theory, we use a technique of compressive sensing to provide some generation back. In terms of application, we want to focus on the data scar gauging, where the number of samples in a lot of plate real data is still less than the Data is still less on the side of the basis required to sufficiently represent the feature space, which is very rich. So, one of the applications we talk about, which is the first one, the half one, it is a high dimensional surrogate model for scientific and engineering problem, where the dimension here could be equal to 90, while the number of measurement may be only like 200 or maybe 2,000, not so much. For the second kind of example of in terms of application that we see. Of application that we see from the first one, it is in terms of signal decomposition where we have the input data, this time series, where we know the time, and it could be sampled randomly. And then at the end of the day, we want to decompose that signal into different modes, integer mode. And the third one here, it is also in terms of pandemic forecasting. So in that scenario, for the last one, it is a combination of spa fan feature time delay, and method would also be the token theorem talking about. Talking about. So, the first two rust solid light is more in terms of prediction where you have both input and output, and then we want to learn the function. For the signal decomposition, it's another kind of interesting that I'm talking about using the sparse feature one. And then, the second part I want to emphasize in terms of application, it is we want to make it as a practical machine learning algorithm. In the sense that in so many cases, we don't really know how to verify all the assumptions of the theorem. On the assumption of the theorem, and then you want to see that with a real data set, then how it behaves well with the forecasting in terms of real data set. Okay, so in terms of formulation, it's quite easy to formalize. That is basically in terms of function f, it is nonlinear. So we write now as a linear combined generation of some nonlinear function phij. And most of the time in the past, it's a very well-known method, where the phij here could be polynomial or polynomial chaos. Of polynomial chaos expansion that we did some work in the past about polynomial chaotic system. For this talk, we want to use also motivated by the work by Ryan Millrich, where we use the random feature to represent the function phi. So, in this case, the function phi now is actually parameterized by the random weight, Umenaj, and the phi here, it is somewhat called an adaptive basis in the setup. Adaptive basis in the setup, we take the dot product or the inner product between the data and also the random weight. So the idea behind a random feature here is we shift the unknown distribution from the input data who don't know to the known distribution from the width widget that we are free to choose because we can pick randomly and on our choice. And then after that, so the problem now is in turn turns requests into finding the Recast into finding the coefficient C now. So now it's a linear problem. We only need to find the coefficient C. And then the idea behind here is where we use another random feature. And here is very, very large. So that we can capture the enriched property of the function. And then in that case, we have to compress the solution to C omegrams. Some of them are non-zero. So that's why we use N1 or N0 regulation on C to robustly identify C active variable. So, in terms of those, the idea for the predefined feature, which is for predefined function phij, for example, in the past, people have theory like IP are the condition for the function phi is built from the Legend polynomial. But the drawback of those methods here is we need to build those dictionary methods octagonal with respect to the distribution of the input data. In our case, we don't know the distribution of the input data. Know the distribution of the input data, then the idea here we have it is using the random future, we can get a free choice of shifting the distribution that we don't know from x to the known distribution from omega. Okay, so in term of the visualization, so it is in the function congruent approximate is actually uh um can borrow from the shadow neural network when the first uh hidden layer here is first initialized randomly. Here is first initialized randomly based on some different distribution and then it is fixed. And then the trendable one in the coefficient here is on the output of the second layer. Okay, so here's the model for our sparse random feature model, which goes short at SRFE or SPAR random feature expansion model. So the idea, the first one is we just draw n random ways, but n here could be very large, and then we just define the length divergent and the after that. Data matrix, and after that, we create the random feature matrix A, which is some non-linear function phi applied on the inner product between the input data and the random weight, where M here is number of measurement and N is number of random features. For the choice of the non-linear function 5, we can either use, say, RELUP, BCSI, or depending on the application. That I will show later on, that for some case, if we use no more information, but like that, then we can incorporate the information. Like that, then we can incorporate the information to build the nanider function file. And then after that, the problem now turns into like a basic pursuit problem where we want to do just fit. The output that we want to predict, it is A times C, the coefficient that we want to represent, and Y here is just a vector that contains all the output information. For the sake of simplicity, I will just assume that the function here output here is a real number. And then for the sparsity, And then for the sparsity promoting, then we either use the N1 now for the first case. And for the first case, we talk about some tier coverage. For the second one, here it is we using the mix of N0 plus N2. The idea of using the Ticuloop recognization is studied well in the Brahmir Rich paper when the Ticuloop1 is booked well with the random feature. And then we still want to have some sparsity constraint, and then it does. Sparsity constraint, and then in that case, we're using the N0 to control how many non-zero is the coefficient going to be. Because most of the time, some of the case, so the second setting is non-convex, but in the case that we don't have so much data, then we need to have a little bit more stronger consumption so that we can control how many non-zero we have. Okay. So the question interpret theory that we're going to answer is what class of function does this method work well for? Uh interpre uh theory called Galen G. So the class function that the hope for here is actually an extension of the random Fourier feature space that is set in Rahimi and Red paper, which is the budget row number function. So it is also a function so that the row number f here is finite. So notice that if we just give phi here is the Fourier function, then the unphi here simply the inverse Fourier transport f. Times power f. And then the reason why they related those boundaries drawdown functions, it has some relation with the baron function class that people study for the shadow neural network, where they have the bar piece the n of minus one half. So that is a class of functions that we can have a result for. So that's the result uh interval uh analysis. What we say over here is that we assume that the function that we want to approximate based on the input and output data coming from that button program function. Coming from that button function class. And then now also that we can just draw the random quid. In this case, we are using the Gaussian one with the standard deviation sigma and then the support sample here coming from the ensemble Gaussian. The result here, so that we have the boundary result. And then after that, we just use the random feature matrix, solve the basic pursuit problem where we control the sparsity of the solution subject with a concern that we have a noise in. Consider that we have a noise in the output data. Then, after we suck the solution, then we just have a certain requirement. Most of the time for the problem, it is there are some requirement about how big and how large the place that we can sample the width and sample the data. So, that's the first one, it is the relation between the standard deviation of the input data and of the width. It is started by the order of the sparsity. The second one, it is the one that we care about. One is the one that we care most in terms of practical, it is how many random pictures we need in terms of the given amount of measurement. And then for the finally the generalization bar. It's quite complicated, so I just put a simplified version here for the accurate formula. Maybe you want to read our paper that is published. So in the case, that's the one we want to emphasize here is basically when we don't have the noise or the sparsity here, this low sparsity, then the As a low specity, then the generation bioput is basically talking about the difference between f, that is a true one, and then the learned one here is the order of n minus one half plus one d, one over d, and up to some block terms. So just to notice that if we just use shadow neural network where we both have to learn the first layer and the second layer, then the best order we can get is we don't have the one plus or indeed here. But in this case, Because of a nature, but in this guy, we do like a looser, I love loser, sorry, a simpler network where fix the first layer and only change the second layer. Then the thing that we composite is only an n over one over D. So in the case of EP is very high-dimensional, so we can closer to the same approximation of the shallow neural network. And of course, in the real formula, then the error value also depends on how well is the random feature can represent the function. The random feature can represent the function f and also the complexity of the function. Okay, so let's recap that using a convex optimization problem that we just proposed, then we have a low cost than channeling the neural network. The problem now, it is a convex optimization problem. We have some THO guarantee and it's much faster. And then in terms of sparsity of effect that we talk about here, it's a combination. The good thing about that is we can sample many random weights that we want. Many random weights that we want, and then to compensate that kind of overfitting them, then we have a scarcity effect to kind of like steam leverage the structure of the data scattering when we don't have so much data and also steams retaining some kind of accuracy. And of course, there's some other specific effects that happen in real application. Okay, so now I'm going to talk about some application. So the first one is what I call this high-dimensional. What I call these high-dimensional surveillant model for scientific and engineering problems. So, those are some data sets from the UCI data set where the dimension of the input data ranging from 12 to 90, maybe not so much high compared to the real version, and it could be like 1000. But in this case, for the size effect, I think it's quite high. Number of measurement here ranging from 200 to 2,000. And then the first one, the first column, it is. Like our model, combining the part thresholding with sparse value feature. And then the one that's most of the last is the one, like the original one when we're using the L1 BB minus model. And then the one in blue is the one with the smallest error. And we compare with other state-of-the-art sparse additive models. And then you can see that in all those calibrates here, most of the time we get the lowest error in terms of a surrogate modeling when we want to approximate the function in calibrate. To approximate the function in Haramunction setting. Okay, and now here is another type of plugin. So, in this kind of application, of course, in this case, just to demonstrate the idea, and then more real example, we know later on. So, basically, here is the input data. We only have the input data in the sense that we have a pair of time and then the signal value. And as you can see, that is a challenging one internal. We have some discontinuity, and the true one formula is actually given in the In the first report, which is a very difficult one, we are not a mode deconversion uncle. And at the end of the day, we are given the data, we are given that this data is actually a summation of three modes. The number of modes here is most attempts is a hyperparameter that more or less has given beforehand, or probably if you have more modes, then you have a small decomposition, but you can combine together. So we have that input data and the number. That input data and the number most that we want to decompose. And then, as yet, so the scheme work of our method is this: like we have an input data we put into the framework of the sparse random feature expansion, and then from that expansion in the time-frequency domain, we have very sparse and nice representation of all the time and frequency that corresponding to the non-zero coefficient that we might solve. The formula I will explain in the next slide. So, after you see that, you just do a simple graph. After you see that, you just do a simple clustering algorithm. We can capture those three more right away, and when we go back to the single form, exactly those three. Each colour here represents to the function that we want to decompose. And it will compare a lot of popular signal decomposition methods and with algorithm order. So in terms of mathematical formula, for the first application, you the features are predetermined. The features are predetermined, right? No, no. We only have the data which is the number feature. So the input data in the previous one is the feature vector. And the output here is some value that is given from the UCI data set. Yes. So the number of features here is the dimension of feature we have is the variable ID. So like the way that from those data set is already have. From those they just have this already have a fair input and output, but input here is a vector in RD, and output here is a real number. Yes. Yeah. Okay. So in terms of the mathematical formula, actually the inspiration of this second application is actually coming from that music example that I'm talking about. Where for the music, the original music, we want to do something with that music decomposition. And then for the music piece, it's only localized in time. So that's why when we build Tap. So that's why when we build a function, we don't just use a rare loo or side function. We use some localized window function. So in that context, we first start the short-time Fourier transform representation of the signal F. F being exactly the time series one. So we just have 1D signal. So the first formula is the short-term Fourier transform of the signal F. And then, but we don't know anfar in this context. So because we don't know anfa, we just denote it by Cj. That's the thing we have to find. That's the thing we have to find. And then for the localized, because music is localized in time, so we represent by window function w. And then for the last part, it's about the housing. And then for the windows one, one of the simplest one we can think about is using the dawshi paper. The thing is, we don't know where the center of those harmonic is. So the best we can do is we just randomize those ones. And then the algorithm, we randomize a lot of tau J1. lot of tau j one, I mean a lot of the omega j. There's a random line and the product is fixed. So now the CJ unknown one that we need to find and with the sparsity will help us to identify where the local, where the window happened, where those harmonic or the piece of information is happening. So those are the two ones that is our non-Lear function file and the inspiration we have if it's a localized information from the data. And then after that, there are seven, simply random features, random Samples, simply random features, randomize those and fix. Of course, it needs to be in the reasonable domain. For example, for the tau, it should be in the time interval. And for the omega, it should be from zero to the maximum frequency. Where the maximum frequency, sometimes we know, sometimes we don't know. If we don't know, we use the lightest one. But most of the time, maybe it's sometimes fixed, we just speak like we know the highest frequency. So, how did you determine the delta? Determine the delta, the variable. Identity is a higher parameter. Yeah, yeah. And then, after that, we just solve that optimization problem, and then C would have us to decide on the localized information. Okay, so here, just to compare most attempts to the first one, it is a reconstruction one, and then the arrow, and as you can see, the arrow only appears most of the time, but the discontinuity part. So, that is it for the synthetic example, and then for the real example. So, I have a couple of real examples. The first one is for The first one is for gravitational waves where we get the data from the LIGO contact. So, in this context, what you see in the picture on the top is very hard to see, but you have two black holes. And they wanted to see, like that, they can record at two different locations of two cities in US. I forgot which one it is. Now, I want to see when those two black holes merge together. So, in terms of the from the TIGO physics point of view, then if we just overlay all Then if we just overlay all the short-term fluor transform, but that is overlay the blue one at the at the background, and then all the bright region inside the bus just to indicate the timing when the black hole, those two merge together. And then the orange one that we brought over here, it is actually to represent the pair of time and frequency where the learned coefficient here is non-zero and it matches exactly with like the called the true one. Called the true one. And in this case, I want to emphasize that it is a help a way, like the model is a way to visualize noisy strain data. Noisy in terms of physics here means that most of the time 90% is noise. The concrete example we have to remove the opposite one. Noise here is 90%. And then when we keep only the top 5% of non-zero coefficients, that is the picture we get. And then, in terms of the theoretical comparison of the reconstruction, Of the reconstruction, the true one, two-year internal play theory, it is a black line, and then the one that we just got by using the top 5% and 0, it is the blue one, which is closed up. And of course, if we cut more number percent and 0, then we may have a more noisy data. But what we are happy here is we have just solved a fast optimization problem, and then we can capture where the black hole merge. And another one is, as I said about, is a fluke beta separation. Is a flu-beta separation where we have the input here, it is the pair of time, and then the output signal at that time. And then the benefit of using the random feature in this scale here is that not like other kind audio process, we need to have an equally in time if we want to have the data. But using our method, we don't need to have an equally spaced in time for the data. We just can fix those input data here randomly. And then, as you see over there, I just split the I just play the music one more time. Hold the load one. That is the load one when we use a random. So that is the first part. That is the lone one. Just a little bit of background noise, but that's a I think that's acceptable. I mean, uh when we dance them for uh fif fifteen percent and then the we dive up. And then the Vita part is a little bit lower. I mean, it's a little bit tricky here because typically flute and guitar, they are different frequency. So now we are working on some extension of the working with more complicated music piece and then we want to decompose those instruments. Okay, I think running out of time, so another final application I will talk briefly in five minutes. It is about everything. Minutes, it is about every forecast. So, the time relay equation is like bothering me for quite a long time, and I'm trying to see how I can use the time relay equation. And then the token theorem can tell us something about that, saying that for chaotic system, then the token embedding theorem saying that if we just evade that from one trajectory, then we get some recover one up to a diffeomorphism. But most of the time, we are curious to see exactly what is the real value. We don't want that up to diffeomorphism, we want to find the. Up to ephemeris, we want to find the real value in the future. And then, in this context, the idea here is actually from the mathematical biology, where we only, for example, for the COVID data or the Ebola, Zika, flu data, we only have report number of active case or number of cases that is still sick or something. And then we only have those data most of the time on the real data, we only have those. And then what we're going to do is we're going to forecast the research. Forecast the return of bad money in the next seven days. And why seven days? Most of the time, that is the kind of sleeping mode. And after that, we now invest. So that's why they only need to focus on the next seven days. And the idea behind our model is as a first line. So basically, the idea you want to say about that is actually want to say that the rate of change of that variable that we want to know the forecast is a function depend on like the time delay of that variable. So in and so that's why. So that's why in the first slide we just first make the random chain in the infectious variable as a function of the timely line mapping and then for that function we use a sparse random feature expansion. And in this case for the phi function we don't know exactly which one is the best, which value it seems work better than the psi function most of the time because the pattern data is maybe not oscillated that much. Okay. So here's the result on the real data set. So here's the result on the real data set. The COVID-19 has so many data, so that's why. But of course, you also have data with the other daily with other data sets. More data, which means that is 200 or something, because they only have it daily. They don't have it for every time window. So in this case, that is the second one in Canada. And then for the first one, it's actually that represent: assume that we have a double amount of data is given, we need to predict the next seven days. Assume that we have not that amount of data to predict the next. Not enough data to predict the next effect. And then the remaining figure is actually we want to compare with the bench dark matter and compare with the brown tube. So the black curve in all of those, in the garage, the blue is our prediction in the next seven blanks that I explained. And for the other color, it is a benchmark or not. So as you can see, with a clear data set, our method, which is the blue line, is so close to the real one on the next seven blade. Which is kind of surprisingly, and even especially during. Even especially during the peak of the pandemic, where they really care about how the channel is. Then you can see that our method slide here is like basically the next seven days, it captures the check of the pandemic. We also have some kind of like error with the archive data set, and then in most of the methods, our space fork is also a vector smallest error compared to the other part. Okay, so I think I can end my session here. Yes, the term of language. Time for maybe one or two quick questions. Sample is uh flute plus guitar. Okay, yeah, so if there's no uh like Yeah, so i there's no uh like listening to Mona Sartre and then converting it back into machine music. Oh rather than another kind of application, maybe. It's more or less I only have one piece of music that a combination of like maybe in the uh uh idea at the R is in an orchestra, like so many instruments and it wants to separate people instruments. Composer and composer. Exactly. I'll just ask in private. Any other questions? And if we want to our next talk. So our next speaker is Oscar Leon and he will be talking about leveraging common structure for image reconstructions. Thank you so much for the kind introduction and thank you so much to the organizers for inviting me to this workshop. I'm really excited to be here and it's wonderful to meet so many folks who have seen their papers but now it's great to finally meet. I've seen their papers, but now it's great to finally meet them in person and talk about interesting ideas. Yeah, so today what I'm going to do is I'm going to talk about some of our recent work in trying to solve imaging inverse problems in the case where you don't have access to a good prior on hand and when you don't have ground truth, clean data to learn prior. So this is going to be joint work with Angela Gah, who was a PhD student at Caltech, Cha Sun, who was previously a postdoc at Caltech and now at King University, and also Katie Bowman, who was at Caltech. And also, Katie Bowman, who was at Caltech. And also, to kind of get in the spirit of things, I think I was inspired by Molly's talk earlier. If I had to give a different title for this presentation here, I would say that this is all about your structure is all you need. So hopefully I can try to motivate that and to convince you that that is indeed all you need in some cases, which is usually what actually happens in machine learning. Okay, so let me go ahead and go into, just try to set the stage. To just try to set the stage and talk about what exactly the types of problem I'm going to be interested in. So, I'm going to be talking about imaging inverse problems. And so, the goal in these problems is to try to reconstruct some ground-truth clean image, which I call X, from some noisy, corrupted version of it, Y. And so, there are a number of different types of problems that arise in these various domains, and I'm sure a lot of folks here are very familiar with that. But I just want to set some of the mathematical notation I'll be using in this talk. So, the goal in these problems is we're given some measurements y, right? Given some measurements, y, right? We're going to observe x through some forward model, which I call f, and we might also be corrupted by noise. And I'm going to assume we also have access to this forward model, f, the way that x was corrupted. So the goal is given these measurements, and the forward model is to reconstruct my image x. And so, what are the overarching challenges when it comes to these types of problems? The overarching challenges essentially are that, and to do this inversion process, right, there are Inversion process. There are a number of images that are consistent with the measurements that I have. And so I need some additional structure in order to overcome the ill-poseness of these problems. And so typically, a lot of folks in this room have used this word many times for the past couple of days. We need a prior on our image in order to ensure uniqueness or to better regularize our inversion process. And so, a couple of different ways that one might try to do this is maybe you want to solve something like a Maybe you want to solve something like a maximum a posteriori estimation problem. So, this would be trying to solve an optimization problem like you see here on the bottom, where you want to maximize essentially the posterior p of x given y, and you can use Bayes' rule to essentially simplify this into this problem here. And so, here I'm denoting m by certain parameters that sort of governs the prior for my image x. So, you can think of these as like, you know, if you had a Gaussian prior, these Like you know, if you had a Gaussian prior, these correspond to like the parameters of your sort of Gaussian prior that you have here. So, this is showing you sort of an example of how you use a prior and sort of the importance of different priors that you might have. And then another way, maybe you want to, instead of doing solving this MAP estimate, maybe you'd want to instead sample from the posterior. And so, this still corresponds to incorporating some information about your image in the form of a prior here. And so, I want to talk about just a couple of different kinds of priors and compare and control. Different kinds of priors and compare and contrast them that are going to be quite important for this talk. And so, throughout the talk, I'm going to call this sort of structural information about my image. I'm going to call it an image generation model. So, this is kind of borrowing some terminology from the imaging processing community. But I'm going to kind of coin this sort of structure as an IGM or an image generation model. But for the talk, you can think about this essentially as just your prior on your image. All right, and so what are some different types of priors that folks have? Of priors that folks have used throughout the years. And so, kind of on one end of the spectrum, I would say that some of the most successful priors have been hand-crafted priors or hand-crafted inverted generation models. So, these correspond to things like where we have a mathematical description of the prior that we can use in our inverse problem. So, for example, sparsity in a wavelet domain is a successful prior in the problems and has been sort of the foundation of things like JPEG compression and compressed sensing, where we've seen that natural images are. We've seen that natural images are going to be sparse or compressible in a Fourier domain. And here's just a pictorial example of that where I took an image and I computed its wavelet transform and kept only 10% of the largest wavelet coefficients. And this is the image that I get. So there's a high fidelity between the two. And then the way that I would use this as a regularizer or as a prior in an inverse problem is I might solve something like the following here, right? So the first term corresponds to fitting my data, and then the second term here is a regularizer. Second term here is a regularizer that, in this example, would say, I want to find an image such that its wavelet representation is sparse. And that would sort of promote that structure here. But there's a bunch of different types of examples of things like this. But there are a couple of issues you run into when using a handcrafted image generation model. One thing that you need to do to solve this problem well is to tune this hyperparameter to ensure that you have a good solution. So, hyperparameter tuning is really imperative to success, and this is a trial and error. Success, and this is a trial and error sort of process. But in addition to that, this approach is quite prone to human bias. So you need, when you're solving a problem, to enforce a prior that you believe the image to obey, and you're already encouraging some types of biases that you believe the image to obey, which may not necessarily be the case. So that's sort of like a couple of issues that you might run into when using a handcrafted model as your prior. Okay. And so on the other end of the And so, on the other end of the data spectrum, I would say, are learning-based image generation models or learning-based priors. And so, here are a couple of examples of these, but the overarching idea is that you use a large corpus of clean data and then you learn a prior directly from that data. So, here's an example of that in the form of what are called generative models. So, generative models are sort of a recent advance in deep learning and machine learning, which are essentially able to take a large corpus of training. Take a large corpus of training data, let's say like images of celebrity faces, and then they're able to generate new samples that look like these celebrity faces. So they're able to try to sample from the distribution of celebrity faces by training on a large corpus of data like this. And they do it in a low-dimensional way, which is quite nice. So they often take a low-dimensional simple distribution like a Gaussian, and then they learn a mapping G that takes this simple Gaussian and generates samples from the more complex. And generate samples from the more complicated distributions. And the way that you would use this as a prior is you'd say: if I want to reconstruct some image that I believe to be natural, I want to constrain my image to live in the range of this generative model. So that's one way to incorporate this prior. And maybe what you'd do is you'd solve a problem like the following here. So you'd find an image X that's consistent with my measurements, but that also lives in the range of this generative model. So that's just one example of a priority like this. But there's one really But there's one really huge problem when using this type of prior in more scientific applications. You need a ton and ton of clean data to be able to train one of these. For example, this example of celebrity faces, you might need on the order of 100,000 images to be able to train something like this. And in a lot of different applications, like seismic imaging, medical imaging, or astronomical imaging, this might just be invasive, time-consuming, or even just outright impossible. And that's actually one of the motivations. Possible. And that's actually one of the motivations for our work. The biggest motivation, I would say, is in this realm of astronomical imaging, where it would be literally impossible to get access to ground truth clean data. And so this is sort of one of our motivating applications here, which is in black hole imaging. And so just to provide a little bit of context, sort of about this, I think Katie will be able to explain this better than me, but I'll do my absolute best. So, you know, you probably have seen the sort of recent advance of we've been able to generate. Recent advance, so we've been able to generate the first image of a black hole. And the fundamental idea about how this was done, it was a really big computational imaging challenge. And so the idea was that if we wanted to try to sense or measure this black hole, it's extremely far away and it's also extremely large. And so due to the physics in terms of the measurement systems that we have, if we wanted to use a telescope to measure it, we would need a telescope actually that's about the size of the Earth to be able to do that. Size of the Earth to be able to do that, which is computationally, I think, infeasible. But the ingenious idea behind the Event Horizon Telescope project was instead to say what we can do is we can actually pick different locations on the Earth, have small Earth telescopes, and then we can measure the black hole from these different locations as the Earth is sort of rotating. And so what you're seeing here on the right is actually the Fourier modes that these telescopes are actually generating. Telescopes are actually generating. And you can see they're quite sparse in this 2D plane here. And so the goal in this inverse problem is to take these sparse measurements and to try to reconstruct what would be this image of the black hole. And one of the biggest challenges here is that you might have, for example, this might be the sort of ground truth, quote unquote, black hole that generates the measurements that you get to see here on the right. So if you do this over a span of a large amount of time, these might be the collection of measurements. Time, these might be the collection of measurements that you observed. But the overarching challenge here is: we have no access to what ground truth clean data would look like in this example. And so the goal is, how would we try to solve a problem like this where we have no access to clean data like this, and we don't have maybe a great prior that would not introduce too much bias into this problem. So that's sort of the motivation behind this question. Can you not try to simulate? Yes, you can try to simulate, but that still is. Yes, you can try to simulate, but that still is also going to incorporate some structural biases into the problem. So here we're trying to see if there is some other type of structure that maybe we can try to sort of exploit. And here's precisely that kind of structure that I want to sort of motivate in this talk, which is that the images that sort of end up generating this black hole, you would imagine that if you try to sense it for a very long time, the black hole itself is not going to drastically change over the span of time that you sense it. So here's like, for example, in So here's like, for example, in these, just like, I just took different stills of this, a bunch of these images of the black hole, right? And they all have some commonalities between them in terms of their structure. And you would imagine that perhaps if you looked at this class of images, that they might live on some very low-dimensional manifold or low-dimensional structure in high-dimensional space. Maybe that's something that we can try to capture in trying to solve the inverse problem. So you might imagine that all of these images are going to be very high-dimensional. That all of these images are going to be very high-dimensional, but taken together as a class of images, they might lie on, in this case, maybe not actually like a one-dimensional manifold, but they might lie on some lower-dimensional structure that's embedded in high-dimensional space. And so, is there a way that we might be able to capture this?