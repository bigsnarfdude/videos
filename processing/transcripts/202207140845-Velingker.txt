Thanks, everybody. Yeah, thanks for the introduction, Ladislav. Yeah, it's a pleasure to be here. And first of all, I'd like to thank all the organizers for inviting me and putting together this great workshop. It's been great listening to all these exciting works. Yeah, so my name is Amay Velinker, and I'm a research scientist at Google. So I'll be speaking today about affinity-aware graph networks. Okay. So, first of all, I'd like to give a shout out to my amazing set of collaborators that I've had a chance to work with. So, my colleagues Ali and Trina Wasat on the Google side, as well as my collaborators in DeepMind, Petr and Ira, who is one of the organizers here. Okay, and just to begin with an overview, I'll start with an introduction. Introduction and sort of motivate the question we're trying to answer in terms of the expressivity questions about GNNs. Then I'll kind of discuss what we do in our work regarding affinity measures and some of the computational issues. And then hopefully I'll share with you some experimental results that will bridge some of the theory with practice. And then I'll conclude the talk. I should note that. I should note that, since this is located at the Banff Institute, I tried to find some appropriate scenery here. So if you don't take anything else from my talk, hopefully you'll be pleased with some of the nice images of mountains and lakes. Anyway, let's proceed. So start with the introduction. So I don't think I need to motivate graph learning too much. Motivate graph learning too much to this audience. I think most of you are convinced that this is an important area of research that has definitely sprung up in the last few years. I'm just going to highlight some of the applications and tasks to which graph learning problems have been applied. So things like molecular graphs, knowledge graphs, road networks, etc. And these tasks can be framed as a variety of You know, a variety of different prediction tasks, such as graph prediction, node prediction, link prediction, etc. And as we all know, graph neural networks have emerged as a powerful tool to solving these kinds of problems on various applications. And they've had a lot of success in these areas. Okay, so hopefully I don't bore you too much because I think some of you will be. Because I think some of you will be familiar with this, but in case there are other viewers, I'm just kind of gonna motivate GNNs, right? So I think the way I like to sort of see them is a generalization of this intuition we have from the convolutional neural network world, right? So here we have this convolutional filter, and this filter kind of gets moved around an image, for instance. And at each sort of section of the image, we aggregate. Sort of section of the image, we aggregate some pixels according to the pixels that are exposed by the filter, right? And there's a couple of like interesting properties here, right? One is this weight-sharing phenomenon. So here, basically, the weights that you use to aggregate in different parts of the images, these are kind of fixed, right, according to the filter. And one way to view this is that the filter is essentially aggregating. Aggregating pixels according to a neighborhood on a certain graph. So, in this case, you have a grid of pixels, so it's just a grid graph, and you're aggregating all the neighboring vertices according to the edges in this graph. And GNNs essentially formalize this intuition to general graphs, right? And one of the challenges is that you don't have this structured graph with this nice grid where every node kind of looks the same. Every node kind of looks the same with respect to its neighbors, right? The neighborhoods can look different at different nodes. So that's one of the challenges, but DNNs have stepped up to address this challenge. Okay, so that kind of inspires our model in which we operate, which is the familiar message passing neural network. Right, so in a nutshell, what this does is combine node-level features as well as. Combine node level features as well as edge level features. And each step has an iterative message passing rule. So each node kind of takes messages from its neighbors. It does this aggregate step, you can see here, and then it kind of combines these messages into this HI. Okay, so each step is going to do this iterative message passing. And our framework is essentially going to be to Essentially, going to be to incorporate some additional features into message-passing neural networks. And by the way, if there are any questions as I move on, feel free to interrupt me. I'd like to make this as interactive as possible. Okay, so let's move on to expressivity of DNN. So the reason I bring this up is this is kind of going to be a guiding motivation for the kind of problem we're looking at. For the kind of problem we're looking at, right? So I think there were some previous talks that talked about this in more depth than I did. Christian just talked about the Weissfeller-Lehmann test in the previous talk, and I think Christopher also discussed it yesterday. I won't go into as much detail, but just we'll try to recap the salient points there. So essentially, there's this question of what exactly can GNN? What exactly can GNNs capture, right? How expressive can you make them? And it turns out that there's a long line of literature investigating this, and you can show that GNNs are bounded in expressivity by the so-called Weispeler Lehmann test. Okay, so this was a test to kind of decide whether two graphs are isomorphic. I think it was believed actually. I don't know too much about the history, but I think when it came out, people were thinking this might. Out that people are thinking this might actually solve the graph isomorphism problem in polynomial time. Of course, we know that didn't end up being the case, but it can kind of distinguish various, a good number of pairs of non-isomorphic graphs. But there are some pairs of graphs that are actually non-isomorphic for which this test would output that they're isomorphic. But the essential test, what it does is it kind of assigns colors or values. It kind of assigns colors or values to each of the nodes in the graph, and in each step, it takes the aggregated multi-set of colors of the neighbors and then it kind of hashes them down. So you keep repeating this procedure. And the idea is that at the end of this, if two graphs are non-isomorphic, hopefully they'll have different sets of colors. So just to illustrate this, start with this graph on the left where I assign all nodes the blue color. Nodes, the blue color, and then what I do is I'm gonna sort of hash the neighboring colors, right? So, so for this node, I have three neighbors that are blue, okay? And then you have the color itself. So, these two nodes are kind of identical and they get mapped to this red color. You can see these three nodes are different because the multi-set actually has one fewer color. Okay, and then you can sort of keep going and hash this, and then you get more distinct. Hash this and then you get more distinct colors. And then, just to give you an example, here's an example of two graphs that are actually non-isomorphic, but they still get mapped to the same colors if you kind of run a few iterations of this test. Okay, so you can see the top graph has a six cycle and the bottom graph has two five cycles. So, you know, they're clearly not isomorphic. Okay, so it's known that sort of vanilla standard GNNs are limited by whatever this graph isomorphism test can do. So there's been a lot of research centered on trying to improve expressivity of these GNNs to go sort of beyond the limits of the one Weissweller Lehmann test. Okay, so I like to think of there as being, you know, there's several different opinions. Being, you know, there's several different approaches to this, but I sort of categorize them into three main types of approaches. So, the first approach is to add either node or edge-level features. Okay, so these features will allow you to kind of discriminate different nodes that you would not otherwise be able to do without this information. So, this is actually the approach that we're going to take, and I'll talk a bit more about this later. Okay, but you know, there's also just to give you Just to give you an example, there's various works, for instance. There's this paper by Boritsis et al., which counts substructures in graphs and kind of uses those as features. So that's an example of an approach that does this. The second sort of main approach to enhancing expressivity is to actually modulate the message passing mechanism itself, right? So if you look at the Look at the aggregation and combination rule of message passing networks, you're kind of isotropically taking messages and combining them. So you can try to maybe combine them in a different way, you know, do something anisotropic. And this is sort of the approach that is taken by works such as directional graph networks, where they actually compute flows according to Laplacians and try to anisotropically aggregate messages. And then the third approach is to kind of modify the underlying graph on which the message passing happens, right? So, this is a good example of this is these k-order GNNs, such as the one that Christopher was talking about yesterday. There you're operating on this graph that involves tuples of vertices. Okay, so again, I mentioned that we're going to take the first approach. That we're going to take the first approach. But I should note that a lot of the existing approaches involve sort of handcrafting features, right? I mentioned that there's this work on counting substructures where you encode, you know, count, say, three cycles or four cycles, and you can use these as features. But one of the issues with this sort of approach is that these features are often really kind of handpicked according to the sort of tasks you're trying to solve. So there's an art to kind of figuring out which. Figuring out which substructures or what you want to actually encode. And our goal is to really find a more general purpose set of graph features that enhance DNN expressivity, sort of task agnostic to the extent possible. Okay, so then I'm going to sort of introduce the main idea here, which we call affinity measures. Okay, so what I want you to think of is that affinity measures essentially capture some kind of structural information in the graph, right? So you want to capture some kind of connectivity information that looks globally at the graph. And the idea is to kind of incorporate these as node or edge features in the message passing mechanism. Okay, and what we're going to do is kind of And what we're going to do is kind of compute these affinity measures at the outset using a pre-processing step. Okay, and then once you've computed those, you can do the message passing step without much additional overhead. Okay, so the idea is that these affinity measures are going to encode some kind of global structural properties about the connectivity of the graph, and you're still able to keep this local message passing that happens in MPNs. Message passing that happens in MPNs. Okay, so the first example of the affinity measure that we use in our work is the so-called notion of effective resistance. So again, these are actually something that are used widely in theoretical computer science. And I'll talk a bit more about this in the next slide. But basically, I think they're widely studied. I think they're widely studied in TCS, but not as well known in ML literature, as far as I know. So I think this is kind of a nice instance of using some ideas from TCS and bringing them to the GNN world. And the basic idea is this. So I have a graph and I can sort of view it as an electrical circuit. Okay, so we're going to go back to sort of physics 101, you know, when you learned about circuits. And we're going to replace every edge. And we're going to replace every edge with a resistor. Okay. And the idea is that I can connect my nodes of interest, say U and V, to a voltage source, say like a battery. So you pass some current from U to V, and then you can kind of measure how much current is actually passing between the two nodes. Okay, so you can hook up a multimeter to kind of measure this. And via Ohm's law, you can kind of figure out what the effective resistance. Figure out what the effective resistance is between these two points. Okay, so this is essentially what we're doing. If you have an unweighted graph, all the resistors have the same resistance, say one, but you can also handle weighted graphs where you adjust the resistances according. So is that clear? Okay, so Okay, so why are we looking at effective resistance? Well, it turns out it's a nice measure that gives us some notion of similarity between pairs of vertices in the graph. Okay, so it has some nice properties. I'm not going to go into too much detail there, but first of all, it is a true distance in the sense that it satisfies the triangle inequality. And then, you know, you get the usual properties like symmetry and the fact that the You know, the fact that the ER between a node and itself is zero. You also have this monotonicity property, which is nice. So, if you increase any of the resistances along the edges or increase the edge weights, you can never actually decrease the effective resistance between any parameters. So, on the previous slide, I mentioned that these are used widely in theoretical computer science. So, there's this pretty spectacular work. There's this pretty spectacular work of Spielman and Srivastava, where they do graph sparsification using effective resistances. If you're not familiar with the work, I highly recommend looking at the paper because it's actually quite interesting. So the idea is you have some graph, it might be dense, and you want to kind of sparsify that graph in a way that you preserve the spectral properties of the Laplacian. And it turns out you can actually do this by sampling edges according to the effective. Sampling edges according to the effective resistance. So, this is actually a very nice way. And that has applications for linear system solvers. You can use factor resistances to kind of speed up numerical linear algebra. And then more recently, there's this work of Alev et al., which shows graph clustering applications as well. Okay, so I just want to make one remark here because I think. Uh, one remark here because I think you mentioned that effective resistance has these distance properties, and you know, a natural question that people often ask me is: okay, why is this different from shortest path distance? You know, why, you know, is it kind of similar? Is it really doing anything that you can't just do with SVDs? So, just to give you a sense of why ERs are actually buying you something, I should note that if you're kind of concerned with just the edges of the graph, Concerned with just the edges of the graph, the original edges. Shortest path distances are kind of trivial, right? Because let's look at the unweighted case in which all the edges have the same weight. Then the shortest path distance between two nodes that are endpoints of an edge in your graph is kind of trivial because you just go along that single edge. In the case of ER, it's not at all trivial because it uses some kind of global connectivity information. So as a result, a lot of the architectures that use shortest path distances. Use short dispatch distances, they often construct some set of landmark points in your graph to which you're computing short dispatch distances. So this is the approach taken by some works like position-aware GNNs. Or you kind of densely connect the network. So there's the work known as GraphFormer, where they use shortest path distances, but they're densely connecting network and Network and doing an attention mechanism over pairs of notes. Okay, so we don't really need to use these techniques because ER is capturing something more sophisticated. And the other thing I want to note is that if you're looking at sort of single source metrics, so single source shortest path distances, you can actually kind of get this from a simple GNN, at least if you only care about shortest path distances to nodes. Shortest path distances to nodes that are not too far away. Okay, so within the K-hop neighborhood, you can get all distances by just doing K-rounds of message passing with the suitable kind of aggregation and combination function. And this, you know, all you need to know really is that this MPNN that I've defined here is pretty much simulating Bellman Ford. Okay, so this is actually quite similar to this work on algorithmic alignment. That's by Xu et al., okay, where they algorithmically align to Bellmanford. Okay, and then just to give you another example, right? So I mentioned that shortest fat distance you can actually get via some DNN, but in small number of hops. You cannot actually do this with ER. So here's an example. Here's an example. We have this left graph and a right graph. The left graph is a cycle. The right graph is actually the same graph, except that we delete one of the edges in the cycle. And what you can show is that there's no way to kind of, you know, if you pick V0 as your source, there's no way to kind of get effective resistances to graphs even to nodes even close to V0, like V1 or V2, using just a finite number of message. Using just a finite number of message passing steps. And this is because the message passing can only see things that are within a K-hop neighborhood, but the presence of this edge actually alters the effective resistance. This is not the case with short dispatch distance. Okay, so just some more intuition, right? So effective resistance is capturing some kind of global connectivity structure. Global connectivity structure. And so, so, just giving you this example. So, if we take this simple graph here, the shortest bat distance is three. The effective resistance turns out is you can compute it. It's eight over three. And let's say I take the same graph and add some more edges denoted by these red ones. So, I've essentially created more paths going from U to V. And it turns out that this actually reduces the effective resistance. Actually, it reduces the effective resistance to something smaller than the one on the top. Okay, so sort of if you have more parallel paths between pairs of nodes, then that tends to reduce the ER value. Okay, so what we're able to show is that if you incorporate these ER features into MPNNs, you can actually go beyond the one-week Spoiler-Lehman test. So the fact that it's So, the fact that it's at least as powerful is not too hard to show because we already know that MPNNs can kind of hit the WL expressivity. And all we're doing is simply adding new features to the MPNN. So it can't possibly get worse. However, there are examples where ER can distinguish some nodes in the graph in a way that standard MPNNs cannot. So if we take this particular example, I've If you take this particular example, I've sort of color-coded these nodes. So, nodes of the same color are pretty much indistinguishable by MPNs. And you can't really hope to do better because essentially there's some automorphism in the graph that takes any green node to any other green node, for instance. Okay, so just by this kind of simple message passing structure, you can't really distinguish those nodes. But the question is, can you distinguish nodes of different Nodes of different colors and a standard MPNN cannot do it, but with effective resistance, we can actually distinguish this. Okay, so now I want to, yeah, go. I mean, I just numbered the notes. Yeah, it doesn't mean anything. One, two, eight. Yeah. Yeah. Yeah, maybe it's better. Yeah, maybe it's a bit confusing. Yeah. Okay, so I described effective resistance in terms of electrical circuits, but there's also a way to kind of look at this in terms of random walks and commute time. So I'm going to just briefly describe this interpretation. So you can think of this notion where you start at a point U, okay, and you do this random walk, and you try to measure the expected time it takes to get from U to V and back to U. U to v and back to u. Okay, and it turns out the effective resistance is actually more or less proportional to this quantity up to some constant in terms of the number of edges of the graph. Okay, so as an example, I can get from u to v via these green edges, and then maybe I will come back using these red edges. So this would be a commute of size six. Alternatively, I can take the following commute, you know, which takes only five steps. Which takes only five steps. So, commute time is basically going to average the number of steps it takes over all possible choices in the random walk. Okay, so then I can also define hitting time, which is sort of the one-way counterpart of this. So, again, I start a random walk at U, but I only care about reaching V and not coming back. Okay, the commute time is kind of naturally the sum of the hitting times in. The sum of the hitting times in both directions. Okay, so just as an example, here's one potential random walk trajectory. And here's another one that takes fewer steps. Okay, so hitting time is going to be another affinity measure that we use. Okay, so how do I incorporate these? I kind of alluded to this already, but here's your message passing network where you do this aggregation, and then I. Aggregation, and then I combine these messages from adjacent nodes. All I'm going to do is basically take my affinity measures and incorporate them into this edge feature that we're using in the aggregation. Okay, so it's quite simple. Okay, so you might ask, okay, why is this actually good? Because these seem to be very simple scalar values that we're using. Okay, how much can you really? Using okay, um, how much can you really buy with this? Um, so the result I'm about to show you here is uh, actually, I was somewhat mind-blown by this because, um, actually, it turns out you can go quite far with just these scalar features. So, we tried to train an MPNN on a large-scale molecular graph data set. This is the PCQM4M. This is actually in the KDD Cup contest last year. And what we were able to get is the best published single model result. The best published single model result that beat all the entries there just kind of using these affinity measures. And one of the things I should note is that many of these other approaches, like conformers, you know, they use some carefully constructed molecular coordinates and various features about the molecule. Okay. Yet we're still able to beat them without using any of this kind of specific, these specific molecular features. So this suggests, you know, this might be a good idea. So, this suggests, you know, this might be a good general approach where you might not have access to all these like privileged features or kind of computationally expensive features. We also beat GraphFormer. Actually, I should note that I think in this contest, the top entry was based on Graph Former with some ensembling. And, you know, GraphFormer uses a densely connected attention network, but we're sort of able to meet that too. Okay, so this is quite nice that just using scalar features we can do quite well. Okay, but there's still this question: can we do something more richer than just scalar features along edges? Okay, so we'd ideally like to maybe use some node embeddings that are actual vector embeddings instead of just one or two scalars. Okay, so I defined a Okay, so I defined this notion of something called a resistive embedding. So the next couple of slides are probably going to be the most mathematical slides. So, you know, hopefully you stay with me. But if not, don't worry about it. I think it's just the next couple of slides are good. Yeah. Yeah, so I'll talk about this in a couple of minutes. Yeah. Couple minutes, yeah. So just hold on for a sec. So it turns out I can compute this thing called a resistive embedding for each node. Okay, so I don't want to go into too many details here, but L here is the Laplacian. And the thing of note here is that L involves, you know, this computation actually involves taking a pseudo inverse of the Laplacian. So this is actually potentially expensive. You know, it can take like n cube time, which Take like n cubed time, which might not be that good for large graphs, okay? But we'll show how to get around this. And one of the nice things about this is that you can actually derive effective resistance from these embeddings. Okay, so turns out that for nodes u and v, the L2 squared distance between these resistive embeddings gives you precisely the effective resistance. Okay, so this suggests kind of using these node embeddings, these resistive embeddings as node embeddings in the Embeddings as node embeddings in the MPN. Okay, and then there's this question of, you know, can I efficiently compute this? So we can get to this. Okay, so again, this is just a reminder. Okay, so I have this resistive embedding and, you know, the L2 squared distance between these embeddings at the node. Between these embeddings at the nodes gives me precisely the effective resistance. Okay, and now there's this kind of nice idea that I can use the Johnson-Lindestraus lemma, which basically tells us that if I have n points in Euclidean space, I can pretty much map them down to about logarithmic number of dimensions and preserve the square distances between points. Distances between points. Now, again, since I want to get effective resistance and kind of preserve those, and those are given by L2 squared distances, I can, the idea is that I want to kind of just use the JL lemma to map these vectors down to a smaller dimension. And what I'm going to do is actually use a constructive version of this JL map. Okay, so it turns out I can, first of all, compute this projection matrix. Okay, M is sort of the number of edges. Okay, M is sort of the number of edges in your graph. And then I choose some K, which is logarithmic. This corresponds to the dimensionality reduction in the JL lemma. And there's various choices for Q. You can take like a matrix of random Gaussian, or there's other things you can do. But the idea is that you can speed up computation. And I'm basically going to do this projection. Okay, so I will get a result. Okay, so I will get a resistive embedding, but a smaller dimension version of it. And it turns out, you know, there's a lot of work on this in theoretical computer science, but you can use fast Laplacian solvers to solve some roughly m log n number of Laplacian systems. And you can pretty much compute these resistive embeddings for all the edges in running time. That's roughly your number of edges. Okay, so if you have. Okay, so if you have sort of a sparse graph, then you can save on this n-cubed. Yeah. Yeah, that's actually a good question. I was actually searching around for this, and I saw one paper. This and I saw one paper that was kind of talking about it. I think it was on open review. I don't know what happened to the paper afterwards, where they were sort of looking at this, but it's not something that I've really investigated. But I think that's an interesting question. Like, can you use these effective resistance dupes by swaying that? Maybe even in these sort of graph attention architectures where you have a lot of like dense architecture, densely connected nodes. Densely connected nodes, maybe you can do something there. Yeah, I think it's an interesting question. Okay, so basically in a nutshell, these are your sort of true resistive embeddings. They involve some matrix inversion that's expensive, but we can use this JL to kind of get a fast version of this. And then using my sort of approximate embeddings, I can get I can get back the actual effective resistances up to this one plus or minus epsilon factor by simply taking the L2 square distance again. And then it turns out I can also approximate hitting time. So this was something that we couldn't quite find in the literature, but we were able to show that you can sort of define some weighted mean of the resistive embedding. And this pi is given by some stationary distribution of a Stationary distribution of a Markov chain. But basically, your hitting time can be approximated by taking some inner products, and inner products are related to sort of L2 squared norms. So by this JL property, you can kind of preserve the hitting time to a reasonable approximation. Okay, so basically the punchline is with these dimensionality reduced embeddings, you can still compute a Compute approximations to our affinity measures. Okay, so then I'll just share some more experimental results here, just the last couple minutes. So, one of the things we looked at is this PNA data set. So, this was in this highlighted in the principal neighborhood aggregators paper. Neighborhood aggregators paper. So, this is a synthetic data set of actually consisting of six different tasks. So, there are three node-level tasks and there's three graph prediction tasks. And the idea is that you want to see kind of how well you learn embedding such that you can kind of do well on all these tasks together. So, the idea is to jointly train these node and graph level tasks. So, what we saw is that. So, what we saw is that, you know, just using effective resistance itself, you get results that are beating some of the existing methods like directional graph networks. So here, this is kind of a logarithmic scale. So it's logarithm of the mean squared error. So basically, the smaller number is better. The more negative it is, the better. So with effective resistances, we were already able to get for. Already able to get fairly good numbers. But turns out when we incorporate these resistive embeddings as well, they're clearly giving some additional structure and we were able to sort of blow it out of the park with them and actually get sort of the best numbers on many of the individual tasks as well. Uh no, this is this is supervised, yeah. That's it. It's just a pre-computation stuff that we do at the beginning. Right. Yeah, on the graph structure. Yeah, it's just features that you have. Features that you have. Yeah. So, you know, with the ER, we got some wins, but using the resistive embeddings as node features, we were able to get more structure and sort of do well on these tasks. We also have some other experiments. I'll just highlight a couple here. So, these are from the OGB collection of data sets. Data sets. So the above table is for mall PCBA. So using hitting time or effective resistance, we were able to kind of get the best result. And one thing I want to highlight is that we were actually able to do so using a shallower network in the message passing steps. Okay, so somehow, one of the nice things that we're getting out of these affinity measures is that even when maybe the actual accuracy is not. The actual accuracy is not much better than previous methods. We're still sort of getting them with a fewer number of layers. So, for instance, in multi-CBA, we're getting roughly close to what directional and graph networks get. And we're also beating the standard MPNNs without these features, which is maybe not surprising. Okay, so just to conclude, we've introduced these affinity measures, which are a general purpose set of features that you can use in any sort of MPNN. And the nice thing is that you can take your favorite type of MPN. You know, there's a bunch of different types out there, and you can just kind of use these features in conjunction with that and see some performance gains. So if you care about So, if you care about scalability, you might want to just use the scalar features. So, this works well with very large graphs. But if you're willing to tolerate some overhead, you can also use vector node embeddings based on these resistive embeddings. So this has some versatility built in here. And somehow we're able to bridge this gap and give good theoretical performance, but also have scalability. Also, have scalability and do well in experiments. We're able to get sort of the state of the art on the best single model results for large-scale molecular data set. And we get similarly good performance for other types of molecular graphs. We also did experiments on citation graphs. I didn't quite show it here, but we're able to kind of do as well as other methods. We're able to kind of do as well as other methods there as well. So, I think one question maybe for the future is: can we design tailor-made architectures that use affinity measures in a more direct way, right? Sort of maybe directly integrate them into the message passing rule somehow, or maybe use these as like positional encodings and attention-based architectures and so forth. So, I think there are a lot of things to explore here with them. To explore here with them. And some of these we're looking at currently. So, but I'm hoping there are more directions we can go in with this. And that's all I have for you today. Leave the floor up to questions. Thanks for the live talk. So, I have a question regarding all your A question regarding on your uh sorry the random walk um interpretation of the effective resistance. So, if if we use a random walk to compute the effective resistance approximately, is there any requirements on the transition curl of the random walk? Also, like because I, if I understand correctly, you can simulate a bunch of random walk and then compute the heading time. Compute the heating time approximately, and then use the heating time to get the effective resistance approximately. Yeah, would that be an alternative way to compute that? So just to try to understand. So what are you suggesting as the alternative method to? Yeah, so basically first simulate a bunch of random walk. I see. And then you can compute the heating time. And compute the heading time for each pair of nodes approximately. And then from heading time, you try to compute the effective resistance rather than compute the basically avoid the pseudo-inverse. Right, right. But I'm not sure that that's more efficient. So that's why I want to get your opinion. Yeah, my feeling is that you might need to sample a bunch of these locks from different nodes. So I don't know if that would necessarily be more efficient. More efficient. I think here we can sort of get provable guarantees with the epsilon. You can sort of get the desired approximation qualities. Maybe you can do that with just simulated walks as well. But I would imagine that you kind of blow up in the number of walks you need starting at each node. Right, right. That's more like a kind of spatial thing to run on the graph. But if you have some degree of freedom to design. some degree of freedom to design the curl like transition curl of the round walk you might achieve some i guess fast uh yeah i think i think that's quite possible so you see here for the transition matrix we're just um we're just kind of uniformly going to a neighbor of each node um but maybe you can try something more sophisticated yeah thanks yeah i had a question on the experiments that uh in uh Experiments that in many tables you show that you use noisy nodes as the yeah, pretty much here and in the previous experiment. So I was wondering: is noisy nodes that important a factor? Because it seems that it should be something with it. Otherwise, without this self-supervision, all those scalars are like very hard to make something useful of. So I was wondering if you had any ablations without noisy notes. Yeah, actually, actually, I should have included this other table because I actually updated. This other table because I actually updated this and then forget to change it here. But we actually had the similar result without noisy nodes. I think this is in our archive app perhaps. But yeah, so even without the noisy nodes, you're still beating the other benchmarks. But I think it's a good question, right? Because it's not clear whether the win you're getting is due to the affinity measures or just the noisy nodes. But actually, But actually, you are getting wins even without the noisy notes. I think these are just sort of the best results we got, which are, you know, you get slightly better with noisy notes. And maybe, ah, throw more questions. So, you have this method to embed the To embed the nodes to some features, and then the distance between the features gives you the resistance, right? And but eventually, you only use the resistance between pairs of neighbors, right, in the message passing network. Correct, yeah. For the scalar features, we only use edge features, right? So, in the message passing rule, there's a way to integrate the edge features. So, we only use them, yeah. So, is there a way? Yeah, so is there a way also to somehow propagate information not only along edges and use the resistance between pairs of nodes which are maybe some two-hop neighborhood or three-hop neighborhood? Yeah, that's a good question. I mean, I think, yeah, I think it would be good to try that out. I haven't done it myself, but yeah, either taking more hops or maybe combining with this idea that you see in other papers where you construct some kind of set of You construct some kind of set of anchor nodes, and maybe you want to compute effective resistances to these specific, you know, from every node to these anchor nodes. So there's a few things you can probably try there. But yeah, I think it's a good question. Thanks. Yeah, thanks for the interesting talk. So I was wondering if you can precisely quantify what kind of permutation invariant functions GNNs enhance with ER are able to. Uh, ER are able to approximate. I mean, you have this linear system, but you have this pseudo-inverse in there, so I guess it's hard. Yeah, so you're saying what sort of so as in, okay, we're going beyond one WL, but can you characterize what sort of functions you can capture? Yeah, it's a good question. I mean, in short, I don't think I have an answer, but I don't think I have an answer for sure because I haven't really thought about it. But yeah, that's a maybe a good theoretical question. Hi, thank you for a nice talk. I was wondering, so like in the talk of Stefan Guineman, we saw that like the direction also of the molecules was having an importance. And so it seems that here like your model is doing very well. So when I'm seeing like resistance, I'm also thinking like, could we also use impedances? Use impedances and so then go in the complex domain, which actually would encode like the directionality of like the different edges. And so, because you know, like when you were like interpreting this as an electrical circuit, so would that be would that be making sense at all of like using those complex? Yeah, I think this is a good question, actually. So, so one of the things that you know is not quite natural with this effective resistance is that it's it's a symmetric function. So, um Metric function. So, you know, if you have a directed graph in your data set, it's not clear how to really distinguish between an edge going from U to V versus V to U. This is just kind of how inherently effective resistance works. I would be interested in seeing what extensions you can do to kind of incorporate the directionality. But yeah, I think this is a good feature work. Thank you.