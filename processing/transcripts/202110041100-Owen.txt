This is joint work with Hal Varian from Google and Dan McKluger, PhD student here at Stanford. And I have a more complicated disclaimer than usual. So Stanford insists that we talk about, you know, if something wasn't done at Stanford, then of course we should say it. So I did, I got started on this problem when I was consulting for Google. But when it was my turn to chair the department, I had to stop doing. Department, I had to stop doing both of those things. But I kept on working on this problem. I think it's a cool problem. And there are now like three different students talking to me about different angles and different takes on it. And here are the papers that are that are out. This one with Halvarian is in the Electronic Journal of Statistics. And Dan Kluger and I have a follow-up on archive. Well, the take-home message is Message is where it's feasible and, of course, ethical, shoot some randomness into the decisions that you're making. And of course, the context is one where you're going to get to see some follow-up. It's not like a one-shot deal. But if you have a repeatable thing, then it makes sense, if you can, to put in some randomness. And what got me thinking, well, one of the examples is the customer loyalty plan. I have this in the back of my mind. And I had this in the back of my mind before I started chatting to Hal. So I read a story that said: you know, airlines, they don't just give an upgrade to their best customers. They're going to try to give an upgrade to the customer that might become more loyal to them later. So they want some kind of causal thing. And so instead of just giving it to the end most loyal customers, you know, why don't you try to guess who would start flying more if you gave them that upgrade or whatever nice thing you want? Upgrade or whatever nice thing you want to do. And I got to thinking about this some more. Maybe as a customer, if you have one of those fancy credit cards from one airline, you should buy tickets on a different airline. Now both airlines know that you're up for grabs and they might get them into a bidding war. So we can turn this to our advantage as consumers. And there are plenty of other examples. So hotels can give upgrades, car rental companies, Uber could do something. Companies, Uber could do something. Uber keeps asking me if I want $30 off of a restaurant meal. So I don't, maybe somebody else is getting a $50 offer. Though there are, and there are e-commerce platforms. So you could just imagine like what might, what might YouTube be able to do for their content producers? Or what might Amazon do for reviewers? Or what might somebody offer to their advertiser? So the things that you can do that make it a little bit nicer for one of your counterparties. For one of your counterparties. And then you have two goals. Well, first of all, if you're doing this, you want to get some value out of that offer. You know, you don't, you know, you could let that first class seat ride empty. Or if you're going to dedicate it to somebody, move them from business class or whatever, then you want that to make a difference. There's a cost, so you want to get the most of it. It might be a small cost if the thing was nearly empty. If the you know, thing was nearly empty, and then on the other hand, you also want to measure it: like, does it do what you want? Like, you know, maybe if you upgrade somebody, they come to expect that, and now they're a worse customer than they were before, because they're only going to get something that's on sale or a special. How do you know? Or maybe it interacts with features of the people, so you want to know who to offer something like that to. Well, I have. Well, I have for the two goals, I have for you two acronyms: RCT, that's a randomized controlled trial, and RDD, that's a regression discontinuity design. And obviously, it's better if your acronyms are the same length and rhyme. So that's what we have. So there's an RCT and an RDD. And what we're going to do, we're going to smush them together and try to use a little bit of both. I'm not going to see. I'm not going to assume that you've seen regression discontinuity before, so let me give you an overview. And here's a famous example of regression discontinuity. Students write some standardized test and their test score is X. If they get a high score, then maybe they're a national merit scholar or they get some sort of honor, some way to recognize who's got a top score. And you would do that like maybe if they get 1700 or more on their 1700 or more on their test, then they get some sort of recognition. And then, if you want to measure the effectiveness, you could look at something like: well, did they go on to grad school, assuming that's good for them, or how are their future earnings, or what other impacts are there on that person later? And so it's tricky because the people that were high scoring were already more likely to go to grad school. And the logic of a regression discontinuity is. Regression discontinuity is, you know, there are some students that score 1701, others are going to get 1699, and that's basically the same kind of people. There's really the 1701s, you know, maybe it was half a glass of orange juice that morning or some minor difference or, you know, somebody got a better or worse night's sleep. They're essentially practically identical. So if you see a difference, now you can attribute it to Z. So it's attributable. So it's attributable. And I consider every question of discontinuity the second most believable causal inference method. It seems like the logic is pretty strong for this one. Now, they actually use these kind of things in other settings, like in, you know, who gets a scholarship, for instance. Here's a famous example about small classrooms. Is it better to have a small classroom? Is it better to have a small classroom? You'd think it would be kind of obvious, but what if you want to be data-driven? It's not easy if you're data-driven. And a small classroom, if you just look at observational data, it's kind of a mess. You know, maybe what they do is they cluster together a small group of gifted pupils into one room. And then it's a small classroom, but they would have done better anyway. Then again, maybe the same principal figures that Principal figures that the gifted kids are going to do okay anyway, so they put sort of a mediocre teacher in that room. Or maybe what they do is small classroom is the most difficult, disciplinary people with a drill sergeant kind of teacher. So the fact that it's small, it might mean that they're destined to do better anyway, or it might mean that they're destined to not do as well anyway. And so you can't really, you can't completely trust an observational study. You'd have to adjust for any number of other things. Adjust for any number of other things. If you like, you'd control for other things, but nobody controls. You only adjust. So in Israel, there's this thing called Maimonides rule. And a classroom is supposed to be less than or equal to 40 students. If it turns out you get a 41, you split it into two classrooms, maybe a 20 and a 21. So now you can compare the 40s to the 41s and see whether the big classroom is worse than the small. Classroom is worse than the small classroom, or vice versa. And this is exactly what Anguist and Lavi did. And they were comparing the classes that had been split, just barely split versus not split by that rule. And they were looking at reading scores and verbal scores for third, fourth, fifth grade students. Okay, here's some more about regression discontinuity. It actually goes back quite a ways. Wow, it's 1960, so we're like. So we're like, you know, we've had like 60 years of regression discontinuity. It comes from econometrics. And imagine here's a hypothetical running variable going from minus one to one. And here's the response. And suppose you get the treatment if and only if x is bigger than zero. So here's their cutoff x equals zero. And let's take a little window. We couldn't, you wouldn't just do the 1701s versus the 1699s. Versus the 1699s. There's not enough 1701s. You know, you could do better by gathering more data. So we could take a window to the right of the discontinuity and a window to the left. And here I've done that with some made-up data. And then this difference here could plausibly be the causal effect of the intervention. Well, it's a plausible estimate for people right at the running variable. It doesn't mean that the people over here would have the same difference. You didn't do anything. Difference. You didn't do anything over there, so you don't really know. And then, so pre-treatment, you can compare the barely negative to the barely positive. You know, people don't normally do it quite that way because it's leaving out all the data over here. So what you might do instead is fit a linear model with a change in intercept, maybe a change in slope as well. So you could fit a linear model and then look at the difference here. And now. And now the beauty of the regression discontinuity is starting to get unwound by practical complications. You don't really know if it's linear out here. So, and then you could try fitting a quadratic, except then the variance just shoots up astronomically. So it starts with a very compelling idea, but when you get down to brass tacks and start doing details, you face some very hard decisions. Okay, so how big of a window? So how big of a window should you use? That's actually not easy to say. I'll mention a few of those things later. Okay, so here are the variables in the formulation that Halbery and I had. The letter I, that's the customer ID. XI is a running variable, or some people call it the assignment variable. And ZI for Banff is the treatment, and it's a plus one or a minus one. Z is also the same as Z. Z is also the same as Z, so I may slip back into that later. And then there's an outcome Y. And for an e-commerce company, Y might be the revenue or profit or some other measure of what happens for that customer later. Now, what's this running variable? It could be like last year's revenue, or it could actually be, and more realistically, I think, some machine learning prediction of how. Learning prediction of how responsive that customer is likely to be. But for us, it's just a number, it's a real value. There might be a thousand features here, but they got boiled down to one. And let's make it simple. When the data come in, you don't make it simple, you make it more complicated. But we're talking about what happens before there's any data. So why don't we make it simple? So say half of the Z's are plus one, half of them are minus one. One half of them are minus one. We can look at different choices later. And let's just sort everybody and rank them from least to most likely to be a good customer. And we can do this by redefining xi is just going to be 2i minus n minus 1 over n. So these go between negative 1 and plus 1. So that's our running variable, equispaced linear, and we'll fit a two-line regression. So we can have. We can have a baseline regression here, and then we can have an intercept change and a slope change. And, you know, obviously the model is wrong. I think it's useful because you can plan with it. Then later when the data come in, you do something, you know, you can do something else. But what are you going to do before you have the data? I think designing simple makes more sense. And then you can analyze in a complex way. In a complex way. Okay, so how does the cutoff or the tiebreaker design work? We'll pick two cutoffs, A and B. And anybody that scores higher than B gets the incentive. Anybody that scores lower than A does not. And in between, we randomize. Okay, so you think of your customers, we've got the good, the bad, and the random. So here we have the running variable, goes from negative one to plus one. All these people, subjects. These people, subjects, customers get the treatment over here, they don't. In between, it's a coin toss. And in the extreme case, if you take A equal to B, then you squeeze this interval in to nothing and you get a regression discontinuity design. At the other extreme, you can send A to the left of your least observation, B to the right of your greatest observation. Now, everybody's in the middle random group, and you have a randomized controlled trial. So this tied. randomized controlled trial. So this tiebreaker fits between, it interpolates between an RDD and an RCT. And these are also called cutoff designs. And here's some examples. So one of them, it actually surprised me in Nebraska, they offer a scholarship to students that do really well in high school. And of course, if they don't do that well, they don't offer it. But in between, it's random. And you wouldn't make the National Merits. You wouldn't make the national merit scholar random. That would be kind of weird. And as a statistician, I think it would be slightly weird to randomize who gets a scholarship. But these are economists, and they're a little bit bolder than we are. So they'll do it. And then the response is like six years later, did they finish their degree or not finish their degree? So that's one example of a tiebreaker. I found another one where whether somebody goes to rehab, some people clearly need it, some people clearly don't. Clearly, need it, some people clearly don't. You don't actually know where the line is, so why don't you randomize around the line? And there was another example about who should get a remedial English class. I want to mention a classical experimental design thing. It's the Lanarkshire Milk Experiment. And this was done, I believe it was done in the late 1920s, but analyzed by student in and at student in 1931. And they wanted to know whether it would be beneficial. wanted to know whether it would be beneficial to students and then you know children in the uk in the 1920s they're pretty poor compared to you know like how things are now in say canada and the us and there was this idea what if you give people some milk every day they get a glass of milk it could be um what it was either pasteurized milk or unpasteurized milk or no milk at all so they that's what they did but a lot of the principals looked at this as let's just give it to the kids Let's just give it to the kids that need it the most. And you know, you can you could well understand if some principal might want to do that, but of course, that messes it up statistically. And conceivably, a tidebreaker would have been the thing to do. Okay, so here's how a tidebreaker works. I have some hypothetical data. Red ones are treated, black points are untreated. And if you did a randomized controlled trial, the way I did it, all the red dots land above the black dots. And so here, it's pretty clear. The black dots. And so here it's pretty clear that the treatment has a positive effect. It's clear visually. If you did a regression discontinuity with the same model, you can't quite tell. But if you do a tiebreaker and you make them overlap, so one-third of it is experimental and two-thirds observational, now you can kind of see that the red dots tend to be above the black dots. And if you did a two-thirds experiment, it's even clearer. Okay, so visually, we can kind of tell the more experiment you do, the better. You do the better. Well, let's do it mathematically. So, this is from the joint paper with Hal, and we have a two-line regression function. And this variable said it's actually plus or minus one. It's not zero or one. And then here's x. Well, we have an intercept, that's a column of ones. We have our running variable, that's a column of x's. We have our assignment variable, that's this column of z's. And then we have this interaction term. And then we have this interaction term, which is a product of the prior two terms, prior two columns. And so what's going to be the variance that beta has? Well, it's just x transpose x inverse sigma squared. Of course, we're assuming constant variance. But, you know, at design time, you make simplifying assumptions compared to analysis time. And what's the chance that z i equals one? Well, it's zero if x is really small. It's, you know, again, it's the good, the bad, and the random. So we have an experiment. We're going to experiment. An experiment, we're going to experiment on a fraction delta of the data. Okay, well, what does x transpose x look like if you divide by n? Well, you know, one times one, so one squared, you know, over n is just one. The x is by design sum to zero. The z is by design sum to zero. What about x squared? Well, I kind of know what that is. x squared, it's like, you know, we know what that moment is. The only one that's left is, well, this is the. one that's left is well this is the this is the average of one times xz but this one is the average of x times z this is the average of z times x this is the average of xz times one of course they're all the same multiplications still commutative so that's kind of that's a critical quantity this moment and what does that moment look like well we're going to take the We're going to take the integral of x, and then we'll take the expected value of the incentive given x. And when x is below negative delta, this is always negative one. So we plug in a negative x. Above delta, you get a plus x. And in between, it's 50-50 plus or minus 1. So it's zero. Okay, and then the error in this wiggly thing, if you did everything at random, it would be one over root n, but you don't have to do it at random. You can actually stratify. But you don't have to do it at random. You can actually stratify. You can pair things up and do a plus one and a minus one. You can basically make this one over n, the error. So we're at the time we're designing the experiment. We don't know x, but we do know x transpose x. You could bank on this being pretty much x transpose x. So you know what the design matrix is going to look like in the future before you even get the data. Okay, well, let's rearrange that. There's a few zeros in that. So we can, if we change the order and go one, and then the interaction term, and then the other two variables, then we get x transpose x over n looks like this. Well, I can invert that matrix. I could do it with my bare hands. And so here's the inverse, and we get one third minus phi squared in the denominator, some minus phi's, and of course, sigma squared stays there. And then what is that phi? Well, we worked it out in the prior slide. We worked it out in the prior slide. It's one minus delta squared over two. Okay, so we know what the design matrix is going to look like. And it's only going to depend on our delta. Well, it'll depend on, well, this will depend on sigma squared in the variance. Okay, so what should be delta? Well, let's look at the variance of like C transpose beta hat if we use this delta compared to that delta. And you get to pick your vector C. Well, sigma squared cancels out of that ratio. So from here on, we might... out of that ratio. So from here on, we might as well have sigma squared equals one. And what c should we use? Well let's think about the c that we want. Suppose we give somebody the incentive, so z is plus one, then their expected return is beta one, beta two times their x, beta three times their z, which is a one, and beta four times xz, which is x. But if instead they're left out and they get the negative one, then it's beta one, beta two, negative beta three, negative beta four x. 3 negative beta 4x. Now we only care about the increment. We care about the difference between getting the plus 1 and getting the minus 1. So we'll just subtract those two things. And then what we get is twice beta 3 plus twice x beta 4. Okay, so clearly beta 3 and beta 4 are the important things. They affect our decision. X matters too. Okay, so this is a linear combination of our betas, and let's work out. Or betas, and let's work out what its variance is. Okay, well, that's we can assign those that to our undergraduate class, and then you get one over n, this thing. Now, let's look at that. Well, it's something about x divided by something about delta. And so, what that means is if you got the best delta, it's the best delta for all the x's. Or if you get the worst x, it's the worst x for all the deltas. For all the deltas, because the problem is separated. Now, let's look at how that variance goes versus delta. So, here's delta. I can do delta from a regression discontinuity. I'll sweep it all the way up to a complete experiment. And the coefficient of z, here's the variance of that thing. It starts off at four if we did a regression discontinuity, and it drops down to, oh man, I think that's two, but no, it's no, sorry, it's some. It's no, sorry, it's some, it's a third of, hold on, I'll show you later. And then what about Z times X? Well, its variance also drops off as we, the more experiment we do, the less variance we get. And the beta three hat variance is actually exactly three times the beta four hat variance. Here's the impact. The impact is two beta three plus x beta four. So what happens? So, what happens to the variance of that impact? Well, if we did a regression discontinuity, the variance would start off here at, you know, x equals zero, it's the least, because that's the center of your data. But as you move towards the fringe, you get out to x equals one, the variance goes up. So that's regression discontinuity. Let's do 10% experiment. We get this curve. 20%, 30%, 40, 50, 60. If we did a full experiment, then we get this lower curve. This lower curve. So, the more experiment you do, the better off you are at all x. Interestingly to me, the worst thing that happens in the experiment, which is x equals one, that's better than x equals zero in the R D D. RDDs have a lot of variance. Maybe you have a lot of data, but you need more than a lot of data. RDDs are very inefficient. Okay, so what do we get from doing this? Like at the end of the year, Like at the end of the year, say what did we gain from that customer? Well, for the average customer, the expected value of, let's still look at the average y in the future. Well, of course, there's beta one as a gimme. Beta two x, you know, if we average that, it's going to be zero. And then here's beta three times the expected z, beta four times x times the expected z. And beta four is pretty important. That's the one that that's the one that says, you know, does it. That's the one that says, you know, does it really make a difference? It's the interaction is what makes a difference. If there would be no interaction, then it just wouldn't matter what you did. Okay, so then what's the expected value of Z? It's negative one, zero, or plus one for the, again, the good, the bad, and the random. And so what are we going to get? Well, let's replace this by its integral or negative or its expectation. And a little bit of algebra or calc, you know, first-year calc or something. You know, first-year calc or something. The gain that we get on average, if we're going to use delta, it's beta one. We're going to get that anyway, right? That doesn't depend on delta, plus beta four times, uh-oh, one minus delta squared. So if beta four is positive, and we would think it would be, otherwise, why would we instant the upper end of the distribution? The more experiment we do, the less we gain this year. The more experiment we do, the less variants we get, but also the less gain. So it's an exploration-exploitation trade-off. The more you explore, you get more information, but you damage your short-term efficiency. And we can reduce the regression discontinuity variance by some factor ranging between one and four. We get a four-fold reduction from the experiment, one-fold reduction if we don't do anything. Okay, you know, it's if the If the RDD itself is a one-fold reduction of the RDD. So let's look at this variance ratio. Efficiency is a variance ratio. So here's the variance of what we would get if we did an RDD. Divide that by the experiment. Chug away. And you get this quartic function of delta. And so if you want a specific So if you want a specific efficiency level, you can solve that quartic for delta. What's nice about that quartic is it only involves, it's actually a quadratic and delta squared. So you solve a quadratic for delta squared and then take the square root. And so if you want efficiency at least rho, you have to experiment at least this. Okay, so here's the efficiency. If we did the RDD, we'd have one fold efficiency. We can get as high as fourfold. We can get as high as fourfold. Here's the delta that we would need to take to get that. So, if we want 1.1 fold reduction, we'd better experiment on 13% of the customers. If we want a three-fold, we'll experiment on 65% or put it another way. If we were going to do an RDD, we could be 20% more efficient just by experimenting on 18% of the subjects. Subjects. If we want to double the efficiency compared to the RDD, we have to experiment on 43% of the subjects. At the beginning of this project, we kind of thought we'd need a sliding scale. So maybe, you know, 100%, 75, 50, 25, 0. Maybe we thought there might be a sliding scale, or maybe a continuously sliding scale. And what we found. And what we found in that paper is that carpentry like that doesn't help. All you need is 0, 50, and 100%. And then just a week or so ago, a student here, Harrison Lee, said, you know, it's not true. If you're only going to incent like 5% of the customers, then 0, 50, 100 is not the best way to go. You can do a different kind of a scale. So that's work in progress. Okay, so let's look at. Okay, so let's look at that non-central region. And, you know, and we did this just with 1%, sorry, ZE's one for the top few, random for another few, and negative one for pretty much everybody. And then what would you do? Well, when the data come in, you don't really believe the linear model is going to extend from, say, United Airlines' best customer. Airlines as best customers all the way down to their worst customers, it's probably not really right that linearity would extend over that range. At the time you're designing it, you might not know where linearity extends. So let's suppose that 15% get the treatment. That wouldn't be true for first-class seeds, but it might be true for other things. So you could give the top 15% an 15 percent an incentive, and your variance will be 223 over n times sigma squared. But suppose instead the top 10 percent get it and then half of the next 10 percent. Well, that cuts the variance from 223 down to 137. So it's about 60 percent more efficient to do a little bit of tiebreaker. Okay. Okay, so now one of the criticisms that we got from that work, and we got it published, but we got pushback too, because people said, well, nobody really fits this global linear model. Because I wasn't going to do that anyway. I would just fit. I would wait till the data came in and then pick a linear model. You have to design it not knowing what's going to happen. But what's much more common, at least in the literature, is that people Is that people use kernel regressions? So they do some kind of local linear smoothing. And this is some of the work that Dan Kluber has been working with me on. So the expected return, it's not really this two-line model. It's more realistic to say we have a smooth function. People that get the incentive, there's some function mu plus, which is what their average return will be. And those that don't get it, well, we'll call it mu minus. call it mu mu minus. And so it's more common in the literature now to do a kernel regression at the point x equals t, right at that threshold. And so one of the major articles in this is Imbans and Kalyan Araman, and they actually tell you how to pick the bandwidth parameter. So what we'll do is we'll take a we'll take our two-line model. Model. So there's y, and then there's, oh, I've got two beta fours. It's identification. Okay, so anyway, call that, deem that to be beta one, two, three, and four. And we'll take this local squared error, and we'll put a kernel weight right around t, our target. And this kernel weight, it could, you know, could be any of the non-parametric regression kernel weights. And then we'll argmin over. min over we'll find the minimum squared air variance and of course we have to pick a bandwidth h and what could be the bandwidth well it could be a boxcar kernel where you know it's just one for the things that are close to the target and zero for the ones that are far from the target okay so that's that's a local linear model incidentally we found if you go local linear and you redo the You redo the 15% versus top 10 and half of the next 10, we get pretty much the exact same 60-some percent efficiency gain from a local linear with a box scar kernel. Okay, so let's look at the classroom size data. So we have enrollment in fourth grade and the 40s didn't split, the 41s and above did split, and here is a measure of average verbal score. Measure of average verbal score in all these different classrooms. And here is a measure for those other classrooms at their sample size before they got split. Obviously, the 50s got split into like, you know, maybe 225s or something like that. And if you pick a local linear regression, here's how local turned out. The regression turned out to be this red line. These schools were not in the local. were not in the local regression. These ones were not, these classrooms were not, but these ones were. And so you get this effect for your confidence interval. Okay. And using this approach from Inbins and Kalyan Araman, H would be 14, essentially 14. So let's see, what's that? 14. Let's go. 40 minus 14. That sounds like 26. Okay, that's kind of where it goes. And then the nine. Okay, that's kind of where it goes. And then the 95% confidence interval for the effect of splitting the class went from negative 1.4 to plus 7. So, you know, you did maybe did this amount of benefit or maybe did that amount of harm, or maybe did nothing. It's okay if you're 95% confident that it won't include the origin. Okay, so what's the optimal kernel? Now, in a regression discontinuity, the target point T, it's actually not in your data interval. You're going to get In your data interval, you're going to get μ hat plus from all the observations above t. So then you have to do a kernel that extrapolates from above t all the way down to t, just outside the end point of the end of your data. And likewise on the other side, you have to extrapolate from below the threshold up to the threshold. So, and then you take the difference of those two subtractions. And there's some theory that says, well, if you're going to do that extrapolation. Says, well, if you're going to do that extrapolation, of course, we often tell our students don't extrapolate. But if you have to extrapolate like that, then it's known that the best kernel to use is a triangle kernel. And this comes from Cheng Fan and Maron. So if you did have to extrapolate, then you'd use a triangle kernel. So it's pretty common in regression discontinuity to use a triangle kernel instead of a box scar kernel. That's commonly recommended. So let's go back to. So let's go back to the classroom size and use a triangle kernel. And so with the triangle kernel, it kind of looks the same. It's a little bit steeper on the right, and H is only 9, it's not 14. So we're going to use fewer of the classrooms. That was what was selected. And we still have a confidence, oh, our confidence center is actually a bit wider. I think it gets wider in both directions. Again, it's not a teleclass, it's not our job to get P below 0.05. Job to get people 0.05. Somebody else can do that. That's not something statisticians have to get involved with. Okay, there is bias and variance in this estimate. And later, not this talk, but Dan Kluger has done a deep dive into how the bias works. But a policy question really requires a confidence interval. So what Kalanako, Cabneo, and Kalanako, Kapneo, and Tatuna do. They choose an H that's slightly undersmoothed. And by choosing a smaller H, then variance dominates and you can trust a confidence interval. So you can trust a confidence interval a bit better. So if you need a confidence interval to make a policy decision, of course, that's the trade-off. You might be less accurate. You might get a worse mean squared error, but you get a more reliable coverage level. I think that's a tricky trade-off. That's a tricky trade-off, and it's tricky whichever side of that you're on. Anyway, if you go with the idea of undersmoothing in order to get a confidence interval, then it makes sense to study the variance. In the paper with Dan, we do study the variance. He, however, has gone on to figure out how the bias works too. Okay, so we'll do a local tiebreaker. We still collect up our running variables. Then we choose a distribution for Then we choose a distribution for the incentives. Then we assign those incentives. Then we observe. And then we pick H and run a kernel smooth. Now, at the time you're designing the experiment, you're up here. You can actually see what's going to be your X transpose X matrix. You can see into the future what X transpose X will look like, but you can't see what Y is going to look like. You can't see what H is going to look like. So you can't see what the kernel symbol is going to look like. So we're up here at step two. All the Y's are in the future, so is H. The future, so is H. Let's use the boxcar kernel. So x goes from negative one to one, ek was spaced, and h is the bandwidth of the smoother, and we randomize in the middle. And there's this notion of the local experiment size. So delta is the say the half width of your experiment, and h is the half width of your kernel, and it's the ratio. And it's the ratio, little delta, that really counts. And with the boxcar kernel, we know what the efficiency versus the RDD is. It's this quartic function of the local experiment size. And it's exact same for the two lines. And the efficiency is four. If that little delta is bigger than one, well, then this stops being quartic and it starts just being four. Because now the if delta is more than one. Is more than one, well, it means that your whole thing is an experiment. And so, here's the efficiency if you did a tiebreaker and did a boxcar kernel. So, here's the efficiency ratio going from one to four. Here's how much experiment you did going or local experiment going from, if it goes from zero to one, you max out at four and then you stay at four. Okay, so that's if you do a box cart kernel. you do a boxcar kernel. You don't know what the kernel, you don't know what H is gonna be, but you're always better. You don't know how much better you're gonna be, but you know you're gonna be better. To me, that seems like that's a good place to be in when you're planning. We know we're gonna be better, we just don't know how much. If you use a triangular kernel, you get a very similar looking curve. It doesn't go quite as high. It only goes a little bit past 3.6. It doesn't go up to four and starts off at one. And it starts off at one, because that would be, this is the tiebreaker, this end. And, you know, that sure looks monotone. Okay, so triangular kernel. We have equispace data from negative one to plus one. H is the bandwidth of our smoother. We're going to randomize in the middle. Local experiment, it's still capital delta over h. And here's the efficiency. And here's the efficiency. So, the efficiency, I showed you that curve back here. Here's the formula for that curve. And if you poke at it, it's a rational function. That's good. The numerator has degree 12. The denominator has degree 7. And this function is strictly increasing. So the more experiment you do, the better. And to prove that, if you take the derivative with respect to delta, you get a positive. Respect to delta, you get a positive denominator, and you get a numerator that's an 18th-degree polynomial. And how do you show that an 18th-degree polynomial is non-negative over zero to one? You can't reliably even find the roots of a, at least theoretically find the roots of a higher order polynomial. And I want to say that Dan Kluger did some epic work to actually show that this is an increasing function, strictly increasing. This is in our appendix. This is our appendix. Okay, let's go take a look at the classroom data. If we went into that classroom, you know, the classroom sizes, they're not really equispaced uniform. But turns out it doesn't really matter that much. The efficiency curve that you get looks pretty much the same as if they were equi-spaced. And we have some theory to back that up. And then also the efficiency isn't that random. Efficiency isn't that random if you stratify. So, what we'll do is to take two adjacent classroom sizes, toss a coin, one of them gets a plus one, one of them gets a negative one. If you do your randomization that way, then here's, say, the theoretical efficiency. Here's like the worst it ever got. So, you know what the efficiency is going to be at the time you're designing the experiment. This is what happens if you period. This is what happens if you vary the bandwidth. So let's see. H, this is the bandwidth recommended by Inbens and Kapalyanaderman, and that is a triangle kernel. And so here's what happens with their bandwidth. And then if you went with half that bandwidth or more than that bandwidth, you get broadly similar things. So you tend to get, you know, you tend to get more efficiency. You don't know exactly how much more you're going to get, but you get more. More you're going to get, but you get more efficiency. Okay, so more realistic setting, and this is something that is work in progress, the running variable is on x, one, but you don't usually have just one variable. So let's say we have 100 variables or 1,000 variables, and we would have a model that predicted y from, say, all the data that's sitting around. That's sitting around. And then there'd be a black box that maybe predicts how responsive that customer is. So you wouldn't really use a linear regression or a two-line linear regression. You might have to use that when you're designing your experiment, but you don't have to do it once the data are in. You can fit anything you want. And suppose you did that in the past, and your running variable might actually involve all of the data. So f of x becomes your running variable. Actually, comes in your running variable. And so now you can randomize around a threshold defined on your running variable. And then later you're going to re-estimate your black box model, your linear model, your black box model. And I'm pretty sure that a year later, when it's time to re-estimate that your tiebreaker design was actually better for getting you the coefficients of x2, x3, up to x100, will be probably be better for fitting around. Probably be better for fitting a random forest or something in the future. I think the tiebreaker pays off more than just this one very simple situation. Yes. I'm confused. Is the tiebreaker designed just with respect to one of the running variables? Oh, the tiebreaker would be with respect to some ranking. So some function. Oh, okay. The ranking transformation. I forgot about that part. Yeah, no, my slide here. That part. No, no, my slide here, it's actually a little bit it says X1 was incoherent, yeah. And I that was my confusion coming through to you. So, you know, we had what would really be the running variable that you would make your decisions on would really be some ensemble measure. Okay. An expectation. Yeah, which if you like, you can rename to be X1, but. And then, okay, so then, yeah, actually, I think my first slide was more about if you're fitting a linear model. My next one is more about if you're using a black box. And so, yeah, in the tiebreaker, you'd randomize, you might randomize in the borderline. Then it's a little, you know, how do you design for how do you design for random forests? You know, that's not. Not obvious how to get an optimal design for it. Okay, so that's what I prepared. I see I'm like a smidgen minutes early. I have people to thank. So it's been a pleasure to work with Hal Varian and Dan Kluger on this. And a couple of new students are starting to work on it now. I want to thank Google for allowing work to be published. They're pretty secretive about a lot of things and justifiably so, but also they let a few things come out and get published. Come out and get published. I want to thank David Banks for leading this whole enterprise and Beerus for hosting. And I should thank my NSF grant for funding this and thank everybody for joining in and watching. Thank you, Art. That was fascinating. And you made it seem so easy, which astonishes me. You always do.