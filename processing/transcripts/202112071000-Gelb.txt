Okay, so thank you. I'm so happy that to be included in this meeting, and I really wish we could do it in person. And I hope that we'll have that opportunity at some point. I don't think I've ever been at a women in something workshop. So this is a first for me, but I'm just really impressed by all the science that's going on, and I'm happy to be a part of it. And I'm happy to be a part of it. I should make a disclaimer that I come to this field as a little bit of an outsider. This is certainly not where I started, but listening to the comments of others about the love of learning and the love of just trying to do interesting problems, this is the direction that I've gone. So here I am. So I'm very happy to answer any questions as we go along. And I'm really interested in what people have to say. People have to say. So, I want to make sure that I give credit to the two other people that worked on this. Teresa Scarnati was my PhD student in Arizona State. She was my last student there. She actually spent some time at AFRL, but now she's at Qualis Corporation. And Jack Shang was an undergraduate at Dartmouth, and he started this work as an honors thesis, and it really took off. So I'm grateful for them. Okay. Okay. This is not. Is not why won't it? Oh, I see. Okay, so before you guys had the idea of the map, I actually put one up because most people don't know where Dartmouth is. And so if you can see my mouse here, our city isn't listed here. It's Hanover. And it's beautiful. It's right by the Appalachian Trail. So if you come here, you can actually go there. This is the Connecticut River. I can walk to Vermont from my house. House, and it's really lovely. We have beautiful mountains, and maybe we'll be able to have a conference here sometime. I would really enjoy that. So, I wanted to make this more of a general talk because you are who you are. And fundamentally, I just like fundamental research and developing algorithms. So, of course, there are motivations in applications. In applications that we're interested in, but this is really more general, and we haven't really tested it on too many applications yet, but we have some things in mind. So the problem we're considering is a linear inverse problem that's modeled in a typical way. I certainly don't have to go through this with this group, but we define X, Y, and E as random variables, and E is complex additive. Complex additive complex Gaussian noise. The important thing about what we're doing, which is an assumption that we make for the model that, for the numerical algorithm that we do, is that the data are given as multiple measurement vectors. So, this is, we came to this problem actually from a sparse sampling problem where they use multiple measurement vectors and Multiple measurement vectors and joint sparsity, and these kinds of notions, and we adapted it into this sort of Bayesian framework. So, there's a lot of overlap there. But the point is, is that there are actually a lot of real-world problems that you can model in this way. So, for example, you can think about any kind of sequential indirect data collection of a static image. And so, maybe your data are Image and so maybe your data are obstructed in some way, or you don't have some measurements available for you from the sensing equipment or something like that. So you have multiple measurements of the same thing, and you hope to take advantage of this redundancy to create a better recovery, whether it's your image or some other feature that you're trying to get. Okay, so fortunately, you actually had the expert on SAR right before me. So I'm not going to spend too much time with this because Margaret Chini already talked about it, although she went into. Already talked about it, although she went into passive radar, which is a much harder problem than the one I'm talking about here. So, our primary motivation comes from spotlight synthetic aperture radar, and we do assume that we know where we are when we emit the signals. Although we have worked on, especially with Teresa Scarnani, my former PhD student, we have worked on a lot of focusing problems, which comes from when you don't know exactly where you are, but nothing nearly as complicated as Margaret. Are, but nothing nearly as complicated as Margaret was talking about. But the multiple measurements come from the fact that you are at slightly different angles that are close to each other. So you basically assume that you can see the same image multiple times. Anyway, so this is what synthetic aperture radar is, and the idea is that you can synthesize the effect of a large antenna by using more. Of a large antenna by using multiple observations. Anyway, the main idea here is that you want to do a better recovery of some kind of SAR image or some kind of classification of that from this raw data that you collect from SAR. And if you can see, and I honestly don't remember at this point. Remember at this point what kind of recoveries these are. But there's a lot to be gained from developing better numerical algorithms because you want, this is a case where we have all the information we need. There's no occlusion. There's nothing like that. But still, it's really hard to see where the road is, what kind of cars they are. There's all sorts of streaking and speckle and all sorts of different things that are inherent in any kind of In any kind of coherent imaging problem. And so we really have our work cut out for us in this SAR image recovery. So our goal is primarily to do some kind of feature extraction or classification or something like that from raw, from SAR face history data. Okay. So again, there are certainly other experts here about SAR. Here about SAR. So, and I'm not really interested so much in talking about the particular problem, but just so you can see where we get the problem from, I put this slide up here. So there's some signal, this frequency modulated chirp signal is sent off at every some angle theta. Angle theta, and this is the way, and this is the way it's described. And then we receive, we have the emitter and the receiver as the same, and we receive the signal. And that's what we want to use to recover the underlying image or the reflectivity function. So I won't get into the details there because Ms. Mara really. There, because Ms. Maria really described the sort of general sense of all that. But this is the problem itself. But from the problem, you actually get a model. And there are all sorts of shortcuts that are taken in the model. And to make these shortcuts, so we can actually solve the inverse problem, we have to make a bunch of assumptions on the SAR model. So, first of all, you have to assume that this trip rate, this You have to assume that this chirp rate, this frequency modulated chirp rate, is small enough so that you can so that it can be effectively mixed with the wave. And this allows us to get a kind of system that we can actually solve for. Essentially, it gives us Fourier coefficients. Another assumption is that the distance of the flight path to the receiver is assumed to be, well, this is the far-field assumption that it's sufficiently. Is the farfield assumption that it's sufficiently larger than the radius of the ground scene, so you can neglect the wavefront curvature. A third main assumption is that the scene contains only isotropic scatterers, so you don't care what angle you're viewing from, you'll see the same thing. A fourth major assumption that's generally always used is that there are no scatterers outside this main ground scene. And the fifth, And the fifth is that you know the platform location exactly so that you can um so that you can measure this this trip, this round trip time. Okay, so probably most people would agree that the first assumption is fine and the second assumption is fine. The third assumption is complete nonsense. These scatters are not isotropic, which causes some problem. Which causes some problem. The fourth assumption is probably also fine, and the fifth assumption is also not true. And I'm not even talking about the issues that Mario was talking about with passive radar, but even in this case, you don't know the platform location exactly. And as a result, there are phase errors and people typically use autofocusing methods, which Teresa and I have actually worked on, but that's not the point of this talk here. That's not the point of this talk here. We've also worked on how to deal with the anisotropic scatterers, but also not the goal here. So maybe next year we'll talk about that. Okay. But that's the motivation for what we're doing now. Although, like I said, we've really cut everything down to a very fundamental problem that we want to talk about, and then hopefully we'll build back up to the actual application. Back up to the actual application. But one of the things that makes what we do relevant is the Fourier slice theorem, because it means that data such as SAR data, or you can think of ultrasound data or MRI data or anything like that, you can view it as incomplete Fourier data, which makes the modeling much and the numerical programming much easier. Of course, there are constraints on this data, which because you don't have Because you don't have all the acquisition trajectories that you want. So, you know, that so there's also noise, there's significant under sampling. But at the end of the day, we are able to write a model that we're given this data F hat, which is a Fourier transform of the underlying reflectivity function F. Reflectivity function f. It's important to note that f actually is a complex image, not a real image necessarily, although I'm not doing that in my talk today either. Okay. Now, what, what, so what, so this is a tough thing to actually do. So, so we have some, we have some, we have some things in our tool. Some things in our tool kit that we can use. And we have some things about the data that we can exploit. And one of those things about the data is that we have multiple measurements within a small aperture radar window. So that means that maybe there's some kind of redundancy that we could use somehow to get a better reconstruction or recovery of this reflectivity function f. And of course, this is true with all numerical methods. Course, this is true with all numerical methods. You want your method to be efficient, accurate, and robust because you need to be able to, well, because it's a numerical method and you need that to actually be used. And also, it has to be good enough. What you do has to be good enough so that even if the image might look okay in some kind of low order way, you may need some of this information for some kind of downstream processing. So you don't want to have. Stream processing. So, you don't want to have a lot of data information of information loss as you do whatever reconstruction you do. So, I would say, arguably, that the most popular way to recover SAR image is to use any kind of standard technique such as match filtering, back projection, non-uniform F of T, or something like that. And the reason why people do that is because these methods are fast and well understood. Understood, and you know, and you can see a lot from that recently, though. Um, well, I guess not so recently, probably in the last 10 or 15 years, compressive sensing has also become more prevalent. But still, compressive sensing is not a cure-all. There are a lot of issues in compressive sensing, especially with regards to robustness, lack of automation because you have to pick parameters, and computational efficiency because you have to solve iterative methods. Have to solve iterative methods. But regardless of whatever method you use, sort of direct sort of NFFTs or a filter back projection or anything like that, or compressive sensing techniques, iterative methods like that. No matter what, you always have a problem that the assumptions aren't always true. So you're going to get some error. And this model mismatch will get worse as. Model mismatch will get worse as aperture angle increases. So, because of that, you get well, all the sort of bad stuff we see in these image recovery, which is speckle, streaking, excessive or diminished brightness, and the phase error causing the defocusing of the image, which comes from not knowing exactly where the platform is. So, we can, the first idea that I want to talk about here is how we can mitigate. We can mitigate this problem by using multiple measurement vectors. But the other thing that's important in this talk, and I hope I get to it, I tend to be ambitious in the number of slides I give, but after 25 years, maybe I'll start changing that about myself. These techniques that we have are all point estimate solutions, or you can think of them as map estimates. But as everybody knows in this As everybody knows in this group, we really are interested in being able to quantify the uncertainty because we want to be able to make decisions and know more than just a point estimate solution. So the goal then is to actually try to use a Bayesian approach to be able to sample from a posterior as opposed to just getting a point estimate. Okay, I should point out that the method itself actually only requires Actually, it only requires one measurement from which you can generate multiple data sets by adding noise directly to the data. And this is this is personally, I really like this result because it shows the power of numerical analysis and understanding convergence properties within numerical analysis. But it's not the focus of today's talk either. So we'll have to do that again. Okay. So, oh. So, oh, all right. So, here's the forward model. In our SAR example, this F is this forward Fourier transform. The F is the underlying image, F hat are Fourier coefficients, and N is the noise. So, we have these multiple observable measurements. So, we each of these F themselves are typically. F themselves are typically undersampled in some way, like maybe they have bands of frequency information missing or something like that. In this presentation, I'm only considering one source of information, which is Fourier data, but I have some students and postdocs that are working on data fusion problems. So our method isn't really limited. We just haven't gotten around to doing it yet. And like I just said, we actually only need one measurement. Measurement. And then we could, and we can manipulate it in different ways. But for now, we'll consider we have multiple measurements. The rest of this talk, I'm going to do two things. One is I'm going to start with a compressive sensing technique to show how we recover a point estimate solution. And then we show specifically how using multiple measurement vectors, which I'll just use MMB as short hand notation. Use MMB as short hand notation improves the robustness and accuracy of these methods. Because we're interested in quantifying the uncertainty, we'll move to a Bayesian approach. And we actually use what we understand from our compressive sensing approach using multiple measurement vectors to do something to To generate a new prior, which we call a support-informed prior, and then in our examples, we'll show why actually the support-informed prior is a good prior. Oh, actually, I have results in 2D2. I just put those in last night. But by the way, the method is also parallelizable and yeah, and it's pixel-based. So that's not an inherent limitation. Okay. So as every So, as everyone who's familiar with compressive sensing knows, the whole point of compressive sensing is that you use information that the underlying signal is either sparse itself or it's sparse in some domain, like the gradient domain or the edge domain or even the wavelet domain or something. It doesn't really matter. But you're able to get at that sparse domain information from the data that you actually have. The data that you actually have. So, just to make sure we have all the definitions. So, a vector is sparse if its support is less than some finite number S. Oh, this should say, this should be a lowercase n here. And yeah, so this zero is the L0 pseudonorm. Now, the way compressive sensing works. Now, the way compressive sensing works, right, for any given observation is that you can recover some approximation to X, so point estimate to X, by using this form here, right? This nice convex optimization form here. And the key, the thing is, is that, so first of all, so L is. So, first of all, so L is something that puts you in the sparse domain, and you're able to make this kind of approximation. And as well, no, now people use the one norm as a surrogate for the zero norm, since we otherwise we would have an intractable problem. So, the tricky part comes in this parameter lambda, which heuristically makes sense if you have good data, right? You have good data, right? You would just use a sort of a least square solution, and you wouldn't use this regularization term to, you wouldn't need this regularization term to sort of get a better solution. But as the data, is the fidelity of the data gets worse, if you get, if you have more and more noise in the problem, so that's why sometimes it's called denoising. You have, you want to rely on what you believe the underlying solution. The underlying solution is. So, this lambda is essentially a way to balance these two terms. So, a smaller lambda means you have more confidence in the data, while a larger lambda means you have more confidence in your prior belief of the solution. So, all of this is well understood. But there are some major flaws in this method, mainly in the thing. Mainly, and the thing practitioners don't like is that there's a real lack of robustness in parameter selection, even within the same data sources. And this prevents automation of these methods for when it comes to real problems. The other thing, though, that's probably not talked about as much is that any, this sort of standard sparsity assumption, because it's a norm, it's a global assumption. It affects the whole solution. The whole solution. So it doesn't actually capture any local features in the sparse domain. What is probably better to do, what makes sense to do, is to make this lambda spatially varying. So it should penalize only the sparse regions where the solution is truly sparse, but where the solution is not sparse in the sparse domain, it should let that value through. That would be more accurate. Value through that would be more accurate. And people have known this. And a lot, and probably about 10 years ago, a number of iterative reweighting methods were developed. So basically what that is, so if this is a one-dimensional problem, W would be a diagonal matrix that weights the strength of the non-zero components in the sparse domain so that you actually So, that you actually pass those through and in this optimization problem. However, we played a lot with these and they also have issues with robustness for some obvious reasons. If you get the wrong weight, then you'll end up reinforcing that throughout the optimization and you'll end up with an actual worse result than you started with. It will be reinforced. Reinforced. And second, this is horribly inefficient because you have to calculate W, this weight matrix after each iteration. So, this is where we got involved. And we thought, okay, well, if we actually have multiple measurements, maybe we can pre-process and design a weight in advance that actually figures out where the support is in the sparse domain ahead of time. So then we can use. So, then we can use this weight more effectively. So, that was a very simple idea that we had. So, and that's it. So, this is work. So, I've done this a few times separately with Teresa and then also I have a colleague Ben Adcock, Guo Hui Song, and Ben Sudan Yi Sui. We also worked on this in a couple of different contexts. But anyway, the idea is that you can use additional. That you can use additional information from these multiple measurement vectors by employing the notion of joint sparsity. So, what does that mean? That means across the signal, the underlying signal across the different data collections should look the same. So, they should be sparse. All the data collections should point to the same support or the same sparsity. It's two sides of the same coin. Sides of the same coin, no matter which you use, as long as the underlying signal is the same. Okay, so we have to define this idea of what joint sparsity is. So it's basically that if you have multiple measurement vectors, so remember this is in the sparse domain. So not the signal itself has to be sparse, but there has to be some sparse domain of the signal. Excuse me. So it's S joint sparse. If you take the union that there's If you take the union that there supports and it's still bounded by this S, which is bounded by the number of components in the vector. So if you have J equals one, you're back to this original framework of just a sort of standard sparsity assumption. So now we want to exploit this idea of joint sparsity. So yeah, so what we came up with is this idea of how do we get at this joint sparsity? How do we get at this joint sparsity? And we came up with this idea of looking at the variance of these measurements in the sparse domain, which I'm going to talk about in a minute. Right. Actually, I'm going to talk about it now. So now, so this is the idea. We want to develop this weighting matrix W so we can rewrite this convex optimization problem. Convex optimization problem. So instead of just having the sparse transform, we hit it with this weighting matrix so that we allow the anytime there's support in the sparse domain, we don't clamp that down in the solution. We don't penalize there. So we only penalize where the problem is truly sparse. So these weights are designed to essentially be inversely proportional. Essentially, it be inversely proportional to the variance, right? Because if you have a large variance in the sparse domain, then that means that likely there's going to be some support there. But if you have a small variance in the sparse domain, that means likely that's a region where it's truly sparse. So we kind of, we just, we just look at that, and then we just, and that's how we effectively determine what this weight is. What this weight is. So, a big advantage of that is that you don't have to calculate these weights iteratively. You can do it once and then it's offline and then you employ it every time. And second is that it's more robust because since it's done as a preprocessor, you're not reinforcing bad solutions. Okay, so we already talked about what the definition of joint sparsity is, and now I'm going to say. Definition of joint sparsity is, and now I'm going to say why we how we can use the variance for calculating these weights. Okay, so remember, these are spatial entries, so you have data collections which go from one to j, and then you have the spatial entries for this vector that go from one to n. So we look at the we calculate the variance of across the Across the data sets, J in the sparse domain. And as I said before, if the variance is small, then it looks like you're truly sparse. And if it's large, it looks like you're in the region of support. So you construct this weight to be inversely proportional to the variance. So that means that you're not penalizing the You're not penalizing the regularization term. So you have smaller penalties wherever there's really support. So, and you only use this heavy penalty when it's truly sparse. I want to make an important point here because I know that this is a crowd that knows the Bayesian framework really well. And it's that the term support and sparsity are not really appropriate, but that's just sort of how they have. Just sort of how they have been discussed in the literature. Really, they're just colloquialisms in this approach. Really, what we're talking about is separation of scales. So, sparsity just means really small values in some underlying domain, like a gradient domain or an edge domain, whereas support means larger scales. So, that I just want to be careful because that's I just want to be careful because that's because I know that when we talk about priors later in the Bayesian framework, people will say, well, you really shouldn't use a sparse, you shouldn't use the Laplace prior for a sparse prior, but it's not really sparse. It's just really small values. Okay. So this is a picture to show how well it works, right? So if you use what we call the variance-based joint sparsity, The variance-based joint sparsity, and you can see that this you get a very good error here. And we should, I should say also that in these numerical experiments, we had all sorts of noise and missing data and different data sets and that kind of thing. So it works really well. The other thing I want to point out is that in the sort of standard L1 regularization, well, hence it's called L1 regularization or compressive sensing, you need an L1 regularization. Regularization. But once you use this weight, right, you're not saying that you're doing your regularization is not based on sparsity. Your regularization is based on something that you want to be small scales. So this whole idea of the P should be one or less than one or all sorts of things people do, it's no longer relevant. You just want some small value here in regions that the data values are supposed to be. That the data values are supposed to be small. So you can choose. So that's really nice because then that makes your method much more computationally efficient because you can use a two norm here instead of a one norm. So this is a picture of how it works, right? So we have different data here. And in the sparse domain or in the edge domain, you can see that there's much more variance, right? More variance, right, wherever there's an edge. Oh, yeah, here's the picture of that, and that corresponds to these weights. So you put more weights in sparse regions, and you put much less weight to let the support of this sparse operation go through. So it's a very simple idea. Right. Oh, yeah. I wanted to talk about one other thing. Well, anyway, so this is the method. So F is a complex signal, right? And it has a sparse vector G, sparse domain vector G. We acquire these data sets. We are able to approximate the sparse domain vector G from the data that we're given directly. The data that we're given directly. This is important because there's no information less information loss. If you had to do an initial reconstruction of this vector f or the signal f, then you would have more information loss in this construction of G. Then we determine the spatial varying matrix, as W, as I pointed out, by using the variance of these J vectors G, which is an approximate. G, which is an approximation to the sparse domain vector G. Then the weighted L L the weighted L P regularization, L1 or L2, is then solved by solving this convex optimization problem. Now, this LQ here is not the same as this BJ, but it's another approximation of the same sparse domain. So this one has to do with the under. One has to do with the underlying image, right? F or signal. Whereas in this case, we get the approximation to the sparse domain using the actual data that we collect. So there's a lot. So you don't have extra information loss in this way. All right. So that's why I said I wanted to make a different, I want to make a quick detour. Tour about this comment because this is important to me. So if you have to compute the signal originally to compute the edge vector, then you're going to have loss of information. This is especially true if you're starting from Fourier data that's noisy because you have to get rid of the Gibbs and you have to get rid of the noise and you have to get rid of all these other things. So instead, we actually use the Fourier samples directly. Use the Fourier samples directly to compute the approximation of the edge values. And this is done by way of a band pass filter, right, which I write, which I wrote here. So this is in the Fourier domain. And this gives us this reconstruction. So that was really, that was a nice moment for me because it brought up this work that I did a long time ago with Eitan Tadmor of why this is useful. Moreover, you actually have provable results. Have provable results for how this converges to the edge domain. And you can use that when you go to determine your variance. So this is a whole nother story or sometimes it's a horse of a different color, but it's important to know that you can really get at this edge information from the data that you have. Okay, fine. So now let's see how far I'm off. Let's see how far I'm off. Okay, I'm looking at the time. So, why do we want to use the empirical? Why do we want to use the Bayesian framework? It's true that we can improve the robustness and efficacy of compressive sensing algorithms by using this approach. However, no matter what we do, we're always only going to have a point estimate and we won't be able to quantify the uncertainty. And this disadvantage is non-trivial because in many applications. Because in many applications, especially if you're talking about SAR or the military or something, they want to actually know how reliable the recovery is. And they know that their data is not always that great. So, you know, they want to actually know, they want to be able to quantify the uncertainty. Moreover, even within the same application, the compressive sensing algorithms don't always do a good job. Of sensing algorithms don't always do a good job of tuning this regularization parameter. And small changes can lead to drastically different outcomes. So we're interested in having some kind of uncertainty quantification for our method. Okay, so we're back to the original problem where we consider this forward model, but instead of trying to recover a point estimate, now we want to be able to sample. Um, we want to be able to sample from a posterior. So, again, we have a particular observation given by this data point y, and we're going to consider again that we have multiple measurements. Okay, so we're going to use we're going to use the Bayesian approach, right? So, everyone here is familiar with Bayes' theorem. Um, so since the noise is additive complex Gaussian, we we can we have a we can we we have a um the the likelihood um density and then this is the and then this is the prior so um and oh yeah and we don't worry about uh the the evidence because um you don't have to calculate it when you when you try to do the sampling um so so it's just not normalized when when we do the sampling which is which is fine okay so the right so the the whole meat of every Right, so the whole meat of everything comes in this calculation of the prior. And the rest of this talk is what we call the support form prior, which is an extension of what we did with these weighting matrices in compressive sensing to find the support, which will give us a better prior. Okay. So, Yeah, so the likelihood, we already said what it is, and this is the prior. So, of course, there's a lot of research on how one works, one uses a hyper prior, but we'll just use this hyper prior that you can determine empirically. And we borrowed this idea from something that these three authors, Sanders, Plate, and Skiel did. Did for compressive sounds like for Lambda. So we just use this as the hyper prior here. Yeah, that's not really important to this talk either. Okay, so then once we have this hyper prior figured out, now we just need to know how to calculate this value, this prior here. Yeah, I just. I just because I'm using sigma squared here, then I'm going to take the sigma squared out in the denominator. I just wanted to make that point. Okay, so that's the goal of the rest of this talk. This is the work that I did with Teresa and Jack Shang. And the main goal is to do the support inform prior. So we want to make two assumptions. One is that the prior enforces the small scale or what we colloquially call sparsity. Colloquially call sparsity and some transformation of X, and two, that we have multiple observations of Y for each X. So, and it's possible, of course, to relax this assumption, but I'm not, but I won't do that in this talk. Okay. Now, in the MAP estimate or in the compressive sensing technique, people basically use the Laplace prior, which is this L1 prior, which we discussed earlier. Which we discussed earlier extensively when we talked about compressive sensing. But again, as the issue is also compressive sensing, using a prior like this doesn't take into account anything about the locations of the support in the sparse domain. So we had this idea that the same way we used a weighted L1 regularization in compressive sensing, we can again exploit. Exploit the joint sparsity to make this prior spatially adaptive. So it's a better prior so that you actually use the locations of support. So it's empirical, right? It's an empirical Brayson framework, but that's what we do. So recall, this is the definition of joint sparsity. And so we, this is what we do again. We use the variance. And from this variance, we develop a binary mask. Binary mask. So it's a little bit different than the weight we used before, which we calculated as one over the variance. But that's just to allow a broader space to explore for our sampling method. But it's comparable. So what we do is we use, we generate this binary mask wherever we decide that this New, I guess that's a new is larger than a threshold. And then we don't enforce the prior whenever this variance is large, right? Because that would be the wrong prior, because this prior says that you're supposed to be small in the sparse domain. But if you actually have a place of support, that would be the wrong prior. So we don't, so we make a space. We don't so we make a spatially adaptive prior so that it's appropriate for where you are in the domain. So, here's an example, right, of some signal. And we borrowed this from, shoot, I just forgot his name, a paper. It'll come to me later. So we just use the same example that he used. So let's look at these sort of interesting points, right? So this would be the sparse domain. Sparse domain of this, uh, well, our calculation of the sparse domain, and then this would be the variance, so this would be the weight that we would get if we were to use regular compressive sensing, and this is the mask we recover. So, wherever there's support, right, then you use, um, you have a zero mask, so you don't enforce that prior. And wherever there's no support. Where there's no support in the sparse domain, right? You want to enforce that prior. So that's the idea. Okay. So that's what we do, right? So we approximate this, the, this, I don't know what this, what, what I wrote here, but the idea is that we, so we do the same thing. We come up, we compute the variance, we calculate this hyper prior. calculate this hyper prior or this hyper parameter for the hype for the for the hyper prior so that gives us the the um this this um support and form prior here and you can use p equals one or two okay so now that we have that we have we can choose different posteriors i'm almost done so i just call this the laplace prior posterior the support informed l1 a gaussian like prior it's not um it's It's not really truly this sort of standard Gaussian prior because this matrix L is not SBD, but it has to do with transforming the problem into the sparse domain. And then the support-informed L2 prior, which is related to sparse Bayesian learning, if people are interested in making those comparisons. Okay, then we use MCMC, we use this Gaussian proposal distribution, and so that gave us this. And so that gave us a symmetric proposal that gave us this acceptance ratio here. And then we use sort of standard burn and rates to what we found in the literature. Okay, so here's the results. You can see in the sparse signal example that when you compare our new, we're in blue and the standards are in red. So the idea is if you use a sort of adaptive support inform prior, you get a much Support and form prior, you get a much, these are the confidence intervals, you get a much tighter confidence interval here. And these are, as you change the SNR, right, our results. So we're again in blue with the two is the two norm is dotted and the one norm is solid. And this on the left is whenever we're at what's a truly sparse region or a Truly sparse region or a low-value region, and this is where the region is a presumed region of support. So, I would say that this is actually a really important graph here because it's telling you that this the standard prior is not the right prior to use wherever there's support. So, we actually get the really significant improved results in those regions. So, this is that function that we had here. And again, you see comparable behavior. It's not much. It's not much difference when you have this sort of nice smooth region in what you use, but it's a pretty big difference wherever there's actual support in the sparse domain. These are what the final sort of distributions look like at these points. So you can argue that, so we're the blue and the black, that, so for example, this is position one here. So, for example, this is position one here. You can see that it's sort of more centered around the mean here. It's less important as you go to this fourth position, which is here up here. Okay. And then, oh, this is the acceptance ratios, which are sort of as expected. These are the autocorrelation plots. Again, we're in blue, so you can see that the support informed. Support informed looks like it's converging better for its appropriate case in the L1 and L2. And it also gives an argument that maybe now that we have this ability, we can make the actual prior itself adaptive. So we can use L2 in some regions in L1. These are trace plots. So I'm almost out of time. Oh, yeah. I want to show. Yeah, so this is comparable to the other picture we had before with the relative and Relative and absolute errors. So, compared to the SNR level, we're again in blue. And then these are the errors with respect to the position that we're at. So, again, the most sort of clear incidence is where there's true support that our method definitely outperforms the other, and it's no worse where they basically look the same underneath. This is the 2D results. Results. So, this would be the ground truth. And if you use sort of standard L1 prior, I hope you can see all those extra dots in a year. And then this is our method. These are hot off the press. We just, my student just came, did them yesterday. You can see that we have much better brightness here for ours. And this is that same plot, except these are the standard deviations of the posterior. So you can see again. So you can see again, like this is expected, right? Because these are where there's brightness here. So the standard deviation should be higher in those places. And in some ways, it's a way to detect where the edges are by using this posterior. Okay, so yeah, so just a couple of concluding remarks. What we did here is we introduced a new support inform prior. It's an empirical Bayesian inference method. The nice thing about our method. The nice thing about our method is we were really able to capitalize on the provable convergence properties of edge detection methods. So we feel confident that what we're doing is mathematically sound. And our method compares favorably to sort of these more standard priors, especially in regions where you're not enforcing a prior that doesn't seem to be the correct one. Seem to be the correct one. We also seem to have tighter confidence intervals. So, in some sense, you can compare our method to a sparse Bayesian learning because that's also support-informed in some way, but it's typically limited to sparse signal and not images with sparsity in some other domain. Also, it's not very computationally efficient in high dimensions. In high dimensions. So, our method is pixel-based, so it's parallelizable. And you can also modify it if you want to include like neighboring points and use correlations there. And then, as I said before, this work is pretty much in its infancy stages. And what we want to see is like, well, we have ways of understanding what the underlying image looks like a little bit more. Underlying image looks like a little bit more from this. So we can, so we can localize the prior, like using L2 or L1 or something like that. We can even, this is helpful also because, oh, I didn't write that here. Okay, but I'll get to that in a minute. You can, we didn't do this. We didn't do this for different sources of data, but our method is not limited by that. So we can use different sources of data. Oh, I thought I wrote this down here, but we can also. I thought I wrote this down here, but we can also use our method to help practitioners decide where they want to put more emphasis because they can see that there's more uncertainty in certain regions based on it. Anyway, that's all I have. I want to thank you very much for your attention and again, for inviting me.