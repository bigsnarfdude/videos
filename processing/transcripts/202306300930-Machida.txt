Okay, I would like to thank the organizers for inviting me, and I need to apologize for not being there. But when I accepted the invitation, there wasn't a baby in my life, so the life was easier in organizing travels, research visits, and so on. So anyway, let's start talking about the talk. And today I'm going to talk about how to combine more or less traditional Combine more or less traditional methods for reconstructions in post-problems and inverse problems in particular and double-driven methods. So let's start from our motivation. We start giving an operator equation, F u equal to Y. So the idea we have some knowledge on the operator F, we have Y, in particular we have noisy measurements, so we have a perturbation Y, why that? A perturbation y, y delta, and from these noise measurements, we want to reconstruct, we want to find an approximation of u, the quantity u. In particular, we will work in deterministic setting, so it means that we have a control of the difference, of the norm of the difference between y and y delta in terms of the noise level. Okay, so our goal is to approximate you. The setting that we will consider, especially in the first part of Especially in the first part of the talk, is the following one. So we will assume that the operator F acts between Hilbert spaces. And in the first part of the talk, we will assume that F is a linear operator. Then in the second part instead, we will need to focus on a linear operator and also on injective operators. But I will explain later what we need. Okay, so we know that. So, we know that when we want to solve the inverse problem, in particular, when we want to identify you, I would say that until the last 10 years, more or less, there were traditional approaches. Let's call it this way. So, the methods applied in order to reconstruct the U. And basically, these methods were based on the fact that in the reconstruction procedure, they use the explicit knowledge of the operator. Use the explicit knowledge of the operator F, starting from, of course, the measurements y delta. So these are the so-called physics-driven reconstructions, more or less. So we start from by y delta, we use in some way the explicit knowledge of the operator F, and we want to reconstruct U. In the last years, we know that new approaches are appearing, and in particular, they are based on data-driven approaches, as we saw in many talks during this week. During this week, and the idea is the following: one: so we assume that we don't know explicitly the operator f, or better, we have only partial information on the operator f, but instead we have some extract data, so some pairs ui and yi, which are solutions of the operator equation, so f ui is equal to yi, and from these informations and the measurements, y delta, we want to reconstruct u. So, these are the so-called data-driven reconstructions. Are the so-called data-driven reconstruction methods? Our idea is more or less to try to combine these two words, so using traditional approaches and rephrasing traditional approaches in a data-driven setting. So the question is, how can we use this space UI and YI in the reconstruction procedure, combining these two methods? So in this case, we start thinking about We start thinking about this framework in 2018, more or less. And the starting idea was to, okay, let's start from a traditional approach. In particular, we consider the Landweber iteration, the well-known and well-established Land Weber iteration for solving Ilpos problem. And let's try to modify the Landweber iteration in order to get a data-driven. Get a data-driven land weber iteration. And this is the first part of the talk. In the second part, instead, we move towards the more or less machine learning and deep learning philosophy. That is, we wanted to use more or less only the training phase available. And we wanted to consider the case where the operator F. Say the case where the operator f is not explicitly known or is only partially known. Okay, and this is the second part of the talk. So let's start from the first part. In order to present more or less the data-driven land web iteration, I will start from the beginning. So recalling you what the land weeration is. In particular, it is an iterative method. So you can see here. So, you can see here the iteration. And for in particular for implementing the Land-Lebre iteration, we need to assume that the operator F has a continuous Frechet derivative. And here you see the Frechet derivative and the adjoint. This is the adjoint of the Frechet derivative of F. Of course, since we have an iterative method, we need to give, to assign an initial guess, U0, in order to start. Ute zero in order to start the iteration. I don't give you the details regarding the convergence of this method and so on. We will see something later regarding the data-driven iterative iteration. But for the moment, the important thing is to say that in order to act as a regularization method, the land vibration needs a stopping rule, in particular when you consider. Rule in particular when you consider noisy measurements y delta. So you cannot get exactly the convergence of the Land Weber iteration, but you can only find an approximation of the solution using the so-called discrepancy principle or Morodov principle, that is, we stop the iteration after a certain number of iterations. Let's in this case, a case star, let's indicate it in this way. In this way, in particular, when we have that the residual is less or equal than a certain level of the noise, okay? So, after when we reach this level or we are below this level, we stop the iteration. Okay, so Scherzer proposed in 1998 modification on the landweb reliteration. Modification of the Landweber iteration, the so-called iteratively regularized Landeweber iteration, adding to the classical Landweber iteration a dumping term. In this case, we have this term here, U0, which is more or less en a priori information that we have on the solution that we want to reconstruct. And sometimes it happens in the inverse problem that we have some information on what we want to reconstruct. What we want to reconstruct. So we can add in the land of every iteration this term. And of course, with U0, we denote always the initial guess in our setting. Well, the addition of this dumping term in the algorithm stabilizes the algorithm in the sense that one can prove that this iteration converges to a solution of the operator equation F u equal to y delta, which is the close. equal to y delta, which is the closest to yield zero. And okay, so this is a good modification of what of the Landweber iteration. Just before going on, I want to say that in equivalent form we can rewrite this iteration in this form. Okay, so now the question is: let's start from this point. start from this point we want to use in some way the expert data that we have in that we have in hands so the pairs ui and yi so we want to try to utilize these training pairs in the reconstruction procedure how can we do that the first idea basically based on the iteratively regularized and web iteration so the idea proposed by shelf is to add many many dumping terms Many dumping terms, as in this case, containing the UI. So we can imagine that we have a lot of information on our solution, so we can add this dumping term inside the land-variable interactions. This is an idea, but we have two drawbacks, more or less. The first one is the fact that we are completely ignoring, in our case, the knowledge of the Y. And I would say that this is the main issue in this case. Main issue in this case. And the second drawback is the fact that we can expect that an iterative method like this one will converge to a solution which is the closest to the mean of UI. So in this case, we don't have, in some sense, a good result. Okay, so in order to insert also this yi inside the iteration, Inside the iteration, we thought, in particular, following the path of the machine learning and deep learning philosophy, to use a black box strategy that is to identify a non-linear operator, a hex, which maps each UI into the YI and consider the following iteration. Okay, so in this case, we are using exactly the expected data that we have in hands. Of course, requiring some conditions, in particular on the differentiability of the operator A, we can rewrite this iteration in the explicit form. Of course, as done for the Langweber iteration, since we are dealing with an iterative method, we have to add to this iteration also the discrepancy principle for the operator F. Okay, so what about? Okay, so what about the analysis of this iterative method, which we call the data-driven iteratively regularized random level iteration? Okay, so for the analysis, I will give you only some details. So the most important conditions that we need to require in order to do the analysis. Of course, we have to require that both the operators F and A should be controlled. Should be continuous frechet differentiable, and we need to have a control on the Lipschitz constant for both the operators. And as usual in the Land Weber methods, we need to require the tangential comp condition in order to prove the convergence of the methods. In the case where we have the exact measurements, so measurements without noise. Measurements without noise. This is the following one. So it's a sort of, we can think to the tangential condition as a sort of quasi-local convexity. More or less, it's not exactly the quasi-local convexity, but more or less we can think about that in this way. And moreover, we require that A at is such that it doesn't contain information regarding the quantity that we want to reconstruct. That we want to reconstruct, otherwise, the method will be, of course, straightforward because we have the information inside A, you know, otherwise. Okay, using all this information, we can also prove that starting from a sufficiently close to U dagger with our iteration UK, then we have that also A at UK, so in the range, we are closest, we are close to. We are close to y delta, which is controlled by a constant depending on the time also the leach is constant of the operator A. Okay, so in this setting and adding some technical assumptions in particular on the lambda k. So the lambda k are just to come back. So the coefficients, the sequence related to the dumping term. The dumping term, so adding some additional information is lambda k, which should be controlled by the residual of the operator f, we can prove that the iteration is a monotone. So we have a monotonicity result. And in particular, from that, we can find that when the noise is zero, so we don't have noise in the measurements, we can prove a strong convergence theorem. Theorem. And so we converge, we can prove that the iteration will converge to a solution of the operator equation f u equals y. Instead, when delta is greater than zero, as I told you when I presented the land of abilitation, we cannot expect the convergence, but of course we can expect some stability results. So we can find an approximation using the discrepancy principle of a solution of the operator equation. Of a solution of the operator equation u equal to y. Okay, well, this is these are more or less the theoretical results that we have about this iteration. And just to show you some results, some numerical experiments that we did. So, we used for first of all, we used the as database the MNIST data set. So, it's really famous. So, I don't need to explain. So I don't need to explain much about these data sets. And in our setting, so we consider for the moment the linear case, both for the operator F and both for A. So we assume that F is the Raman transform, and there were some talks during this week related to the Raman transform. During this week, related to the Ranon transform. So I will skip the part related to the Ranon transform. And for A, we assume that it is linear. So we can construct in our, for numerical purposes, we are able to construct only the, we are able to consider only the case where A h is linear. Okay, so I will try to explain what we have here. So in order to create a hat, we consider in this case We consider in this case 150 images of the data, the MNIST data set. So they are images similar to these ones, more or less, of the end-written digits. And so in our setting, this is the numerical experiment. So we want to reconstruct this picture, which is not contained in the 150 images which we utilize to create a X. To create a x, we start from the sinogram, so these are the y delta, so the measurements that we have that we have, and from these measurements, and using our iterations, so the latter even iteratively regularized land level iteration, we want to find the reconstruction. So, here you can see comparison between our method, the classical iteratively regularized land of every Iteratively regularized land web iteration, the land web reiteration, and this is the results obtained by the application and the feeder back projection directly. So in this case, the result is quite good. These are just to show you the resilience. And you see that in our case, our method could converge with less iterations with respect to the others. Okay, the other methods. And moreover, we tried also to. And moreover, we tried also to consider the case of partial data. So, the setting in order to create a hat is exactly the same as before. We didn't change anything in this case, but now we want to reconstruct this picture, starting from partial data. So, it means that we have only some information on the cyanogram. So, you see that in this case, we cut some of the strips in this cyanogram. And from this information, we want to reconstruct. From this information, we want to reconstruct this picture. Okay, so you see here again the comparisons between the different methods. So, our method, the iteratively regularized Land Veber iteration, the classical Land Veber iteration, the filter vector projection. In this case, it works quite well, I would say, with you see that in this case, the method converges sufficiently fast, I would say. I would say. Okay, so now the question is: in this case, we need the explicit expression of the operator F. What happens, for example, when we don't have the explicit knowledge of the operator F or we have only some information on the operator F, but we have some expert data in available expert data. Okay, so we consider these in this case. So, we consider these in this case these settings. So, now for this analysis, more or less, we need to require, and it will be clear later the reason why. But in this case, we are using as operator a linear operator. So, in this case, we have to assume that we have a linear operator. So, the main goal in this case is to find a stable algorithm which is able to find an approximation of u or u. An approximation of U or U in case we are working with noiseless measurements when we don't have the explicit knowledge of the operator L, but we have only training pairs, so the expert data, and the noisy measurements, Y delta. So in particular, so we are working exactly in the data defense reconstruction setting. In particular, the question that we want to address is: okay, is there a regularization method? Is there a regularization method which is capable of learning a linear operator? There is a spoiler. The answer is yes, there is. And there, of course, some suitable assumptions on the operator L that we are considering. In particular, we have to require that the operator equation has a unique solution. So we have to require the injectivity of the operator L. And for example, a regularization method, which is able to do that. Method which is able to do that is the projection method onto finite dimensional spaces. And I will explain the two possibilities. Okay, so just to fix the terminology and the notation, we will call with UI the training images and the YI the training data. So just to explain, because we have to consider two different projections method. With the UN and YN, we instead And Yn, we instead denote the spanning set created by the UI and YI respectively. And since we have to deal with projection operators, in particular orthogonal projection operators, we denote by PUN the projection, the orthogonal projection onto UN and with PyN the orthogonal projection onto YN. And we denote with U data the solution of the operator equation U equal to Y. Operator equation u equal to y. Okay, so what are the main assumptions? I told you that we are able to prove that there is a regularization method which is able to learn a linear operator, but under suitable assumptions. So in this slide, we will see the assumptions that we need on the underlying operator on the operator L and on the data that we have. Regarding the operator, Regarding the operator, we considered again the setting where we are working on Ilpert spaces. And we assume that the operator L is bounded, linear, and injective. But of course, since we want to deal with Ilpos problems, we assume that L minus one is a bounded. Regarding the data, we need to assume some independence for the data. For the data and some density results. So, in particular, we need to require that the data that we have are linearly independent, they are uniformly bounded, and also we need to require some sequentiality condition in the sense that we start from a training set, in this case the PASC UI and YI, some of them, and then we can add to this set other. These sets other training pays, but we cannot remove from these starting sets and we cannot remove the pace, okay. Some of the pairs that we have or we have considered. In particular, we need to require that there are this relation between the training images sets and the training data sets in particular. So, this is the reason why we can only add without removing and And regarding the also, since we want to deal with the projection operators, this is the classical would say condition. We need to require some density results. So the training image is basis, the UN are densing mu. Some consequences of these assumptions. We have that also the training data. Have that also the training data yi are linearly independent because we are requiring the injectivity of the operator F L and also the linearity, and they are uniformly bounded as well as the UI. And also, we have some density results for the closure of the range of L. Okay, so now we say that we want to deal with the projection operators. What is the philosophy of the projection operator? Is to the philosophy to approximate the solution of the problem, in our case, that we denoted by Eudaga, using the minimum non-solution of finite dimensional problems. In the literature, you can find two different ways in order to do that. The first one is the so-called least squares projection method. So, in this case, you project your problem. project your problem onto the finite dimensional onto finite dimensional space of u. So you consider the operator equation L P and U equal to Y. On the other hand, you have the so-called dual least squares projection method, where in this case you projected a problem onto a finite dimensional space of y, so on the image space. Okay, well these are the two methods that Well, these are the two methods that we have in the literature. Now, the question is: how can we modify more or less these two methods in order to consider the expert data that we have? So, in order to insert in these methods the data that we have. Okay, so the idea is to choose as a orthogonal projection, not a generic orthogonal projection, orthogonal projection, but a specific orthogonal. Projection, but a specific orthogonal projection that is the as we consider the orthogonal projection onto the finite dimensional space created by the training images. So we project onto UN. For the dualist squares projection method, we project the problem onto the finite dimensional space created by the training data, so onto Yn. In both the cases, we can prove that there is an explicit expression for the minimum non-solution. In particular, we have that u nu as exactly this expression in terms of the data y. But for the least squares projection method, as was proved by Simon in the 1980s, there is a drawback. The fact that without additional assumptions, Without additional assumptions on the data and the operator, usually you don't get the convergence of the minimum non-solution to the solution you are searching for. And on the other hand, instead, for the dual least squares projection method, again, we have an explicit expression for the minimal non-solution, which is related to the minimal non-solution of the least squares projection method. Squares projection method. And how are they related? In particular, in this way. So we have that the minimum absolution of the dual least squares projection method is linked to the minimum absolution of the least squares projection method, projecting the problem onto the range of the adjoint operator. And you see here the main drawback of this method. Drawback of this method: the fact that in order to implement this method, we need some information on the adjoint operator. And I will explain in a few slides what we need in particular. So, in this case, we have this straightforward in the literature to prove that the minimum solution of this problem, without requiring other assumptions, is converging to the solution we are searching for. Okay, so let's. Searching for. Okay, so let's examine these two methods because at this point, one would complain, saying, okay, but you said your goal is to find something which is where you don't have to need the explicit knowledge of the operator L. So you want to use only the expert data. But in this case, you see that we have L minus one, so you don't have the knowledge of L. So in particular, you don't have the knowledge of L minus one. Knowledge of L minus one. And also in this case, you see that you have the adjoint of the operator L. So how can you overcome this issue? So how can you write the minimum non-solution in terms of the expert data only? Okay, so let's start from the least squares projection method. In particular, one, the two main goals are to give an explicit expression, as I told you, of the minimum solution. You of the minimum norm solution in terms of the training pairs only. And the other goal is to, of course, find since we do not have a guarantee in this case that the minimum norm solution is converging to the solution we are searching for, we want to find conditions in order to guarantee the convergence of U and U to U dagger. In this case, I will give you only details on how to rewrite more or less U and U in terms of the training periods only because more or less The training periods only because more or less the other part regarding the convergence, we use the classical results. So we rewrite more or less what is known in the literature regarding the convergence in order to guarantee the weak convergence and the strong convergence of this method using the data-driven settings. So nothing new, more or less. Okay, so in our case, how to rewrite this minimum non-solution in terms of the training pairs only? The key ingredient, more or less, is a The key ingredient, more or less, is a simple ingredient, the Gram-Schmidt autonomalization procedure. We start from the Yi, so we apply the Gram-Schmidt autonomalization procedure, we get an autonomous basis for our space Yn, and then we identify the U bar I, which are the solutions of the operator equation corresponding to the data Y bar I. Of course, at this point, one could complain again, saying, okay, but you can. Again, saying okay, but you cannot use L. But in this case, it's not necessary the knowledge of L, because using the Gram-Schmidt or to normalization procedure and requiring, of course, the injectivity of the operator L, we can get a recursive formula for getting the U bar i in terms of the Y bar I and the U bar K. Okay, so this is the recursive formula that we can use. We can use. So once we have the war i, the u bar i, we can rewrite the minimal non-solution in terms is simple to check this fact that one can rewrite this term here using only the training pairs on the training pairs. So, in particular, the u bar i and the y bar i. Okay, in case where in this case we In this case, we did all the calculations with the precise measurements y. When we have noise measurements, we can repeat exactly the same procedure. It's exactly the same, substituting y with y delta. So nothing happened in this case. What about the dualist squares projection method? In this case, I recall you that the expression of the minimum non-solution is this one. As I told you, we have in this case. In this case, a drawback, the fact that an issue, the fact that we need some information on the adjoint operator. And in particular, this means from the point of view of the data that we need, that we need to require, we need the following training pairs. So the training pairs related to the operator equations, the operator equation, which involves the adjoint operator of L. Okay. Okay, so we need the training pairs DI and YI. So we cannot use the, in this case, the UI. So in this case, we cannot do that. And so this is the main drawback because it means that you should have access to some data of the adjoint operator. Well, in any case, if you have these pairs, okay, then you can rewrite as we did before the training. The training, you can rewrite the minimum absolution in terms of the training pairs only and using again the Gramschmid orthonormalization procedure. So in this case, you autonormalize again the Yi, you get the corresponding V bar I, and then you can see that this problem here is equivalent to this one. So more or less to find the solution of the minimum. The solution or the minimum solution, the minimum non-solution of this problem here. Okay, so in the case of noisy measurements, you can do exactly the same thing and it's exactly the same in the calculation. Now, we saw that both the methods have some drawbacks. So the least squares projection method has some issue regarding the convergence of the minimum solution. Minimum solution. The dualist quest projection method has some issue regarding the data that we need to use in order to implement the method. So, what happens, for example, when we don't have information regarding the data coming from the adjoint, or we don't have assumptions which guarantee the convergence of the least square projection method. In this case, we can. In this case, we can overcome these two issues considering some variational regularization method in a data-driven setting. In particular, what we have in mind is the following one. We start again from, in particular, the least squares projection method. So, we projected our solution onto a finite dimensional space. We have our data. We consider this. Our data, we consider this variational method where we have the projection, of course, as before, but now we regularize the problem adding a regularization term. Okay, and again, it's simple to show that this optimization problem can be rewritten only in terms of the training pairs. In fact, again, using the Gram-Schmidt autonomization procedure, you can Procedure. You can start in this case from the UI, you apply the Gram-Schmidt orthonormalization procedure to UI, you get an orthonormal basis for UN, and then you find the Y underline I. And then you can express, it's simple to check that you can express this part here in terms of the training pairs only. Okay, so in this case, we have an optimization problem which involves only your data. Only your data plus a regularization term. Regarding the existence of the minimum and so on, this is classical, so nothing new in this sense. So I won't show any results regarding the existence of the minimum because it's classical. Okay, so instead I will show you some numerical results, which is based always on the Radon transform. So we consider this operator equation: Ru equal to Y. R u equal to y and I want to explain what in this case we are what we are using in this in this case. So for the least squares projection method, we are using a training page like this one. In particular, in this case, we are using another data set, which is the data set of the phases of UNITE in the United States, phases like this one. So we have a similar So we have a similar data to this one, and we use the corresponding sinogram. For the dual least quest projection method, we assume that we have access to the sinogram and to the data related to the adjoint of the operator of the Radon transform. Okay, for the variational regularization that I showed you in the previous slide, we use in our case. Previous slide, we use in our case as a regularization term the total variation, but of course, you can use whatever you want. So, other regularization terms. Okay, it's not important in this case. Okay, these are the results. So, we start from, in this case, we want to reconstruct this picture here, which is, of course, non-containing the data that we are using. And in these three columns, you see there is a In these three columns, you see the results of the three different methods. So in this column, you see the results of the least square projection method. In the second column, the results of the dual least squares projection method. In the third column, you see the results of the variational regularization method, the data-driven variational regularization course. In the first line, you see the results using 1000 training pays with and And imposing one percent of noise in the cyanogram of this picture that we want to reconstruct. In the second line, instead, you see the results using 2,000 training pairs. So you see that there is an improvement of the numerical experiments, adding more training pairs in the reconstruction procedure. But there is an issue because it can be proven that. Because it can be proven that in the case where we increase the number of training pays in the least squares projection method, in the dual least squares projection method, we have that the results are worse. In this case, you see, for example, the results using 5,000 training pairs. So you see that the least squares projection method fails more or less. And the same is true also for the dual least square projection method. Is a square trajection method. Also, in this case, we are comparing with the same training pace, the same number of training pays. But if you increase to 6,000 training pays, you see for the dual least squares projection method more or less the same results as this one. Instead, if you increase the number in the variational regularization, the driven variational regularization method, you get better results. You get better results because, in that case, you are regularizing the problem, adding a regularization term inside the optimization problem. And in this case, we are comparing the results given by our approach, so the data-driven variational regularization method, what I showed you before, with the classical variational regularization method, so the model-based regularization method, because in this case we are using explicitly the knowledge of geo-variation. simply the knowledge of the operator R. Okay, and so you see that the results are sufficiently good, I would say. So in the case of the data-given variationalization. Okay, so just to conclude, because I think that it's time to conclude. So we have seen that we can rephrase some of the traditional methods, methods, the classical methods, in another dimension setting. For example, in the first part, we saw the Example: In the first part, we saw the lambweber iterations, in the second part, the projection operators, projection methods. I'm sorry. And we also saw that some of the classical methods, model-based regularization theory, can be extended to purely data-driven setting. In particular, in the second part, we saw the projection methods. Projection methods, and we saw that in that case, if we the number of the training pairs that we are using acts in the problem as a regularization parameter, in fact, if we increase a lot the number of training pairs that we use, in that case, we compromise the stability of the algorithm. We saw that instead for the variational regularization, in that case, adding some Is adding some regularization term, we have that the results are better instead. And also, we addressed more or less the question, the starting question, that is, there is, in fact, a regularization method which is capable of learning linear. Of course, we have some restriction on the operator that we need to consider, the fact that the operator should be injected. The fact that the operator should be injective, but anyway, it is a first answer to this to the question. So I stop here and I thank you for your attention.