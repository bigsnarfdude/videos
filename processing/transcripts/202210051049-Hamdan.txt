Thank you so much. So, my name is Adrasiz Hanga. Today I'm going to introduce mixed finite element methods for semicolon crystals. This work is joint with Patrick Faril and my PhD supervisor, Scott McLachlan. I'm going to start with an introduction. So, recent years has been significant and successful effort in robbing numerical modes of various liquid crystalline materials and equilibrium. Materials and equilibrium states of liquid crystals usually correspond to minimizers or subtle points of given energy functional. And these energy functionals can be discretized using finite elements or other variational techniques. But in this talk, we are specifically interested in the equilibrium states of smittic A liquidic crystals. Therefore, I'm going to start with the PSS model where they This model where they presented this energy functional for the semi-tick A liquid crystals. We are going to restrict ourselves for the two-dimensional problems. U represents the variation in the liquid crystal from its average density. U is the unit length director of the liquid crystal. A1, A2, A3, QK, and B are real-valued constants determined by the liquid crystal under consideration. Then Shaital adapted the PSS model to make use of a tensor-valued order parameter in place of the director field, and they write the energy function in this form. So they replaced the outer product by this tensor. And these terms are the same. They are finding extremizers of this energy functional over this space where H10 omega s. Where H10 omega s is the trace distinctor valued order parameter. Because we are in 2D, this trace distensor has only two degrees of freedom with each component in H10. So it has this form. Now, compared to the PSS model, they are adding this function and they are adding this function so that the minimizer of That the minimizer of this function is maxiq of the form nu after nu minus I of 2, where i2 is the identity matrix. Looking at the energy functional, Euler-Lagrange equations lead to a coupled system of PDEs with fourth-order parameter on U and a second-order parameter on Q. Now, discretizing second-order parameters is easy using a continuous Lagrange elements, but Uh, elements, but uh, discretizing the order parameter, the order parameter u is more challenging because of the high derivative. Now, if suppose that in this energy form that we are fixing a q and find and we are going to find the first order optimality condition, so we are going to take the gate derivative of u in the direction of attest function phi, then the function phi, then the first order optimality condition is of this form. Where again, in this condition, we assume that Q is fixed. Therefore, as a beginning, we are going to start with a simplified minimization problem. So we are going to consider this minimization problem with suitable boundary conditions and The optimality conditions for this form is almost similar to this one, but in our case, A2 and A3 are both zeros and we enforce a source function here. So it's pretty similar to it's somehow similar to the condition here. Now, T is again, the tension of T is Again, the tensor of t is two by two bounded bounded tensor with the Frobenius norm equals to one. Q and M are both positive constants. And for this problem, we restrict ourselves for the unit square domain. Now, extremizers of this energy should satisfy the following fourth order problem. So, first, we are going to consider to provide mixed finite element methods for this fourth order problem. Then, we are going to try Problem, then we are going to try to use the mixed formulation to the Phil Smichtic model. Because this is a fourth order problem, the boundary conditions that arise naturally from the variational formulation appear in pairs. So, on any segment of the boundary, we have to provide one pair of these boundary conditions. One pair of these boundary conditions. Now, the notation is related to the order of the boundary condition. For example, gamma 02 is the boundary condition on the zeroth and the second order derivative. And so on. In addition, we denote gamma 0 for gamma 02 union gamma 01 and gamma 1 is gamma. Gamma one is gamma zero one union gamma one and so on. Okay, we are going to provide mixed finite element methods by transforming the fourth order problem into a system of lower order problems. So if we write v to be grad u and alpha to be dev grad grad u plus q squared tu, then the fourth order problem is equivalent. Then the fourth order problem is equivalent to this system. We can get this system by from here. We have v minus graduate equal to zero, plug in this restriction here, and therefore we get the second equation. And finally, writing the fourth order problem in terms of uv and alpha, we get the first equation. We usually, in this case, multiple equations. Usually, in this case, multiply by this function and integrate by parts. And with the help of these boundary conditions, the weak form is equivalent to the settle point problem of finding uv alpha in L2 V H div omega H gamma 3 div omega such that this system is satisfied. Where here we have the sublive space, the H div space with the Drinked boundary condition on the normal component. Drink with boundary condition of the normal component, and we have the function space v is the vector space each component in each one with v equals zero on gamma one and the tangential component is zero on gamma zero. Excuse me, give me a second. The bilinear and linear forms A and B are defined here. So the question becomes if about will possessness of the saddle point problem we provided. The weak form of finding UV alpha in the suitable space such that these equations are satisfied is well posed. We need to prove the coercivity of the Coversivity of sorry, this is a typo, should be A over the kernel of B. We have to prove coversivity of A over kernel of B and we have to prove the M sub condition. At the discrete level, we consider discretizations with UH Vh alpha H in D G K V H R T K plus 1. Note that D G K is a subspace of the square integrable function. This is the space of discontinuous Lagrange elements of order k. VH is this space. VH is this space, it's a subspace of this space, the space of continuous Lagrange elements with suitable boundary conditions. Now, if we define this weighted norm for the pair UV, we have this theorem that states the error estimates. First of all, the problem at the discrete level is well posed. And if the solution at the continuum is If the solution at the continuum level is smooth enough, both U and the given tensor T are both smooth enough, then we have these error estimates. I have some observations here because VH and CGK plus 2, but the error is big O of H to the power K plus 1, we have suboptimal convergence rate for V for the L2 norm of L. For the L2 norm of alpha, we also have a suboptimal convergence rate, but we have an optimal convergence rate for alpha in the edge of semi-norm. This is an example. This is a typo in the example number. This is an example. We take the tensor T to be nu outer nu, where nu is unit vector, u exec to be sine q nu xy dot xy. The constant b is. Y. The constant B is equal to this is the constant from the semi-trick model, but it doesn't appear in our equation. M is equal to 10 and Q is 40. Unit square domain with these boundary conditions correspond to this exact solution. Then we have this figure. I don't want to go through this figure line by line, but I would like to say that these experimental convergence rates are exactly what we expect. Convergence rates are exactly what we expect from our analysis. And as a reminder, V and alpha approximate grad U exact and dev grad grad U exact plus Q squared to T U exact. Now from this experience with mixed finite element methods, I can say that we have some drawbacks. First of all, it's not trivial to choose a finite element discretization that satisfies all positive conditions. That satisfies all positive conditions, the coercivity of the bilinear form and the M2 condition. And some finite element methods can converge to a superior solution on non-convex domains. A well-known case, the biharmonic operator with simply supported boundary conditions. Some mixed finite element methods fail to convert to the right solution in this case. Now, back to the Shait Al model for Smyctic A-liquid crystal. They have used C0 interior penalty method to secretize their energy functional. So they are using continuous Lagrange elements for UH and continuous Lagrange element for QH. But because we have the Hessian in the energy, they penalize the inter-element jump with the weight we go of one over HQ. One over h cube, and they are using quadrilateral meshes. However, we try to use the mixed finite elements we developed for fourth order problem to the false mixtiq model. So we employ a Lagrange multiplier to enforce V is equal to grad U, and then we add this term to the energy functional with the correct boundary condition. the correct boundary conditions we have alpha dot v dot v minus grad u is equal to alpha dot v plus u div alpha and in the energy we replace the Hessian by grad V then the energy that we deal with is has this form and again we replace the Hessian by grad V and we add the Lagrange multiplier terms and we find subtle points for this for this energy over this For this energy over this space. Now, if we take the first order optimality conditions for the energy, they are non-linear and therefore we apply Newton's method. So, suppose that upper U is the equilibrium point of the energy, and if upper U naught is a close enough initial guess to U, then Newton's method finds a sequence. Then Newton's method finds a sequence of approximations of the equilibrium points. And in our case, at each step, the current approximation is updated by solving a linear system. So at each Newton step, we have to solve for this system, for this Hessian system. And the Hessian matrix, again, in our case, is of settle-point type and can be written in this form. And this form is. And this form is similar and close to the saddle point we get from the fourth-order problem we solved minutes ago. Now, instead of solving the saddle point systems for the updates at the continuum level, we are going to use finite elements to numerically approximate these updates as delta upper UH is equal to delta UH, delta VH, delta QH, and delta alpha h. Dr. QH and Dr. Alpha H. And at the discrete level, we conside the discretizations with DGK, CgK plus 2 squared, Cgk plus 1 squared, and RTK plus 1. And I have used CGK plus 1 squared here because QH in two-dimensional problems has only two degrees of freedom. In the first example, in the numerical experiments, we have two examples. In the first example, we consider the square domain from minus one to one and from zero to two. And the constants Q is 30, A1 is negative 10, A2, 0, A3, 10, B is 10 to the power minus 5, K is 0.3, and L is equal to 1. And from Jingmin's paper, this is called the oil streak scenario. The oil streak scenario. Now we apply Driclet boundary conditions on Q such that Q is this tensor on y equals zero and Q is this tensor on the other sides of the square. And the corresponding director field for these tensors is new, is the unit is the unit vector 10 on y equals 0 and 01 on the other sides of the square. Our initial guess for Our initial guess for this case is sine q over 2y, grad u, alpha equals to 0, and this q. And we perform Newton's iterations until we get 10 to the power minus 8 residuals. Now, if we use dg1, cg3 squared, cg2 squared, RT2 at h1 over 2 to the power 7 using 16 processors, we needed more than 100 iterations. We needed more than 100 iterations, and the elapsed time is 45 minutes, which is somehow unsatisfying because 45 minutes is too much, and 100 iterations is too much. So it can be for many reasons. One of them that we start with, let me say, are not good initial guess. So we augment our calculations. We augment our calculations and we instead of using the standard Newton LU, we use nested iterations. So we first solve the nonlinear system to convergence on a coarse grid with a fixed initial guess, then interpolate the solution we get from that level, use it as initial guess on a uniformly refined mesh. We repeat the process. Find mesh. We repeat the process until we reach the desired finest grid mesh. In this example, it's the same example as before, but instead of using the standard LU, we used a nested iteration in Newton LU. It showed that at the coarsest level, which taken to be equals to 32, we needed 90 iterations with 1.3 minutes. But interpolating the solution we get from here and use it as an initial guess at this level, we needed only 10 iterations and then again interpolate the solution we got and use it as an initial guess in this level. We needed only two iterations and so on. So at the finest level, which is in our case 2 to the power 8, we only needed 2 iterations. Needed two iterations. Now, compared to this case, if we find the accumulated time here, we need only three minutes, which way bigger than the standard LU, which did take 45 minutes. Now, this is the solution I got from the previous, the stated previous. In addition, this is the solution. In addition, this is the solution I did get from the initial guess sign q over 2. But changing, I have noticed that if we change the initial guess, we get new solutions. For example, for this one, I have tried sine q over sine qy. And for this solution, I tried sine 3 over 2 qy. So we have at least three solutions, and this motivates us to use the deflation. Motivates us to use the deflation techniques that presented by Patrick yesterday. He had a nice talk yesterday about deflation. So I think the clever next step would be applying deflation techniques for our formulation. And now we have to try to change the scenario. We took the same domain, square domain. We changed the constants a little bit. instance a little bit but we implemented the boundary conditions such that the director field is the radial is the radial vector with this initial guess so we have tried sine 5y with v equals to grad u and alpha equals to zero and again this q as an initial guess and we got similar uh behavior for mystered iteration in newton lu so when Iteration in Newton LU. So when we start with, we started with H1 over 16, we needed 52 iterations, interpolate the solution, use it as initial guess for the next level. We needed these iterations and so on. Now, the finest level we could have is one over two to the power seven. When we tried one over two to the power eight, The power eight the LU solver was unsuccessful due to the memory requirements, but fortunately we developed nested iteration Newtonikrylov multi-grid. We didn't discuss it here, but that method can solve the arising nonlinear system at the final mesh. Now, with the initial guess and the constants we have presented here. Presented here, we did get this solution. We tried to use the radial director field as an initial guess and implemented its boundary conditions, but with different domains. This is what we did get when we used circular domain, and this is what we did get when we used the int square domain from 0, 0 to 1, 1. Finally, conclusions and future work. We consider Conclusions and future work. We considered mixed finite element approximation of solutions to fourth-order problems. This enabled by transformation of the fourth-order equation into a system of lower-order PDEs. These mixed formulations are applied to find extremizers of the semi-ticket A energy functionals. We developed Newton, a NISTIC iteration, Newton LU solver to solve the arising nonlinear systems. For future work, we are interested in extending. Future work: We are interested in extending our experiments to 3D models and applying deflation techniques to generate multiple solutions. And in addition to this, I will start a one-year postdoctoral position in the Imperial College next month. And yeah, that's it. Thank you so much. Great, thank you, abilities. Any questions from the audience? Any questions from the audience? In the last simulation, thanks. In the last simulations, you said that you have changed some of the constants. Why was that necessary? And how did you decide what constants to use? Standard question. Did you hear her? Yeah, I could hear her. I don't have really a good answer for this, but usually in 2D we take the constants. I fixed these constants, but maybe Scott has a good answer for. Answer for A1, A2, and A3? I was just trying to look that up. I think we stole these from maybe one of the experiments we did with Jingmin or something like this. Or I'll blame Tim and say, we had some discussion with Tim about what these constants should be, and we tried some different values. And of course, we have no idea what these constants should be in terms of actually matching real data. In terms of actually matching real data or anything like this, right? And so we tried some things. Yeah, this is like the biggest problem is that we don't have great there's so many constants respects. In fact, at least for I'm not very sure about the first one, but B and K, we know it. You do? For HCB? Yes. I can take a look and if you A look and give you values that I used to see. I see, but if I am right at least dilation in case it goes off to try and read, but of course, I see that it is normalized to something because usually k is of the order of 10 to the minus is a little. Minus is Eleven Butter, like that. But only if you can use it, do you? It wouldn't be great to build up a line to me. Just like Randy suggested. Just as Randy may have suggested. Something, anything. I think the guilty secret is that most numerical people go to the back of Ian Stewart's book where he nicely wrote a bunch of concepts for the mattress to look at that. That's been my go-to source. There's old things about like persistence lengths and kind of fish and depths that I kind of use to get the ratios fine-ish. So in this work, I mean, Q is the phenomenological thing that we're setting here, right? Q is determining how many layers we get packed into a unit length. And then from the numerical analysis and from what we've run before, we have a pretty good idea that B should be something like Q to the Something like q to the minus 4, because that gives the right sort of phenomenological balance between terms and so on. And the rest of it is all guess. Because the relationship between B and K, we know it very well because if what I believe, if lambda gala square root of k or k over v you agree and and if You agree. And it's of the order of the lambda, it's size Carnegie and space. It's quite a screen anomaly. So this relationship is true. So if you put V as in relationship to Q, then you know pay. But even the value of V, But even the value of B and the value of K are well known for it. That's to say. The roadophil of Christian people. What do we say to them? Yeah, just not for a fruit one. One thing that I usually do, I don't know if it's super well depth to that, but order of magnitude, root A or B, the symptomic scale. Like scale ought to be not too different from the pneumatic defect core size, right? They're all on a molecular scale. So, those are two, since you have those, both of those in this kind of theory, you can kind of see everything. And how are you ever going to do, how are you ever going to do, on SCPs? It's not a secret, it's not a different thing. How are you ever going to do calculations where you don't go down to the molecular scale if there's a parameter in the energies? It's the molecular scale. In the energies that's the market scale, you do a continuum, you do some continuum modeling, right? But one of the parameters in the continuum modeling is the molecular length scale, right? So until you start calculating strains and variations on the molecular length scale, that term is completely irrelevant. The curvature term is basically you could throw it out altogether, right? If you wanted to, you always say. You're always saying, but even for like just slowifications. I'm just saying one of our, well no, why don't you even finish your finish your answer? It was not a complete problem. Right, but if there's a parameter, and it's true, everybody always says that it's lambda, right, root K over B is the scale of the spacing, which is the molecular scale. The spacing, which is the molecular scale. And so, you know, some people give me a hard time. They say, well, how can you write down a continuum theory for things that are having oscillation at such a short scale? Well, I mean, you know, that's up to me, right? It turns out it works perfectly. Not only does it work perfectly, you can see those few spots in X-ray scattering, right? So it's not, I'm not imagining, but you can see the dots. So if that number is explicitly in the energy. That number is explicitly in the energy, don't we have to be able to simulate across many, many orders of magnitude in order to see that turn play a role? Then the gen has the same problem, right? With the defects core scale of, I thought the answer is just we have small simulations. Right, or overly big defects. Or we have overly big defects, yeah. So supercomputers? Like for instance, what we saw yesterday, the numerical construction of focal comic domains, right? You could probably get that by turning the curvature energy off. Might even be easier, right? To turn the curvature energy off, unless it's there and it's regularized. Could be regularizing something by being a higher rate of the term, you know, trying to keep variation bounded, not. Variation bounded, not bound variation. I mean, one of the things that we can easily do in these simulations is just compute. Here's a solution. Here's each of these pieces of the energy. And then that sort of answers that question, right? Is that that K term contributing anything meaningful to the energy in these solutions in comparison to the B term, right? And I don't know the answer because I don't know. I don't know the answer because I don't have the data in front of me, but we can compute it. Right. But I mean, certainly in a focal comic company, obviously B wins. Right. So, and you see it's 10 to five times bigger than K, at least in the five times smaller. Yeah, whatever. But on the other hand, I can do simulations of Darcy's flow without resolving down to the light scale. Down to the length scale of a pore size, even though Darcy's friction in the front will produce a length scale by balancing the friction coefficient with a viscosity, right? And just because you have small length scales doesn't mean you have to. I totally agree with you. So that's what I said. People are critical sometimes. Say, why are you doing continuing elasticity theory on a theory where the variations are at molecular order? Well, I do, and it works because I'm agnostic to what the length scale is. To what the length scale is, I say it's continuum all the way down, right? But there must be some justification that's better than me saying because that's because that's the model I'm considering. There must be some reason why we can do this. Can we evaluate, can we answer the question of why is it that we can get away with doing, why does it work so well? Not, not, not, right? In other words, we can respond to all the people who say, well, this is nonsense. How can you know continuum theory? How can you have plutonium theory for molecular scale with molecular scale link that? Well, it works, but there must be some better reason than just that it works. I mean, that's been a question on our minds for a while now, right? I suppose that we should, we should, maybe the output of this of this workshop should be, you know, or if you know you were if you were frozen and woke up 100 years from now, what would be the first question you would ask? Your Shabana, what would be the first question you would ask? Which would be the first question here, right? Any other questions for those? I think in the results that you showed before, did I understand that you say that depending on the initial solution? Depending on the initial solution that you take, you obtain different results. And then what is the solution plan? How to solve this problem? Because it's a little bit a problem because how can you choose, oh no, I say differently, how do you choose the appropriate starting that would be the correct one? I don't know if it is. Can you show me street slide again, Belaziz? Yeah, sure. And then the multiple solutions of oil street. Yeah, the two or three slides back with the oily street solutions. This one? Before? This one. Yeah. So, I mean, in. So, I mean, in the work with Jingmin, of course, we found tens of these or something like this, right? That are all stationary points of the energy, right? And now, again, we don't have it on the slide, but each of these has a different energy associated with it. And so you can say, okay, there's a ground state. The best solution we know is the one with the lowest energy. Cross our fingers and hope that it's stable. If it's not stable, we have ways of going downhill to try and find a stable solution below it. Below it. But you know, the allegation is all of these are possible solutions. Depending on the dynamic path you took to get to an equilibrium, right? So the answer would be, okay, it is the one that we see experiments. But then it would be more interesting to say, okay, why do we see this one in the experiment and not the other one? Does it mean that there should be the both solutions? Does it mean that there should be the whole solution, but then you should see them together? Because I guess it'll depend on how you get to the equilibrium in your experiment, right? Yes or not, because sometimes we always find the same. So the system depends on the way we produce it. Sometimes we have a feeling that there is always the same. So not so few or few, but that's but I think experimental protocol is. But I think the experimental protocol is very different. I think our perspective is if we didn't see multiple solutions, if we always got the same solution out of this energy, we must be doing something wrong, right? Because it's this horrible looking energy and we expect multiple solutions from the PDE variational perspective. And then the question is going to be: well, how do you get into these states in a gradient descent sort of way or whatever? From the popperian falsification point of view, it gives us, you know, we've got 10 shots to be right before we match up with that experiment. I think the point is that numerically, we often have to start from a really immediate place. It's convenient to often start from a medium place. But you don't have very good initial conditions, initial guesses. Any other questions for Adelaide? Any other questions for Abdelaziz? Any other questions from your secret source of questions, Ren? Okay, all right, in that case, let's uh thank you Delaziz again. You're welcome. You did ask me about onto the West End Mall. I think it's about the five. And so I guess it's lunchtime and then it's special o'clock time. Special part, yeah. It used to be, I don't know, the biggest, it probably wasn't the biggest, yeah, yeah. By far the biggest, it's one of the so there are possibilities if people wish to join. Uh, we're gonna hike up Tunnel Mountain relatively. It's relatively easy, you know, not to bad. Very beautiful views. You don't have any group, otherwise it will walk. Yes, so it's the more the more people that go, the more likely we'll come back, okay? And so, so, do we want to set a time for that? I don't know. It could be like one o'clock, thirty. I don't know. It's very late. We're about to eat. Yeah, oh, let's take it in. Oh, is it taken to go? Or we can go with it? That's what I'm saying. Oh, no, really. Shall we say 12:30? That's just a good idea. I'm not very much too much. Yeah, let's do that. Let's meet at the front end. That's sort of cool. So, the first thing we will do is meet at the front end of the plate. 12:30 will be 12. Yeah. Yeah, so yes. Oh, yeah, you're there.