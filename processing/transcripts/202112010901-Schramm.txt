Everyone to day three in which we'll discuss over-complete tensor decomposition. So let me overview the problem that we'll be focusing on. Okay, so in this problem, we're given a tensor in Rd Q. So it's a So it's an order three tensor, you know, looking like this. And we're promised that t has the following form. So we're told that t is equal to the sum of rank one tensors, which I will call components, I'll call the vectors that form the rank one tensors the components. Components. It's a sum of n of them. Okay? And we'll also take a distributional assumption on the AIs. Let's say that the AI are drawn uniformly from the sphere independently. Okay, this assumption can be relaxed, but it will be convenient for us today. It'll make some calculations nice, but essentially, you know, like. Is nice, but essentially, you know, like any sub-Gaussian distribution, the same results that I'll present today should hold. Okay, so our goal, I guess, which I will write down here where you can barely see, is to estimate AI through AN, the components, okay, in L2 or Okay, in L2 or whatever. So we want to find vectors that are as close as possible to these as we can. Okay, so this is the over-complete density composition problem, and I guess that one reason that people, I mean, so people have studied this for decades, you know, dating back to 60s, 70s. Or sorry, sorry, I forgot to say one thing. Okay, so what does overcomplete refer to? So over complete just means that we can take the number of components larger than the dimension. So in particular, if you take the AI, they'll more than span RD. So this is what over-complete means. So, and the sensor decomposition problem has been studied for decades. One really nice property of tensors, as opposed to matrices, is that even when the tensors are overcomplete, their decompositions can remain unique, right? So their minimum rank decomposition remains unique. Not for every tensors, but certainly for tensors with distributional assumptions like this. It's true with high probability. And so, you know, this is a very nice thing because if you want to estimate some mixture, like let's say the centers of some Let's say the centers of some mixture with identity covariance or something, you can compute the moment tensor, the tensor will look something like this, right? And even if you have more mixtures in the dimension, you can recover the covariance. So this is one of the reasons that this has been studied a lot in a recent decade or two, but I won't go too much into these applications, just to give you some orientation for why we might want to study this problem. But n should also be smaller than b squared. Smaller than B squared. Right. So, okay, right, that's a good point. So when is the decomposition, okay, when is the min-rank decomposition unique? So it depends on the tensor, but in this case, it will be whenever n is less than. n is less than order d squared. Okay, once you allow me to take order d squared, then I can start to just take the matrix slices of the tensor. Each one has a rank D decomposition. And then I can take like everyone with the standard basis vector gives the thing. And it won't be a symmetric decomposition, but then you can argue that you can also get a symmetric decomposition from this. Decomposition from this. So, okay, so this is kind of the regime in which this problem is interesting, or maybe more interesting than matrix decomposition. Great. So I'll start by presenting an algorithm which is called Yenrick's algorithm, and it dates back to, I mean, it's mentioned in papers from the 70s, not in Yenrick's papers. But, okay, I'll give you a simple version. Okay, I'll give you a simple version of this algorithm that's that's whose analysis is specialized to this case when the AI around the sphere, and that will be a useful primitive that we'll use later. So I'll put it over here so that I'm not tempted to erase it. Okay, so suggestively, I will now write a different tensor, t prime, and it will be equal to sum i from 1 to n b i tens of 3. Okay, so it's this box. Sorry, the bulk should be square. This is not, it's still symmetric. There we go. Okay, and what I'm going to do is I'll sample, let's say that the B I live in R L now. So what I can do is I can sample a standard Gaussian vector. In all dimensions. Okay, and then I take this tensor and I contract it in one of the modes with coefficients from g. So writing g this way, I'll take a sum. So what I'm computing is I'm computing the matrix M, which is just T prime with G in one of the modes. If you're familiar with this notation, then this will help you, otherwise it won't. But let me just now also write what this gives you. Also, write what this gives you. So this gives you some from i equals 1 to n vi g vi vi transpose. Okay, so now you take this matrix and in the case where your bi are orthogonal to each other, it's pretty clear that if you just compute the spectral decomposition of this matrix, the bi will be exactly your eigenvectors. Your eigenvectors. So you'll recover the components exactly. Now, even if your BI are only roughly orthogonal to each other, this algorithm works. So that's the situation that we'll be in here, right, when we sample the AI from the sphere. They're not actually orthogonal, but they're roughly orthogonal, as long as n is less than d, or at most d. And let me just say what the analysis could look like. What the analysis could look like. So with probability at least 1 over n to the, okay, something like 1 plus epsilon squared. You'll have that B1 inner product G is at least 1 plus epsilon times the max over over i greater than 1 di g. Okay, so the coefficient of the first component will be 1 plus epsilon larger than the coefficients of any of the other components. And now we can run power duration and recover a vector that's pretty close to be 1 since they're all roughly orthogonal to each other. Okay, so now you know I could spend some time dotting either. Some time like dotting I's and crossing T's and telling you exactly what the approximation is, but I'm not gonna spend time on that because hopefully the conceptual way you do this is clear. Okay, so that's fine and well whenever you have that n is less than d. But when n is larger than d, the bi's are no longer guaranteed to be the close to the eigenvectors of this matrix. Of this matrix, okay, because in particular it'll be over-complete, so we won't have linear independence anymore, and that will be a problem. So what can we do instead? Well, here is a thing that you might hope to have. Okay, so suppose you had instead We had something that I'll call T6, which is the sum of the AI tensor 6. Okay, so instead of having the third order tensor, I have the six-order tensor of the same thing. Now, what I could do is I could just set BI to be AI tensor 2. To be AI tensor 2. Okay, and then the whole argument that I did over here essentially repeats: as long as the BIs are roughly orthogonal to each other. And in this situation, they will be as long as n is less than d squared. So, we would really like to get our hands on the sixth sensor. Okay, but if you think about applications in statistics, approximating a sixth tensor well from data requires much more data than approximating a three-tensor well. So, okay, so if you have a limited data budget, you're not going to go around computing T6, right? So there's this algorithm that's a combination of several works. I guess it's really like a long line of works. But here's a theorem. And maybe it's best traced to, okay, works by Barack Kellner Stoyer from 2013, I believe, and then G and Ma from 2015. 2015 and then culminating in Ma Shi Store 2016. Okay, so a degree 12 SOS algorithm. I guess there exists a degree 12 SOS algorithm which recovers A1 through AN up to air order and over three halves. Okay, so meaning for each AI, we can find some AI prime such that some ai prime such that ai dot ai prime is at least one minus this quantity. So in particular, if n is most t three halves, there's a polytime algorithm for this problem. Okay, so so this is great. We significantly beat the you know the the primitive Yenerich algorithm application, right? We're greatly exceeding the n equals z threshold. We don't get quite the d squared. Okay, but this is a this is really great. This is really great. Like, you know, algorithms that in polynomial time decompose pretty over-complete third-order tensors. So, nothing is known after details we have? No. Yeah, I sort of conjecture that it's that there might be an information computation gap, but there's no evidence for that either, actually. Yeah, we can talk about it. I think it's really fascinating. We'll talk about it afterwards if you'd like. Okay. Okay, so great. So now I'll prove this theorem for you guys, unless there are more questions, which takes a question. Have people tried hard to go beyond 3DAR? It's been open for a while, right? Like at least five years. Yeah, it's been open for a while. I think people tried hard. I think, yeah, I know of people who tried. And I also tried a little bit. And yeah, I mean, I think, okay, there's another. I think, okay, there's another comment that I should make, which is that this algorithm is actually robust to noise. So if you take this tensor and you add to it a tensor whose Frobenius norm is bounded, let's say, or you could take any other number of restrictions on this noise, but still the same guarantees that this algorithm will work. So it's quite robust. But, okay, maybe in the case when it's completely noiseless, there could be a special. When it's completely noiseless, there could be a special algorithm that uses some kind of, I don't know, like LLL thing or something. So I don't know if people have tried to attack specifically the noiseless case in that regime. Yeah. Do you mind if I take a photo too for the photo while you're talking? Oh yeah, no, I don't mind. I'm being videotaped, so I did my hair this morning. And Slip, remind me, what are the critical points of the Remind me, what are the critical point of this function? Like I could also try to run Yes, okay, great. This exactly brings me to the next point. And I'll address your gradient descent comment after I say the next point. Okay, so you guys remember the problem? Okay, so the idea of the sum of squares algorithm for this problem is the following claim. So let's look at the function or the maximization problem. Max over vectors on the sphere x tensor 3 dot t. Okay, so this is like if you Okay, so this is like if you take the entries of t to be coefficients of a homogenous degree 3 polynomial, what is the maximum of this polynomial over the sphere? Okay, so let's call this f of x. Okay, so the claim is that sorry? Oh, sorry, sorry, sorry, yeah, yeah. Let's call this f of x. Okay, now Okay, now the claim is that the maximum of f over the sphere is one plus or minus order and over d to the three halves and maximizers. And uh maximizers are uh one minus where I know we're d to the three halves close to the points a1 through a n. Okay, so in particular, when n is less than two to the three halves, the maximum Is less than 2 to the 3 halves, the maximizers of this function over the sphere are exactly the components, or very close to the components. Okay, so this exactly addresses Seb's excellent question from a moment ago. And in fact, people do run, I mean, they have tried, people extensively studied gradient descent for this problem. And I guess it's a very natural, you know, non-convex optimization problem. And actually, it seems like the guarantees of It seems like the guarantees of gradient descent, ignoring robustness concerns, are comparable to this. So empirically, whenever n is at most 2 to the 3 halves, gradient descent converges to one of the components. However, the analyses of gradient descent don't really extend beyond D, which is linear or N, which is linear in D. So they can get at least like 2D? In 2D, I mean, for 2D, it's all about, like, you know, 2D, it's all about like, you know, finding. I mean, if you can, if you randomly guess and end up in the basin of attraction of one of the AIs, so it's not really like, it's not, I wouldn't call those not like properly polynomial time maybe, but I don't know. I mean, maybe that's unfair. Yeah, like the running time, the analysis of the running, here's the precise thing. If you look at the running time for gradient descent when n exceeds d, then the running time is dominated by The running time is dominated by trying to find a starting point. There are bad long-haul maximizers. No, there aren't. I think empirically there are not. Okay. But in the analysis, no one knows how to show this. So establishing, so empirically, gradient descent seems comparable to this algorithm. It's actually much faster, so it's a I think that makes it a better algorithm. But in terms of our theoretical analysis, we can't come close to matching this guarantee. Come close to matching this guarantee. Empirically, no matter the starting point, if n is less than e to the three-half, you get to one of the areas. Right. Yeah, I can show you my code. Yeah, it's actually, like, also the dynamics are pretty weird. Like, you know, you'd think maybe that you converge to the AI that's closest to your initialization point. But in fact, once you're in the over-complete regime, Once you're in the over-complete regime, actually, what happens is you bounce around for a long time, and then eventually you get into the basin of attraction at one of the points. So it's like, okay, this also maybe explains why the analysis is so complicated, right? Like, how are you going to explain something like this? And how is everything you've told us specific to three? Nothing is specific to three. Everything, here if you have an order k tensor, take k over two everywhere. And think about the scale between. I think about the scale, by the way. What about why is it like you can flatten the problem? Like the tensor, you can represent it like the D to the K tensor into two D to the K over two matrices. And then try to do something there. Okay, hold on a second. Could be a non-sense comment. Oh, no, it's not a nonsense comment. Okay, great. So this is a good point. But yeah, so Seb is asking, suppose Okay, so Seb is asking, like, suppose that I have a matrix that's a four, or a tensor that's a four-tensor, can I not think about it as a matrix instead? So, yeah, so in this case, you can do something kind of special. So, I have a matrix, and it would be like the sum of AI tensor 2, AI tensor 2 transpose maybe. The problem is that if the AI tensor 2 are, okay, so actually there are algorithms that work here. Are algorithms that work here basically by just looking at the subspace spanned by the rows or columns and identifying symmetric, like you know, rank one vectors in this subspace. So n-square dimensional vectors, which when reshaped are rank one. But these algorithms aren't as robust, I guess, as the. But they get to the same. Yeah, but they get to the same. I see. Yeah. Yeah, I'm being very lax with citations here, but if you look up any of these. If you look up any of these papers, they'll have a good survey of the fryer works. Okay. Great. So, okay, so fantastic. So this is the claim that will let us guess an SOS algorithm. I'll be able to prove this in a sum of squares way. And once I do that, that will give me that the Will give me that the pseudo-expectation operator, when evaluated on the tensor of degree 6, has to be close to this object. And then we'll be able to essentially run Yandrich's algorithm on it and get back components. So that's just to say where we're headed. Now I'm going to erase this degree six thing. Okay, so here's the proof. So let's take the polynomial f of x. So let's take this this quantity and just rewrite it in a slightly different way. This will let us see how to apply the Cauchy-Schwarzen quality. Okay, so we can also just write this as the inner product of x with sum from i equals 1 to n ai. A i Ai x squared. Right, believable algebraic manipulation. Okay, so now I want to apply Cauchy Schwartz, so let's square everything. So applying Cauchy-Schwartz, this is going to be at most the norm of x squared, big times. Times the norm of this vector. Right? And skipping a step and just writing out the norm of this vector, I get that this is a sum over ij ranging from 1 to n AI Aj AI AI AI x squared AJ. Squared Aj X squared. Okay, now the next step is going to be to take this and rearrange it so that we'll get a good spectral certificate which upper bounds this quantity, right? That's like our two tools that we are using all the time, like Cochi Schwartz and spectral certificate. Okay, so by symmetry, okay, actually, sorry, another object. Uh okay, actually sorry, another observation. So if we take that x has norm one, then we can just drop this term and be happy. Okay, so this is one. And then let's rearrange this only slightly. So I'm going to write this as the quadratic form of the vector x tensor x. Here I'll have sum over i j from 1 to n and i AI AJ AI AI transpose tensor AJ AJ transpose. Okay, whole thing extensor X. Alright, so, you know, here I just, like, I could have chosen any number of ways to write this. This part, right? I could have done like AIAJ transpose, sensor AI, AJ transpose, but that would have resulted in a matrix that is low rank, and that would be bad. So writing it like this instead, I get a matrix that's high rank. And that will ensure that the spectral norm is small. Okay, this is kind of a subtle point, maybe, if you don't feel like you got it just ignoring for now. All right, now let's look at this. Now let's look at this matrix. And I want to write it as a sum of two different parts. So notice that the terms when i and j are equal to each other, that's just the order for a tensor, right? So I can write it as sum over i, norm Ai squared ai ai transpose tensor ai ai transpose. Transpose. Okay, this, I'll just write it more simply, right? So this is one, and this is just a tensor four. So I'll just make some space for myself on the board. Okay, and then here I'll have a sum over the terms where i and j are not equal to each other. Okay, so what do we have here? We have the AI tensor 4 part. This is good. This is like a higher order tensor. So we're pretty happy. And here we have this term, the cross term where i and j are not equal to each other. Okay, now I'm not going to make any argument. Now, I'm not going to make any argument for why this is, but suffice it to say that, okay, in this paper by Guer and Ma, they proved that the spectral norm, so, of this matrix is at most order n over d to the 3/2. All right, so in particular, when I evaluated against a unit vector, x tensor x, Here, x tensor of x, I'm going to get something of magnitude most n over d the three halves. Okay, and then so really what this all says is that this is equal to sum over i a i x. I'm reapplying this x to the 4 plus something of order n over d to the 3x. Okay. Okay. So okay, so this is great. So now let's just uh finish. Sorry, this is also a uh sum of squares. Yes, every step that we did was a sum of squares proof because we. And the ones you didn't do any. The cheating the ones from cheating. Oh, right, right, right. Okay, so this proof actually, it doesn't need to be a sum of squares proof because the AIs aren't variables in the program, right? They're the input. So saying that the operator norm of this is bounded is the same with the operator. Norm of this is bounded is a statement that holds with high probability over the AI. And now we apply the fact that a bound on the spectral norm implies a bound on the quadratic form of the vector. We apply that sum of squares proof from day one to conclude the bound. Yeah. Alright, so all of the steps that I did here were sum of squares. Okay, so we've seen that the value is bounded by one plus of is bounded by one plus of n over z and three halves. Let's now argue that it's achieved by the AIs, that such a value is achieved by the AIs. So okay, let's look at f of A1, or of AI, I guess it's all symmetric anyway. So right, so this is just going to be AI A i Ai cubed plus sum over j not equal i A i a j cubed. This is just one because I have vectors on the sphere. Okay, and here with high probability, I claim that this is bounded. Now, A i and A j are independent vectors from the sphere, so the magnitude of this will be. Sphere, so the magnitude of this will be like 1 over root d, and I'll have random signs. So I'll get something like 1 over root d cubed. Okay, and then there's n squared terms, but because of cancellations of random sign, I would get something like n. Yeah, this looks correct to me. Okay, so you can see that you're going to get that the AI is going to be. That you're going to get, that the AIs themselves will give you, will achieve the maximum. So already we know that the maximum is of this order. Now I only have to say that the maximizers are close to the AIs. But this actually goes back to the fact that Sev was saying earlier that the square reshapings of the tensor have the AI tensor twos as their component. So this is exactly what's going on here, right? The eigenvectors of vectors of this matrix, when it's shaped to a d squared by d squared tensor, are exactly the AI tensor 2's. And there are no other vectors in d squared dimensions that look like rank 1 matrices when they're reshaped to d by d vector. I mean d by d matrix. Does that make sense or should I write something? Comedy, yeah. Some nods, so I'll keep going. Okay, so fantastic. Okay, so fantastic. So we proved this. I guess, okay, I should say this like with high probability. This thing that I always am implicitly meaning, but never saying. Yeah, so we have this and we have a sum of squares proof. All right. And actually, we can revise this claim a little bit. We can strengthen it a little bit. So we've shown We've shown that if norm x is one, then f of x is actually equal to sum over i x a i to the four plus something of order n over d to the three halves. And this is an SS proof. And this is an SOS proof. This is degree, I guess, six, because we just took Cauchy Schwartz once, SOS inequality. All right, so that's kind of fantastic, right? Like in some sense, what we're saying is What we're saying is, we're saying that if sum x AI cubed is equal to 1, then we get this implication. And this means that I guess if it's this plus or minus order, you know, n over d to the 3 halves. Like, I guess we're saying, so. Like I guess, I guess we're saying, so, okay, so this is f of x, right? And this is also f of x. And if I enforce in my SOS system that f of x has to be equal to its maximum value, then I get that f of x also has to be close in value to this range. Like in particular, I guess what I'm trying to say is that I now know. Is that I now know that the okay, actually let me not try to do this. Let me just write down the polynomial system. I think that'll be more clear. Sorry for getting ahead of myself. Okay, so here's the polynomial system that we'll take. So the variables of our program will be just a vector x, x taking value in Rd. Okay, and we'll require that the norm square of x is 1, because we're trying to optimize over unit vectors. Okay, we'll require that t dot x tends to 3. Sensor three, okay, f of x is equal to one. Okay, you can have some slack here, but I'm just not going to write it to keep things from getting too ugly. And already if we have this polynomial system, then this SOS curve that I've given here implies that the pseudo-expectation of of uh x uh tensor four dot a uh sum over i a i tensor four is at least one minus o of m over d to the three halves. Okay, so now in particular if I just take In particular, if I just take this object, which I can just, you know, like run that CP and then get the sensor, I know that it will be, that it will have a large inner product with this. Okay, so, and I'll use the same trick that I used from yesterday. I'll add some regularization here. So I'll ask to minimize the Frobenius norm of the order four parts. Okay, and then I'll be able to. Okay, and then I'll be able to conclude that these actually are close to equal to each other in for V S norm like yesterday. Alright, so this is all fine and well, but now we're holding in our hands the four tensor, but I told you that we wanted the sixth tensor, right? So, okay, instead of all this, what you can do is you can repeat this argument one more time. It will be approximately the same. You'll have to prove a different concentration for a different matrix certificate. But it's more of the same flavor of thing. And then you can replace this by six. Okay, so here's another claim. If x equals 1 and s of x equals 1, then that implies that x answer 6. x tensor 6 sum of k9 tensor 6 at least 1 minus order. And over to 3 halves again, okay, in degree 12 SOS. And I'm not going to prove this claim, but the proof is very similar to the proof of this other claim. Okay, so from this implication of this claim, we can conclude that If I take this sixth tensor, resulting from my program, and I subtract from it the empirical sixth tensor, here with this divide by n, the difference in Frobenius norm will be little O1. Okay, and now I'm exactly in the situation that I wanted to be in, right? I basically know the sixth tensor. So I can So I can run the NRX algorithm on it. Okay, so I know that's geo expectation x tensor 6. If I take this and I evaluate it at some vector g prime, where g prime I'm sampling uniformly at random in d squared dimensions now. So this is going to be sum over i tensor two g prime. Okay, and then also I'll have some error matrix, which I'll denote by Evalue. Evaluated G prime as well. Now, what do I know about this error matrix? I know that its Frobenius norm is bounded, right? I know that it's, because this is just the tensor, I guess, E over N. So I have a bound on the Frobenius norm of this tensor, but you might worry that that's enough to mess up Yannrich's algorithm, and you'd be right. But then it turns out. But then it turns out that if you add some constraints over here, it will let you reduce the error in Yannick's algorithm. So I'm not going to prove this unless you... Sorry, say again, what T-Zip programme? Okay. Right, so I have this tensor. I know that it's close to this sixth tensor that I wanted, infra-benius norm. Okay? Yeah, which is. Okay, but then I do have some error. I mean, like, there is, there are, there is. I mean, like, there is some chance for error. So, what I want is, and I know that if I just had this term purely, this analysis would basically guarantee that I can recover the AI tensor twos. But I don't just have this term purely, right? I have this noise. And you might worry that this noise will mess you up. Okay, so this is a legitimate worry. And in fact, it is an issue. In fact, it is an issue. But what you can do now is you can add a little bit more constraints over here in order to make sure that this isn't a problem. So you add constraints of the following form. Okay, you constrain that the operator norm so just to clarify: the problem is if the noise is aligned with certain direction. Yeah. I guess the problem is, like, imagine that this noise ends up being a rank one tensor or something. Then it always is going to be like a huge thing that's going to prevent you from finding it. And then you can say, okay, if it's really rank one, then I can peel that off. But okay, but then like, what if it's like rank root n or something, right? Then it's bad news. So, okay, right, so what you're gonna do is now you're going to add constraints that constrain the operator norm. The operator norm of rectangular reshapings of this tensor, because that ends up being what governs the spectral norm bounds of this random contraction. Okay, so I don't really want to go into this because it's like this whole other layer of things, and somehow it's not that central to the concept of this. But just to show you, like, that you know, you have recourse in situations like this, what you would do is you'd say, okay, I'm going to take, here's some sketchy notation for this situation, I'll write x tensor 6. X tensor 6 d squared by d to the 4 to denote the reshaping of this tensor into a d squared times d to the 4 matrix. Okay, and then I ask that the operator norm of this is bounded by 1. Now, will this be satisfied by our idealized solution, which is the I Which is the uniform weighting of the sum of Ai tensor 6? Yes, it will, right? Because the Ai's are approximately orthogonal vectors and there's much fewer than d squared of them, so summing them all up and putting them in a rectangle is fine here. I guess, like, you know, as always, there's some little slack here on the line. Okay, and then when I do this, it will ensure that this error has bounded operator norm in its rectangular reshapings, which is what controls the total spectral norm of this. The total spectral norm of this random contraction. And this is just some random matrix, you know, Gaussian matrix series fact. Okay, so great, so that will let me recover one of the AI tensor twos. If I if I round uh If I round the answer to the semi-definite program using Generich's algorithm, okay, I can, you know, here there's some, I guess there's always some failure probability that I won't hit one of the vectors, but because I know that the maximizers of this polynomial, which I can actually evaluate, are actually the AIs, I have a testing procedure, right? So I run NRICS algorithm. I test if I got one of the AIs. Okay, then I repeat. And then every time. And then every time I, okay, then I kind of want to peel off the component so that I can proceed again. So in subsequent iterations, you can do stuff like require that, you know, x and your component that you already found, let's say A1 squared is at most, you know, 1 over 100. You can add constraints like this, and then that will guarantee that you stay away from A1 from then on, and you can make progress this way. Going to make progress this way. Right, and in such a way you, after polynomial, many iterations of this, you recover all the components. Yeah, I guess that's all I had planned. I guess I'm done early for one. And the robustness to, you know, like if you don't have the sum of the AIQ, but you have a little bit of Of the AIQ, but you have a little bit of noise. How does that happen? Oh, yeah, yeah, okay, okay. So that is so effectively, if you can get a bound on the spectral norm of the noise part, or on the operations that we did to the spectral norm of the noise part that you have here, then everything's fine. Then, like, you know, you absorb that in this order, and it also can, I guess it will tell you how large you can. Will tell you how large you can afford to have things. And depending on what kind of error you expect, you can get better or worse bounds on the operator norm of the operations that we've done to it. So that's the robustness. So it can be totally adversarial, but as long as it's bounded in spectral norm and for Benius norm, then In spectral norm or for Benius norm, then you're okay. That's the kind of thing that gradient descent, for example, isn't very robust to, right? And also algebraic algorithms that are based on checking if there are things in a subspace. So you only need this claim once, but each time you're adding new constraints, you're getting a new pseudos expectation. Yeah, exactly. Yeah, exactly. Right. With a guarantee that that pseudo expectation stays away from all of the vectors, isn't large on all of the vectors that you already found. Yeah. Great. Good. Thanks, guys. Any questions from the people of the internet? 