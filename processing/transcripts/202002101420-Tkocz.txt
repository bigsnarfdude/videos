I'm gonna talk about a joint project with Han Juan, Bos Swomka, Beatrice, Helen Witzu. Okay, let's get started with the statement of a nice question. Due to Levy, Hadwiger, Goeberg, Markus, which is known as the covering or you know. Known as the covering or elimination conjecture. So let K be a convex body in Rn. So the question is, suppose you want to carve a K with translates of its interior. How many translates do you need? Equivalently, how many external light sources are needed to illuminate the boundary of K? Okay, I'm not making this precise. Anyway, these two numbers. Precise. Anyway, these two numbers are equal, and the conjecture says, I'll denote this, the number of minimal number of translates by capital N of k. And the conjecture says that this is at most 2 to the n in dimension n. With equality, moreover, if and only if k is a cube up to an affine transformation. The problem is a fine invariant, of course. Alright, so. Alright, so yeah, so the conjecture is n at most 2 to the n. There are many partial results. I will refrain from mentioning them. The question remains open, though, for n at least 3. Levy sorted it out. For n equals 2, it remains open, even in dimension 3. Today, I would like to take the asymptotic point of view: what bounds can be done. Point of view, what bounds can we put in high dimensions on this number? So, what has been known before is, well, there are some estimates due to Rogers, Shep, and Zong. So, there's this inequality, for instance, that this number of translates needed is at most. So, here is volume of K minus K. Okay, I'll use the absolute value to denote the volume. So, here is this. Volume. So here is this ratio times another quantity related to the convex set K, which is, again, I'm not making too precise, the covering density of the whole space which translates of K. So yeah, so there are two components in this estimate, right? The volume piece and this covering density bit. So Rogers showed that the covering density is at most n log n. is at most n log n plus log log n plus 5n plus 1. Okay, and then we can handle this volume bit separately. When k is symmetric, it's very easy. k minus k is equal to k plus k. Its volume is 2 to the n times the volume of k. So this is just 2 to the n. So in the symmetric case, well, the best upper bound is almost there, right? To the desired answer to the... The desired answer to this dm. So it's off by this n-log n factor. In general, we have Roger's Shepherd inequality, which tells us that this volume piece here is upper bounded by 2n choose n, which is an optimum, by the way, if you just care about this ratio, it's an optimal bounded 10 for the simplex. And that gives you an upper bound for n of k of the order 4 to the n root. order 4 to the n root n log n. Okay, if you get a constant if you care about it. Alright, so what are we going to do today is okay so this is again what is known for to the n root n log n and our contribution is incremental incremental improvement of this of this lower order term. Okay, we can improve it to something sub-exponential. So Sub-exponential. So I'll try to sketch this upper bound. And of course at most absolute con okay, C little C capital C will denote absolute constant. So how do we go about showing this? So okay, so I'm sweeping a lot under the rug here. I'll just take as a black box this beautiful inequality. Box this beautiful inequality, which was derived by Shiri and Boaz, and then improved, and with a different approach handled by Martin Nassodi. So there's this following general bound, that the covering number is at most absolute constant, times for certain quantity infimum over trans of the points x and Rn of X and Rn of volume of K divided by the volume of K intersected with the reflection of K about point anyway so you reflect about a point but you get to choose the best point about which you reflect okay in this in this component and then there's this additional factor 2 to the n n log k log n 2 to the n n log n. And I will study just this inequality today. I mean this quantity. I would like to put some bounds on this incimum. So this is known in the literature as the Kervina-Basikovich measure of a symmetry. Right, if k is symmetric about the origin, we take x to be the origin and we get We take x to be the origin and we get one, right? No discussion. But how big can this be, right? In in dimension n? How big can this can this infimum be? So I will try to sketch a proof of results saying that if we even okay, if we want to be explicit about the choice of x, we'll choose x to be, we'll reflect about the barycenter. Will reflect about the barycenter of K. So the theorem that I'll show is if K is centered, so the center of mass is at the origin, then this ratio is at most 2 to the n times sub-exponential correction, e to the minus c root n. And then if you shove this in here, you get immediately the estimate for the covering number, of course. Estimate for the covering number, of course, right? This n log n is that goes away because of this sub-exponential correction. Alright, so once again, so these are the quantities we care about. I'll denote this Kevin-Basikovich thing by delta of K. And delta of z delta zero of k will be the one where we specifically choose the the best point to be uh the w reflection about the embarrassment, right? Reflection about the barycenter, right? Okay, so I should mention that the version with the barycenter was upper bounded by 2 to the end by Milman and Paja. Okay, and we will show, again, just to recast, we'll show this upper bound. 2 to the end times some will shave off a little bit. Unfortunately, we can't improve the base of the exponent here. However, The exponent here. However, not too ridiculous conjecture perhaps here is that simplex is the worst case scenario. And then indeed, if you do the computation for the simplex, delta 0 of k has a better exponent for the simplex. It's e over 2 to the m. So if you could crack this conjecture, you would improve the base of the exponent in the Rogers' upper bound for... In the Rhodesis upper bound for the covering number as well, right? It won't be 4 to the end anymore, but a little bit better. Okay, so how can we go about putting some bounds on this measure of a symmetry? So, okay, as a warm-up, let me try to convince you that delta of k, the version with freedom of choosing the point about which we reflect, is at most 2 to the n. And this can be done by Be done by rather straightforward averaging type of argument. Okay, so suppose the volume of k is 1 and let's denote the indicator function of k by f. So what is the supremum norm of f convolved with f? Well, f convolved with f at point x is this expression, right? I just wrote down the definition of the convolution at point x. Point x. And then we take soup over x. But if you look at these, this is this integral just picks up the volume of k intersected with x minus k, right? So we take soup over x of the quantity that sits in the denominator here exactly. Alright, so let's record this on one hand. On the other hand, here is the averaging type of argument, right? Let's have a look at the integral. Let's have a look at the integral over the set 2k of f convolved with f. So this is upper bounded by the maximum value of the function we integrate times the volume of the set over which we integrate. But f convolved with f is supported on 2k, so I can write this as equal to the integral over Rn of F convolved with F, which then spits out the volume of K. spits out the volume of k squared, which is 1. Here we get 2n times volume of 2 to the n times the volume of k. So this tells us that what, this convolution is lower bounded by 1 over 2 to the n, right? From the last inequality. Which is exactly the statement that delta is at most 2 to the n. Okay, so what if you want to improve this a little bit? You can imagine. This is a little, but you can imagine, you know, averaging in a better way, right? You can maybe average over a more carefully chosen set than just 2k, which is the support, right? You can choose a better set. What if you care about the point about which you reflect? So you can also choose a, sorry, and you can sort of choose a different proxy if you want. So here the proxy was like the L infinity norm, L1 norm. So let's try to. So let's try to use the entropy, okay? So again, this is our goal. We'll bound now the quantity when we reflect about the barycenter. So the entropy of a random vector x with density f is minus integral over Rn f log f. Okay? If you want to look at this probabilistically, this is the expectation of minus log f of the vector x, right? Evaluated at x. Evaluated at x. Okay, so here is the first basic lemma which will let us put some bounds on delta k in terms of the entropy. If x, y are IID uniform random vectors on k, then log of delta 0 is upper bounded by the entropy of x plus y minus the entropy of x. Let's try to see why this is true. This is true. So the left-hand side as well as the right-hand side are a fine invariant in this statement. So we can, you know, we can make some adjustments that will be convenient for us. So let's assume the barycenter of k is at the origin, which means the expectation of x is 0. Let's assume the volume of k is 1, which means that the entropy of x, which is just, for uniform vectors, just log of the volume, is 0. It's 0 under this normalization. Okay, so what, so S of X is 0, so what are we trying to prove? Delta 0 again is the volume of K, which is 1 divided by this thing. So we want to show log 1 over volume of K intersected with minus K is upper bounded by the entropy of X plus Y. And why is this true? This is just a consequence of convexity here. Just Jensen's inequality. Because what do we know? F is log concave, F convolves with F is log concave. F convolved with F is log concave, so minus log F convolved with F is convex, right? This is just per copper line there. So if we use this convolve, okay, now if we write the definition of the entropy of X plus Y, we get the expectation minus log F convolved with F, that's the density, right? Evaluated X plus Y. So this is a convex function, so I can put the expectation inside, right? Just Jensen's inequality. So I get greater than or equal to 0. So I get greater than or equal to minus log F convolved with F evaluated at zero, right? Because we put the barycenter at the origin. So we get, but F convolved with F at zero, remember the formula from the previous slide, right? Well, this thing will just give us the volume of K intersected with with minus K, right? So there we go. We prove we prove this. Okay. All right. Okay. Alright, so this is our lemma one. Okay. Now the second step is we want to upper bound delta zero. We we use the entropy. So now it's time to bound the entropy from above. How do we do that? There's a very standard way of upper bounding the entropy. So it's just again consequence of Jensen's inequality. I wrote it as lemma 2 here. Entropy of a random vector x is upper bounded by expectation of minus expectation of minus log h of x plus log integral of h, where h is any arbitrary non-negative function, integral. In the definition of the entropy, we have entropy equals expectation minus log the density. And this lemma says that if you swap this density for any other function you like, you don't get anything smaller than the entropy. And if the integral of the function you choose is not one, you pay. Of the function you choose is not one, you pay some price for that. Okay? And this is really just Jensen's inequality. And this is what we are going to use. So lemma 1 and lemma 2 combined, if we just treat, you know, this, this, s of x is just the volume of k, so you know, this is something concrete here. We are not going to touch it. If we use lemma 2 to the entropy of x plus y, we get the following correlation. X plus Y, we get the following corollary. You will have to also play a little bit about, I mean, we know that X plus Y is supported in 2k. So that's why here, instead of integrating H over Rn, we can integrate over 2K. So that's the corollary you get directly from combining these two. The quantity... Oh, there's a type, alright? I'm sorry, it should say log delta 0, okay, is less than or equal to this stuff. For any function. To this stuff. For any function h, non-negative function. Now we get to choose h and play with this, right? So if we choose a function which is constant on Rn, equal to 1, okay, what do we get? Again, there's a type which should be log delta 0 of k. Less than or equal to this term vanishes because log 1 is 0, right? So there's no expectation. We just have log of this stuff which... Just have log of this stuff, which is what? 1 over volume of k integral over 2k gives you volume of 2k, so you get 2 to the n. So this simple choice immediately recovers previous bound on delta 0. And now let's try to improve it a little bit with a better choice of h, right? You agree that constant functions shouldn't do to do spectacular things, right? So okay, so how to improve? Okay, so how to improve? So suppose again, because you know left-hand side is a fine invariant, let's assume that X is centered. Let's suppose that the covariance matrix is the identity. Okay, so let's say X is, so we say X is isotropic. So what's the philosophy here? So we know what the best function H is, right? The best function H is just simply the density of X plus Y. That's the best choice which one. That's the best choice which which won't cause any loss in lemma two. But we don't we don't we can't do much with that function. It's not explicit, right? What is the density of x plus y? We don't know. But the idea here is that, you know, you take sums of independent copies, you get Gaussians, right? So that's the idea. Let's choose a function that will be close to the density that we want to approximate with h. Let's say it's Gaussian. Let's see what happens with this function. So now we will use h, which is So now we will use h, which is e to the minus lambda Euclidean norm of x squared over 4. We choose something proportional to standard Gaussian because we align it with the covariance structure of x, which was fixed to be the identity, right? Okay, and then we'll try to optimize over lambda, usual story, and we'll see, right, if we can improve a little bit this 2 to the n. Okay, so that's the whole idea, right? The density of x plus y is almost Gauss. y is almost Gauss. So if we plug in this function in the corollary, what happens? Well the first quantity becomes just lambda n over 2, okay, because minus log h is just Euclidean number of x squared and covariance matrix is the identity, so we get precisely this. Now the second term, what is the second term? It's we change the variable so that we won't integrate over 2k but over k. So we have log 2 to the n over volume of k. have log 2 to the n over volume of k integral over k of h of 2x. h of 2x is precisely e to the minus lambda x squared, right? And because we divide by volume of k, I'll write it probabilistically as the expectation. And this 2 to the n comes out of this factor. So we get the bound for delta, oh here it's correct, log delta 0 is at most log 2. log delta 0 is at most log 2 to the n plus lambda n over 2 plus log of e to the minus lambda x squared. Alright, so our dream is to make this the last two terms here together negative. We get to choose lambda, right? And once we convince ourselves that there is some lambda which will make this negative, we'll improve to the dm. So let's see why how How can we show that this? Okay, so the final claim here in the argument is that there is a choice of lambda of the order absolute constant over root n such that the quantity we care about is at most log of, okay, absolute constant e to the minus c root n. And this will come from the fact that, okay, x X, the Euclidean norm of X is concentrated, right? This is the tin shell estimate that we have tinial bound due to Gedon and Millman, which tells us that, you know, probability, the chances that the Euclidean norm of x deviates from, you know, the second moment by an amount greater than t root n, okay, decays in this way, right? This absolute constant e to the minus. This absolute constant e to the minus c t cubed times root n. Okay, and this is valid for every t between 0 and 1. So that's the idea. x gets e to the minus lambda x squared, we will have concentration for that. So we split this expectation into two parts, right? Where x is small and where x is large, okay? Where x is small, we know it's a small probability event. So we just upper bound this e to the minus by 1. e to the minus by 1 and where x is large e to the minus is small because x is large. So we neglect the conditioning here we just have the deterministic bounds coming from this restriction, right? So we put the bound on this probability from potential estimate. So we get some explicit quantities AB and then you just get to optimize, right? So you have to choose lambda. So you have to choose lambda, you have to choose t. So, okay, something pretty close to optimizing this happens when a equals b, and then lambda is of the order 1 over root n, okay, times this constant. And then you play around with t to absorb this linear factor. Note that this linear factor, because of our choice of lambda, is now of the order root n. Okay? And to absorb it, you can absorb it. And to absorb it you can absorb it with one of these two guys and then and then you get uh you get the the the lemma and and that's it right so so this is just right we we shoot from heavy alternative right this is powerful weapon potential estimate that does the job for us okay I keep scrolling okay uh this is the last slide so let me make a few of Slide, so let me make a few final remarks here. Okay, so right, so we were trying to choose a good lambda, right? If we manage to say that there's a choice of lambda such that this is, you know, we show that it's at most log e to the minus c root n, and that gives us subs exponential improvement. Suppose there was a choice Suppose there was a choice with this approach, which would give us a constant improvement in the exponent. Well, this sort of statement turns out that it would imply, or almost imply, slicing. If you had this claim for every uniform vector x on a convex body k, then the isotropic constant would be bound. And there's an argument which can link this sort of statement to isotropic constant. To isotropic constant, and there would be a logarithmic bound. So I think this approach is okay, maybe it's doable, but it's quite difficult, right? Okay, then some technical remarks. Okay, you can try to generalize this to some, or if you know something more about k, can you get some better bounds for the covering number? So let's say if you knew that k was psi alpha, then you can improve. then you can you can improve again, uh you can improve the bound on delta zero and consequently you can improve bounds on the covering number. Particularly if you know it's psi two, you get e to the minus constant n, n. So the desired exponential improvement will take place from psi 2 only. And you can also think, okay, we get to choose this function h, right? Maybe you can choose some totally different functions which will do something good. Different functions which will do something good to you. And recall, the Euclidean choice, I mean the Gaussian function, worked well because we had concentration. If we chose h instead to be e to the minus lambda, the gauge function, okay, okay, there are also concentration results for these guys. And I can give you some exponential improvements for k's which have positive modulus. Which have positive modulus of convexity. So it's exactly the same approach, you just use different tools here and there, and you can get, of course, the exponential improvement will depend on the value of this module convexity. And the last remark is just more like a fun question. So remember, this quantity entropy of x plus y minus entropy of x, where x and y are iid. This is the thing. IID. This is the thing that we want to maximize. We want to say that this is less than or equal to something. If you like, you know, elementary questions, I don't know the answer to this problem, sharp answer, even in dimension one. What is the maximum of this quantity where you take the max over low concave random variables? So dimension one concave density. This has some, it's less than or. It's less than or I conjecture that the extramizer is standard exponential. I don't know how to show it. And I think that was the last slide. Thank you. Any questions such as Lisa? For the last question, I will try localization, dimension one, I think it's exponential. Yes, so it involves F controls with F. It involves F convolves with F in a very non-linear way, right? So I don't know how to. The computation would be too rough, right? I don't know how. I don't think you can, because it's a non- doesn't have the right structure to start. Of course it's linear, right? It's the entity. The entropy functional is okay, but entropy of x plus y is the guy that causes trouble here, right? Because it involves f convolved with f. That's the nasty nasty questions, suggestions? If this is not the case, let's take it. And we'll have a bit of a break for 30 minutes.