So next speaker is Casey Mendel. And whenever you're ready, please begin. Hi, it's great to follow Renee because she's giving you an overview of transient astronomy, which is very useful. So I'm going to talk about a particular type of transient, which we think will be really important for addressing current issues in cosmology. And these are called strongly lensed supernovae. And I'm going to talk about a model we've developed. This is work led by my second year PhD. My second-year PhD student Erin Hayes, along with several postdocs. And I guess you guys should all apply for grants so we can hire her in two years. Is this plug in? Yeah, okay. So there's something in cosmology called the Hubble tension, which we discussed yesterday at the transience discussion group. But the basic idea is that when we try to measure the expansion of the universe, we try to constrain the rate of expansion encapsulated in the Hubble constant. Constant. There's basically two major ways to do it. One is by looking at the distances and velocities of relatively nearby objects to calibrate the distances of further objects, to calibrate the distances of even further objects. And at each point, there could be errors in the calibration. And the value you get, which is about 73, conflicts with what you get if you study the radiation from the early universe and use our knowledge of the cosmological model to extrapolate. Of the cosmological model to extrapolate to the present day. So, this is kind of a big deal because if there is in fact a discrepancy and the data is correct, then it could suggest we don't understand the physics and universe. That'd be kind of a big deal. Or there could be a lot of bugaboos in each side of the data analysis. So, we need independent ways from those methods to actually check the estimate of the public constant. And so, one promising method is strong ELAN supernovae. Strong ELAN supernovae to do what's called time-delay cosmography. And so here's a pretty picture of a few systems now that have been observed. And so we've been, my group has been involved in about two of these. And you can see in each of these pictures there are spots, usually in groups of two or four, which are point sources which seem to be replicated in a pattern. And those are basically supernovae, so exploding stars. Of exploding stars, that where the light has passed through the mass of an intervening galaxy or galaxy cluster, and that light gets split into different beams and produces four images of the same supernova. And so here's a clever animation, which is now playing, which shows how this works, where this galaxy in the middle acts as a lens and makes it appear that there's four images of the same supernova. An important thing to note is of those four pathways you see forming there, Those four pathways you see forming there, they actually cover different distances because they're slightly different curvatures, and which means that there will be a delay between the time that the explosion will appear in each of the four replicated images. And that's useful because it turns out if we measure the time delays between the supernovae in the different images, we can get at the Hubble constant because that time delay is proportional to something called the time delay distance times some model for the lens mass, and that time delay is distance. For the lens mass, and that time-delay distance, in fact, is inversely proportional to the Hubble constant. So, it gives you a way to measure the Hubble constant independently of the more two standard methods. So, just to give you an intuition of how that works, these are the light curves or the graphs of brightness over time of two images of one single-lens supernova. This is not real data, by the way. This is a cartoon. But the main thing to see is that. The main thing to see is that one of these images is delayed relative to the first, and that's what we want to measure. And the other thing is that, in fact, there is relative magnification between the two, because the lens can magnify or demagnify, in the case demagnify one of its relatives or the first. So those are two effects we need to model, but there's something, there's kind of a problem called microlensing, which in fact is the problem that that lens in the middle, which is the galaxy. That lens in the middle of the galaxy has stars moving in it. So, if a star passes through the line of sight of one of these images, it can add additional lensing to the problem. And so, if we take this picture and then imagine a star has passed through the line of sight of one of the images, this constant magnification actually becomes time-dependent and it will change the shape of the, say, the second light curve relative to the first. Light curve relative to the first. And that's a problem because if you were just being naive and saying, well, I'm going to measure the peak of one and the peak of the other, and say the difference of the time delay, well, now this microlensing has actually shifted the peak of the second image, so you get the wrong time delay in fault. So that's a problem. So we need to model that. And people use kind of realistic simulations of stars moving in galaxies to produce magnification maps and figure out what the microlensing signal looks like that's computationally intensive. Look like that's computationally intensive. But if you kind of look at the results, a lot of them look kind of like S-shaped curves. So we decided, why not just use a sigmoid function, which a lot of statisticians and machine learning people might know from neural networks. So we're just going to adopt a useful and simple prescription with some parameters to put into our model. And so we've developed this framework called Gaussen, which basically has these features. It models the underlying flux-light curve with brightness. Lying flux light curve with brightness light curve known parametrically as a realization of Gaussian process, which means that we don't need to know the type of a supernova because different types of supernovae, there's several different types, have different shapes of the light curve. But to know the type, you actually have to get a spectrum, which is hard to do, as some people have mentioned already. So we're just gonna model it in kind of a non-parametric way. We're gonna treat the microlensing and the time delay. Microlensing and the time delay within the same model, so you can marginalize over the different parameters and different effects. We can apply this to any number of images, two or four or five, in any number of bands, which is different colors of light, so we can get a full treatment of uncertainty. So, just as this is a dentist, these are simulations for a future space telescope called the Roman Space Telescope that had been produced with realistic. With realistic cadence and depth and microlensing treatments. So, we're just going to apply our method to this. This particular example simulated supernova observed in three bands, those are infrared bands actually. And so, when we apply it, we can do it using SS sampling, using Josh's, Josh, using Dynasty. Have you guys heard of it? Have you guys heard of it? So it works pretty efficiently, only takes less than 20 minutes to analyze this particular supernova. And we can get a contour plot of marginal posteriors and marginalize over, in fact, that microlensing signal to get pretty good recovery of the time delay. So actually, there's like thousands of simulations that have been made. And so we can, for each type of supernova, we can measure the error difference. Measure the error distribution of the fitted time delay versus the truth, as well as that weighted by the uncertainty to see how well we're capturing the uncertainty. And just focusing on type 1As, which are my favorite type of supernova. We actually compared to a template method where you have to know the type, our method, when you don't have microlensing or include the sigmoid model for microlensing, actually improves the coverage of recovery so that we're reflecting the uncertainty of the The uncertainty of the time delay estimate better to better. So it approaches what you would expect for a Gaussian, about 68% or 95%, well, slightly less. That's pretty good. Now, the Roman is in space, so you have more control over the cadence. For an observatory like LSST, it's on the Earth, so you have seasons, you have weather. So you get more challenging time series where there's gaps and irregularities. Those gaps and irregularly an incomplete time series. So we can apply it to that and it works well there as well. So we can also get good recovery time delay. It's a bit tarter and more uncertain. So that's basically it. We've developed a stoical model which has a non-parametric part to describe the underlying light curve without having to get the spectrum of the supernova. And we can account for, we can compute fully Account for, we can compute fully Bayesian uncertainties on the time delay when Marshall is in over the light and light curve model and the microlensing prescription. And it works pretty well. Oh, and there's a thriving group of community of astrostatisticians at Cambridge. And if you want to join them, there's in fact a job opening here where the deathline is indeed on Sunday. So I'm happy to talk to you about that. Time for