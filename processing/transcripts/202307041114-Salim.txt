Organizers for inviting me to this workshop, this very beautiful place. So, I'm going to talk about non-notiation for single-cell RNA-seq. Just a recap. I mean, typical thing with single-cell RNA-seq is I can't see it now. Anyway. Anyway yeah, I don't see the it was there, but it's gone. Okay, anyway, look, you you sequence your cells and then usually what you get at the end is a matrix here where features or genes are in rows and cells represent the columns. And you can do clustering using some low-dimensional embedding, and then to get some marker genes, you do differential expression. Micro genes, you do inertial expression analysis. Now, before I continue any further, just like to clarify what I meant by normalization, and here is removal of all kinds of unwanted valuation, not limited only to library size. So that includes artificial and what's not. Um motivation why we want to develop another normalization method. There's been a lot of things out there, but There's been a lot of things out there about normalization. First, we noticed that current normalization methods remove biology when unwanted variation, so the batch effects, the library size, are associated with the biology itself. Second, most methods also only return what we call the dimensional deduction or the seven bearings, and that's if you want to do the analysis. That's if you want to do the analysis, that's that's unsuitable. Okay, so with RD53B, the aim is we want to take into account the potential association between the biology and anodic variation and the return-adjusted data for all genes. So, you can use this for downstream analysis like the E or even pseudo-HON. Alright, to give you an example, what I meant by removal of biology, when Biology when biology and modification is associated. Here is the data from non-small cell lung carcinoma, 10x data. So I normalize this using SA transform. This is colored by block library size. And you can see that epithelial cells there has much larger library size compared to the rest of the cells, right? Because it's just bigger. So the biology here is correlated with library size. Library size. Now, you use IUV3NB, one thing that you'll notice that now, see here that the monocyte is almost kind of joined up with the rest. Here, we managed to kind of separate monocytes better, and the ectophilian cells is also more tightly clustered. So, here we still do some kind. We still do some kind of normalization. So, I wouldn't say I don't do normalization with this kind of data, but our normalization is making sure that we do not remove all the biology associated with library size because that biology is related to cell type separation here. In fact, if you look at the biological silhouette, you'll see that if you do You'll see that if you do scan or modification only, that's actually quite good. And AdivitLB managed to improve that. If you use, sorry, that's Dyno, sorry, that's Dyno. And if you do scans here, that's not too bad. I mean, Dyno and Artific DND managed to improve that. But if you use, say, SI transform and ZIND with geophysic uses with an ecological silvent in that data set. That data set. Okay, this is the second example. Cell line study. You've got two cell types, HuelCut and 293, again the next data. Three batches, but each batch, two of the batches only contain one cell type. And you can see here that there's a strong batch effect, right, for this one cell type here. You can see the cell type separation, but for this particular cell type here, you can see this being separated by bar. Have been separated by batch. If you use that, you see it removes batch effects, but it also removes biology. So with IBP to the NB, we managed to kind of remove the batch effects at the same time by uh preserving the the biology. The biology. And that's the UMAC version. That's the PCA. So how does RDV3ND actually work? So I'm talking about the model here. So we're using negative binomial model. So imagine you have n cells. So your negative binomial mean, okay, so mean, okay, so the mean will be a vector. So each cell will have its own mean. So for a particular gene, this is model for a particular gene, each cell will have its own mean and a gene-specific dispersion parameter. So to run VP3NB, we need to have the cell state information. So cell state information can be cell type, for example, or a combination of cell type and treatment. Of cell type and treatment factors, basically the biological factors that you care about, that you want to preserve. So you need to have this information for about 3,000 of cells. And you can get this for cell types, you can do annotation using software in silico using software like Singlet, for example. Get highly confident annotations after just some simple library size normalization. Decise localization. For other factors, of course, you will know because if it's treatment, then it comes from your design of experiment. So with the annotation, so cells are separated into two parts, the annotated cells and the unannotated one. For the annotated cells, the model looks like this. So you've got intercept, and here is the matrix that contains the biology. Contain the biology basically, right? Just a dummy, dummy framework representing biology. And WA here, that's just to represent the annotated set. That's the unwanted factors. And alpha is actually the effect of that unwanted factors on a specific gene G. So there's a gene-specific effect of the unwanted factors. So if your unwanted factors is batch effects, then Factors is batch effects, then the batch effect affects its gene specifically. And that's W is unknown. We do not make any assumption about W. That will be estimated from the data. For the unannotated cells, it's very similar, right? Obviously, you still have to estimate W for that unannotated cells, but here, because it's unannotated, Because it's annotated, we do not have the M matrix there, so we're just going to have a kind of an additional intercept for each specific gene and cell. And in addition to that, we also assume that we have some negative control gene sets where there is no biology. So you see that it's able for unutnated and unanotated cells. Annotated cells, we do not have the beta there, there's no biology, right? So, why we need the negative control? Set that's for estimating W, right? I'm not going to go into detail about that. And why we need the annotation for some cells, and we need the M matrix that's for estimating alpha, basically. So, the alpha estimation comes. So, the alpha estimation comes from the annotated cells, and for estimating W, we use the negative control set. So, how do we get the adjusted data? We get the adjusted data by, so the input is count, okay, but the output is also count. So, what we do is we fit the model, we got all the parameter estimates, then we take when we calculate the We take or we calculate the percentile for this gene and cell under the negative binomial, that negative binomial model. Once you get the percentile, then you invert that percentile to get a negative binomial quantile under a negative binomial. Under a negative binomial distribution, where the unwanted variation now, unwanted factor now has been set to the average unwanted factors. So instead of having cell-specific unwanted factors, all cells now have an average unwanted factor. So in a way, they don't have any unwanted factors at all because everything has been set to average. Right? So this is the workflow. Basically, you've The workflow basically you've got the raw counts, and as I said, you need to identify the subpopulations to get that M matrix. But then you'll go through this interactively weighted least squared iterations where you estimate the alpha and the W with the aid of the negative control gene sets. Once the iteration finishes, you get a normalized adjusted count, and that normalized adjusted count. And that normalized adjusted count can be used for a variety of things like clustering, pseudo-climb or DE. So, as I said, the parametric estimation, we are using iterative real d square. So, these parameters, the first here, I mean, they are estimated all of this here, they are estimated using. They are estimated using only the annotated cells, so that's pretty quick because I only allow a maximum of 3,000 cells that are annotated, and then for the rest of the parameters, you estimate that using the unannotated cell. So that allows the algorithm to be relatively quick, I mean about 25 minutes for 100,000 cells on a 16-gigabyte RAM. Okay, so just to recap, to run RTV3 and B, we need the M matrix, okay? This is, and then we also need negative control gene sets. And we also need to specify the number of unwanted factors. And I'm going to show you here that RDV3NB is actually quite robust against all of this required. Okay? And we'll go back to And we'll go back to the semline study here. Send line studies. Remember, we do not assume anything about the W, right? But you can see here that the first W is actually correlated nicely with library size. That really tells you that you need to perform some kind of library size normalization here. The W2 and W3 that captures the batch effects, basically. Just the batch effects, basically. You look at the biological silhouette. You can see that you can vary the K in W3 and B, and the biological silhouette doesn't change as much. This is in terms of negative control sets. This is a biological silhouette, batch silhouette, and this is just correlation between or R squared between the PC and the log library size. Library size, and you can see that we have five, six here, six negative control gene sets, and performance is quite stable. I mean, overall, they're quite stable. And for DE, because the count can be used for DE, you can see again here we are comparing. Once we get the JASTIC data, we compare the same cell type across batches. So if you remove the batch effects well, you will see that the Well, you will see that the percentage of DE gene will be very low, and that's what we see here with RDV3 and B. Okay, we also have extension recently to DINB. This is not on GitHub yet. So, that has been used to kind of integrate UMI and non-UMI data because non-UMI data require zero inflation parameters, and you can see. Zero inflation parameters, and you can see it works very well merging the two batches here. And we also did an experiment where we pretend that these two batches, sorry, these two cell types are actually one, so we misspecify the annotation, the M matrix. We see whether RDP3MB still managed to get that. Separation back, and it looks like that these two clusters are still feasible there. So just like to conclude a few points. So the method remove remove unwanted variation of the cell biology when the two are associated. So it returns percentage adjusted count that can be used for downstream analysis without further normalization. Now it has a degree of robustness, okay, and it's published last year in NAR and packages of. In NAR and packages available on GitHub. Future immediate works, we want to extend this to SE Multi-Omics and also special transcriptomics. To finish, I'd like to thank these people that have contributed this work. Thank you. So we have time for one quick question while Matt gets himself set up. So anyway, my question. So my question is that I think the model has a lot of Is that I think the model has a lot of similarities with combat for batch effect removal, if my understanding is correct? Could you explain the major difference between the two? Well, I think combat assumes that you know the batch variables, doesn't it? So here you don't know. So that don't be okay. You do not assume that we know the unwanted factors. That don't be okay, basically. Yeah, okay. It can be anything. I think that's what one I think that's what one difference is that I can think of off the top of my head. And how sensitive is it to your prior specification, the Gaussian prior variance? Oh, yeah, so we use that basically reach penalty, right? I mean, the population, just to stabilize the parameter estimates. In the paper, we shown that it has some degree of robustness. I mean, against so yeah, we in the Yeah, we we uh in the in the package we have a default kind of uh parameter for that. But yeah, in the paper we show that uh i if you vary that a little bit it it's still okay. Fantastic. If anyone has any further questions, I encourage you to go to Slack and thank you, August, again. Stop.