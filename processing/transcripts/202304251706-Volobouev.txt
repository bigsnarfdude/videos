I mean, if you start off, if you combine a few flat regions, you get a very nice galaxy. Yes, I fully agree with you. I'm just saying, if you hear theorists, some theories will somehow insist very strongly that their theory uncertainties get added linearly. And the reason for them is not because they think those two uncertainties are 100% correlated, and that would be another reason I added linearly. They somehow think those are two boxes, and therefore they should be added linearly. That's all I feel. I'm just saying. I feel clear. I'm just saying this is what some viewers have in mind. So please educate those people. And then on slide 60, you've got an ellipse that for a two-dimensional situation, it's labeled delta chi-squared equals one. So that's a little bit confusing. So yeah, so I didn't explain as well. So what we do is we fit, so this is the view related I'm showing you here. We fit this thing to data, right? To data, right? I'm not showing you the data. And the ellipse is basically the data chi-scale which comes out of our chi-scat filter data. So this is the uncertainty, this is the data uncertainty in our in my prediction. Okay, so that has nothing to do with, just ignore the ellipse, that has nothing to do with my theory uncertainty. And all the points are then the varied central values that I get from. So basically, each of these, if you want, each of these points has its own, you know. Has its own ellipse from the actual fit to the data. Follow up on just a table. Each of those points somehow is representing your uncertainty, like not the point. The ensemble of those points represents my theory uncertainty. The theory of uncertainty. And at the time, you said it's just kicking the can down the road, I remember. But it strikes me that kicking down the can down the road is exactly what you want to do. If you kick the can. To do. If you kick the can down the road to the experimentalist, insofar as you can take that uncertainty, represent the uncertainty in your theory in some way, that's exactly what we mean, right? Okay, what I meant by kicking the can down the road is that if I, this is the scale variations, I should say. Okay, so those 243 things are 243 bad things. Okay, but you know, I'm, I confess I've been doing scale variations also all my life. I've been doing scale variations also all my life. But at least I'm doing a lot of them. So that's what I mean. The thing is that this ensemble has no statistical meaning whatsoever. I can add another 500, and even the distribution here has no meaning. So that's why the only thing I can do here is some envelope, and I don't know what that envelope means. But if you add another 500, are you saying the distribution? If you add another 500, are you saying the distribution would look different? I have no idea because what I'm doing is I'm doing 500 different B0s. And as I said, the zero is like a random whatever. It doesn't mean anything. So I can give you another 500 points, which all very closely here. It doesn't mean that there's a peak or a higher probability or anything here. That's why, with the scalarization-based stuff, that's why we take envelopes, because all of We take envelopes because all of those things hope the same source. It's just, I continuously do, I'm gonna do this 100 times with lots of numbers. It doesn't mean anything. Do you choose the numbers? I mean, you sample them randomly according to which distribution? Yeah, but what does that mean, right? Again, because it's. So, how do you choose the V0s? Are we coming? Coming to you. Because 25 years ago, some very smart physicists wrote papers that I said use log2. And he's still doing that. And they had a very good reason for that log2, which we can talk about in the copyplay. I had a question on the proposed solution. To what extent is this just kicking the can down the road? So basically, what you're saying is you're So basically what you're saying is you're parametrizing the f prime, f double prime, with a finite number of parameters. And so now you just say, okay, you have like some more parameters, you stick it into your likelihood, you can compute your predictions as a function of these additional finite number of parameters. But then your theory uncertainty becomes F triple prime, right? Ah, no no no well, okay. No. The uncertainty is still F double prime, right? only is still f double prime. Because what I'm not doing, so I'm not trying to estimate the correct value of double prime so that I don't need to calculate it. I don't want to bias correct. And I don't want to go to a pseudo NNLO order. I just want to estimate the uncertainty by figuring out how large is this F double prime. And the reason it's conceptually much better is because if I or any theorist does this, we are forced to really think We are forced to really think about what is the true size, right? What can it be? And then I can use information from existing derivative series like this to really get a good estimate as opposed to 2 times the previous one. So it makes it much more reliable. And moreover, I get correlations because I can really think about which of these, you know, I mean, these are, again, these are real. These are again, these are real parameters as opposed to fake parameters, right? And you know, W mass measurement, right? You want to know, you want to use the ZPT in order to get the WPT to get the pigeon solutical size and W's, right? You really need to know the correlations between the W and the ZPT spectrum. And you need them to like, you know, it makes a difference whether they are 99, 98, 95% correlated, right? But you have absolutely no cruise air agents. Inner agents, but with this, you will know exactly. Because I can tell you exactly: you know, this one is the same in W and C, this one is the same, this one is the same, whatever. Like, some of them. But I'm not understanding correctly. So if you knew these Fi double prime, God gave you these values, you knew what the next leading order function is. Right, if I, if I, if, you know, in five years, somebody calculated all of them to Somebody calculated all of them to me? Then I'm sticking them in, and of course, then I need to go and estimate after the prime. Now I've just upgraded my entire calculation to the next order. Yeah, so that's in some ways it's like kicking the... No, no, no, no, no. That's improving my field predictions by hard calculations. That's not kicking out. Okay, Nick Fixed. Yeah, just j just to understand that maybe it's really good. Here you're talking about estimating uh also maybe some of the next slide is on my estimating these uh parameters. Experimentalists, okay. And the correlations. But then, as an experimentalist, what I want to know is: well, is there a is it effectively something I can plug in as a prior now? So can I put in a distribution that effectively these are good estimates of these parameters and also some uncertainties of those parameters that use a theory? Or do I need to go and do some measurement to actually constrain and create a yeah, so I mean can't hide it's I'm not saying it's easy, right? I'm not saying it's easy, right? I'm just saying that what we're currently doing is not up to task. So, you know, in the case of something like the resumpted spectrum, like the PT spectrum, right, where I can reduce things to numbers, I know what I'm going to do. Let's say you're asking me about the Trajan rapidity spectrum, which is a spectrum which is much harder, then I really need to know about those functional forms of what is the NMLO rapidity-dependent coefficient. Or rapidity-dependent confliction, what can that be? And that's a hard question, right? And the point is, you know, my point here is that we need to ask a question, and by thinking, by changing the way we think about it, at least we're now in a position to ask the correct question. I don't have the answer, and in some sense, you know, my NNLO colleagues should answer that question because I'm not doing those types of calculations. But once they've, you know, so you guys should, you should go to your favorite NLO theorist and sit him down, right? Him down, you know, I took Glenn at some point, you know, we had a discussion at this workshop in Hamburg. And I like your way of saying, you know, you take your best theorists, you step them down to a chair, and you ask them, what can this thing be? And I can't do this for everything, but yes, I mean, once you have this, right, then you can say on the next slide, for example, where you had the bad debt. So here, my assumption was that this thing got Was it this thing that you've got the plus delta the gala path? It comes somehow from a combination of those parameters that you estimate. Yeah, no, so this is really just an example. So this I2 is just a specific f double prime of something, okay, which just happens to be a function of x as opposed to all the numbers, because the number of the syllabus case by function is more complicated. This is just an example of a function that we used by any questions. Let me ask, are there any questions on Zoom? Okay. So I wanted to get back to this thing you said, Frank, about the, on the next, come here. Next slide. The next one? The one that had the scatter plots with the color dots. I wanted to make sure I understand what you meant when you said that something could be escalated to within an order of magnitude and probably even to 50%. Is that the nuisance parameters themselves, or is it the uncertain theory uncertainty? The theory uncertainty would be called SPUs. I want to know what the appropriate error on the error is. So what I'm saying is that, I mean, of course, some of them are tiny compared to one. I mean, there's this whole spread here, right? So obviously my estimator is not good to get the precise value. Sometimes if you want, I'm off by a factor of 10 for some of them. But my point is that. Some of them. But my point is that, you know, this scatter plot could have looked like one is here, right, and the thing goes all over one, right? And then my estimator would be underestimated. So what I'm saying is that the estimator is, right, at some nth order coefficient is something like, you know, Cf times C A to n minus 1 minus N F C A times N minus 2 with here's an N, here's an N times N minus 1, or something like it. n minus 1 or something like don't quote me on this something like this right so this is sort of the leading colour in f dependence and just using that formula I get I get this plot okay so that basically tells me is that perturbation view basically works as you expect it to work right like at the next order you the leading colour is the dominant contribution right and you get a you what I'm saying is that you get a very good estimate of the typical size. made of the typical size. So the uncertainty and the uncertainty I would say now is really like small as opposed to with the scale variations where it's maybe 50% like or 100%. Really nice and I think that really is a way for the future of base games at India. There are any ways something similar to World Cup could be other for us today. I mean, maybe I mean if I mean if you what you need to do is you put the pattern char onto a more sophisticated level, right? So if you you know I mean let me say yes and no. Know, and maybe this is for the discussion section with our works. But I mean, in principle, you could try imagining: you know, if you add MLO type things into the pattern shower, right, then some of these coefficients you might be able to do that, right? But obviously, it's a more complicated project. Yeah, I think I should just under that part. Let's under it, yes. Well, actually for me, it was a slightly different question because. MetComplete next order, yeah. So I'm confused because when I need to implement complete next order, I hear putting the f double prime in green. No, no, yeah, sorry, no, no, that is what I mean. So what's the difference? No, no, what I mean is you need to implement the next read. Need to implement the next reading-order structure in terms of the unknown coefficients. Like in a. Right, okay, nice. Thank you. So it's, you know, it's if it's really just f double prime of 0, y and I know no nothing, then you just put that function. But in reality, right, there's cross terms from lower orders, you know, there's the scale dependence which cancels, like there's a lot of stuff that I know at the next week in order that I have to put in to make it consistent. So this connects to then what you were saying? So this connects to then what you were saying is if somebody goes and computes one of those coefficients, we can then go to our likelihoods and fix the value to that. And your questions about that. Okay. How many of those unknown parameters are there typically? Like what's the size? I haven't seen that. That totally depends. So in the ZPT spectrum, also depends how many functions you have, right? So in the ZPT spectrum, I mean there's one, two, three, four, five, six, seven. Okay, this one's fine. This form, right? But you know, once you let's, you know, here I'm also cheating because I'm looking at fixed rapidity or maybe rapidity integral. If I do the QT spectrum in bins of rapidity, then I would have sort of a couple of them like these B's, for example. So those B's are kind of related to those I's. So the more complicated you get, the more you can get the functions, and the ones you have functions, it depends on how good you can approximate those functions. Depends on how good you can approximate those functions. But, you know. Not the thousands that the experimentalists are used to deal with. Okay? So, but if I answer this correctly, in principle, we could go in today and sort of put all of this parameterization in. And then the question is, you know, how many different processes do we need to fit somehow simultaneously before we can be able to save it, right? Yes, yes. Which is going back to the question about the hydronization, which in France and early is kind of the case when you know even less about this. Case when you know even less about this I don't term or this extra term. So, this, like, overall, this reminds me of the problem that in some fields is called model discrepancy. Like, like, there's this whole area of like computer model calibration in applied mathematics and certain parts of statistics where they are dealing with this kind of model discrepancy. It's basically the same problem. You have kind of a theoretical model, which you know is not perfect, has some unmodeled things in it, and then you have your data, and you're supposed to somehow fix your data. To some other things on your data. So, what they do in that area is that, so sort of the same thing that you have some extra terms and you want to add to your model, what they do in that field is that they usually model those extra terms as a Gaussian process. So they basically say you have your theory model, there's kind of star view and model, and then there's this unknown Gaussian, there's this Gaussian process that's supposed to capture the mismatch between your theory model and the data. And then, at least to some extent, you can, well, first you need to write down. Can, well, first you need to write down a thousand-product model, and what it does is that it kind of reflects the epistemic uncertainty of this unknown term. And then, at least in some cases, you can try to learn that from your data one year the aftermath. If we have basically unknown parameters, then you can maybe somehow figure that out that we have a measurement. So, have you seen anybody do something like that? Like, has anybody tried to write those unknown terms as some kind of a stochastic process that would reflect the epistemic constraint, for example? Not that I'm aware of. I mean, you know, I can count the number of people who have started using these in their calculations on one hand, right? And most of those are postdocs of mine. Sorry, I can't resist the scatter plots as possible. I mean, finally, I mean, you know, I know you say it should be the no, not the other one, the one is the rotation. I think somehow they're not. No, I mean, you say this is based on the sampling based on the block two variation. I thought this is, although it's so debated, but still kind of the common recipe you see, right? And then what can't I still, I mean, of course that was wrong, but I could still now say using this as one sigma uncertainty, and then I take just the RS of the spread, right? I mean, of course, it makes a list out of this multi-point gauge, two-dimensional gauss. I mean, we were so hesitant, but people would still do this, right? I can't stop. No, I mean, look, you can. The problem is that if as I explained, scale valuation is conceptually flawed. Now you can go and add many different scales, right? Different scales, right? And what's happening here, right? I mean, you know, when I do scale variations, right, you know, I have six different scales in here that I'm varying. I'm varying them in certain ways, and I'm actually putting enough that I know I have enough scales to probe F primes, which actually starts capturing how many Newton sparometer I know I should be using. Okay, now in 99% of cases, people just have mu r, mu f and then you're done. But just that producing that plot on the right way doesn't trigger any task to spread of it. I understand. Yes, so apologies. We should have spent another n years on this paper and put in theory nuisance parameters, and I could have shown you that part because it's nothing because that took all the seven years. That's a good question. So, is it possible to put some constraints on the theory nuisance parameters and data? Depends, right? So, let's take a separate. So let's take the ZPT spectrum, right? So if one of them has a significant impact, let's say beyond, let's say I add my, you know, the estimator, and the impact is 1% and the fuel and the measurement is 1 per mil, right? Then yes, now the question is, what are you constraining? Are you constraining PDFs? So are you constraining my nuisance parameters? It depends on how sensitive they are. But you will eventually be able to constrain a certain direction in that. A certain direction in that space. Your basic expansion evaluates the second derivative of zero. But we know that it's an exact expansion if you evaluate for some point between zero and x, because using the Lagrange form of the remainder. The problem is that I just don't know the x-dependence there. Yes, but what I was thinking was can you bound the difference by Balance the difference by estimating the second derivative by methods already suggested, and then just allowing x to vary between zero, the expansion point, to vary between zero and f of x. Yeah, but okay, but remember is that I don't actually need to know f of x. What I need to know is f of x equals 0.1 or something. So I'm not actually trying to estimate the whole functional form. Okay, the x here is really my coupling constant, and I know its exact value. Okay, so I really only. Okay, so I really only need the function at f. I really need f at 0.1, right? So I know. So I don't really need to construct some local approximation of how the neighborhood is. But that's what the expansion does. You take f of x of 0.1, plus equals f of 0, plus 0.1 times f of prime 0, plus 2.1 squared 2 times. 0.1 squared 0.2 times some term, the second currency, evaluated somewhere between 0 and 0.1. Yeah, but I can't, the thing is, I cannot evaluate f double prime anywhere but 0. So the way the expansion is, I can fundamentally, the only thing that I can calculate is f prime at 0 and f double prime and 0 and f triple prime and 0. That's just how the expansion works. I cannot do anything. If I could do anything in between, Anything. If I could do anything in between, of course, yes, then I could do, but I can't. So I wanted to maybe connect to what Philip was saying. So now you're basically elevating them to these new parameters, right, that we just stick into our likelihood. And it's like the Higgs mass or something we don't know what the true value is. There is some true value out there we don't know. We just have it as a parameter. And so should we just basically measure those these things? Or is this something like a possible target? Is this like a possible target for experimental measurement of these numbers? I mean, so the way I would, I do not want to, so okay, I do not want to elevate them to really like a parameter of eight. Because of course, at the end of the day, you know, that parameter, so you know, at the end of the day, right, when I estimate that prime of zero, if you, you know, if I put You know, if I put a x squared here and I'm telling you to fit a, what you're truly fitting is not just a, but the whole zeros, right? So that parameter itself will really have again an uncertainty, right? So I don't want to go that far that you can now literally go and measure the three loop class per normal dimension, right? But what I am saying is that you can have a well-defined notion how the measured ZPT spectrum can reduce. Can reduce the WPT spectrum. By constraining the size of the variations of my Newton's parameters in the ZPT spectrum, I'm reducing the uncertainties in the WPT spectrum to the extent that they are the same. I don't want to go so far as that you can literally quote a number for them. And then, of course, if somebody goes calculates, you can compare and see, oh, you got twenty five plus minus five and my calculation was twenty, so ah, you're happy right. But I yeah, don't don't overdo it, right? Don't overdo it. Unfortunately, we have run out of time, so let's say thank you again.