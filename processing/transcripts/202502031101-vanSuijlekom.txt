We're looking forward to this meeting to extend and explore also these interactions and these new kind of appearances of operator systems in non-communicative geometry in the sense of comp. And I'm also very happy that Matt gave this talk before me, so you know everything about operative systems. I can kind of enrich that with many examples which arise from the Which arise from the junctions. But I will start very sort of actually also quite similar in sort of with the classical story. It will be very much pictures and so on, so it should be quite relaxing, at least at the beginning. So I would start with uh with the Dutch uh guy, which you may know which is this guy Heinrich Lorenz. Which is this guy, Hendrik Lorentz. So he was giving lectures in GÃ¶ttingen in 1910, end of 1910, about physics and about radiations of black bodies and so on. So physicists had quite a good understanding of what was going on when you look at the frequency spectra of black body radiation. And so then, sort of, the idea that arose was the following: The following is that if you think about these higher overtones, I think I will use this one. This will be this one. The overtones, if you listen to the audio, if you just analyze the overtones, so the frequency spectra, if they're high enough in this frequency spectrum of a radiating body or whatever it's resonant. Or whatever is resonating or making some sound, then the higher overtones would actually not depend so much about the actual shape of the thing that is radiating or vibrating, but only on the volume. And so then in the audience, there was Hermann Well. And in just a few months, actually, he figured out that he solved this mathematical problem, as it was. Solve this mathematical problem as it was meant to do. So that's the famous law of Vel that said, okay, actually, indeed, if you look at the number of wave numbers up to some scale lambda, that actually goes proportional, because now I did the opposite, I count, up to lambda, proportional to the volume of the M, the membrane, or the body that's radiating or sounding. And then there's some independence on the dimension. And then there are some independence on the dimension as well. So you can actually get some information, and you get some information on the volume, but otherwise, this is the behavior that you get. And I'd like to illustrate this with something I already mentioned yesterday over dinner, is that I do this with my other hobby, which is percussion. So I play percussion, and usually on these kind of drums, but of course. But of course, mathematically, you can easily talk about these membranes of different shapes to vibrate. As in the D Different problem, I'm trying to solve the Helmholtz equation where there's a fixed boundary, so I have a DFTA condition at the boundary. And then you can just analyze and compute, as an exercise, what will be the spectrum. And I plotted here the first 25 frequencies. And you see, indeed, you already kind of get this idea of the law of value. Of the law of well, some evidence that indeed they grow similarly. They're of the same volume, of course, area. What you also see is that if you listen carefully enough to these drums, and of course these are overtoned, so it may be difficult to get that, that they are different actually. So you can hear the difference, the difference between these three shapes. So given that you know it's one of these, then you could actually theoretically Actually, theoretically sort of figure out if it's a triangular drum, the square drum, or the circular drum, which is sounding. Of course, then we jump to the other famous question in mathematics, 1960s, Mark Koch, that actually put that as a paper. So, can we hear the shape of a drum? It's very nice. Actually, the answer was already known at the time, but in any case, this is a very nice description of what can you actually do. Of what can you actually reconstruct from the vibrating spectrum of a membrane of some shape? And more precisely, this can be expressed for manifolds through many manifolds. Does the spectrum of wave numbers, K, in this Helmholtz equation, determine the geometry of M? So if you have a list of numbers, just numbers, can you get to M? So that answer that was around actually That was around actually um they saw it snow because there are uh isospectral drums so wi which are actually identical. And again, they will be not very easy maybe to fabricate, but theoretically they can well you can just solve or at least but you can do not solve of course you can sort of move one eigen function to the other side and make it in such a way that it actually is compatible at the boundary and that all. It's compatible at the boundary and at all intersecting triangles or the connecting triangles. Here's the first twenty-five frequencies which are the same. So the answer is no in that sense. And you need more information if you want to sort of capture the geometry. And this is sort of what I want to explain to you. And that's also the underlying principle of non-commutative geometry originating in spectral geometry. So I would start again. We jump Again, we jump back to the 1920s or so, because these numbers of wave numbers, which appeared as k squared in the Helmholtz equation, so I'm interested in the wave numbers k. And so, actually what one should be doing, because also these plots are there, is to take some kind of square root. And then I would like to explain in that context. So so k squared is an eigenvalue of this Laplacian that I showed you on a on a manifold m. On a manifold, M. And the Dirac operator is a square root, so that the eigenvalues are actually those wave numbers, k. Now, that's not a very strong kind of motivation, maybe, to analyze such a Dirac operator. In fact, Dirac, the person who sort of invented it, who found this solution, he was trying to kind of address the following question. If you look at the Schlerdingen equation in quantum physics, then there's a first derivative in time on some wave function describing. On some wave function describing the distribution of where a particle is. And the right-hand side is a second-order in space derivative. So you see a mismatch, and if you take kind of the idea at the time, 1920s, series, that space and time should be merged into one object, so there should be some symmetries between space and time, like in special relativity. And you have to deal with that. So either you make the derivative in time like second order, that was a solution also around Klan-Worder. A solution also around Klein-Worden. That's what you get. And the other thing is to take the square root on the right-hand side. So take the first order derivative in space on the right rather than squaring the time. So that was his motivation, and he succeeded. So let's do that in the very kind of cases where we know mathematically very well, and I don't have to do too much kind of physics as well, what's going on. What's going on? So, this is the Laplacian, say, Laplacian on a circle, minus ddt, and it's describing how the kind of string, well, as in a circle, would vibrate. And so if so the this is a positive operator, so the the eigenvalues are like k squared, and I want to know the an operator with eigenvalues k. With I can produce k. And so, what I have to do is to take essentially a square root of this operator, so it should be d dt, and then there's a few choices you can make to get to minus, and that's either i or minus i, and for some reason I chose minus. And the square is exactly Le Passo. So you can analyze the spectrum. Of course, this is a Fourier theory, easy IST. Just pro like this, and maybe that's also kind of preparing for something that is to come. That if you consider this operator, the ddt, as an operator on L2, of S1, and you look at functions as well as operators on L2 of S1, but they just act by pointwise multiplication, and the commutator of this will be the derivative of the function f, because you just have sort of a chain rule, a product rules for you to get to this. To get to this, which is a bounded operator. So, this commutator, even though this D is unbounded, the commutator will be bounded if F is C1 or smooth. Okay, so let's make it slightly more complicated. Two-dimensional torus or two-dimensional R2, it works in the same way, except the spectra may be different. So, the Laplacian is now the following: minus d dt uh d d t one squared and d d t two squared. squared, d d t squared. And now the task is to write an operator which is something like, well, it should be first order. So I have a ddt one and a ddt two, and I have some A and some B. And it should square to that that operator. And that was sort of the the exercise that the rock was was facing. Maybe The rock was phased, maybe in four dimensions, but that's the same sort of problem. And of course, if you square this operator naively, you get a squared should be minus one, and b squared should be minus one, and then the cross term should vanish. Then you find, okay, this is impossible to solve. I cannot find an operator like this, such that it squares to the Laplace. Then Dirac came up with a nice solution to kind of extend, of course. To kind of extend, of course, what these A and B could be, because they could be matrices. So if you take them to be these matrices A and B, like that, then it actually works, because A squared still minus the identity and B squared still minus the identity, and they're anti-commute. So they actually, the cross term that I sort of struggling with, it just disappears. So this is the type of operator should be should be looking at. And then if you compute it, it could be, I think I have it. I think I have it. It's a 2x2 matrix with derivatives in it, and it squares to this operator, and that's actually times the 2x2 identity matrix. And so this means that this operator D is not acting on L2 or something like this, L2 of S, L2 of T2, two torus, but it's actually acting on like vector-valued L2 functions. Two functions. And you can solve what the spectrum looks like. Well, of course, we know it because it's these wave numbers, and we know that what they are for the Laplacian on the torus. So you get square roots of these numbers, n1 squared, n2 squared, and the commutator is actually precisely if you compute what the commutator with f is, and you compute the norm of that, use maybe the C-served identity, you find that you get the leap norm of F. So you control by putting it. So you control by putting it into a matrix form the derivatives in all directions. Of course, that's what I would like to have, because I want to control not just in a single direction and letting it be sort of free in another. I want to control it in all directions. And so again, this is two-dimensional, so you have a similar kind of parabolic shape. So this is now I jump, so I have like a circle, torus, and then the rest. So this is my So, this is my general situation where you have this, which is when the Riemannian manifold is actually a spin manifold, or actually spin C before it as well. And then you have a differential operator in this L2 space, and there's not an L2 of M, but there are an L2 of some spinner bundle. And that's sort of replacing this C2. And in fact, the solutions of the equations I was looking at, I had AB, got BA equals to zero, and so on. But PA equals to zero and so on, and they square to minus one, they are Clifford relations. So the underlying algebra is Clifford algebra that you have to sort of find and that you can do and realize them to act on some spinner bundle only for a spin C manifolds. So that's exactly defining a spin C manifold. Spin manifolds, it's an additional uniqueness of this operator. The point is it squares to the plus. Squares to the Laplacian almost because there is a sort of a defect. And that's also nice because you can interpret curvature as a defect now. So kappa is a scalar curvature of the manifold. So you do get something which is close enough, it resembles the La Gaussian, plus a zero of order term. And indeed, this is a general result that this commutator of this first-order differential operator with matrix valued coefficients, if you take the commutator with f, If you take the commutator with f, you get the leap norm of the function f. Leap norm being the smallest leaves constant. Okay, so any questions, remarks are very welcome, of course. There will be a steep curve at some point, as you may expect. So, let's think in this Think in this context of how you get like how to resolve that question I started with. Can you hear the shape of a drum? Apparently, you cannot. We can add some information to our set of data and see if you can get a metric back, for instance. So you get this sort of tackle that these isospectral domains that actually you can kind of listen to it if you add some local information. Local information. And so, this I will do as follows. So, first of all, to measure the distance, as usual, we just take lengths of paths between these two points. Now, that's a very geometrical kind of approach, and I want to sort of let that go and to have everything sort of defined in terms of operators on order spaces. But what you could actually do is instead of measuring the distance between x and y, you would move to function space by taking function values of f of x and f of y. Values of f of x and f of y and compare these to one another. If you take the absolute value of f of x minus f of y, this is actually the distance if you let this gradient of f, if you have that under control, so you keep it small or equal to one. And that gradient happens to be exactly what I can capture by this commutator with dm. So that's this deep constant that controls the gradient. What you find is that this distance that I'm trying to capture is actually Trying to capture is actually the following. It's the supremum over functions with controlled Liekball norms, so these are what it's commentator about. And the delta of x, these are the pure states, so the states that evaluate at the point x and at the point y, which is f minus fy. So you sort of capture in a sort of a more C-struct algebra. Well, this is like sitting inside some C-stra algebra of continuous functions, so they will be smooth, so they have this condition. And delta of x r is. And delta x are in effect the pure states of that C-star. And the combination is what we put together. I think the name is not yet there yet, but this is called a spectral triple. It will come in the next slide. So we put an algebra of smooth functions together with an unbound operator, this Dirac operator, and they sort of meet each other by acting on an L2 Hilbert space of sections of some spin of R1. So this is Sejan. Some spin of R1. So this is their stage, is where they sort of connect. And if you think about, for instance, if you think about Gauffin duality, so from C infinity of M, or let's move to C of M, you get M back as a topological space. But I want much more than that. I want actually to capture the metric on there, so I want the metric. So that I have a formula for. But the other challenge is to get the manifold structure, of course, of M. And that was a l long uh standing And that was a long standing problem, not really, because Alan Kwan did state that as a theorem in the 90s, but the proof came about 2008 that you can actually capture the manifold structure back from this data, and that was a rather technical consideration. It's the reconstruction theorem of Riemann spin manifolds. So in general we do the following. So now we let go and and so s soon we'll also sort of make this even more general, but we take a unit of star algebra that acts as bounded operators in the Overspace. There will be some self-joint operator D and the nice thing about these operators that I showed you is that their spectrum is sort of what's under control. So it's discrete, it kind of grows in the right way, sort of a control of the Sort of a control of the dimension, in a sense, and it has bounded commutators. And this combination can be captured by putting the condition that D has a compact resolvent. That's exactly sort of combined with this being balanced commutators, captures the ellipticity of the operator. But this is kind of the way to sort of move into an abstract framework where you capture sufficient information, sufficient structure actually, about the test. Structure actually about the testical situation. And in fact, you can even do this. So we know already the states, of course, what they are. And on them, you can write the same formula. You say, okay, let me take just the supremum over all elements in the algebra, for which this commutator of D with A is small or equal to one in norm, and then evaluate the states. And I get a distance function on the state space. That's something that sort of All kinds of questions that arise here. For instance, is this a metric? Does it matter either the weak set topology? So there's all kinds of things which are addressed in this, where you have like quantum metric spaces in the sense of Liefeld to work there. This is sort of the connection to the theme, of course, is that actually everything I just said makes sense for operator systems, states, but also all the States, but also all these kinds of things acting on the hope of space. I mean, everything is completely fine, for example. And that's actually what I will use. But not just for the sake of generalization, but it's because there are some cases in, especially in physics applications of non-communicated geometry, where you wonder, do I actually have an algebra available as sort of data? And this is how it's available now. So, well, that's the status. So, you need to know A, H, and D. And in some cases, you may assume A is commutative, and then you can sort of reconstruct the geometry. But you need to know a lot of information. If you think about physics, then actually you may not have that information available because you could detect, for instance, only in some energy range or some resolution. So, you may have only a black part. So, you may have only partial information. Think about hearing the shape of a drum. Even the problem is stated sort of abstractly, if you know the fullest of Ico frequencies, then the question is sort of yes or no. But of course, in practice, that's not at all what we can kind of reproduce. First of all, we have a limited hearing range, also some resolution. So, there's all kinds of approximations, so limited information of the spectral data. Of the spectral data that we have. And we want to sort of analyze what happens if we do that with only part of the spectrum or with some resolution. And earlier work was already done a long time ago, well, ten years ago, Dondre Alizi Martinetti, which worked with this, actually the type I'm about to introduce, but they didn't sort of insert operator systems yet into the game. Glasser and Stern in my group, they were doing computer signals. There and in my group, they were doing computer simulations on spectral geometry. And if you do a computer simulation, also, you have to sort of limit yourself to turn it into linear algebra, essentially. And that means that these Hilbert spaces should also be kind of cut, truncated. So you have the same kind of question is, what does that mean? Can you truncate this? And can you still do something reasonable with that? So in Wikipedia, we work this out in two papers. And so many other people in votes. Involved. So, first, appearance of operator systems from a spectral triple. So, we have A, H, and D. And now I say, okay, let's limit ourselves to only part of the spectrum. So, what I do is I take a spectral projection for D, which is just sort of projecting into some Hilbert subspace corresponding to these eigenfunctions. And let's take that finite dimensional, so we don't have to do much analysis. We don't have to do much analysis, but uh good I suppose. Uh then of course if you uh truncate D or uh you it just kind of acts on that uh that subspace again, so P d P is fine. A, so the algebra typically does not sort of respect, it does not act on that subspace. So you have to force it to act on there. So you have to compress the algebra to make it into PAP. And this is not an algebra. So it's not like a corner, so it's because P is not coming Corner, so it's because P is not coming from the algebra, it's coming from the other kind of data, which is this Dirac operator. So, this really sort of in a sense pathogenoff. It's in another direction that it's supposed to act. And so, this is not an algebra, but it's a perfectly fine operator system. Because the star, of course, it is respected, and in fact, in this case, it's even so it's just sitting inside the bounded operators on this P of H. On this P of H. So it's even concrete, it's just given to you, and you can analyze it. So that's the first instance where you find operator systems. And another one, which is sort of, so physics, in terms of physics, this is sort of the opposite thing you can do. So either what I'm doing in the first example is I kind of restrict to a part of the spectrum. So I only have sort of limited frequencies available. Frequencies available, and with that, in terms of what you measure, if you want to sort of measure a space, it means you can only see the space up to a certain scale because of the frequency being sort of fixed or limited. You can also do the other thing, is that say, okay, let's take a metric space, and I consider two points to be sort of equivalent when their distance is smaller than a fixed epsilon. So epsilon is fixed, that's my resolution, but I cannot distinguish. My resolution, but I cannot distinguish these parts. So I get a relation, but it's not an equivalence relation, because it kind of propagates, of course, to larger epsilons. And that's not what I want. I want to keep the relation at this epsilon to be fixed. And that's a tolerance relation, so it's symmetric, it's also reflexive, but it's not transitive. And this is what I would like to consider. In fact, the operator system that I will also explain are integral operators that are. That they are like defined on the support only on the relation R epsilon around sort of the diagonal in, let's say, x cross x. X is the metric space. And the diagonal has this width epsilon. So that's what I'm considering. So that's, of course, if you multiply these, this would grow. This sort of band. So this R epsilon would become R2 epsilon and so on. So that's why it's not an algebra, but it's an operating system. But it's an operator system because this is a symmetric relation. Alright, so I was very happy that Matt did the operator systems definitions and so on, and so I couldn't do it better. And what I would like to sort of connect to is this Caesar envelope. Because even though operator systems appear naturally to sort of analyze their structure, the Caesar envelope turns out to be very convenient. So here is Arson. I'm very quick at this because that's already realized also in a geometric way. Just to sort of mention that this was maybe introduced in the 69th and 10 years later, Hamana showed the existence and uniqueness. And he did that sort of by an argument which was more about ideals that you sort of take out of the C-Star extension. Maybe we should show that as well. The C-Star extension that sort of you like or you found. Sort of you like or you found, you look for an ideal that you can take out without losing the structure of the operator system in that CSER extension. And that then is the Caesar envelope. A more kind of constructive way to do it is by looking at the boundary representations and taking direct sum of all of these. That was done in a series of papers, and it was kind of more incremental or not that that's not meant as a negative point, but as a sort of step towards kind of the non-separable. The non-separable situation that was discussed in 2015. So, this is what I maybe also to stress, the Caesar envelope is the smallest among the Caesar extensions with a certain property. So, that's where you also see that if A is my Caesar envelope, any other Caesar extension just kind of subjects onto it, and that's exactly looking for this ideal. Looking for this ideal, shield of boundary ideal. And maybe an example here. Yeah. So this is a nice one. It's an operator system which is harmonic functions on a closed disk. And then, so, of course, you can look at it inside C of the closed disk, but that's actually way too large in a sense. But what you find is that the C-star envelope is C of S1. It's because if you restrict to the boundary. It's because if you restrict to the boundary to S1, you know everything about the harmonic function on the inside, because you could just solve it actually by a Poisson kernel. And that restriction is compatible with positivity. And that's a very nice sort of situation where you see that actually the Caesar envelope can be much smaller than the Caesar extension that you would kind of be given naturally. Yeah, so in order to compare operator systems to Operator systems to Cetr algebras. I sort of put it as a question: how far is an operator system from a Citrus algebra? So what I did with Connor in his first paper is to introduce an invariant, which sort of invariant for operator systems, in order to distinguish these different sort of operator systems that we were analyzing, one coming from spectral truncation, the other from resolution, because we thought they were the same, but they were not. They were the same, but they were not. So, physically, they look rather similar, but apparently, they're not related in this sense at least. Okay, so let me first define this. So, the propagation number is just telling you how many times you need to take products before you end up with the Caesar algebra. So, if you take, this is to be the norm closure of the linear set of products of n elements, multi-elements, then the propagation number is the smallest among those inside the C-slam. That you end up with the C-slamm. That you end up with the C-structure. And for instance, well, this is an almost trivial situation, the restriction to the boundary, that's already sort of sufficient. So you're already, that restriction is giving you the C-stra algebra. So that's the propagation number being equal to one. Then there are some small products that sort of came out of this. So, first of all, I showed with POM that it's invariant on a complete order isomorphism, as well as when you take a tense product with a complete. As well as when you take a tensor product with the compacts, it's also the same thing. And then we were very happy that there was this notion, left-hand skepticalism, and Todorov introduced marital equivalence for operator systems, which is exactly doing this. So it's this kind of tensoring with the compacts is identical equivalent, I should say, to Morita equivalent. That's saying that the propagation number is invariant on a Morita equivalent. Marita equipments. It's also behaving nicely with respect to tensor products, minimal tensor products. So, and that was actually extended more recently by a student in Leuven to the non-unital case, non-unital operator systems. So, that propagation number has good properties, so sort of kind of figure out what are the different um operator systems that we have. Okay, let's look at an example. Uh again back to the circle, so it is uh since I did that before. This is a since I did that before, that's exactly what I'm looking at. So these are the VA modes. And I project onto just the first n of these VA modes. And then I look at the truncation of C of S1 with this projection P, fixed size N, and you figure out that that's a tupless matrix. Because these operators of course act as shift operators, the generator of C of S one, which would be two-sided shift. And that's more or less this T one over there that that you find back. Over there, that you find back. And it's truncated, so it shifts, and then it just kind of falls out of the range. So that's what you realize to become Tibet's matrices. So that's my operator system, consists of these n by n tiblis matrices. And I denoted it like this, with the upper n. And what is then nice is that, first of all, you're in the matrices. So let's see if we can get to the matrices. And what is this here? What is this clear? The Caesar envelope. Well, first of all, the Caesar extension that you get by just realizing it in this Hilbert space is Mn of C. And what you find is that any elementary matrix in the Mn of C can be written as a product or a sum of products of two matrices. And so the propagation number is just two for any n, independent of n, so fixed. So that's a strong invariant in a sense to compare it to other ones. And since mn of c has no And since Mn of C has no neutral ideals, that's also my C-sword envelope. So the C-stra envelope is this Mn of C. But here, Mn of C is of little help in analyzing the structure. Because if I would just work with the C-stra envelope, MN of C, I lose all the relation I have with the circle. And somehow, in this turbulence operator system, there is sufficient information or that yeah, let let's say when N is large to sort of capture the circle. To sort of capture the circle. So that's also something of a stress. Whereas M of C does not. There's not much, I mean, it's just matrices, so more it's equivalent to what points. So what we analyzed is duality of this operator system, and that's for the purpose of finding the state space, of course. We have a duality, a dual operator system, which we call Vegier D's operator system. And now it's called probably more properly. Called probably more properly like a Fourier truncation because it's a truncation in Fourier space. So I take C star of Z, or otherwise C of S1, and I have a support constraint in Fourier. And I say that something is positive, which is positive, is a function under S1. So it's precisely a function space in the sort of old sense saying of Harbour Sun. So you're inside of C of S one and positivity is inherited from there. Positivity is inherited from there. C-star envelope is given by actually all of it, T of S1, because when you take convolution products of these, the support will grow, of course. But it will only get there at infinity. So the propagation number is infinity. So even though I do a similar thing, I truncate, again, same kind of modes, I get now propagation number is completely different. I can analyze the extreme. The extremal points, say, in the positive cone in there, maybe that's the thing I would like to highlight. So, the extreme rays in this cone are given by elements for which the Laurent series that you derive from these Fourier features has all the zeros, one has one. So, that's sort of the extremal thing. And this I will use, because in the dual, this corresponds to the extreme states, right? Because these are the extreme rates in the dual. So, let's do that. So let's do that because the duality we found is at the level of, let's say, the order unit space, is what we figured out in this paper. And then Doug Verenicke extended this to the operator system. So we used a Vegieris lemma, that's why we gave it that name. And Verenick used operator values, Vegieris Lemma. But the point is always that you sort of, what you want to get to is that this respects positive. Get to is that this respects positivity. So that if you take something which is positive, let's say in C star of Z with the support, so this visually, I have a function which is positive on the circle, but it has a certain support constraint, and I want to sort of write it as a as a product, then you may be able to do that, but that product, that's why it's as you read Frema, has can be chosen to have support which is half the support, support zero to N. Of the support, 0 to n. That's exactly what this lemma, this odd lemma, tells us. But that means that I can write the AK as a product, and then the left-hand side simply becomes something like psi. Then there's this Turpless matrix, Psi. And then this is positivity as the Turpless matrix considered as a matrix. So this, by inserting this Fagerie's lemma at that paragraph, the problem. The proposition where I'd like to focus on the second point is that the pure states are given exactly by these zeros that I just described to be the extreme rays, because they would determine, of course, the Laurent polynomial. These zeros on S1, I chose them like this, up to permutation points, because I just choose these points, and they are supposed to resemble something like the circle. So they are points on the circle, and the question is, what do they And the question is: what do they actually resemble? And for that, actually, so I put this question here. So, does it resemble in any way either this or the pure states, something that you are familiar with on the circle? And I would have to sort of have some patience for Thursday where you will see this coming back. In what sense does this converge to the state space of C of S one and then probably also extend it to Tori in the talk of To Tora in the talk of Modelena. Well, it's 15 minutes, so I'm sorry for this. I had all the time to give you more handle, but we're left with that. Because I wanted to sort of talk about this other kind of thing which comes from these tolerance relations. And there's sort of a general setup which I'd like to sketch. So we came up with the name. So we came up with the name Bond, but I think, I don't know, maybe you should tell that, who came up with positivity domain, because you wrote it at some point, which sounds rather reasonable. But in any case, so what I'm looking at, it's in the group point context. If you don't like group points, then you take a group, that's fine. And otherwise, if you don't like groups, you take the groupoid to be a product of x with itself. To be a product of x with itself, that will be the example, just a product of x with x. And what I'm looking for is: I look for a bond which is a subset, which is open inside either this product or in group points. It's symmetric, that's in that sense, symmetric. And it contains the units, and that means it contains the diagonal, the x x. And otherwise, if you know what the group point is, then it's also dead. So I have some kind of So I have some kind of general context where I can do this. And given that, you can look inside the Caesar algebra that you associate to a groupoid and find an operator system by just looking at those elements, which, first of all, they should lie in the C C of G, which eventually end up in C star of G, to have the closure of the subspace for those who have the support on this omega. So I restricted. In this omega. So I restrict the support, and it could be a non-unital operator system in the sense of Werner. But since I'm in a CISO algebra, it's not so complicated to work with non-unital operator systems, because I have it concrete. It may become more complicated if you look at duality, but not at this point. So some examples, as I said, there will be many examples which appear. And and the one I already discussed, so if you take a Lie group G, you take omega in there, so some neighborhood of the identity. So, some neighborhood of the identity, then that's what I just showed you. In fact, it's here by me. It's the second line. It's going to take omega n to be the interval minus n to n, and I restrict the support to lie exactly in there of C star of z. But this you can also do for C star of G for a Lie group. And the default even does it for complex quantum groups. There's a similar kind of structure tool there. So these are called Fourier truncations. And in special cases, the Fourier D system labels support, which is the dual. Support, which is the dual, as I said, of this triplet system. Another one is when you take this into the cyclic group, say let's take M a little bit larger than N, or twice N or something like this, then you get, first of all, C star of the cyclic group, or circulate matrices, and you get banded circulant matrices. So again, some structure that people have been studying in the context of such a maybe even some more linear algebraic. So, more linear algebraic. There's another one which is more of this type. So, you take a set X of n points and you look at the G to be X cross X and you take a band Rn of a certain width. What you get as a Caesar algebra, you get, first of all, you get just all the matrices because that's what the Caesar algebra. But you restrict the support, and that's in terms of matrices, means I have a banded matrices. means I have a banded matrix where the bandwidth is the size n. And again, that's not an algebra, so it's an operator system, but it's exactly the same reasoning as with these integral kernels. It's just a discrete version of that. So the key motivating example that I had is the following. So I have an X with a metric, metric space X number D, and I just take those points in X cross X, which have a distance. x cross x which have a distance more than a fixed epsilon. And then if you have a measure space, you can actually build up an L2. And on there you can act with these elements in x cross x as integral operators. What I do now is I restrict to those integral operators with support on R epsilon. And that's what you get. So you get a closure of these integral operators with support on R epsilon. So this is typically non-unital. So for that, we had to kind of develop a lot of additional theory to help. Additional theory to handle also like Caesar envelopes for this and the propagation number model. But that can be done. And the upshot is, this is a very sort of very quick summary, of course, for everything that happened in between, of kind of approximate order units and so on that we had to use to make sense of these non-unital operator systems. Is that the Caesar envelope is, again, is compact for the same reason. There's no ideal that I. For the same reason, there's no ideal that I can kind of quotient out. And the propagation number is the diameter of x divided by this epsilon, because it grows like that. States, the pure states, are given, I mean, this is again just because I am in the compacts. I know that every state, even every pure state, extends to a pure state on the compact. So I know exactly what I should be looking for. But then I have to make sure which of these are still pure when you restrict them. And that turns out. Restrict them. And that turns out to be those vector states for which, if you look at the support of those psi's in L2 of x that define that state, but just evaluating or the expectation by like this with respect to that psi, that the essential support of that psi should be epsilon connected, so you should be able to sort of jump from the two kind of pieces of the support within a distance epsilon. And otherwise, they split in two pieces. They split in two pieces and it's not pure anymore. That's just as simple as it gets if you think about it. Geometrically. Walter. Yeah. There's a question in the chat. I take it closure. So in that sense, I don't restrict the integral operators, if that's the question. I actually am looking inside C star of X plus. Inside C star of x cross x as a group point. So I don't know if that's the question. Thanks. Yeah, so that was my final statement here. So if the Psari has this kind of support which is further apart, so larger natural, it just splits into two pieces and it's not pure. So that's a nice perspective on what you can actually see with this operator. With this operate system. Okay, so I have these two sort of or maybe three, but this one kind of includes this Fezieris operator system, so these approaches where you take like a certain resolution into account, or alternatively, you take part of the spectrum into account. So, what I'd like to do in the remainder is to see whether you can capture You can capture invariants that are used very much in C-series algebra theory, which is K-theory, on the level of operator systems. And so that's something that is natural from the point of view of replacing C-source by operating systems. But it's also something that is already, there's, let's say, evidence that there should be such a thing. Evidence that there should be such a thing because there is people doing index theory with spectral truncations. And that's actually quite striking in our whole traffic. It's like a wish list. So if you want something like K-theory, then so you want, of course, maybe something there, but for C-s2 algebras it should reproduce K-theory of C-serve algebras, or at least uh in some or refine it at least. Or refine it at least. And now, C-structure algebras, if you look at K-theory for C-structure algebras, you look at projections or unitaries, but let's look at K0. So you have projections, but the projection is, of course, a very sort of algebraic object. So you have P squared is equal to P is equal to P star. So you sort of have to capture this in a different way. And then there's also That's even the second bullet there. This structure of the spectral localizer that maybe some of you are familiar with. But what happened is that if you look at index pairings or index computations, so you're looking at some Fredon operator, typically infinite-dimensional, well, otherwise, well, it would not be much of an index. So you take some operator, and it could be this Dirac operator D here, and maybe. And maybe it's paired with a P, and it's paired in this way. So you look at P d V as an operator, and now P, my small P, comes from the algebra. But this is Caesar algebra. So P sits inside, let's say, the Caesar algebra or in matrices over the Caesar algebra. And you can wonder if I have an operator D from a spectral triple, is P D P threat home and so on, and actually that's the case. And you can compute the. Actually, that's the case. And you can compute the index of PDP. When I say that, I mean the index of the positive, sort of the splitting of P into two pieces, because it's of course a self-government operator. And that's the same kind of matrix structure that I showed you for the tutorial. What's there as well? Now, so PDP may have an interesting index, and it's an infinite-dimensional object, but what Object. But what happened is that in this spectral localizer, because Loring, Schulz-Ball, and many others were involved, is they could sort of, via spectral flow argument, they could reduce this to only sort of the lowest part of the spectrum of D. So they could do exactly what I did, truncate the spectrum of D, limit yourself to some part of that spectrum, and also compress my projection P, so that's not even a projection anymore. Not even a projection anymore, it's a compressed projection, and it's in this operator system. But they managed to find some matrix then, so finite dimensional, and the signature of that matrix, if the truncation is large enough, is that index. So they could sort of do index computations by computing a signature of a finite dimensional matrix, which is absolutely striking that that's possible. So that's one of the maybe even key motivations because they are exactly the type of indices. They are exactly the type of indices that you would like to compute. And actually, what they were doing is motivated by also physics, where the systems are finite-dimensional, condensed matter, physics. Now, one of the observations they also did is that a projection may not be a very good object, because to have a projection, for instance, in a continuous. A projection, for instance, in the continuous functions, it will be very, there will not be many. So you have to sort of go to either a larger algebra or allow yourself to work not with projections but with just sort of invertible elements. Then if you take like an invertible element, then you could take an indicator function that would give you the projection. So that's sort of what they did. And they needed that in order to compute it in a better way. They needed to. In a better way, they needed to take this operator not so much to be with P, but replace it by something which is invertible and maybe homotopic to P, but that's fine. That's also not changing that index, but it makes it easier to compute this signature. That's another motivation to think about, for instance, this, in vertical self-protend elements, because these projections, which may be around, they will not appear in this precise form as a projection. A projection, at least in this context of spectral truncations, just because if you truncate or compress a projection, it fails to be a projection at all. So that's sort of the underlying idea. You know, of course, it should also capture these projections that I don't know if you will talk about this later next week, but of course they should be there. There's many other projections, for instance, like almost projections, like these absolute projections, that should also be there. And then finally, on the wish list is Mauriti equivalence, so that should also be. It says Mauritius, so that should also be okay. So, let me after this patient, this is what I sort of propose. So, I look at Hermitian forms, and they're defined to be uh sort of invertible elements in an operator system. And in order in order to capture that, it's uh of course it's um it's rather hard because I don't have a a multiplication. Hard because I don't have a multiplication. But what I can do is something which has more to do with the spectrum. So I want to have something which is invertible, or let me say it like this, it's larger than G times the identity in each boundary representation. In fact, let me go to the next slide where I have the sort of the more practical way to check this. That's because I can realize the Caesar envelope with boundary representations. This is what's happening. This could also be the definition. This could also be the definition. So H of E and is all the emission forms, and an element X in M and E is of such a type, is an emission form, if it's an invertible element in the C-stra envelope. So it's something that you can check typically by just looking at it in the C-Stra envelope. But it lives in its X and it's not, as I said, not necessarily a projection. The motivation for this definition is because The reason for this definition is because, and why I call it Hermitian forms as well, is because actually you can describe K3 or C-struct algebras as a bitterin, as a bit group. So it's by Hermitian forms on finitely generated projective modules. It's exactly the same thing. You sort of take maybe the indicator on the positive part to get a projection if you want. But in principle, the invertible element itself is sufficient to capture that class of k. And that's what the grid group is doing in this context of C-star. In this context of C-strage mouse. And oh, yeah, that's also one of the reasons to capture this, to have this matrix structure that you use for K3. That's something, of course, we have for our operator systems. That's why it could actually be made to work. As I said, these projections in operator systems, they should be there. And in fact, the projection is not so much what I would look at, but the Hermitian form is kind of splitting into the positive and the negative part corresponding to the projection. And the negative part corresponding to the projection D minus 2p, that would be my emission form. And well, this is quantitative K-theory. These are epsilon projections, so they're like p squared minus p is smaller than epsilon in norm. So that's also in there because they're still invertible if you look at these emission forms of the same type. Then again, if you take a spectral compression of a projection, then you get something which is, as I said, Then you get something which is, as I said, not a projection, but it may still be invertible, provided this is sufficiently small. What's going on? So again, this y, this 1 minus 2t, that enters here, which you compress, and then you check whether that's still invertible. And of course, if you compress something to the spectrum happens, you may sort of be able to sort of still have a gap around zero. Okay, so the evariants are the following. So these are emission forms at level n, so matrix size. At level n, so matrix size n, and I have a homotopy in that class, just the homotopy from the n-by-n matrices with the operating system inside there. That's what I call V. And an example is the following. If you compute this for C, then you find that, okay, what is it actually that I'm looking at? I'm looking for invertible matrices size N up to a mod P. And that's just the same as Just the same as giving you the signature of an index. So that's exactly the signature that tells me where you are, minus n, up to n in steps of two. Of course, because the signature is the, kind of makes a step of two. Then if I want to compare different levels, which is of course what I will do in a moment, so I want to sort of map to the next level, m larger than or equal to n, then I have this where equal to n, then I have this where I add e to it. So e, the order, sorry, the order unit, that would just play the role as a neutral element in Kth, because I look at Hermitian forms. So I'm actually only interested, the projection would correspond to the negative part of this negative eigenstacles. So I have x plus uh plus e m, so I just add this up and that's how it works, and that's how it fits together to give event eventually z. Together to give eventually Z in this whole kind of construction where you build this up out of these signatures over there. In general, this is actually what we do. These maps are generally defined. So this is just increasing the size of the matrices. We get these Vs. And then if you want to go to the Grotony group, then you take the group, you go to the Grotony group of this semi-group. What the identity is apparently E. Well, the identity is apparently E and the addition is plus. That's just it's the same as with the Caesar algebras. So let's then see the properties that we have. So for Caesar algebras, K-theory is kind of reproduced by taking the projection. It's like the inverse of this thing I just wrote when I said that you take the emission form, which is something like E minus two P, this is just the inverse. Minus 2p, this is just the inverse. We just look at the projection exactly on where it's negative. And that's also the thing that you do for mapping the width group to KT. Stability, and that I will not do in full detail, but this is just sort of gives you the hint of how this may work. It's similar to CISR algebra, but also somewhat different because I work with Hermitian forms. So I have this. So I have this map from X into Mn of 2 by 2 matrices, and I just plug in these are the blocks, and I plug in this E's on the diagonal, that's where the trivial elements, and the rest are just extended by zero. Then that's the same argument as usual, so that in, say, a larger, maybe like twice this, you could move this around by right-hand to get that this is actually identical. So if you move all these, so rearranging all of this, then you find. So, rearranging all of this, then you find that this is something like x plus e, and that's taken up by this equivalence relation. And that allows you to show that at least the amplification by two by two matrices, that is the same, at least to the same Kth. And that's the start, and that's the key, of course, to getting some stability results. However, uh stability requires non-unital uh operator systems in k3 carels. So this I also have um uh introduced. And actually that I could show, and then I'm in the last minute I will give you my last slide as well. So here is, but that's maybe only for showing this. So there's a notion where K-theory is defined that's in terms of the unitization, as with CSER algebras, except that for uh operator systems one has to kind of work with a somewhat more sophisticated notion of what it means to take the unitization. What it means to take a unitization. And in the unitization, E plus, a tech emission forms, for which that part corresponding to the sort of added copy of C, this is homotopic to the identity. That's as usual. But for C-service race, you take some map and look at the kernel of it to have it to make sure that that's the same, but here we have to do it without going to these kernel maps. And then you find that it reduces to the same situation as for if you're in the unit okay. As for if you're in the unital case, and you can show that this amplification actually goes on towards me. But as I said, I will stop here with a little summary. So what I did was to talk about non-computer geometry, explain the metric aspect of that, so how you get metrics by operator systems. I hope I give some examples that could sort of, well, I at least appreciate this whole connection to this, all the general results. To this, all the general results that are around for operating systems. And then, in return, there are these examples and these notions of invariants that we try to introduce. Again, motivated by these truncations. The high K groups I mentioned here, that's also being done and under investigation. But I'm very happy to further discuss also during this week. So thanks for your attention. So thanks for your attention. Thank you very much. Any questions? So if I understood the instructions correctly, let's say that you stuck with an operator system key that is already a system. As well, is that correct? And if so, is there a connection between that and editing and the one we're using there, which is finding the C-star rule? That's a good question. So it's theory of the operator system and the K-theory of the C-serve algebra. Yeah, I'll still wonder about that, but my question is more. Let's say that you have a C-fer algebra A and E is inside of there. Then you can look at these. Then you can look at these, how will they call? The invertible guys. I think it's even a finer structure that you get, because what you could always do is to take that element in the C-star extension that you're looking at, in this case, and then you kind of, that you could project it to the C-star envelope. Yeah, so I guess my question is, let's say I apply your construction to my initial embedding and I apply it, as you suggest, to the C-Star envelope. Then is there any connection between the two? Yeah, so I think what you can do is to. So I think what you can do is to, so what I said, so you could, from the Caesar extension, you could sort of subject to the Caesar envelope without losing the information of this being invertible. It will still be invertible in the Caesar envelope. That's right, but there might be fewer invertibles like the fire envelope. That's right. So you may have finer information. Final information. Yeah. In your case. So that's interesting. And in fact, if you think about writing a spectral triple, so then I So then I have a sort of a given realization of the operator system in B of H. So and if you think about the dual theory, so K-homology, that's actually defined in terms of this realization on some Helperspace with some operator acting there as well, this D, for instance. Then you may wonder, so how much does that depend? And then, of course, you would like to have something as well which is more intrinsic. Which is more intrinsic. That's why the CSRAM bots get that. But yeah. For that, if you would write K-homology, which is essentially a spectral triple, there is no difference between writing this for an operator system and the CSER extension it generates. Because i the only condition is bounded commutator. But you have that if you have that for the operator system, it will just extend to the CSER extension by uh Lightning's rule. So By Leibniz Roll. So there's nothing. No more information. So then you have an invariant actually of the Caesar extension rather than the operator system. But I agree, this is sort of a delicate balance. Or maybe to add to that, we discussed this before, but it would be uh uh much better if you have a sort of an intrinsic characterization of these uh invertible elements. And it's of course with the projections you did that. And it's of course with the projections you did that. Uh sort of say what does it mean for an abstract operating system to have that net property. And this I did sort of halfway using boundary representations, but then it amounts to studying it in the C-Strand club. So still it is in the operating system, but it's about spectral the spectrum of the operator inside the C-SRAM. So if there are no more questions, let's take water again. We have lunch and we are back at two. Hi, everybody. Sorry. My name is Jay Pasaki. I'm the first program coordinator. I have some good news for I have some good news for you guys participants. You're going to be quite in this week, so.