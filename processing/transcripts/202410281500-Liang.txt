Good afternoon. I'm Lansa Song, a PhD student from Peking University. Today I want to introduce a novel method to solve the problem of the metric configuration using the neural network. The topics of the talk I want to as follows. First, I want to introduce the basic idea of the mesh validation. mesh effect method using the neural networks. That is to use the cursid mine incursions to constrain the analysis. And the second, in the study, we found that the cursid mine incursions are not enough for solve this kind of problem. The properties such as the singularities, the branch cards, and the reflection in the origin, they are also important in the disc. Insolved of this kind of problem, and we need to consider that with what we need to explore. And the third one, we want to extend our method to the cases with experiential tense. And here we use the method from the weak solution to the strong solutions, just as the method of apprehension in 1990 with a model of With a model of staggering parallel place modeling, parallel model. Finally, I want to discuss this in a higher level to discuss about the capability of the complex functions in the neural networks. So, first is the basic of the Wiener-Hove methods, the constraints of the multiplicative The constraints of the multiplicative kernel fluctuation. There are two constraints here. First is the analytic and the non-vanishing determinant in the respective half lines. And the other is the requirement of the factors are at most algebraically large in the respective half lines. This constraint is most important in the metrics with the exponential factors. Exponential factors. So, in the previous work, how can we factorize these kinds of kernels? For some kinds, we can have the explicit solutions, such as the dynam metrics we will use as the guiding example today. And there are also numerical solutions, such as the rational approximations and the PET approximation. The path approximation and the third is the method that is maybe not mature, it's a neural network solution with the constraint of considering causation and with other complex variables properties. So, this is the question caution, and as we know, it's quite simple. And why we think about using the neural Think about using the neural network to solve such kind of a problem. That's because in these days, to use the neural networks to solve the partial derivative equations is quite popular. And the reason is that the automatic differentiation of the neural network is suitable to obtain the partial derivatives because there is no chunky error and easy to Chung error and easy to implement in the neural network framework. However, in the practice, we found that it's difficult to converge and it's not perfect. And this is the neural architecture. We use the complex variable alpha as the real part and the imaginary part of alpha as the input. And the factorization results. The factorization results, G alpha as the output, the cursing and questions as the last function to solve this problem. And here, although we use the node that works to solve these kinds of problems, but it's not a data-driven method, but all of these methods we do not need to calculate the data. And so here. And so here is the example. It's a non-scattering parallel place scattering model, which comes from HERT work in 1976. Here is the region one software boundary condition and the instant wave with instant angle of theta zero. And here we divide the sputtering. We divide the scattering space scattering field into three spaces one two three and for every field we can get um expressions for the for this from equation five to seven by substituting then into the boundary conditions and the continuity conditions and by introducing the The final function in the respective hoplines u1, u2, v1, v2. We finally get six equations here. By rearranging them, we can get the Winahook equations as the equation 14. Well, the metric kernel is a Daniel form kernel, which has explicit solutions. Well, briefly, of the talk, I don't show that in the slides here. Finally, by using the Liberal theorem, we can by remove the power from both sides. Finally, we can get the results. The only remaining problem is how can we get the session results? So, as I have said before, custom cautions are not enough for solve this kind of problem. Solve this kind of problem. The main idea is that the colonial neural network modeling, although totally for its universal approximation capabilities, it faces difficulties in fitting infinite values and infinite gradients. But unfortunately, we're not hopeful for kernel metric fragmentation problem. There are singularities, there are exponentials, there are branch cards, but that is the But that is the immune network not suitable for SAR. There are two fundamental reasons for this, both related to the nature of the neural networks. The common activation function, such as the RAL and the sigmoid, are continuous. It's not suitable for beating the definite gradients. And the second, the optimization of the gradients fails in Of the gradients fails in the presence of infinite values. Because of these two natures, without these two problems, a basic idea is to we can reconstruct the neural network framework that is developed for the complex variables. I think it's important and meaningful, but it's also a hard work. I will not introduce it today. So first, let me think about the similarity properties. In fact, we sampling the data, we avoid the weakness of the powers. The end is to avoid the event values, but there is also to Values, but there is also geo-ads. The cursing and cautions are not substring conditions. So we add in constraints from curses integral theory. The second method we use here is use the hash constraint for the neural network. That's to say, we add the constraint on the framework of the network. In the current case, the delos and the post exist. zeros and the poles exist at alpha equal to less than the minus k. The equation can be then can be expressed as the equation 17. As we can see, the right hand side of the cursion should that is the G minus. It should be magnetic in the lower half frame, which means it should pre-close alpha equal to minus k to be the power. minus k to be the powers of this one because the beta plus beta plus is on the denominator which means if we do not make um special special issues it will just it will just be infinite values and here we use um here we use an expression as equation 18 here we use um We use episodes like this. By using this, the G plus, this one will candidate with the beta plus and with this one, the beta, the okay, yeah, this one will have this one will become, will be will have um will have beta plus in the dominate which they then it can be kinetic with this one another one we need to discuss is the branch cast if the branch casts are known and in fact in the finite in number such as those produced by the square root function and which is also we often see in the uh in the In the Caucasi problem, appropriate mapping can be applied to alpha, making G add alpha a continuous value. Here, we just use a simple mapping for alpha to alpha height. If there are infinitely branch cards as with functions like tangent h, sine h, additional disc. SH additional discussion is needed here. And so, this is not universal method, but we need to discuss it case by case. And the second limitation is that if we use this mapping, then if we want to use another mapping, there may be some conflict. So, as we have seen before, with the Seen before with the heart constraints and with the loss function. Here we the Cursing equation, Cursing Man equations loss function is just as simple. And here the Cursing integral loss function is just as we I have said before is to resolve the problem of the singularities. And the third one is the loss function that defined from the boundary condition. Each is a little different. It's a little different from both the numerical solutions, such as the pattern approximation and so on, because here we use the boundary conditions to straightforward to constrain the solutions. And here, then, as we have talked about, then here we should use some validation of our method. The metrics here we use is the five-field deductivity approximation. Then we use the DPC decent pace to approximate the five-field deductivity. And here we use the matches of the neural network solution, both of the mean square error, which is usually used in the deep learning. In the deep learning field is to describe the similarity between the analytical and the neural network five-field solutions. And here is the error of the differentiate is to observe the only tools A1 and A3. And as we can see, it has a log 10, which means it can have some difference with the mean square error. And here is operation experiments which we use to validate the neural network. That's to say, we individually set different weights to zero and observe the effects. When the weight W1 associated with L-Crasin is modified, the dismatches displays a significant difference, underscoring the crucial role of The crucial role of constraining the cursing mine equations, and that is here. If we make the W1 to be zero, then this one will be very large. And the absence of the bond condition loss function is the most pronounced impact on the MSAE metric. And here is the And here is the W3 to be zero, then MSCE will be quite large. And as this table shows, we can find that LCR and LBC are quite important here. And the LCI maybe is not that important, but it can also make some function. And here is the popular resource operations. There is the There is the new field comparisons. And the third part of this talk is to talk about how to extend it to the cases with exponential factors. Here we should approach from weak to strong decompositions that is similar to the work of Applehans in 1990. Why we choose this one? Because first, the weak composition. Because first, the weak commensation form, we can just use the neural solution. And in this method, we decoupling the expression tense and it will award fitting infinite values with the neural network. That's why we use such a method. But in the derivatives, there is some difference. There is some difference from the lesion work because the weak decomposition form has numerical solutions here, which means some analog properties may be unknown. Then, due to these kinds of insufficient information, changes were made in the definition of the strong form to felicitate the decoupling. Now, let me have a look at the solutions. At the solutions, and the first is the weak solutions. The k0 is the metric without the exponential vectors. And the k8 is the metric with the expression terms. And we can find that in this talk, we just consider these kinds of exponential factors. They are at They are at the B and C at these two locations here. Because of this, we can realize A alpha as equation 30. With this equation, we can get G alpha T alpha K0 minus alpha and G plus alpha are the weak. Are the weak form solutions? Why is the weak form solutions? Because it satisfies the requirements of being authentic in both the positive and the negative planes, but it does not satisfy the requirement of being at most polynomials only at infinity. So it's necessary to use a strong form. That's to add the k star alpha matrix. K star alpha metric. In our study, the k star alpha is defined as this one, and the b and d can be derived through the density and analysis. But here, the exponential tense still exists in m star alpha. And what we want is that m star alpha s d M star of R, S star of R, there is no exponential tense. So we rewrite it as follows. In this form, all of these complex functions, there are no expression factors in them. So we can write the Write the optimization results G minus K star at this form and these functions are a simple form. And here is there is a mistake and there is a mass symbol here as men symbol in these locations. And according to the nanosity and the polynomial boundary on the positive and the negative half-planes, The negative happens, then we can get the alpha and g alpha is here, p alpha alpha to be zero, and that's to make the and that's to make this and this factor to be zero. And for the positive hopelands, we will get the C plus, we'll get the R and U to be zero. So here we finally get four equations here and and additionally another but here we can see we have eight complex functions here and here we just have four equations and to add more information we can we can just as the appliance work we can see think about the reflection in the origin and then we can by using this And then we can, by using this, we can get these kinds of four equations about the about these four equations. In fact, this kind of reflection in the origin, it requires the K0 alpha, which satisfies some symmetry. Some symmetry by reflection in the origin. It may be not universal for all kinds of measures, but just for the staggering parallel phase model we discussed today, it's just suitable. So with this kind of constraints, and finally, we can get another neural network. Another neural network will train the k-star alpha. Finally, I will give a brief talk about a higher layer about the complex functions. This talk discusses how to address issues such as similarities in the branch cards in the Winnerhof-IP method. However, current approaches require case-by-case. Approaches require case-by-case analysis. And these issues also exist in broader problems involving complex variable functions. As of now, most research on neural networks applied to complex variable functions has been limited to function fitting without addressing issues like singularities and discontinuities and the branch cast. And that's what we want to resolve in this talk. Full result in this talk. Even with the use of complex activation functions, analytical interpretability has not been adequately achieved. So I think it's necessary to develop a neural network architecture headed for complex variable functions. And we need to resign the activation functions. For example, here. And for now, the activation functions is... The activation functions is used for the real variables and such as the radu and the sigmoid. Maybe we can use something complex active functions here. But if we change this one, then the whole framework needs to change, includes the input and the output. The other one is the optimization functions. Optim optimization functions is initial order to change to suitable for the shoot for the complex functions. If we can do this, I think it will be better to handle analytical interpretability concerning singularities and branch cards. And that's all. Thank you.