The invitation first. Pleasure to be here. And so, what I'm going to talk about is based on joint works, different joint works, one with Verling Christ and Tashi Walde, and the other one with Mikhail Kapranov and Vadim Schechtmann. And so, the context for this talk is a certain categorification scheme, which I'd like to refer to maybe as stable categorification. Stable categorification. And the idea is the following: that there will be certain concepts. Mainly, we will focus on or be interested in concepts, results, structures in homological algebra, and not additive, but they will be formulated in terms of abelian groups, let's say. And this categorification process will attempt to categorify them, replace them. Categorify them, replace them by stable infinity categories. If these words, so this is twiggly arrow categorify, it's not sort of a clearly well-defined concept, but it's more like a certain art which will follow certain rules, as we will explain. If the term stable infinity category doesn't mean anything to you, it's just an avatar for a triangulated category equipped with a For a triangulated category equipped with a suitable enhancement, such as DG enhancement, A-infinity enhancement, or stable infinity enhancement. And so the idea is sort of whenever there's a categorification process, there's some decategrification process. And this is typically actually really a well-defined kind of a functor. And this is just obtained by passing to the Gordon D group of this category, which will then produce an abelian group. And so, as I said, And so, as I said, what we'd like to do in this context is an attempt to categorify to the extent possible certain concepts from classical homological algebra and replace them with certain categorical counterparts. And we will follow certain rules. So this is not a really strictly well-defined process in the sense that it's some kind of a functor, but morally, it's supposed to follow the following set of rules, which we're now going to document. A document. So, what are the rules of categorification? So, these are the following. And I'm also writing them to fix to some extent the kind of notation I'm going to use in the rest of the talk. So, you will see a certain concept, let's say, in homological algebra, and it involves an abelian group, which you see somewhere. So, A abelian group. Well, then you're supposed to replace this in the categorification process, of course, by a stable infinity. Process, of course, by a stable infinity category. So, in whatever language you would like to formulate this. So, I will use these curly calligraphic letters to denote them. Then we will see certain elements, maybe elements of the Sabelian group. Obviously, these are supposed to be replaced by objects of this category. So, objects. Okay, so far, that's pretty kind of obvious, a collection of rules. If you think about Obvious collection of rules if you think about this process. And now it's the first, it's probably most important rule. Whenever you see a difference, so that's what we're going to say. So y minus x, there's going to be a difference between elements in some abelian group. Then you're supposed to replace this by, well, first of all, these elements are supposed to be replaced by objects x and y. And then you're supposed to specify a map, or there is supposed to be given to you a map, a morphism in A between these objects. In A between these objects, and you're supposed to take the cone. And this is already somehow the crucial rule. So if you see a difference somewhere in the classical context, then in this categorical context, there has to be given to you a map from X to Y. If there is no map, you cannot take the difference. You refuse. Of course, there's some kind of canonical choices of maps you may imagine, like the zero map, but that sort of doesn't count. I mean, there really has to be some map which you're supposed to use to take. Some map which you're supposed to use to take this difference. Okay, and then there's certain rules which are essentially consequences of this rule. So maybe the next one is this is a kind of formula you will see quite frequently in homological algebra, some alternating sum, let's say, which is a generalization of this difference. So something like i equals to 0 to n minus 1 to the i x i. And so then this is supposed to be replaced by some kind of Supposed to be replaced by some kind of a generalization of this notion of a cone, which is you're supposed to specify a complex valued in the stable infinity category. So think of this. So let's say A, for example, is the derived category of vector spaces over a field. Then you can think of this complex here as just be given to you in the form of a bicomplex. So imagine there's complexes here, and then there's this differential in this new direction. Differential in this neutral direction. And then what you can do is you can totalize this just by passing to the diagonals. And that can be formulated intrinsically in this language of infinity categories. That's what we mean by this totalization. And so then maybe as a last rule, some other type of formula which will appear frequently is maybe that you will take the direct sum of two abelian groups, and that maybe is. And that maybe is isomorphic to some other group C. And then the rule in this categorification process is whenever you see this, you actually have the freedom to provide, first of all, some stable infinity category C, let's say. And it's supposed to come equipped not with a direct sum decomposition, but with a semi-orthogonal decomposition. So it's a pair of full stable subcategories such that there's Such that there's only non-zero morphisms from A to B. That's my convention. And every object in C is in a unique way a cone of a map of a morphism from A to B. This one? Well, yeah, we're going to see it maybe appearing later again, but what it means is that there's two full stable categories, subcategories of C, A and B. There's only the zeromorphism from B to A. There are morphisms. Yes, exactly. That's another way to say it. And so what you see, what you experience from this set of rules is that indeed, whenever you apply K0 to a concept which you will see on the right, you will recover the concept you see on the left. Okay. Okay, that's maybe a bit abstract still. So let's just jump right into an example of how you will apply this set of rules and just maybe this. Apply this set of rules and just maybe the simplest possible case when you can see something interesting. Should we be bothered by the fact that A plus B is isomorphic? Yeah, that's one feature here. Many of these constructions here will be asymmetric. That's going to be related to these terms of laxness here. And that's going to be one of the features which we'll see, which we'll somehow have to deal with. Yeah, that's a feature, not a bug. Yeah. Yeah, so this categorical world will be quite different in some respects from the classical world. In some respects, from the classical world. So, also the difference in terms of objects, like is it sort of reasonable to think the choices of morphisms in that construction as being an encoding of the group law? No, it's I wouldn't go that far. Yeah, I wouldn't go that far. Yeah, I mean. Okay, so let's see, let's look at an example of how this works in practice. Okay, and so the example is just I'm going to specify a certain complex. That's maybe By a certain complex, that's maybe one of the things you would like to do. You have a complex you know and love in this classical context, and you would like to find a categorical counterpart, which is obtained by applying these kinds of rules. Okay, so complex I'd like to focus in is actually the code chain complex, like simplicial code chain complex of the two simplex valued in some abelian group A. So, how does this look like? Well, it looks as follows. You will have Um you will have um right so here's the two simplex zero one two and um now you will have to specify in degree zero functions on the vertices so that's going to be triples of elements in this group x zero x1 x2 and so the collection of all these triples is then isomorphic to to a cubed then at the next level you will have functions on the non-degenerate edges so these will have the form x01 We'll have the form x0102 x12, let's say. So that's again going to be isomorphic to A3. And then at the top, there's one non-degenerate two simplex. That's just x012. And that's just isomorphic to A. And then there's the differentials, which turn this into a complex. And you will see the differential, the way you write it down will, you know, will be given precisely based on these. Will be given precisely based on these symbols that we have used here. So, here, for example, the differential is going to be given by the formula xij is the difference between xj minus xi. That's the differential here. And the differential here is going to be given by x12 minus x02 plus x01. Okay, and then of course it forms a complex and that's. Complex. And that's the gadget I would like to focus on. And now the attempt will be to replace this by a categorical counterpart, which I then maybe want to write like this. So it's some version of this where now instead of an abelian group, I take such an enhanced triangulated category as an input datum A. And now I try to apply these rules to find the categorical counterpart and see how far I can get. Quote counterpart and see how far I can get. So, at the level of, yeah, at the lowest level, what I'm going to start is maybe I first copy the symbols I have used here. Here, these symbols actually denoted elements of this abelian group or families of elements. And on this side here, they're now going to denote certain objects inside this category. Okay, so basically, here I have big letters x0, x1, x2. That's how this is going to look like. That's what this is going to look like. These are not going to be objects of A. At the second level, I'm also going to already write this in a way so that I can easily fill this in later. It's going to be objects like this on the level of two code chains. And at the top level, I'm going to have a single object. Okay, so you can already say maybe this will just be isomorphic to equivalent to just one copy of this stable category A. And then here, A. And then here, the first maybe naive thing I could do is I could just take the direct sum of three copies of A and then define these categories. But then you see, at the next stage, when I'm trying to define the differential, I will have to refuse to define it because I'm supposed to take a difference, let's say x1 minus x0, but I cannot because there is no morphism specified. Okay. And then I remember something that I've learned at this conference is that any proper example is supposed to be a very important thing. Any proper example is supposed to be based on A3, and then there's an easy way out because I'm just kind of smuggling in this additional data which I need in order to define the differential by just adding these arrows as part of the data. So these are just representations of the A3 quiver valued in this category A, let's say. And so this is actually an example of a category which does admit, well, such a triple semi-orthogonal decay. A triple semi-orthogonal decomposition. Let's not sort of worry too much at this point how this looks. It's not a direct sum, but it's a sum in this kind of directed sense here. And if I pass to k0, I do indeed recover just three copies of the Gautenik group of A. And now I'm actually in the position of specifying this differential. So how do I specify this differential? Well, exactly following the rule, I just take the cone of this canonical morphism between these objects. Between these objects. So xij is cone xj to xi to xj. And I see when I pass the Gaudeni groups, it does indeed categorify precisely this difference here. But now I see these cones which I have taken, they're actually not unrelated, right? Because these three cones, which I can take in this sequence here, can actually draw them sort of down here, thinking of them as some kind of. Of down here, thinking of them as some kind of co-fibers, they actually form a complex themselves. In fact, they form an exact triangle, if you wish, right? That's the third isomorphism theorem or octahedral axiom in this case. So therefore, what I'm going to specify here, instead of just having three isolated objects, this is actually going to be the category of complexes, three-term complexes involving x01, x02, x12. Not necessarily. Not necessarily exact. So there's only going to be two morphisms and a zero homotopy between their composite, let's say. So again, this is a version of A3 with a zero relation. It also has a semi-Rothalkin decomposition. It actually does turn out, of course, to be derived equivalent to the A3 cover itself, but this is sort of the specific presentation we would like to use here now. And then I can continue this because this complex is precisely what I need in order to. What I need in order to make sense of this next differential, which is an alternating sum. Now I have this three-term complex and I can just totalize it. So this is just x012. It's going to be given as a totalization of precisely this complex which I'm giving. And now there is this kind of miracle which happens: is that so far I've just written down this differential here and this differential here, but indeed this is. Here, but indeed, this is actually a complex. And we've already just said this because if I start with such a sequence and I look at the corresponding cones of all the consecutive maps, then these cones actually form an exact triangle. And exact triangles in this language are precisely the triangles that totalize to zero. So, therefore, because of the octahedral axiom, if you will, in its enhanced version, the third isomorphism theorem, this thing This thing actually does indeed form a complex. And that's maybe sort of the most illustrative example where you can see already some of the basic features of how this game is supposed to work. Okay, so now what are we doing in this context? Well, as I said, the hope is to categorify as many concepts, results, and so forth from homological algebra to this part here. The current stages that we're The current stage is that we're trying to find just a list of examples of what we can do. So there's classical complexes here, which we can write down. Can we find some reasonable categorical analog? And then the question is, what can we do with it? So that's one thing we're trying to do. Of course, we have certain applications in mind, which I will ignore today. And then the second thing, which we're also trying to do at the same time, is we're trying to somehow find some axiomatic language in which we could then potentially formulate this kind of homological algebra. And so the axiomatic language, let's say in this classical context here, is the language of additive categories. So all these are concepts which I can actually formulate in any additive categories. And that's typically kind of the starting point which I take when formulating homological algebra. And what I'd like to explain today is the kind of axiomatic counterpart in which I. kind of axiomatic counterpart in which I would like to formulate such constructions here. And so this is precisely the concept of lax additivity. So this is supposed to be lax additive. Okay, so and these are not going to be categories, but these are actually going to be two categories. So I'm going to write the proper language you actually need to formulate this, but for actually a large portion of this talk, you can just omit this. Of the stock, you can just omit this infinity here and just think two categories, it's going to be fine, okay? And so, what we're now going to do is we're going to recall a few aspects of this theory of additive categories, just foundational aspects, and then we're going to try to find the counterparts which we need to investigate these kinds of constructions here. Okay, so recall like this one possible definition of a concept of an addict. Of a concept of an additive category. So, a category, let's say C is called. I'm going to call this, we're first going to articulate what means a semi-additive. This means two things. The first one is that C is enriched. In abelian monoids. The second one means that C has finite products and coproducts. And then if in addition, so let's say C is called additive. If, in addition, we have one way to say this is just instead of saying it's enriched in abelian monoids, it's enriched in abelian groups. So, in other words, for all pairs of objects X, Y, and C, C X, Y with this addition law here is an abelian group. Okay, that's maybe not the most elegant possible definition, but it is a definition, right? Because what becomes sort of unclear here is whether this notion here is actually a property, which turns out to be the case, as we all know. But nevertheless, this is the formulation which can be sort of where the categorification can be formulated in the most easy terms. And that's why I've chosen it. And so, one specific aspect of additive categories which I'd like Aspect of additive categories, which I'd like to focus on today, is the notion of matrices, which I can use to write down and understand certain morphisms in such a category. So let's recall how this works. So here's some features which we have in such an additive category. So features in C, let's say. Okay. So the first feature is the following. Is the following. I can formulate certain maps. First of all, if I choose a pair of objects, I can form their co-product. So that's A0, coproduct A1. And then I have another pair of objects. I can form their product, B0 and B1. And now what I can do is I can express morphisms. I can express morphisms from this cop product here into this product in terms of matrices. So just singling out these components, which I need to formulate these maps in terms of the defining universal property. So therefore, that in this situation gives me a two by two matrix, alpha zero zero, alpha zero one, one zero one one, where each of these alpha ij is an element of home from From AJ to BI. Okay, so that's clear. So this is something I can do. I don't need semi-additivity for this. I just need the existence of products and co-products to be able to formulate this statement. But now the second step, that's something which actually requires semi-additivity. That's the second feature. So that's the second feature. And that's the following: FA0, A1. I take the scope product here, and I take the product on the same pair of elements, A0, A1. Then there's a map which I can write from here to here, which for the first observation, I can write in terms of a matrix. Not a particularly interesting matrix. Not a particularly interesting matrix in this case, just the identity matrix. And the statement is that due to the semi-additivity, so what I now really have to use is that it's enriched in abelian monoids, I can actually show that this map is an isomorphism. Okay, so and one way to do this is to actually exhibit this co-product here via this map, which actually. Via this map, which actually specifies for me a cone over this pair of objects A0 and A1, I can show that this cone is actually a product cone. And to show that this actually satisfies the required universal property, I have to eventually produce a map into here. And in order to produce this map, I will have to take a sum of the two components. And that's why I need this addition law to do so. And then once I have specified this, there is a result. Once I have specified this, there is a resulting concept of matrix multiplication. So typically, after having realized that this is an isomorphism, I just identify all these symbols and call this the direct sum. And so there's a resulting matrix multiplication, which we're, of course, all familiar with, which goes from a, let's say, I have. A let's say I have this direct sum a0 plus a1 b0 plus b1 c0 plus c1 and then I can express this composite here by the by the matrix product. Then, sort of, the crucial part I use in this matrix multiplication is at In this matrix multiplication, is at this stage where I really use the matrix to specify a map into the product, I will then have to use the inverse of this map to identify it with a co-product and then follow up with this other matrix so that I get the new matrix. Okay. Right. Um what do you mean? Yes. No why. I'm never taking sums of, these are all going to be morphisms, right? And I'm only adding morphisms all the time, right? And so, right, I'm only adding these more. Yeah, but the formula sort of makes sense. I'm only referring to the addition law, which I have on the home sets. Law which I have on the home sets and never any kind of addition of elements. Well, this is a matrix. And what I mean by this is that it actually defines in this sense by the universal properties, right? So how do you specify a map from this coproduct into the product? You will have to specify, you will have to specify some to specify this map into here. I will have to specify cone. Into here, I will have to specify a cone over B0 and B1. And how do I, you know, for each of these, a map out of the co-product is obtained by specifying a cone over under A0 and A1. And then that leads to precisely a collection of four maps which I have to use in order to specify, to satisfy the requirement given by these universal properties. So, stepping back, point one, which is introducing a notation from Walton Neo exists, then point two. know exists exactly that's just exactly that's just the notation to specify this morphism here in terms of the universe properties and then um by this by this the remaining mambu jumbu i will also understand that here i write a matrix which specifies this map here write a matrix specifying this map i can of course compose these because i'm in a category and then this must again be given by some matrix and if you work out the formula this is going to be the usual matrix product Yeah, it's always refreshing to somehow go back to these basics, you know, especially after seeing what's now going to happen. Yeah, now we're going to basically try to transport everything we have said into this categorical world. Into this categorical world. And so I'm just going to give you right away our definition of first a lex semi-additive category and then a lex additive category, following along basically exactly these lines here. This is now a definition. And so this is due to the joint work with Chris and Wilder. And so, the substantial part is really to formulate the definition, and then sort of the rest somewhat nicely follows from this. And so, we now don't take a category C, but infinity two category. We're going to denote by bold face letters C and this is called lack. Is called lax is called lax semi-additive if certain conditions sold. Number one, C is enriched. Here it's in abelian monoids, and the replacement for this will be C is enriched. In yeah, so one way to say what it Yeah, so one way to say what a two category is, is just it's a category enriched in one categories. And that's also how you can say what an infinity two categories. It's a category enriched in infinity one categories. The actual definition will then be slightly more elaborate, but this is how it strategically works. And so now what I'm requiring is that it's not just arbitrarily enriched in Infinity One categories, but it's enriched in Infinity One categories, which have co-limits, by which I always mean small co-limits. Which I always mean small colonies. Yeah, so in all the morphism infinity one categories, I can take a co-limit of an arbitrary small diagram, and also the composition law respects these co-limits. That's what this means. Second, C admits, yeah, so co-limits kind of funnily now play. Limits kind of finally now play the role of these sums in this kind of a complex. And our C admits lacks limits and co-limits. I will say a few words about what this means. And that's already the definition of like semi-additive. And then if C is in addition, If, in addition, for all x, y in C, I have that this home category, the category of homomorphism between these objects, here was supposed to be an abelian group. And that now is within the collection of rules which we had. Abelian groups are supposed to become stable infinity categories. So the requirement is that all of these categories, which at this point a priori are only. This point a priori are only required to have co-limits. Now they're supposed to be stable. Which, in this context, is, of course, a property. So that's the definition. And now we're going to try to play exactly the same game as we played here. We're going to now investigate certain features which hold in any such a Hold in any such a lax semi-additive infinity two category, trying to sort of steer towards having a description of certain maps in terms of matrices. So, but before before we do this, we kind of have to understand what are going to be the replacements for this co-product of two objects here and product of two objects here in this context. And so, these are going to be certain. And so these are going to be certain universal lax constructions in any, which I can formulate in any infinity two category. And so they're going to be based on the following input data. So instead of starting as here, when I try to formulate this co-product and product with a pair of objects, now I start with a pair of objects A and B. And the prototypical example could, of course, be that C is the two categories of stable categories, and then A and B will just be stable categories themselves. And this one morphism, which I'm not going to write down, is just going to be a functor. So this is going to be one morphism. And now there's going to be certain universal constructions which replace co-product and product, which take this as an input data. And they sort of heavily refer to the fact that we're now working in two categories: so that we have natural. Two categories so that we have natural transformations in addition to just the one-morphisms. So, how do these four universal constructions look like? So, I'm going to write them by specifying how the cones look like that I want to use to specify these constructions via some universal property. So, here's always going to be the map F. And now I could try to formulate some kind of a directed co-product of A and B along this morphism F. And that's what I'd like to refer to as. And that's what I'd like to refer to as the Lax co-limit. That's one such universal construction. And if I specify a Lex co-limit, then the cones under this diagram have to have a natural transformation, which is not necessarily required to be an isomorphism. And it goes in a certain direction, namely this direction. So that's one version of such a co-product in this kind of situation. Then there's going to be a version of a product. Of a product, and that's a lax limit. And that also needs a certain shape of cone where this two-morphism points in this direction. But then I have more, namely some kind of arbitrary choice that this two-morphism has to point in this direction. It could also point in the other direction. And so that's what is typically referred to as an oblax co-limit. Co-limit. And this one here, where I refer to this morphism, is an oblax limit. Okay, and so these constructions here, these I want to somehow think of as some kind of two categorical counterparts of this co-product. And these universal constructions I would like to think of as. I would like to think of as some two categorical counterparts of a product. And now we're going to go ahead and try to formulate maps between these objects here in terms of matrices. That was the first step which we noted here. I'm going to badly run out of time, but let's see. So, how do I do this? Well, so let's focus on these two guys here first: A, F, or let's now say A0, A1. Much of transformation here. And then here, I'm also often referring to this as to really signify that this is a lax version of a co-product. I will now draw some kind of a symbol like this. We're trying to specify a map to P0, P1. P1. That's a Lax limit like this. And then there's F, and I have the natural transformation like this. And it turns out, again, just using universal properties, which define these gadgets here in terms of saying that this is supposed to be a universal example of a cone as depicted, allow me to write down certain matrix data. But now it sort of becomes a bit more interesting. But now it sort of becomes a bit more interesting. Alpha 00, alpha 01, alpha 10, alpha 1, 1. And now, because there's these natural transformations here, which also somehow act on these morphisms and need to be specified, that actually leads to certain, think of this as certain arrows, certain kind of commutative diagram, which I have to draw inside this matrix. Okay, and so how does this look like? Well, first part is kind of just analogous to before. Alpha ij are going to be maps from aj to bi. And then I have this diagram here. So, what does this mean? This signifies certain morphisms and how do they look like? Well, I have alpha 0, 1. That's going to be a map from A1 to B0. And what I can now do is I can precompress this map with F. And then this natural transformation has to relate this to the morphisms from A. Relate this to the morphisms from A0 to B0. So that's going to be alpha 00. So, and that's a natural transformation, and that's precisely what this arrow here signifies that I have to write this down. Similarly, same thing down here. I have a map to alpha one, zero. And then I have these morphisms here, and they're given by composing. Oh, sorry, there's not f here, but that could in general be some g. G alpha zero one, there's going to be some natural transformation to alpha one one, and then similarly here, there's going to be a natural transformation of this kind. So that's the data I have to specify, which means this commutative diagram. And then there's a condition which tells me that it commutes, which basically says that if I apply both G and F to this first guy here, then I can use these natural transformations to produce a square, and it's supposed to commute. A square and it's supposed to commute. Okay, so plus some condition which tells me that this commutes so far, so good. So I can write down again maps from this universal object to this universal object in terms of a certain kind of matrix. And this is exactly what we mean by a lax matrix. So it's a matrix kind of enhanced with this lax data. Okay, now that again works in any category where I have an existence of these universities. Where I have an existence of this universal object. But now we have this additional statement that it's actually enriched in infinity categories with co-limits. And that will now lead to the fact that exactly the analog or an analog of the matrix, which we wrote in this case, namely this identity matrix here, can again be used to identify a priori two of these constructions. So that's the second statement. So that's the second statement. So A0, A1, F. So this is going to be this lax or directed co-product, if you want. And then there's going to be a matrix I can write down, which compares these two universal constructions. And how does this matrix look like? Now that's a bit funny. So the natural matrix you can write down, which So, the natural matrix you can write down, which will then actually turn out to be an equivalence under this assumption, is actually the following. So, we have identity on A0, identity on A1. Now, there's a map from A0 to A1, and that's actually going to be F. And I have to write this down here. So, and then here in this guy, I have no clue what to write because there is no specified functor from A1 to A0, but I have existence of co-limits, in particular the empty co-limit. limits in particular the empty co-limit and that's going to be some initial object which I write down here so there's a just canonical X matrix which I can write down and the first statement we prove and the funny thing is the argument is exactly identical to the argument you I mean with the usual infinity blah blah to the argument you do in the classical case where in order to show that this guy actually also satisfies the universal property of this product construction you have to produce a Construction. You have to produce a map into it, and you produce this map by taking a co-limit of certain data which you extract from the cone data instead of taking a sum of the two contributions. So we're kind of happy to see this. Yeah. So far it's a semi-limit, but so far it's just semi-additive, yeah. And so if it's semi-additive, then this will be an equivalence. So that's the first theorem you want. Want to and then so yeah, so let's say for time reasons, I don't have time to say we get a certain matrix multiplication. But trust me, you don't want to see the formula. I can show you later. It's sort of interesting, but the reason why it must be slightly awkward is because this matrix here, it's somehow quite far from the identity matrix. somehow quite far from the identity matrix. In particular, on K0, it doesn't apply, it doesn't sort of induce the identity matrix. So in this kind of identification which you need between product and co-product to make the matrices composable, you use some really weird identification. And that really weird matrix product, which is still perfectly legitimate. You could also use K0 of such a matrix here to do matrix multiplication in any additive category, but you'll probably drive your students crazy if you do this. Your students crazy if you do this because the formulas will look very strange. It's lower triangular F contribution. Yeah, exactly. So therefore, I'm not going to write this down, but it becomes much nicer in the context when you work in an additive category. So lax additive. So now, if it's lax additive, and that's where sort of the nice Of the nice part begin then. So, what we learn here is, therefore, that two of these universal constructions for semi-additive categories, namely this guy here and this guy here, turn out to be equivalent, but we haven't talked about these other constructions yet. And what turns out to happen, if you assume that it's lacks additive, then also, then all of these four universal constructions will actually become identified. And that's the analog of the fact that you only have one kind of a sum instead of all these different universal constructs. Sum instead of all these different universal constructions. Okay, so if slacks additive, then I can actually also reverse this arrow. And I can write down an identification between these two guys in terms of a certain matrix where now you know the arrows will now point in a different. Where now you know the arrows will now point in a different direction because this natural transformation has been reversed. And it turns out the matrix you have to write down to make this identification looks much nicer than this one because it's given by the negative suspension. So loops or yeah, whatever, negative shift. Now I can write zero because I'm in a stable category. So there's actually zero objects. And this is kind of a fun matrix, which sort of has a feature that, you know, this That this kind of commutative matrix, which you produce at the very end here, it also has the additional feature that it's bi-cartesian. So it's kind of an exact triangle, if you want. So you can therefore decorate this with this funny symbol here. It's a bi-cartesian matrix. And now you see this one looks much closer to the identity matrix up to the sine factor here. On the level of K0, it produces the identity matrix. And therefore, you expect that the matrix multiplication law will That the matrix multiplication law will look much closer to what you're familiar with from linear algebra. Okay, so let's therefore work it out. No, no, no, I think no, it's yeah, that's an interesting, yeah. First linear algebra problem, try to find a criterion which matrices are invertible, or try to write down inverses of matrices, exactly. So, yeah, you can you can sort of now. Well, do we have a couple examples that might I think yeah let's let's let's yeah let's it's there's no clear reason why this yeah let's talk about it later um yeah you you can now go back to sort of the very initial stage of learning math now you're facing the challenge of understanding matrices matrix multiplication how do you invert matrices so okay so and so So, there's a matrix multiplication, and this one is kind of just really fun. So, that's why I'm going to write it down somehow. So, this is the formula. Right, so these arrows mean something similar to what we wrote here. So, it's not literally arrows because that doesn't make sense. These are two matrices. I'm multiplying them. And now I really do matrix multiplication in the usual sense. I do beta 00 hits alpha 00, so I compose them. Alpha beta 10 hits alpha 01, so I compose them. And now you realize from this lax data, which is present in these matrices, there's Data which is present in these matrices, there's actually going to be a canonical map from here to here. So you take this map and you take the cone, right? Because remember, this is a morphism. It's a natural transformation. So it's a morphism inside a stable category. So I can just take a cone. So instead of, oh, sorry, this is one, zero. So instead of in the usual matrix multiplication, where I would now take the sum of these terms, I now take the difference. And that, of course, has to do with exactly the sine twist, which is tweaked. The sign twist which is tweaked into this identification here, and now it just continues exactly like that. So just do beta zero zero, beta zero zero, alpha one one, beta one zero, and so on. You can figure it out. Okay. And so, okay. So five minutes until fifty is the the time step right, yeah. 50 is the times the flight, yeah. Okay, um, right, so now the question is: is there anything you can do with this? Okay, so now we have some kind of interesting description of how to describe matrices between these between these guys. Here I forgot to say that this is also an equivalence. And so we're now really in the situation in any lax additive two category that all of these universal constructions. Universal constructions. They're actually canonically identified by these choice of matrices. And so that's why it makes sense to actually give them one symbol. And that's maybe just direct sum with an F, which we can now use a similar way. Okay. And so now here's some application. Yeah, it's more like a kind of a fun computation. Could also do this in sort of different. also do this in sort of different by different means but um i think this this kind of a technique of understanding these maps um between categories with semi-orthogonal decompositions it does sort of bear some computational power which you could hopefully apply also in other contexts and so the specific application um i would like to explain is um this joint work with misha kapranov and uh and vadim shechmann So, this is with Kapranov and Schiftman. And that's the following. So, we now use a specific example of such a category, which is stable infinity categories. That's, of course, the prime example you may sort of care about, something like abelian groups in this world. And so, this are in the way I formulate this, you need this to be precise. In the way I formulate this, you need this to be presentable stable categories because I wanted to have co limits. But as I say, finite co-limits, then you can just take stable categories. And so that's an example of a lax additive category. And so we can apply this formalism. And now, so here's the situation. If you now have such a functor f from a to b, then this unit. Then this universal construction, it doesn't matter which one I can take because they're all identified. This construction, it actually has the same orthogonal decomposition as SOD with what is called the gluing functor. Gluing functions process of starting with a functor f from A to B. a functor f from a to b and producing this um producing this universal this this lag sum here it is actually the way to recover a category with a semi-orthogonal decomposition and a gluing functor from exactly this data that's the way to recover it um in in in this context here so that's encoded sort of um in here so sod uh let's it has several sods but let's say it has one of exactly this kind as i started with okay and so now let's let's um try to write this so i have this sod I'm trying to write this. So I have this SOD, just AB, and then I'm going to think of this as saying that there's a gluing functor from A to B, which is given by this F. And now, as presumably many of you will know, I can find many other semi-orthogonal decompositions by a process called mutation under the presence of adjoints. So the existence of adjoints of this map F. So in particular, I can mutate to the left. If F has right adjoints, If f has right adjoints, and I can keep doing this. I can keep doing this. Let me say b prime, a prime, and then it continues like this. And if f has right adjoints, I can mutate into the other direction in the sense that each of these consecutive pairs actually forms an SOD with the gluing functor which we wrote down here. And now you can ask the following question. And now you can ask the following question, which seems kind of a kind of a strange question at first, but it turns out to be a very interesting question. And that's the following. That's the following. It's, I can start with this first guy. I can then move in a first step. So, this is this first semi-authority. step so this is this first semi-orthogonal decision which moving functor f then i can mutate to the left let's say b prime a um a prime sorry b prime b prime in the second step so first step second step and now what could happen is after a certain number of mutations um i could just come back to the same orthogonal decomposition which i started with Which I started with. So let's say this happens at the nth step. Okay. And now this, if this happens, then the SOD is called n-periodic. And the funct f I have started with, that's sort of a definition, is what is because everything only depends on f, and we say that f is n-spherical. And spherical. Okay. Now, what's the point? You can use now these matrices. The question is, what is the condition on F you can write down to figure out that this phenomenon actually happens? And so these Lax matrices, they give you a way to just really compute this by brute force. Because each of these guys here, right, in an SOD, that's just some kind of a coordinate system on this category C. On this category C, because every object in this category has unique coordinates in the sense that there is a morphism from A to B. These are the two components which specify the object. That's a different coordinate system. And as we know, as we teach our linear algebra students, it's interesting to compute coordinate change matrices. Okay? And so now we can just compute these coordinate change matrices. We can just write them down. Okay. And so they let me just maybe tell you how they look. This one looks like this. This one looks like this. It's identity here, identity here, and then I have so then it has some F. It has actually the right adjoint sitting here. I'm kind of identifying the mutated copy of B with itself to write down the matrix and these coordinates. So it's kind of a confusing process, but you can eventually unravel it. And then what you end up computing is just Is just a product of matrices which look exactly like this, but they will have these increasing powers of adjoints of this functor f. And now you can just compute this whole product, and this will close up periodically if and only if, that's one way to formulate this criterion, the off-diagonal entries of this matrix are zero. So you just compute the off-diagonal matrices, and there's going to be some crazy expressions. You somehow start with looking at the chain of all adjoints. Looking at the chain of all adjoints from the nth right adjoint to f itself. And then you apply all possible ways of taking the co-units of these consecutive adjunctions which appear here. That produces some gigantic cube. You totalize this cube, you get a complex of functors, and the condition is that this complex has to be acyclic. And that will lead to certain interesting odd equivalences, which are called, you know, which are higher versions of what are usually called spherical twists. What are usually called spherical twists, and it produces some interesting class of hopefully potentially new and interesting odd equivalences, which you can kind of control by exactly this matrix calculus. So I'm already over time. Let me stop. We should change the market. Okay, so what these are. Okay, so is there any question or comment? So this N periodicity is there any connection with fractionally Calabiao property of a triangulated category, for example? Yes, so it's for example if this ambient category is a Calabiao category, but actually what's the interesting condition here is that it's fractional Calabiao. interesting condition here is that it's fractional Calabio, right? Because if it's fractional Calabio where the denominator is n, maybe some power of n, then you can mutate just by applying the sear factor. And then if the sear functor after some steps is just a shift, then that actually means that you have periodicity. And so therefore, for example, any semi-orthogonal decomposition you write down of the derived category of like some Dinkin quiver, right, will have exactly a periodicity phenomenon of exactly this kind. It's this kind. And such a different, such an example, like the Dinkin Creep, that I have the category of the Din King Freebot, can be legal as they example those. Yes, exactly. So in this case, I'm not, yeah, the spherical twists will then, yeah, I'm not completely sure. I mean, of course, you can write on completely random SODs, but they will typically have something to do with these, I mean, with lifts of the wild group, right? So the Coxeter functor, for example, may appear. It already appears as a spherical twist, it also appears as an n-spherical twist. Appears as an end-spherical twist and so forth. And then there's sort of analogues of spherical objects. So there's higher spherical objects. So, for example, structure sheaf in a Calabiao variety, that's an example of a usual spherical object, which by the way, in this terminology, would be four spherical. So, if this closes up after four steps, that's the usual notion of spherical functors due to Rina Ano. That's actually an absolutely beautiful observation of Daniel Haldane. Observation of Daniel Halpen-Leitster and Ian Shipman. Yeah, it's so four-spherical means spherical in the usual sense. Yeah, sorry. So four spherical means spherical in the usual sense. And then if you deviate from in the lower powers, you get statements like, so for example, f is two spherical if and only if it's zero. It's three spherical if and only if it's an equivalence. And then the higher sphericalness conditions are the new interesting ones. So, for example, six. interesting one. So for example, six, so there are six spherical objects. And one example of such a six spherical object is, so an example of a spherical object is a structure sheaf on a Calabiao variety, an example which would be four spherical in this sense. An example of a six spherical object is the structure sheaf of an Enrico surface. And that way you can produce, yeah, so other interesting. So I think in whatever corner you will find. In whatever corner you will find the usual examples of spherical functors or objects, you can presumably find some generalizations here. Will Donohan actually just wrote a paper using some kind of root stacks to also find a different class of examples of this kind Yeah, so my question goes in a similar direction as you indicated already. I mean, what are specific applications? I mean, this application in a sense is quite general or abstract. Is quite general or abstract, yeah? But what was motivating you to develop that sort of machinery? It's, I would say, in general terms, it's just really this desire, hope, dream, somehow, which is completely unclear if this is ever going to really lead to something like really useful in the end. I mean, we now have many examples which are already by themselves interesting, but what's unclear if there's really something like a theory of categorified homological algebra. This would be our dream. Homological algebra. This would be our dream. Just like you do usual homological algebra, deriving functors, writing down complexes by some universal constructions, and so on, you can just imitate this in this context. And why are you interested in this? I would say it's a shift of perspective from trying to understand categories by writing down their objects and morphisms between them and then sort of working internally in a category. What we try to do here is take one step back because we realize that this can be very difficult. A concrete class. A concrete class of examples where this is extremely difficult are for Kaya categories. It's just notoriously complicated to write down internally what the objects, what is the kind of data I have to choose to write down the objects, how do I write down the morphism complexes. It's really very complicated to the extent that only experts could ever do it. And it turns out, essentially, in many of the situations where you can actually really compute these Foucault categories, what you apply is exactly this perspective. What you apply is exactly this perspective that you take one step back and you think about how you can produce the category itself, regardless of how its objects look like, by some kind of universal constructions. And that's really the application that we have in mind in the end, that we can produce such complicated categories in rather simple terms where some of the complexity has gone into these higher structural machines that we actually apply to meteorplate. That's the hope. And these are just kind of fragments. Fragments which we're trying to develop and then eventually piece together to see the whole theory. So Ed Siegel has this result that every auto equivalence is four spherical. And we might ask the question why four is a special number. Yeah. And it's not special. It's also true for n scientists. It's, I mean, from any spherical functor, you can actually. Factor, you can actually construct, there's a canonical construction which constructs an SOD, which will then be n spherical for any n. And that's in fact more or less how we discovered this notion, actually, because we saw this sphericity phenomenon. And then we wondered, okay, is it just another way to encode a spherical functor or is it something new? And it turns out it's something new, and that's what this is. But therefore, every odd equivalence can be realized as such an n-spherical twist. Is there more questions? And also, is there any question from Zoom audience? So if not, let's thank the speaker again.