Well, first of all, I want to thank all the speakers, sorry, all the organizers for organizing such an amazing workshop. I've learned a lot from everyone, and I'm very, very happy to be here. As introduced, my name is Shaun Ying Li. I'm a final year PhD student at Stanford. Hopefully, I'll graduate in a few days. And well, the topic of my talk today is this searching for consistent associations with the multi-environment knockoff. With a multi-environment non-computer. While you may get confused, what I mean by consistent associations and what I mean by multi-environment non-computer, I'll introduce all of that in a few slides. Okay. So this is a joint work. It's a group of great collaborators, Mateo Cynthia, Yanibromano, Emmanuel Candes, and Chiara Zapati. And we are happy to see our paper pop. Are happy to see our paper published in Biometrica in 2021. Okay, so let me just get started with a very, very brief motivation. So in genome-wide association studies, geneticists usually measure hundreds of thousands of genetic variants and wish to know which of those influence a particular trait. So, for example, let's think about the trait being height. And so, we might want to Be in height. And so we might want to know what are the genes that actually influence height. Okay. So in the standard analysis, people usually focus on the variables xj that are marginally associated with y. But this is probably not the real thing that we want. The results are actually very far from identifying the causal genetic variants. And to illustrate that, here is just a very, very simple example. A very, very simple example that shows why this is the case. So, consider here we have variables x1, x2, x3, up to xp. They are, think about them as genetic variants. And we have a treat that's called y. So, let's say this is height. So, in this simple example, we know that x2, x3, and x5 are the genetic variants that actually causally influence this variable y. Influence this variable y. But if we just do a naive marginal hypothesis test, we'll find that actually x1 is marginally associated with y, because x1 is associated with x2, and x2 is the one that actually influenced y. So if we just test for marginal association, we would probably end up reporting x1, x2, x3, x4, etc. But among those, there are a lot that But among those, there are a lot that we don't really want to report. So, the problem then is what can we do in scenarios like this? So, a slight better notion of hypothesis testing is this. We test for, instead of the marginal independence, we test for the conditional independence. Here we say we want to test for the hypothesis HJ, which says XJ is says xj is independent of y given x minus j where this x minus j is all the other x except x j so in words what is this hypothesis says is if this hypothesis is true then the j's variable does not provide information about the response variable y beyond what is already provided by all the other variables okay so if this Okay, so if this HJ is true, then we call this J's variable a null variable, and otherwise we call it a non-null variable. So the goal would be we want to find the non-null variables because the non-nulls are actually the ones that influence y. Okay, so if we go back to the example we talked about at the very beginning, we will see that now if we test for the conditional independence hypothesis, for example, if we look at x1. For example, if we look at x1, then in this time, x1 would indeed be independent y condition on x2, x3, and the other axis. Okay, so this time, if we sort of correctly transfer the hypothesis, then we would ended up reporting the correct ones, the x2, x3, and x5. So here the goal is to actually report the correct null-nose controlling the false discovery rate. The false discovery Discovery rate. The false discovery rate is defined as the expectation of the false discovery proportion, which is the number of false discoveries over the number of discoveries. The sort of amount you make, you select a few variables among the selected ones, how many of them are true non-nodes, how many of them are false non-nodes, and then sort of this false discovery proportion is simply the ratio of the number of The ratio of the number of false discoveries over the total number of discoveries, and we take the expectation. That's the idea. The goal would be to find the non-nodes with IDR control. Okay, so in this project, we actually ask for something that's even more. We ask for a consistence across environments. So imagine we collect data from different environments. Think about in the genetic example, if we have a few different populations. We have a few different populations. This might be like a European population, this might be an Asian population, etc. And then on those different populations, we measure the same variables and the same trait. Okay. And then so we say a variable j is now in the environment E if xj is independent of y given x minus j. Okay, so here we would like to find variables that are. To find variables that are non-null in all environments. That means this variable would be actually influencing this trait in all the populations. So specifically, we call our hypothesis a consistent independent hypothesis. And we say, and for those hypothesis, if a variable is null, then it means it's null in at least one of the environments. And it's only non-null for this hypothesis. non-null for this hypothesis if it's non-null in all emergencies. Just to make sure that we are on the same page, if we have like null, non-null null null, it's still a null. If we have non-null, null null, non-null, then it's like a real non-null we want to count. So sort of we are asking something more. We want things to not only be non-null, but also be non-null across all. So, naturally, you might want to ask the question: why do we care about such consistent independence hypotheses? It almost feels like the previous conditional independent hypothesis is no. So, here I'll just talk about one single scenario that kind of explains why this can be helpful. And if you'd like to look at the paper, there are a few other examples we gave that shows that considering such hypothesis is helpful. Such hypothesis is helpful. So, in this scenario, we consider there exists an unobserved confounder. So, let's look at this example here, this very simple tiny plot here. So, again, we have variables x1, x2, and x3. Think about them as a genetic variant. And we have a trait Y. Here, in addition to the X and Y's, we have one thing we call this C because it's a confounder. This C because it's a confounder, and we didn't like make it gray, we only make it white because we didn't observe it. This is an unobserved confounder that could potentially be correlated with the X. So in this scenario, we see that actually the underlying true model says Y depends on C1 and X1 causal. Okay, now let's say in this environment, Let's say in this environment, think about this as like a European population. Maybe we happen to see that C1 is correlated with X2. So in this case, if of course we didn't get to observe the C1, we only observed the X's and the Y's. If we test the original conditional independence test, we will ended up reporting X1 and X2 because X1 is, of course, not independent of Y given the other X. Not independent of y given the other x. x2 is also not independent of y given x1 and x3 because sort of it's correlated with the c1 and c1 as some additional information about y. So we'll end it up, we'll end up reporting x1 and x2, but x2 is not something we actually want to observe, want to report because y only depends on c1 and x1. x1 is the thing we want to report. Now, if we happen to be lucky that in a different To be lucky that in a different population, let's say this population is the Asian population, and maybe we observe some different covariance structure, different like dependent structure between the X and the C's. Then maybe in this case, C1 is not correlated with X2 anymore, it's correlated with X3. And in this case, of course, if we still test for the original conditional independent hypothesis, we would report. We would report X1, X3. But if we take into account both environments, and if we try to test for the consistent independence hypothesis, as I said before, the goal is to find non-nulls that are non-null sort of in all environments. In this case, the thing that's non-null in all environments is this X1. So we see X1 is now now here, here, X2 is only now here, X3 is only now here. So we would report the correct thing, which is X1. Report the correct thing, which is I hope this gives like a very tiny motivation of why we might want to consider such consistent independence hypothesis. With that, I'll then move on to talk about the methods. What can we actually do to test for such hypothesis? So recall the title of the paper has this word multi-environment knockoff filter. environment knockoff filter so what it says is we our methodology is based on the method of knockoffs and take into account like multiple environments so the method of knockoffs was originally proposed by Rina Barber and Imanium Candice in 2015 and later extended to the model X setting by Candice and co-authors the this method allows people to test the original conditional independent The original conditional independence hypothesis and it probably controls the false discovery. I don't think I'll have the time to go into every single detail of what people do in NORCOF, but roughly, the methodology knockoff can be separated into three different parts. The first part, you make use of knowledge of the X distribution and construct some knock of variables. And in the second step, you make use of knowledge of knowledge. And in the second step, you make use of the constructed knockoff variable and to compute some important statistic. Specifically, it's going to be a vector of length p. P is the number of variables you have. And then the third step is you look at the statistic and apply an algorithm on it, and which outputs a selected set. So we wouldn't touch on the first step. What we'll do is we'll modify. Will modify the second step of the original null computer and make it a multi-environment null computer, which probably controls the false discovery rate for our consistent independence hypothesis. So I'll go into a bit more detail of what we need from this informal statistics in knockoffs. So as I said before, we compute the informal statistics for you based on the knockoffs. Based on the knockoff variables we computed from some machine learning algorithm. Well, I think we, if you want, it can also be like a deep learning algorithm. So I guess that's connects well to the topic of our workshop. But really, what we want from this W is we want the large WJ to correspond to the variables that are more important. The most important property that we need from this statistic. That we need from this statistic is we require a condition on the absolute value of the W vector. Size of the null WJs need to be IID coincidence. And this is something that's crucial for the false discovery rate. And this can be guaranteed from the construction of the knockoff variables. Roughly what it says, here is like a nicer schematic plot of what happens. Are a schematic plot of what happens? So, if we just plot all the variable, all the important statistics according to their magnitude, according to their absolute value. And then the red dots correspond to the non-naw, and the black dots corresponds to the null. Of course, in practice, we didn't get to see what are nulls and what are non-nulls. But this property says for the black dots, that is for the null variables, the sign should be like 50% plus. Be like 50% plus and 50% minus. So, this is the property that is the most important to guarantee the false discovery control. So, just one quick remark I want to say here is the false discovery rate is still going to be controlled if the signs are actually smaller than coin fleet, meaning that if we have a higher probability of it being negative than being plus, then we are still good to go. So, this is because. Go. So, this is because things become a little bit more conservative, but that's still fine for false coverage. Okay. So, in the knockoff world, once you have this important statistic, you'll go to the third step that makes use of the important statistic vector and get a reported set. And this is what they do here. I'll just very briefly touch this. So, again, if we just plot the value of If we just plot the value of W of the important statistic according to its absolute value, and if we move from right to the left and like get a larger and larger region, like the gray one here. And then if we compute how many, if we just count how many minuses we have in there and how many pluses we have in there, and we just compute this quantity, which is one plus number of minus over number of plus. Over number of plus. We find the minimum time t, such a minimum like threshold t, such that this quantity is smaller than q. We call this quantity tau, and then we just report it as hat, which is the j's such that the wj is larger than this stopping time tau. It's a little bit complicated, but really the take-home message is if you just do this and then you're you're and if you're And if your sort of important statistical W satisfy the property I mentioned on the previous page, then the false discovery rate is going to be controlled for your selected sites. So with that, I will then go into what do we actually need to do when we have data from different environments and when we want to test for the consistent hypothesis. So, as I said before, So, we, as I said before, we call our method a multi-environment knockout filter. We have two different versions of the method. Here, I'll talk a bit more detail about the simple version, and we have a slightly more complicated version, which I'll only briefly touch on. So, for the simple version, it's really pretty simple. We just separately treat each environment and just compute, we just construct knockoff separately for each environment and compute important statistics. And compute important statistics for each environment. So we got a few different vectors. So we get like for the first environment, we get a vector of lens p, second environment, we get a vector of lens p, etc. And then we just combine all the important statistical vector into one single vector. What we do is the following. We take the sign of the aggregated W to be the minimum of the sign of the like inverse. Of the like environment wise W. And then for the magnitude or for the absolute value, we just combine them in any way we want. You can take the sum of the magnitude, you can take the product of the magnitude, anything you think makes sense. And then now we just get a new aggregated W, which is a vector of lens P as well. And then we just do apply this algorithm on top of the new aggregated W, and we get a selected set. And we get a selected set. Now, the claim is that using this algorithm, we are going to the false discovery rate is controlled for the consistent independent handout. So you might ask why this is the case. So again, we will be making use of the coin-free property. So recall that for this consistent independence hypothesis, a variable is not means that it's not in. Null means that it's not in that loose value mirror. Without loss of generality, let's say it's not in the first universe. Then the sign of this aggregated Johyo, by definition, is the minimum of the sign, which is smaller than the sign of the first environment. But this, we know that this is IID coin flip. This is just coin flip. So what this shows is that the sign of the aggregate in Java US. Of the aggregated W is actually smaller than or equal to 0.5. So we are still good in terms of false discovery rate control. Okay, so that's what we do for the simple version. Now, you might ask, this doesn't seem super powerful because when we compute the important statistic, we are only making information, making use of information. Make a use of information from its own environment. For example, this W1 would only depend on the data in the first environment. But we have like a few different environments and maybe it's possible to borrow information from each other and make this W something that's more powerful. So that's exactly what we do in this complicated version. This is a little bit complicated, so I just very quickly go through what we did. Did um, this is we do something which randomly swap the null called variable with the original variable. Um, but anyways, with this, we are able to get some empirical prior, which tells us like how important your variable is. So, but more importantly, this information, we got this from like pulling information from all environments. So, this is an So this is, and then we will use this to help boost how we want to compute the important statistic in each environment. Really, so this is we make use of information from all environments, get some value pi here, and use those pi into the computation of the environment-wise important statistic. So, with this, we are able to sort of borrow, for example, for W1 here, it's able to borrow information. Here is able to borrow information from the blue and the green population. Feel free to check the paper and go into more detail what this configuration version does. Cool. Just one final thing. We are able to extend our method to testing for partial conjunction as well. So what the partial conjunction says is now instead of trying to find variables. Now, instead of trying to find variables that are non-null in all environments, we might ask for something weaker. We can try to find variables that are non-null in greater than or equal to, let's say, two environments. Let's say in total we have five environments. We might be interested in variables that are non-null in three environments or in two environments or in four environments as well. So, this partial conjunction is nice in the sense that it's more robust compared to if we only test for the original. Compared to if we only test for the original conditional independence hypothesis, but it's also more powerful compare when you do the test, it's actually going to be more powerful than if you ask for the more stringent non-O environments. So this is especially true when, let's say, some of the environments have very small sample size. Then if you want to find variables that are not known in all environments, the power can be really bad. Environments, the power can be really bad. Because you would imagine if you only test for that environment, the power is already very bad. Now you ask for more, you ask for something that's normal in all environments, the power is going to be even worse. So if you test for partial conjunction, it's going to be slightly more powerful than if you really require the strongest. So finally, with this, Finally, with this, we're going to apply the above mentioned method to a Yoko Baobank dataset. So, in this data set, we have data for whether each person belongs to a specific ancestry, and we divide people into a few different populations based on their ancestries. We have five different populations here: African, Asian, British, European, India. In this case, because this India. In this case, because this is a UK biobank data, you would imagine that the sample size for British is like way larger than the other populations. So here it's a result that I want to discuss. So what we do here is we take our trait Y to be counterplatelet. And then we want to know, as I said before, what are the genes? As I said before, what are the genetic variants that actually influence platelet count? So here we show the results on some parts of chromosome 3. Let's look at this resolution 425 here, sort of the last row here. If we look at the green here, this green says 4. What that means is if we test for the partial conjunction test, that is, if we try to look for variables that Try to look for variables that are not in at least four environments, then we would end up reporting, we would find this. That means our algorithm think this block is going to influence Clean Kant and is now now in that list for electrons. Now, if you look at the blue, it says three. Oh, well, we don't seem to have too much blue. I think it's overlap with the green a lot, but we see like a tiny blue here. Like a tiny blue here. So, for the blue, it means our algorithm thinks it's not now in at least three environments. And for the yellow, that's not now in at least two environments. And for the gray, it means our algorithm is confident that it's now now in Atlas 1. Yeah, I think that's basically it. Here are the references for the two knockoff papers I mentioned. For the two knockoff papers I mentioned before. And that's it. Thank you, everyone, for listening.