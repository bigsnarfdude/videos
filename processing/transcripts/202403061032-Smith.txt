Me, but also I very much wanted to be there. Um, and just in the event, I am not able to, and I appreciate the chance to talk to you and join with you a little bit remotely. Uh, so thank you very much for that accommodation as well. What I want to tell you about today is one of my favorite topics. Um, it's uh, it's about additivity and non-adventivity in the quantum capacity problem. But before we want to do that, I know that's not quite. Want to do that? I know that's not quite on topic with this workshop, but I looked at the people who are there, and I think it's on topic with the people who are at the workshop. So, I hope you'll accept this talk as sort of like some light entertainment while you debate, while you sort of do sort of more serious, more serious and impactful work. Oh, anyway, if I was there, I'd talk to you about what I saw on Twitter today, but unfortunately. Unfortunately. Anyway, what did you see on Twitter today? I saw two interesting things on Twitter today. One was about D-Wave quantum supremacy, and the other one was about Chris Ferry's startup and whether it has a quantum computer. It was amazing. So that was my expectation. But obviously, I have strong opinions about both. But not today. Today, I want to talk about. But not today. Today, I want to talk about the good old channel capacity. And let's just look at the classical one for now. So I have some message M Alice wants to send to her friend Bob. And what she can do is use some noisy channel a bunch of times. And she can do it in such a way, hopefully, that Bob can do a decoder and figure out roughly what the message is. You know, with low probability of error, he'll figure out what the message is. Of error, he'll figure out what the message is. This is a nice operational definition of the oh, no, we didn't get there yet. Uh, the capacity of the channel is just sort of the maximum number of bits that you can do this for divided by the number of channel uses as number of channel uses, as the number of channel uses grows. And this is sort of a complicated uh thing in principle. A priori, it might have been very difficult to analyze because. Analyze because it involves a large optimization over this encoder and decoder. You want to pick the best one, you want to make sure it has certain properties. But the kind of magical thing is that if I just consider what happens when I have some input and output to a single use of the channel, in addition to this operational definition, we can also come up with a really clean mathematical expression that characterizes the capacity. It's just the math. Characterizes the capacity, it's just the maximum mutual information between the input and the output that you can generate with a single use of the channel. And this is like a super simple optimization problem. You could just do it on a computer. So you give me a channel, I'll give you the capacity back, no problem. And even better, I can give you a recipe, a recipe for generating codes that achieve the capacity or approach the capacity at least. I just choose random codes. Choose random codes according to the IID distribution of the optimizing variable. And if I choose a rate to be strictly less than this, anything strictly less than this, then with high probability, it's going to work. Now, there's a lot more to it than that, but that is kind of the zeroth order summary of classical information theory. And what kind of draws me to quantum information theory is the simplicity of this. Theory is the simplicity of this kind of collapse of this super complicated definition into a very clean, simple formula that tells us the answer to the question. And my dream is that we can do the same thing for quantum channel capacities, because when you take something that's classical and you make it quantum, it's supposed to be even more beautiful. And therefore, I sort of have a hope that we can tell a similar story about. Tell a similar story about quantum channel capacities to the one I just told you about classical capacities. But as you probably, as many of you know, most of you all know, we're not there yet. But by adopting a growth mindset, we may get there someday. So, what is a quantum channel? It's best to think of it in this setting as In this setting, as an isometry that maps some input A to an output space B and an environment E. And the action of the channel comes from conjugating the state by the isometry and tracing out the environment. And the reason we look at it this way is that the expressions we get for understanding capacities are going to depend not only on the output of the channel, but also on the environment of the channel in a very, in a very Very in a very essential way. So, considering the isometry is kind of the right thing to do. Again, if I was with you in person, I would talk about Elvir Beer about how that proves that actually many worlds is the right way to think about quantum information because it tells us to consider the environment of the channel. So, what I want to tell you about is sort of how, what do we know or sort of summarize? Know or sort of summarize what do we know about the quantum version of this classical story, and where are we going and what are we trying to do with it? So in order to something's on my screen here, just a minute. Oh, my pen started working again. There. In order to do that, I want to talk a little bit. Well, I'm going to sort of break it up into a few different categories. I'll talk about some coding theorems. Talk about some coding theorems. I'll talk about additivity. And I'll talk about non-additivity. And first, I suppose I should define what I mean by the various capacities. You can draw very similar pictures to what I just drew classically. So, for the quantum capacity, you can imagine that you have some input state. You're going to encode it into a bunch of channel uses. In such a way that hopefully, at the output, you can decode it and get some reasonably good approximation of the original state out again. And then the capacity, the quantum capacity for that is just the maximum number of qubits you can send per channel use. And if you were interested, instead of sending if you were interested instead of sending uh instead of sending uh a quantum state if you were interested in using the same channel to send classical states or classical messages um then you would just look for decoders and encoders uh that send classical messages reliably and you would get the classical capacity as the max number of uh just bits that you can send per channel use and in between you have this private capacity which is the max number Private capacity, which is the max number of bits you can send, but they have to be private from the environment. So an adversary who's holding on to all the environments of the channels can't figure out what message you're sending. And the receiver can always figure out exactly what it is. So these sort of line up naturally. Quantum capacity is less than or equal to private capacity, which is less than or equal to classical capacity. And we have separations for each of these. And what I'm going to kind of And what I'm going to kind of mostly focus on today, though I'll mention private and classical a little bit later on, is the quantum capacity. So, we're looking at how many qubits can you send over a noisy channel in such a way that you can recover the original qubits sent, and what's the best rate that you can do that at. Great, so let's look at some coding theorems. I want to look at The audience, I can sort of see it, but I'm going to ask for a show of hands. Who here has like really proved one of these quantum coding theorems before? Like, not the first person ever to prove it, but worked through the proof of it, I mean. Okay, okay, good. Some of you, okay, good. It's quite fun, and they all have the kind of the same flavor. So once you've worked through one of them, Once you've worked through one of them, you can kind of get through many of them without too much additional effort. Okay, so they're all kind of generally expressed in terms of some state you can make with the quantum channel. So now let's just take some state psi RA, feed it into a channel to make some state psi R B E and And, well, each of these different capacities that I mentioned has a different sort of informational quantity that characterizes the rate that you can achieve by choosing random codes that kind of look like the input state that you've chosen in this picture here. So, for example, for quantum capacity, there's the coherent information. Coherent information of a channel is just the maximum. Difference of the entropy of B and the entropy of E that you can generate with an arbitrary input state here. And there's a better way, a way I like better to express it. It's the maximum difference of the mutual information between R and B minus the mutual information between R and E. This kind of emphasizes the essential privacy of the quantum capacity of the quantum. Quantum capacity, uh, of the quantum capacity, and sort of why you can achieve this rate, uh, I think better than this expression does. Um, so how do you achieve it? Oh, here's the sort of theorem. This is coming from Lloyd Schor and Devatak is that the quantum capacity is lower bounded by this coherent information. And how do you prove it? Basically, random codes achieve it. DevAttax proof is slightly different, and they're kind of slightly more structured codes than the other two arguments contained, and that sort of the most simple arguments we know of contain. But generally speaking, the thing that you get out of this is given the input state, you get a recipe for generating a code that achieves this rate. I heard something. Is there a question in the audience? In the audience? I don't think it's not now. I'm just hearing things then. That's going to be a quick overpassing. I beg your pardon. Okay. Well, okay, anyway. So you can take this coherent information and you can regularize it to generate sort of at least a formal expression for the quantum capacity. So you just take the limit over channel uses of the Limit over channel uses of the average coherent information you can generate with n channel uses. And that's actually gives you an expression for the quantum capacity. And sort of that's also how you do the classical coding theorem. You prove something like that first, and then you show that actually that whole limit collapses and reduces to just the first term. That's not what happens in this case. So, with the coherent information, we know that there exists channels that have coherent information. Coherent information evaluated on K copies of the channel. Oh dear, K copies of the channel that is strictly bigger than K times the coherent information you can generate on a single copy of the channel. So this is non-additivity of the coherent information. And what it's telling us is that if I choose a bunch of random codes, they get this rate. But if I choose somewhat more structured codes, so Choose somewhat more structured codes, so codes with some more structure on K copies, and then I choose sort of random, concatenate that with a random code, I can do better than just choosing a totally random code. And they're really similar stories for similar stories for P and for C. And to me, this is kind of the central challenge in generating sort of a In generating sort of a useful and beautiful quantum Shannon theory, understanding this non-additivity, trying to find basically a systematic understanding of how you can exceed these, I would call them classically inspired rates of coherent information and holovo information and private information. How can you exceed those and quantify how you can exceed them in a systematic way? That's like the main challenge. That's like the main challenge, I would say, to quantum Shannon theory. And in order to do this, I guess what I want to convince you is it's going to have to involve developing a better understanding of the relationship between the information we send to the output and the information that we send to the environment and how we manage those two channels, those two different channels simultaneously. And just to underline how difficult. underline how difficult this evaluating this quantity is. Here's a difficult question that, you know, we have some things, it's kind of slippery and we don't have even strong ways to attack it yet. If I give you n, is the quantum capacity of it zero or non-zero? Well, that's something we don't have really a That's something we don't have really a systematic way to answer. What we can do is go off and evaluate coherent informations. And if you get something positive, then the capacity is non-zero. If you get something zero, well, you can't really say much about it. So before I start now, that's what I wanted to say about coding theorems. Before I start now with some discussion of additivity, I wanted to ask, does anybody have any questions or clarification? Or clarifications or diatribes. You can take some of my time and just do a rant if you like. No. I have a question actually. So what about the computational complexity of the siding is? What do we know about the computational? Yeah, what do we know about the computational complexity of the siding is? We don't know anything. So one thing that we know is. One thing that we know is: like, here's the closest thing that we know about it. If you give me a K, Toby Cubit and his friends can make a channel that has the first K terms of this thing, this limit, this sequence going equal to zero, and then definitely sometime later, it'll jump up. And that's kind of like a very baby step towards trying to show the thing is not computable. But it's a long way off from knowing anything about the. long way off from knowing anything about the the computational um the computational really complexity or even the computability of uh of these capacities so you see that the single shot information says absolutely nothing about the asymptotic content that's that's right the other thing that we do know about computational um complexity is that we know uh holovo information uh is is difficult to calculate just given uh just given uh Just given a channel, you can kind of embed an NP hard problem into it in such a way that if you can estimate the whole of O information, you can solve the NP hard problem. I think you should be able to do that for coherent information too, but I don't know how to do that. I tried to do that before. I have like feelings about how one should be able to do that, but I don't have any theorems about it. So even at the same So, even at the single-letter level, it's hard, and at the single-letter level, that tells you nothing, or a priori, in some cases, at least, it doesn't tell you about the asymptotic thing. So it's pretty open. But not when coherent information is additive. So, let's look at one example of a class of channels we already know where they have additive coherent information. And then I want to tell you about another class. And then I want to tell you about another class of channels that I think is bigger than this class. And I want to give you a specific example of a thing that I think has additive coherent information, but I don't have proofs yet. I only have kind of promising lemmas. Do you know the joke about the promising lemmas? Okay, okay. I'll tell it then. It's worth it. So then, anyway, there's this mathematician, and he says, I'd sell my soul. He says, I'd sell my soul to the devil for a solution to the Riemann hypothesis, or a proof of the Riemann hypothesis. And poof, the devil appears. And the devil says, sign here, please. He signs here. He says, okay, you're back in a day. He leaves. And the mathematician is feeling kind of uncomfortable about this because he's just sold his soul to the devil. But he's going to get the proof to the Riemann hypothesis. So that's something. Hypothesis. So that's something. And next day, no devil. Next day after that, doesn't show up. Three or four days later, the devil shows up. And he looks tired. And he hasn't shaven. And the mathematician says, okay, okay, okay, what's the proof to the Riemann hypothesis? And the devil says, well, I have some very promising lemmas. I have some very promising lemmas. And anyway, so that's what I mean when I say we have some promising lemmas. I'm not sure they're that close, but I really have strong feelings about this channel's capacity. So I hope I can convince you that it's a problem worth working on. But first, let's look at some specific channels that I know have additive coherent information. They're called degradable channels. And they're pretty simple to express. You have a channel, it has an isometry. If I trace out the environment, I get the channel. If I trace out, instead of the environment, I trace out the output of the channel. That's called the complementary channel. And that's basically the environment's view of the thing. We say the channel's. We say the channel is degradable. Sorry, my. Oh, I see. My N is degradable if there is a D such that, that's a channel, if there's a channel such that I can compose it with the output of the channel to give me the complement of the channel. So it means that basically B can simulate E. B can simulate E. And for these, the coherent information is equal to the quantum capacity. It's additive, and that's basically a consequence of the fact that the mutual information between B1 and B2 for degradable channels is bigger than or equal to the mutual information between E1 and E2. You can kind of see that you can take this inequality apart. It follows from data processing and use that to show. And use that to show additivity of coherent information for degradable channels and, therefore, a single-letter formula for the quantum capacity. Now, degradable channels include very simple things like dephasing and erasure channels, but they don't include sort of a generic channel. Like if you choose a random channel, it's unlikely to be degradable. But they're really the only ones or almost only ones that we have that we have a clean answer to the. We have a clean answer to the question of what is the quantum capacity of this channel. So, what I want to do is generalize this idea slightly to something called an informationally degradable channel. And this is, well, let me just define it in terms of some mutual information. It's a channel for which for every input state. For which, for every input state, phi VA, and this is going to, and even mixed states, the mutual information between V and B needs to be bigger than or equal to the mutual information between V and E. Or put another way, if I take the inform over these states, the mutual information of VB minus VA, mutual information of VE, that's always non-negative. This also implies, via a somewhat more complicated argument, that, okay, we'll call these informationally degradable ID, that the coherent information is equal to the quantum capacity. And actually, just like for degradable channels, also the private capacity of such a thing is equal to the coherent information and is equal to also the quantum capacity. And is equal to also the quantum capacity. So these are kind of clean, nice channels. Anything that satisfies this will give you, you'll be able to have a nice expression for the quantum capacity. Furthermore, degradable channels fall into this class. And what we'd like to know is, question, are there informationally degradable channels that are not degradable? So, what do we know about this question? Well, first, we know that classically, well, I think that there really should be. So classically, yes. What does that mean? It means if I have a channel that has one input and two outputs, I can definitely find channels where the mutual information between some auxiliary variable v and the first output is always bigger than the auxiliary. First output is always bigger than the auxiliary variable B, E, sorry, V, and the second output. However, those channels, you can make such channels that are not, where you can't actually process B to get E. So there's always more information with B, but somehow it's a different kind of information with B than the information that's getting leaked to E. And that's what gives you the difference between these, between a degradable channel and an informational. Between a degradable channel and an informationally degradable channel. And the intuition for why they should exist also quantumly is that, well, there should be ways to give always give more information to the first party, but give a different kind of information than the information that you're giving to the second party. But there's a problem. It becomes difficult in the quantum case. And the challenge is basically twofold. So the challenge first is that challenge. Is that challenge, challenge, okay? Is that a priori, there's no bound on the dimension of V. And what that means is if I try to go off and do some numerics, I don't have a good reason to trust any numerics I do because I'm trying to do an intrimum here basically over an arbitrarily large dimension. And that's not something I can immediately figure out from looking at some finite dimensions. Finite dimensions. That's one problem. And the second problem is that basically n constrains n complement. So in the classical case, there's no necessary relationship between the output of the first channel and the output of the second channel that you're trying to compare. But actually, you totally define or you totally identify what the output of the environment is going to be when you tell me what the output of the channel is going to be. So you kind of have a lot less freedom. So, you kind of have a lot less freedom in trying to cook up examples in the quantum case than you did in the classical case. So, let me give you a channel built out of some degradable channels that I think we should be able to show is informationally degradable, and I know is not in the right parameter range that I know is not degradable. And it's going to look like this. And it's going to look like this: informational degradability for flagged mixtures. So, what I want to look at is a channel n that takes rho to with some probability, one minus p. 1 minus p, channel n1 is going to act on it, and with some small, small probability, channel n2 is going to act on it. And what I want to do specifically is make it so that almost all the time, channel N1 is the one that gets applied. Very unlikely, channel N2 is the one that gets applied. However, channel N2 is kind of uh uh Two is kind of well, okay. I have to, let me just tell you, it's all going to be based on amplitude damping channels. So maybe I should just be quantitative in a moment. So, N1 is going to be equal to an amplitude damping channel with a damping parameter gamma. And N2 is going to be equal to sort of an extreme amplitude damping channel. It's going to have it's going to have Going to have it's going to be an amplitude damping channel with damping. Let me just get it right. Well, let's make them both just call them gamma one and gamma two. In this case, gamma one is going to be kind of much, much less than one. In this case, gamma two is going to be kind of approximately equal to one. But we're going to do it in such a way that gamma one is going to be equal to some gamma plus a little epsilon. Gamma two is going to be equal. Gamma 2 is going to be equal to 1 minus gamma. And gamma and epsilon are going to be much, much less than 1. And the probability of getting n2 is going to be much, much less than 1 as well. So what does this imply for what the channel actually does? Let me just write down the channel a little more explicitly. With sort of 99% probability, it's going to be an amplitude dampening with the damping parameter gamma plus epsilon, and we'll know it. And we'll know it. And with some tiny probability, it's going to be one minus gamma only. And the complementary channel of this thing, which we have to consider if we're looking at informationally degradable channels, well, with probability 1 minus p, it's going to be a sub 1 minus gamma minus epsilon of rho, tensor 1. Tensor one, and with probability p, it's going to be a sub gamma rho, tensor two. So if epsilon is equal to zero, then this thing will just be degradable as long as p is less than or equal to a half. Why? Well, if most of the time I get this term, then in order to simulate the output, whenever I get this term, Simulate the output whenever I get this term, which is the best channel kind of in the game, then I can just easily use it to simulate that side. And when I get this term, I can use it to simulate that side. And if I get this term, but I want to simulate more of this term, I can just degrade this damping channel more in order to generate the desired highly damped channel. The problem arises when. The problem arises when I choose epsilon bigger than zero. In that case, almost all the time at the output of the channel, I'm going to get this, which is kind of a good channel, but not a great channel in the sense that some of the time the environment is going to get a channel that's much better than this. Because if gamma and epsilon are comparable, some of the time, very unlikely, but some non-zero points. Very unlikely, but some non-zero part of the time in this fourth term over here, I'm going to get a channel in the environment that this simply cannot simulate. You can't simulate a low damping channel with a higher damping channel. The simulation kind of only goes one way. You can increase the damping by further damping, but you can't undo the damping with any physical operation. So, in this setup, when epsilon is strictly bigger than zero, you simply cannot. You simply cannot simulate the environment using the output. However, if I choose the parameters right, almost all the time, what I get is an extremely good version of what I put into the channel at the output. And almost all the time, the environment gets an extremely bad version of what got put into the channel. Very small amount of the time, I get a bad version and the environment gets a good version. And the thing that they get is better than the good one I got, but they get it like 1% of the time and I get it. But they get it like 1% of the time, and I get it 99% of the time. So, the expectation then, or the conjecture, is that even though the environment, some tiny fraction of the time, gets the best copy of the input state, the mutual information between some auxiliary variable, V, and the output of the channel kind of gets swamped by what happens in the most likely case. So, that's kind of the conjecture. I can write it down, I guess, more. I can write it down, I guess, more mathily, if you like, in terms of just how do correlations decay under the action of an amplitude dampening channel? So let me just do that. If I let A of V B sub gamma be sort of the mutual information, the mutual information. The mutual inference. Oops, let me delete, erase that. If I let A V B be the difference between the mutual information of B and the mutual information of E, of V and E and V and B, then a sufficient condition for the, if you just sort of write down what's the criterion for information. Oh, is there a question? Oh, is there a question? No, yes, it's costly. No, COVID. Sorry. Are you okay? Yeah. Do you need a COVID test? No, no. No, okay. Sorry. Okay. Where was I? Oh, here I am. Okay. So then I can write down sort of the, if I go. Sort of the, if I go back one slide, I can go, I can write down the differences of mutual informations between the output and the environment as actually a convex combination of the difference of mutual informations in this case and the difference of mutual informations in that case because of the flags. And if I do that, uh-oh, if I do that. Then the criterion for this always being positive becomes well the advantage that V has with B at the output of the sort of good channel, the damping channel with the damping parameter gamma plus epsilon, should be bigger than or equal to p divided by 1 minus p times the advantage that I. That I can get with the great channel, the one that has damping parameter only gamma. So, the idea here is that informational degradability of this specific channel is equivalent to the absence of some very rapid decay of the correlations between the correlations between some reference state and the output of the channel under the action of the amplitude damping channel. So, the idea. Of the amplitude damping channel. So the idea is: if this is true, I have informational degradability. And this conjecture, sort of for small enough p, just tells me if I start off with a certain amount of correlation measured by this advantage, then if I further damp it by just a touch, I don't lose everything. I should keep at least 1% of the correlations that I generated. If that's true, then this thing is informationally degradable and I have like a new channel. It's not degradable. Channel. It's not degradable, but it does have additive coherent information. So I'd be delighted. We could add it to the zoo. And this is true, at least sort of for, I mean, this sort of not to fragility or this lack of fragility of these correlations under damping is true in at least some scenarios that I can verify. So for example, Can verify. So, for example, if I make a state V A and I send it through this damping channel now, then I can show that actually the dimension of this auxiliary system V, I can bound it as long as I consider like classical-like states. So, what does that mean? It means, well, I should have classical states on V and I should have states that are diagonal. And I should have states that are diagonal and zero and one on the input to the channel. In that case, this just looks to be true. So I can bound the dimension, I can do a numerical optimization, I can show that this mutual information, that this rapid decay of correlations never happens for the right parameter regime, at least numerically. So it looks like it's true in that case. However, we don't. However, we don't at the moment have the tools that would be necessary in order to show that it's true in the quantum case. So that's, I think, something that could be attacked, but still is sort of like challenging to attack. What time does the talk end? Do I have 10 minutes or am I finished? We need 21 minutes. 21 minutes? Amazing. Oh. Okay. Oh, okay. Let's get something out. Let's keep going then. Great. I'm going to move now to some non-additivity stuff. So we talked a little bit about additivity. I gave you a channel. I think it has additive coherent information. I told a story about some math we might need to do in order to show that. And the math, I think, is pretty, pretty juicy. Math, I think, is pretty juicy. I think it'll be fun. So now let's look at some non-additivity. I want to show you a specific channel that, or family of channels really, that do something very nice, that I think highlight the tension between sending information to the output. Between sending information to the output and information to the environment. And I told you that's kind of my qualitative theme here. We have to be able to manage both streams of information simultaneously. And the difficulty of doing that is what kind of leads us to the challenge of non-additivity. So let me just write down a particular channel. It's called the Platypus channel. And let me write down. And let me write down just the simplest version of it. So it maps zero from A, zero on A to a superposition of zero, zero and one, one on B E and then it maps one on A to some new state two on A or on B and zero on E. Now, this channel, as I've written it so far, that has a single qubit input and a trint output. qubit input and a trit output, this thing is degradable. So it has q equals p equals q1 and uh and I mean incidentally its classical capacity is equal to one. You can just sort of toggle between these two states and you can just distinguish them at the output. You're delighted. You have a classical capacity of one. We can now problem make make things Make things a little bit more complicated by adding an additional state, two, at the input. But we'll try to make it as simple as possible, given that we're adding this. And we'll make a two, it map to a two on B. And, well, it's got to go to something orthogonal. So we'll say it maps to a one on E. So obviously, quote, you should never use this state. Why? Because anything you can send to B with the first two states. With the first two states, you can also send anything you can send to B by sort of using this state, you can also send it by using this state. So you might as well just pick one of these two states, input states one and two, and use them, and then forget about the second one. And that suggests that this is kind of a weird and unnecessary thing to add. That is, I think, good intuition for when you're using the channel together with itself. You're using the channel together with itself. I think it is very bad intuition for when you're using the channel with something else. So, why would I say that? Well, because you can show for this particular channel, I'll call it N, that the coherent information of this channel is really, really non-additive when you put it together with other channels. So, the coherent information of N together with a 50% erasure channel. So, a 50% erasure channel has zero coherent. 50% erasure channel has zero coherent information of its own, but adding it on to this platypus channel, it gives you something strictly bigger than just the coherent information of the platypus alone. So you get something strictly non-additive. Same story for an amplitude damping channel of 50% damping. You get a non-additivity there. And same story also for a depolarizing channel with the one-quarter. Channel with the one-quarter depolarizing probability. So it's sort of generically, whatever channel you want to put together with, at the qubit level, it seems to just work really well with almost any quantum channel in the sense that it gives a lot of non-additivity of the coherent information. Actually, there's a de-dimensional generalization. Dimensional generalization. It's the obvious thing. You just have zero here goes to a d-dimensional maximally entangled state. And then you have a bunch of states that just get mapped to the same output flag and send a bunch of information to the environment. That's it. With that, you can push these non-adactivities away from the place where the partner channel has zero coherent information. And for example, for an eraser channel, if you give me an eraser channel of any eraser, Give me an eraser channel of any erasure probability that's not zero. I can give you a platypus channel back that has non-additive coherent information with that erasure channel. So you can push it. You know, the platypus channel can be pushed to have non-additivities with very low noise channels, which is really very different from the kind of standard intuition that we had for how this non-additivity of coherent information works, because the old lore was that this only happens for very low noise channels. Happens for very low noise channels. What this is looking like now is that, at least for this channel, it happens with almost anything you might like to use it with. So it's very generic kind of thing. And the thing that we don't know how to show yet, but I think also I think ought to be true, is the following conjecture. The coherent information of this platypus channel should be additive when I use it with a When I use it with itself. And we have some evidence for this, but maybe let me, instead of, I'll just give you the feel for why we think this should be true. Basically, anything you can do, basically anything you do that involves this second state rather than using the first state, it seems. The first state, it seems to be similar. It seems to, well, you can replace it with some other state on which you're evaluating the coherent information that has the same entropy of B. So the whole game comes down to do I reduce the entropy of the environment by toggling between these two things rather than just using one of them or the other? And let me just kind of give you a specific conjecture that tells us no, that would tell us no, it doesn't. So that would tell us no, it doesn't reduce the entropy of the environment. The best, the minimum entropy of the environment is what you can get by using just all of only ones and never touching the twos. And we called that the spin alignment conjecture. If we fix some, now we have n qubit states. Have n qubit states. Basically, the n is the number of channel uses that we're considering. And if you fix some weights on subsets of these n qubits, so this i is always a subset of the power set of n or of one to n. And then we say, what is the minimum entropy of? Entropy, what is the minimum entropy of the sum over i, the sum over all subsets of mu sub i some state phi i on i tensor identity over two on the complement. What is this? Let's what's the we want to minimize the entropy of this. What does it mean exactly? Well, we get to control, we can't change these these uh weights, but we can change what state do we choose to put into this linear. choose to put into this linear combination into the super into this mixture what state phi i do we choose to put into this mixture for each subset of states um and what we want to do is minimize this entropy and the conjecture is that this is minimized uh for a specific choice for choosing all of the phi i's to be just equal to some fixed state. Why should that be so? Why should that be so? Well, if you want to minimize the entropy of this convex combination, you want to make all the terms in the convex combination kind of as aligned as possible, as pointing together as you can. And the conjecture is just that the best way to do this is to choose them all to be exactly the same state so that you get sort of the least amount of distinguishability of each of the terms, and that should give you the least amount of entropy. And we can show. Entropy. And we can show this for some examples, for some scenarios. We can prove it. Well, okay, it is proven. So basically, the place to look for this, by the way, I should have said who my collaborators on some of this work were. The additivity stuff that I talked about is a combination of work with Andrew Cross and Koo Lee in 2016 and also ongoing work with. With some people here at, well, ongoing. And this one is about this platypus stuff is with Leditsky, Felix Leditsky, Vakesh Sidhu, Debbie Lung, and John Smolin. And okay, so yeah, okay, I just wanted to. Okay, so yeah, okay, I just wanted to tell you who the people involved were. And in the paper that we wrote, we were able to show this spin alignment conjecture for some cases, like n equals two and n equals three. And in a really great paper that I think, I don't know if any of you have seen it, but it's really great. I would recommend that you read this one. After you read this one, you should read this paper by El Heji and Kunil. This paper by Elhegi and Kinil, where they show that the spin-alignment conjecture is true if we are restricted to classical states and also for any Reny integral, Renyi entropy of integer order. Oh, this is probably 23, but yeah, 23, I would guess. 23, but yeah, 23, I would guess. And they sort of relate the question to some generalization of Kaifan's maximum principle. And it's really kind of a nice paper. So, you know, I can't, yeah. If you take like two things out of this talk, it's that non-relativity and additivity, these are sort of the core questions in quantum Shannon theory. In quantum Shannon theory and specifically quantum capacity theory. And you should go look at this paper because it's quite cool. Okay, so how are we doing here? So that's kind of another nice mathematical problem that I think one should be able to make progress on. I still have some ideas about how to make progress on this. We're not stuck, but it's not trivial. Uh, and um, I don't know, to me, one of the great things about looking at these questions is that we're constantly chipping off sort of substantial mathematical questions that force us to develop new tools that could be of use in other parts of quantum theory. Now, I guess I should summarize now because it looks like we're coming towards the end. Um, I guess the summary is that, well, the capacities uh characterize uh um oops. Uh um oops. The capacities characterize the sort of the uh the rates that you can achieve over uh oh, I forgot to tell you one more thing about the the uh the platypus channel. It's really fantastic. Um, so the thing I wanted to tell you that I forgot to tell you is the platypus channel, it started off degradable, so the quantum capacity and the private capacity were the same. But when you add in these extra states, basically you can use them to uh to send. them to send information to the environment that kind of scrambles her knowledge of which state that you you put in. If you just have these first two input states, then when Eve gets a one, she knows for sure you did not send a one. You definitely sent a zero. And if you have both of these input states, you can kind of make the messages that you send in this subspace look different, look identical to the message you send in this. Look identical to the message you send in this subspace, at least to Eve, while maintaining orthogonality with Ball. So the private capacity of the platypus actually becomes exactly equal to one, equal to its classical capacity, whereas the quantum capacity we think, at least relative to this spin alignment conjecture, stays the same when you add in this new thing. So, okay, I wanted to tell you that. I wanted to tell you that. And then now I want to just say that, well, these capacities, they tell us kind of the value or the amount of communication we can squeeze through a quantum channel, depending on the information that we can say. The additivity tells us we're kind of on the right track with these achievable random coding theorems. And the non-additivity, I want really to push the idea that it's telling us that we're missing some key ideas about how we generate quantum codes, how we generate structured codes. Codes, how we generate structured codes. And the way that we're trying to understand those ideas at the moment is by figuring out more and more simple examples of non-additivity, such as with the Platypus channel. And actually, by doing that, the Platypus channel points us towards some sort of interesting conjectures about just the nature of entropy, which I think is very exciting. In terms of informational degradability, I showed you some plausible examples that are definitely not degradable, but I argued should be. But I argued should be informationally degradable and therefore have additive coherent information. And again, we found that by asking these questions, we're pointed to sort of meaty question, meaty mathematical questions about the nature or about the properties of entropy. With that, I want to say thank you very much and open the floor for questions. Yes, that's it. All right, any questions? Right, any questions from the audience? So, yes, that's one, please go. Thanks for the great talk, that was wonderful. Um, can I ask a devil's advocate question or strange question or politely disagree with the point that additivity is everything? In the sense that I mean, additivity questions like for the quantum capacity ultimately amount to finding good target input data target over all copies of the channel, like in the asymptotic limit. And of course, these are state. And of course these are states that are like economically changing that are impossible to prepare. Would it not make sense also to look at child capacities under meaningful constraints for preparable states of inputs, like having like short range or like tender network type structured inputs, and then ask to what extent the capacity would be reachable within short range the time of inputs? Yes, I think that would make sense. That would make sense. That would make sense as a question. I think I don't think we know. I guess how, okay, here, I guess my perspective is we want to get the most beauty out of this as possible. And if that pushes us a little bit away into having very big states, but as an ex as sort of in exchange, what we In exchange, what we get is a simpler theory. I'll be much happier than if we have a sort of more complicated theory that is slightly more applicable to practice. I think it's, well, I don't know of any scenario, even in the classical case, where we can kind of get even any kind of reasonable, any kind of reasonable theory where we say we're going, well, I want to withdraw that. I'm withdrawing that. Withdraw that. I'm withdrawing that. Instead, I'm going to make this argument that when we do Shannon theory with sort of no constraints on the difficulty of what the coding is going to be, it actually oftentimes leads us to the capacities that we can calculate them. We can go off and find more practical ways to achieve. I also am not certain that, like, the non-additivity of the coherent information is, you know, definitely involves using more. You know, definitely involves using more and more entangled states that you, you know, as N grows, the number of channel uses grows, and then the size of the state that you use to generate the non-additivity does grow, and that may become impractical. I guess the perspective that I want to push a little bit today is that that's not a fundamental thing. That's a thing that is telling us about coherent information, not necessarily the capacity. And it could be for, you know, these non-adductivities that we get for high N with really complicated. Get for high n with really complicated states. We could capture the structure of the codes that achieve these rates in a much simpler way, with much simpler states, if we had a more refined way of selecting our code ensemble. That's my hope, right? It may not work, but that's the goal. Thanks. Other questions from the audience? Alex, please. So you mentioned this information degradable channels as a mechanism to get like. As a mechanism to get like activity. Are there other, like, can you comment on other mechanisms to get additivity for queer information? Yes, I can. I think I can give you even a complete list fairly simply. So there is, if you have a channel that's PPT, then the coherent information of that channel together with itself is additive because the joint channel is PPT and it's always zero. Beyond those examples, the example Beyond those examples, the examples we know are degradable channels only. Informational degradable channels is sort of one criterion that we know. There is another criterion that comes out of a paper by Shunwatanabe that if the complement of the channel has zero private classical capacity, then you also have additivity of coherent information for the channel. So because of the, because of the, basically, the thing that appears constantly when you're talking about additivity of coherent information is what's going on with the environment. And that's kind of what degradability, that's what this Watanabe thing does, and that's what the informational degradability thing does. Also, it kind of pushes us towards considering more explicitly what's going on with the environment. Explicitly, what's going on with the environment. And that's, I guess, another qualitative thing I want to talk about, which is like how do we manage both the messages we're sending to the output and the environment at the same time? And if we could figure out a smarter way to do that, I think we would be able to develop better ways of coding. So long story short, in the classical case for additivity of Holovo information, there's just a bunch of examples where you can show additivity by some like complicated things. In the quantum case, there's just a couple of In the quantum case, there's just a couple of examples or a couple of classes, and they all look the same except for PPT, which looks different. Thank you. Also, is that Alex? Yes. I owe you an email. I'll send it. Thank you. Other questions? And then let's thank the speaker again. Thank you very much. Thank you very much.