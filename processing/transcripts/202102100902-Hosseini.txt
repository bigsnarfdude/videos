And who's going to talk about inverse problems and the connection to GAMPS? So take it away. Sure. Thanks, Lars. And thanks to the organizers for the invitation and also everybody who's listening to the talk today. I also want to apologize, I missed the first couple of days of the workshop, but I promise I'll be here for the remainder. Yeah, so today I wanted to talk about To talk about sort of connecting some techniques from machine learning, in particular, these sort of generative models that have been developed on the machine learning side to essentially to solve inverse problems. So my talk will be focused a bit more on the uncertainty and learning sort of aspects of the workshop today. Just to mention my collaborators, so the talk I'll give today is based on this preprint we have on archive. We're actually about to update it very soon. Updated very soon. A lot of the work is thanks to Nick Kovachki, who's a PhD student at Caltech. Also, Ricardo Battista, who's a PhD student at MIT, working with Yusef Marzouk. I've been told by Yusef that he has hyped up my talk a little bit. So in some ways, my talk will actually complement some of the stuff that he's been presenting earlier in the workshop. Okay, so here's the sort of So, here's the sort of the structure of the talks. I'm going to start with the introduction. Spent quite a bit of time here just to make sure it's clear sort of what we're trying to do and what are the main goals and how our method compares to different methods in the literature. And then I'll move on to section two, which is mostly sort of laying out the methodology and the algorithm and a little bit of theory. And then I'll close with some discussion of sort of interesting extensions. Discussion of sort of interesting extensions and future directions, of which there are many. Okay, so let me start with the introduction. So, I kind of want to tie together supervised learning with inverse problems and then also sort of measure transport and conditioning. I want to sort of tie all of these things together. So, let me start with supervised learning. If you're in machine learning or inverse problems, you look into the other field, you see a lot of commonalities. Field, you see a lot of commonalities, right? So, sort of classic supervised learning problem in machine learning, you have input X and output Y. So, I'm gonna have to ask you guys to remember this notation. It's gonna be a little unintuitive, maybe. So, output is Y, input is X, and you assume there's some map, some ground truth map, Psi dagger, so that the output Y is Psi dagger of X, right? And then given a new input, X star, you A new input, x star, you want to predict the output at that new input. And typically, you do this by approximating the map psi dagger. So, like, if you think about linear regression and methods like that, so there's an approximate map that's psi star, and then you hope that, okay, if this is a good approximation of psi dagger, I can recover the output at the new input. If you do inverse problems, so this is where the notation is going to get a little non-standard. You have an unknown parameter y and you have a parameter. An unknown parameter y, and you have indirect measurements of that x. So you do have a forward map f of y equals to x, and then given a new data x star, you want to approximate y star, right? So you're trying to invert the forward map, essentially. And you typically do this with some kind of optimization problem, right? So these two problems are similar, but not exactly the same. But you can actually cast them in the same framework if you think about the probabilistic version. So if you're doing probabilistic. Version. So if you're doing probabilistic supervised learning, now you have pairs of input and output. So this is your training data. You think of them as samples from a joint measure, nu. So this is now a measure on the product space of input and output. And then given a new input, X star, you want to identify your sample the conditional of the output on the new input. So that's probabilistic supervised learning. So this now coincides with basically the Bayesian approach. coincides with basically the Bayesian approach to inverse problems. So in the Bayesian approach, you have the measurement and parameter pairs, x and y. Then you have a prior measure, nu naught, on the parameter y that you want to recover. Then if you think about how you derive Bayes' rule from scratch, you actually consider this joint measure, which is the law of x and y together, where y is drawn from the prior and then x is the forward mapping. Prior, and then X is the forward map evaluated on Y. So it's the data corresponding to the draw of the Y from the prior, right? And then Bayes' rule tells you that the posterior that identifies the parameter y is just the conditional of this measure new at the new x star. So now you're just doing conditioning to do both probabilistic supervised learning and solving inverse problems. Okay, so that's the inverse problem side and supervised learning side. Supervised learning side. On the other hand, you have sort of connection between generative models and measured transport. So, generative modeling is sort of an idea that's become very sort of, there's a lot of excitement around it in machine learning. And it's essentially a statistical sort of problem where you have samples from a measure nu. So I'm changing notations here because in generative modeling, there's no input and output. Generative modeling, there's no input and output, so you can imagine just concatenating the input and output to get the variable z. So you have samples from the measure ν, and then you want to generate new samples that are approximately distributed according to that measure. And typically, this is done by choosing a reference measure, which is maybe a Gaussian. And then you train a map that pushes the reference to the target that you're interested in. And then you can just generate samples from the new target if the map T is a good sort of. map T is a good sort of push-forward map. So you generate samples from the target, the reference eta, and then you just evaluate T at the ref at the samples to get approximate samples from the target. Now, of course, I mean, this is not almost like immediate to see that this is a measure transport problem, right? So how do you train this map T? How do you construct it? You can do optimal transport. You can do techniques from image registration. What we're going to do is to use triangular transformation. It is to use triangular transformations, which you've seen some examples of these in use of stock. And I'll tell you why, because I mean, the reason is that these are sort of specially sort of useful for doing conditioning, which is our ultimate goal. So essentially what I want to achieve sort of in this talk is to fill in this missing link here. So we have generative models up here. We know their connection to measure transport. We also know that they're very sort of capable in sort of science. Of capable in sort of sampling very complicated, high-dimensional measures. Maybe if they're on a manifold, sort of in like imaging problem, there's a lot of empirical evidence for their performance. So they're pretty good. And the bottom, we have supervised learning and inverse problem, and they have a clear connection to conditional sampling. So we want to fill in this missing link in between so that we can use essentially the flexibility and strength of generative models to solve inverse problems and probabilistic supervision. And probabilistic supervised learning. That's sort of the overarching goal of the method. And what we propose to do is essentially the following problem. So you have the joint measure nu on the product space of input and output, so x and y. You pick a reference measure, which is in this bit, I'm just going to take this to be a measure in the y space. So this is on the output that you try to predict, and then you're going. You try to predict, and then your goal is to find a transport map S that goes from the product space of X and Y back to Y itself. So that if you fix the first input of this map at X star, then the new input on which you want to sort of condition, or I guess the conditioning variable here, then S of X star, which is now a map from Y back to itself, pushes the reference on Y. Pushes the reference on Y to precisely the conditional that you're interested in. So, this is what we're trying to achieve here. Okay. And of course, this is not, I mean, on the surface, if you kind of look at this problem, it seems a bit strange. It may not be, I mean, to me, at least when I saw this first, it was not at all trivial that this should be possible. But turns out this actually kind of classic. So the North Rosenblatt map, again, you may have seen this in Yusuf's talk. Seen this in Yusuf's talk, it's actually explicitly constructed to achieve this. So the North Rosenglad map is a triangular map. Let's say you have pairs of X and Y and R M and Rn. So it's a map that goes from the product of product space of X and Y back to itself. It's triangular because it has this kind of structure. So if you take its Jacobian, it's a triangular matrix, right? And sort of the way it's constructed already tells you that. That already tells you that how it can do conditioning. So, in sort of 2D, what you do with the north horizon blood map is essentially that you know how to transport one-dimensional measures, right? Because you just compose the CDF of one with the inverse CDF of the other one. That's the optimal transport map. So, what you do is you start with the marginals of the measures. So, imagine they have sort of Lebesgue densities in 2D. So, you take the first marginals, maybe on the first variable Z1. Maybe on the first variable Z1, then the first component of the map is just the optimal transport map pushing the 1D marginals to each other. So that you can write down exactly. Then you do a disintegration at every new value of Z1, the first variable. So the disintegration is the same as conditioning, right? Then you take the second component of the map in such a way that for any given Z1, it's the optimal map sort of pushing the disintegration of one of. The disintegration of one of these measures to the other one evaluated, but now it's sort of the disintegration at a slightly different point. Okay, and just it turns out you can verify this by hand that then you can sort of compose the two components of the Northwestern Vlad map together in such a way that you get that desired conditioning map. It's all by construction. I mean, the details don't really matter here, but I just want to make the point that this idea of like having a map that given a new input, it can give you any. Given a new input, it can give you any new conditional. It's not surprising. It's actually like goes back to the 50s and the 60s at that point. Okay. But what we're going to do, and this is sort of like an overview slide, don't worry about the details too much here. I just want to tell you sort of what to expect, sort of coming forward going in the remainder of the talk. Is we're going to take this idea of like the idea of like the the of the triangular maps and and and the north hozenblatt map and we want to construct sort of a more practical algorithm that leverages the the flexibility and strength of generative models so basically you know we're going to have sort of again finite dimensional input and output you you pick your reference to be just a gaussian we will work with block triangular maps which i will tell you soon what exactly what they are but they essentially have a parameterization of this form Essentially, you have a parametrization of this form in terms of input X and output Y. Then you put neural networks on K and S, and then you need a third neural network F, which actually shows up in your optimization problem. So you have to solve a min-max problem where you sort of maximize some kind of empirical loss involving the map F and the map T that we had up here. And you have some constraints and penalties on top of this as well. Penalties on top of this as well. So you may already recognize if you just look at the max and then these two empirical sums, because these points are being drawn from nu and eta, you might already see there's a Washerstein one distance in here. So you're kind of minimizing some kind of distance between the push forward and the target measure. And then you solve this optimization problem, you get a map S, and you use that for conditioning. That's sort of the plan going forward. All right. All right. So, why would you want to do all of this, right? So, we know how to do conditional sampling. You know, we all, many of us know probably MCMC, we use it all the time. So, why are you doing this? It seems kind of maybe not the most intuitive way of solving the problem. So, there are some, at least, motivations for me, pretty clear motivations. So, one is probabilistic supervised learning and UQ. So, UQ sort of uncertainty quantification. UQ sort of uncertainty quantification is something that's largely, it seems, it's missing from the machine learning literature. And a lot of it is because, you know, if you want to do probabilistic supervised learning, you need priors on the inputs and outputs and on the parameters and so on. And those are pretty hard to sort of construct. So you typically want to do this in a data-driven way. So this method promises that if you have enough data, because you just need to. If you have enough data, because you just need to compute these empirical sums involving samples from the input and output. Another reason, and I'll talk about these all in detail, but another reason is sort of doing model agnostic learning or likelihood-free inference. So you might be in settings as you would do in approximate Bayesian computation, where you only have samples from this measure nu, and then you want to condition it, right? So you don't have a likely. You don't have a likelihood or a misfit term that you can evaluate. Maybe your output is, for example, stochastic, right? So, in those cases, you cannot really do the standard sort of optimization or sort of MCMC type approaches. You have to do something else like ABC, but this method can actually tackle problems like that as well. And then another reason, which is sort of more unique to this approach, is that you can do conditional sampling with many. Can do conditional sampling with many new inputs. And the reason is: suppose that you have a good map S that does this sort of approximation I have written at the bottom of the page. If I had any new sort of input, if I had multiple input X stars coming in, right, so multiple inverse problems I wanted to solve with different measurements, the map doesn't need to change. If I had trained a good map, I can just change the input of the map and just keep pushing forward samples. And in principle, Forward samples, and in principle, it will work fine. So, that's you have to compare that, for example, to an MCMC or optimization approach where every time a new data comes in for the inverse problem, you have to run the algorithm again. Right here, we can kind of just train the map offline and then we can do sampling with any new input. Let me take you a couple minutes to talk about the literature because this is the important part, and you need to sort of be clear about how. And you need to sort of be clear about how the method fits within different areas because we're combining things from machine learning and applied math and inverse problems. On the machine learning side, you've probably heard of GANs a lot. So these are generative adversarial networks. They're sort of what our method is based on largely. So they go back to 2014. The main problem in generative modeling is, as I discussed earlier, Is, as I discussed earlier, it's just sort of generating samples from a measure. So conditioning is not really the main target, at least not originally. Related to that are normalizing flows, which are also very close to what we're doing. In particular, there's something called an autoregressive sort of normalizing flow, which they essentially construct triangular maps. The main difference here is that typically what they minimize is like a KL diagram. What they minimize is like a KL divergence as opposed to something like a GAN loss or a Wasserstein distance. And we actually realized that some of the maps people construct here are capable of conditioning as well, but somehow it's not mentioned at all in the literature, with the exception of sort of, you know, Yusuf's work. Of course, I'm fitting it here because of its similarities to normalizing flows. But on the side of machine learning, it seems that people are not aware that. That people are not aware that the maps can already do conditioning for them. And then, a little bit more removed from these two methods are variational autoencoders, where the goal is really dimension reduction. So, you have a map that typically goes from a high-dimensional measure to a low-dimensional one, and then you have the decoder that goes from low-dimensions to high-dimensions. So, the goal really is not so much conditioning there, but rather it's more sort of dimension reduction. But still, they solve very similar types of problems as us, and also these two other methods. On the inverse problem side, of course, you know, more and more people are using neural networks to solve inverse problems. But a lot of the effort, you know, Ozan Octam, Carolo Schondeb, Rebecca Willet, there are a lot of people working on this these days. A lot of the effort seems to be focused on. Seems to be focused on using networks to approximate either the regularization term in an inverse problem or to sort of learn the inverse map or some kind of regularization of it. And that one you have to be careful. There's this paper of Gottschling where they show that you cannot just directly train a map to invert a non-invertible map. I mean, that, I mean, I say it like that. It sounds almost trivial, but it's, you know, you need to make it. But it's it, you know, you need to make it clear. It's a very nice paper, I think. They kind of argue that you know, if you were inverting the pseudo-inverse of a non-invertible map, you'll be fine. But if it's not the invertible, it's the map itself is not invertible, you shouldn't try to use a network to invert it. That's sort of the main message, I think. So, where does our method sort of fit with all of this? Again, the main point for us is conditional sampling. There is such a thing as conditional. There is such a thing as conditional GANs, which is solving a very different problem. I won't get into that to save time. So the name suggests it's doing the right thing, but it's a very different problem they're solving. The closest thing in the literature is a method introduced by Adler and Octam, which I will compare to. It's very, very similar to what we do. And then there are certain other methods where they try to learn all the conditionals. They try to learn all the conditionals of a measure, so exponentially as many. These don't scale very well if you have sort of a PDE-constrained inverse problem. So, it's again, it's a much, much harder, much bigger problem than what we try to solve. Very recently, there's this method came out of the group at UT Austin. They actually tried to solve something very similar where you have sort of a GAN in the middle that is fixed, and then you want to wrap another network around it to condition. Around it to condition the input-output of that GAN, essentially, do conditioning with that GAN. And that's very similar to what we do, but the main difference is that they have to retrain their outer network every time that a new data comes along. Okay, so in our case, we want to train the map once and be able to condition on many new inputs. And in addition to that, we're going to have some theoretical guarantees that we at least have a hope of getting the right condition on. And I'll show you some of the other. Conditional, and I'll show you some of the other methods that don't have these guarantees. Actually, even in baby sort of 2D problems, can sample the wrong measures. All right, so let me now move to the sort of the meat of the talk and talk about, okay, what is this method that I keep claiming is doing conditional sampling. So, the first notion we need is block triangular maps. So, you saw the KR map before, so a block triangular map is just So, a block triangular map is just a slight modification where we say it basically has this kind of shape as a function of x and y. I'll just assume they're all finite dimensional for the talk. And we say it's blocked triangular because it's Jacobian is a block triangular matrix. So it has a zero block to the top right corner. All right. And just to tell you in picture sort of what's about to come in the next few slides. To come in the next few slides, the idea is that we're going to train a block triangular map that transforms a Gaussian on the product space of y and x to the target measure, which may be a sort of complicated sort of measure we're interested in conditioning. And turns out if you do this, and this is supported by theory, if you do it under the right constraints from this map T, you can extract a map S that does exactly the conditioning that we want to do. So it pushes a reference. That we want to do. So it pushes a reference Gaussian on y, the output, and if you fix it at any input, any new input x star, it pushes this reference Gaussian to precisely the conditional you're interested in. And again, you train the map only once because x star only shows up as an input. So we're going to sort of approximate the map using an optimization approach. So you minimize Approach. So you minimize some divergence between the push forward of the reference eta and the target nu. And this divergence is just very general. So it's a positive function that takes two probability measures as input. And it's zero if and only if the two measures are the same. And then you take this calligraphic T to be some appropriate class of block triangular maps. So the main sort of theoretical foundation is the following. Foundation is the following. So, suppose you have a map T star, it's a block triangular map, and it's a global minimizer of this optimization problem achieving cost zero, which means that it's pushing exactly eta to nu. Okay, so it's blocked triangular. If T star is surjective, then you can show, and the proof is actually not that complicated, but then you can show that for any new input X star. Input X star, the S part of this map T is precisely the conditioning map that you're looking for. So the parametrization here is important. The fact that this K part, the top part of the map appears in the bottom part is sort of intentional. It's so that you can avoid having to invert, compute the inverse of sort of high-dimensional maps. I mean, then the existence of these maps and so on are not too hard to show. They're essentially the same as showing the existence of. The same as showing the existence of KR maps, and those you can prove under fairly mild conditions. The main issue here, sort of in terms of the practical application, is this surjectivity constraint. You can do this sort of with neural networks. People have constructed things like invertible neural networks, but practically they're not the most convenient thing to work with, also, because they can sort of. With also because they can sort of limit the sort of approximation power of your map here in sort of pushing the measure to get very close to the target. So what you can do is essentially a corollary of this theorem, which is what we will use to construct our algorithm, and that's to replace the surjectivity constraint using boundedness, continuity, monotonicity, and coercivity. So this is just an application of a classic result in. Of a classic result in nonlinear functional analysis. And the key here really is the monotonicity constraint. It turns out that, you know, boundedness, continuity, and coercivity, you can kind of sweep under the rug when you're constructing the algorithm. But monotonicity is important because it will come up. That's why I'm kind of repeating it here. All right. So in that light, we now sort of formulate the problem at the continuum level, sort of at the level of the measure. sort of at the at the level of the measure so we we want to we have to pick a a a divergence and we also have to pick the approximation class for our map right so for the divergence we we choose the w GAN GP so this is the Wasserstein GAN with gradient penalty which is I mean it's a it's a mouthful but basically what it is is it's waserstein one distance if you think about these expectations that I have here between the push forward and the target The push forward and the target, you will notice that you know, in standard Wasserschin one distance in dual form, you take the soup over Lipschitz one functions. Now it's taken over continuous functions, but the Lipschitz one constraint is replaced with a gradient penalty. So it's a relaxation of the Wossin one distance. That's why they call it WGAN GP. And then the class T, we take it to be continuous and strictly monotone block triangular maps. Monotone block triangular maps. So they're maps that are parametrized in this particular form that was in the theory. And then you have a monotonicity constraint as well. So coercivity, we're not considering. The reason is that coercivity is a tail condition on the map. And when you have finite samples, you don't need to worry about it as much. I mean, offline, I'm happy to talk about this more. I think it becomes relevant if you want to do all of this on the function space setting. But in finite dimensions for now, But in finite dimensions, for now, we can ignore it. And now you implement it basically by relaxing the problem a little bit more. So we had two expectations inside the WCAN GP function. We replaced them with empirical sums. So you have samples from the joint measure. This is your data. You draw new samples from the reference, which is a Gaussian. You have the gradient penalty on F, and then the monotonicity. Monotonicity, I'll talk about this again, but something we found that works really well is to just add average monotonicity constraint using a Lagrange multiplier. This just for most sort of high-dimensional imaging problems, it just seems to work out of the box. And also, it has the added benefit that you can track how monotone the map is while you're sort of doing the optimization problem. So, you already get like an indicator of whether or not this penalty is fine or you need to do something else. Or you need to do something else. Yeah, so the average monotonicity is convenient. It allows you also to retrofit existing architectures. I guess I forgot to mention this, but so the maps, the K, S, and F, of course, are neural networks now, right? So this is the practical algorithm we saw already. So the benefit of average monotonicity is you can also retrofit the architecture. So if you have neural networks that are working really well, you can just add a Well, you can just add a monotonistic constraint to it and you can track it as I said earlier as well. The cons are that it is, it can be sensitive to the Lagrange multiplier that you use to impose the penalty. Interestingly, it's not such a big issue for like high-dimensional image problems. It does become an issue if you had a PDE problem. So, for PDE problems, this may need some work. You have to be more. Some work, or you have to be more careful with them. Yeah, so it may be violated in some cases, and it's not very well supported theoretically because now you have a map that is not monotone everywhere, but it's monotone in most parts of the domain. So theoretically, it's a little bit shady. But you can remedy this if you really don't like the average monotonicity. You can use sort of input convex neural networks. And the idea here is. And the idea here is: it's basically you combine two well-known facts about convex analysis and compositions of functions, and it allows you to construct neural networks that are convex in their inputs. And the way you use those is that you just simply replace the class T with the class of maps that are block triangular again, but you don't have the monotonicity constraint. Instead, you take the K and S maps to be input gradients of input convex networks. Of input-convex networks. I mean, the S-map is partially input-convex, it's only input-convex in Y. But this roughly achieves the same thing as that you wanted before it achieves the monotonistic constraint. It's a slightly smaller class of maps than the strictly monotone maps. But this works equally well, too. But most of the results I'll show today will involve the average, actually, all of them will involve the average monotonicity constraint. But we implemented this. But we implemented this as well, and it kind of works well, too. All right, and this is just a reminder slide. I know I already mentioned this a couple of times, but you train this map however you want with whatever monotonistic constraints you want. And then when you want to solve the inverse problem or do conditioning, all you need is the S part, right? So you draw samples from the reference on Y, and then you just evaluate S star of X star at the samples you drew from the reference. Samples you drew from the reference, you get conditional samples, and then you can do UQ with that, right? You can compute statistics. All right, so let me now switch over to the numerics. So the first question you might have is, why do you need this monitor? Like how important is this monotonicity really? Turns out it's actually pretty important. So if you look at even in 1D, so you're mapping a Gaussian to a uniform, you can train again. You can train a GAN, or you can train an M-GAN, which has the monotonistic constraint. And if you look at the maps, the GAN map is clearly not at all invertible. In fact, the M-GAN ends up approximating the optimal transport map. So you do actually need monotonicity if you want an invertible map, which is crucial when you want to do conditioning. You can see it also in sort of synthetic problems. So here we have sort of three models of. Have sort of three models of input and output. So here you have sort of the nonlinear function of x plus noise, which is non-Gaussian. You have the noise showing up inside the nonlinearity and the multiplicative noise. So the important thing to note here is that the training of the map does not require any knowledge of the dependence between x and y and the noise. It's completely agnostic to these relationships. All it needs is for All it needs is for you to be able to evaluate or simulate y as a function of x. So, what we do here is you generate a bunch of input-output pairs by evaluating these models, maybe like 50,000 times. And then you look at, first we look at sort of the joint measure of X and Y. So, these are the truth measures in the top for each of these problems. The next row is the M-GAN approximation, which visually you can see sort of it's doing a pretty good. Visually, you can see sort of it's doing a pretty good job. I mean, you can give quantitative estimates of some distances between these guys as well. But let me just keep the pictures for now. And then at the bottom, I have the method of Adler and Octum, which I mentioned earlier. If I could simplify it, oversimplify it, it's basically our method without the monotonicity loss. It also has the monotonicity penalty. It also has a slightly different cost function because they need to. A cost function because they need to deal with issues of mode collapse and so on. But you can see here that their method ends up overestimating correlations in different parts of the domain. And that's, again, an issue that you keep seeing if you remove the monotonistic constraints. So if you're not careful with that, then you may not get the right basically push forward of the measure. And also, sort of comparing the conditionals, and here I'm showing that. conditionals in here I'm showing that the histograms of the M-GAN conditionals for each of these methods compared to the exact conditionals so those are the solid lines and then the histograms are the bars so it does a good job of capturing the conditionals here and again the important thing if you're thinking about inverse problems is that it's completely agnostic to the model for the noise and so on so it doesn't need to assume like do I have additive Gaussian noise or not it's it's just it just needs samples all right Examples. All right. Something maybe a little bit closer to what we typically deal with in inverse problems. We looked at the Darcy flow example. This toy problem still. It's just a two-dimensional inverse problem where the permeability field is piecewise constant. So it's small in the blue region, it's large in the yellow region. And then you assume that you know the two regions, but you don't know how large the permeability field is. And you use pressure measurements. And you use pressure measurements to recover the parameters A and B here, basically. Add some additive noise to this. And here we compare it our method to MCMC. So I'm saying here at the bottom that it's 100% monotone. This is a setting where the monotonicity constraint is just enough. The average sort of monotonicity penalty is enough to get 100% monotone map. And this monotonicity is computed using sort of samples from the reference. From the reference. But when you compare this method to MCMC, you can see sort of this is this is sort of for in the you know um spirit of being honest about what the method can and cannot do, right? So it it struggles towards the boundaries. So here we use a uniform prior on A and B. So there are hard constraints sort of in terms of both parameters. So the measure is concentrated within a box and the MGN estimate. And the MGAN estimate does a good job inside of the constraints, but near the constraints, you don't get the sharp drop-off of the sample. So, and that's basically because of the monotonicity constraint and the regularity of the map that it doesn't allow you to have sort of sharp constraints here. Okay. And now off to sort of maybe more sort of higher-dimensional realistic problems. We're going to look at an Problems. We're going to look at an impainting example. So, this is the Celeb A data set. It has 162,000-ish images. Each one is 64 by 64 by 3. They're RGB colors. So, here we take the input or the observation to be the corrupted image. So, it's the image of Samuel L. Jackson here with the bottom part cropped out. And then the parameter that you want to estimate is the full image, right? So, it's the Is the full image, right? So it's the inverse problem of finding the image given the corrupted image. And yeah, so we train an MGAN on the full data set. And then given a new image from the setup data set that is corrupted, you generate a thousand conditional samples, a thousand conditional reconstruction of the image, and then you can compute variances and means and so on. So that's the inverse problem. And this is sort of some of the results that we can get. So, the first column are the original images that you try to reconstruct. The second column are the blocked out images. The next four are samples. So, you sort of see, I mean, these are not surprising sort of images, maybe if you've seen these kinds of in paintings using GANs. So, you get sort of, you know, different facial expressions, different colors of the shirt, and so on. And so on. But most importantly, because we're doing all of this in the probabilistic setting, because we're doing conditional sampling, we can compute statistics, right? So you can compute the mean of the images, which interestingly roughly tells you what is the most likely, for example, expression to show up in the samples. It gets a little bit more blurry in areas where things change too much, right? Like the outfit here. But that's then reflected in the variances. So in this, in the So, in the Samuel L. Jackson example, you get a lot of variance in terms of the clothes. In the top row, you get a lot of variance in the color of the shirt and so on. So this is, again, like, we don't claim that the method is like the most competitive sort of in-painting method, but rather the focus is the fact that you're doing conditional sampling and you can do it supported by theory that tells you that, at least in the, you know, if you had a good enough map and you trained it enough, you're. Enough map, and you trained it enough, you're getting the correct conditionals. Uh, same thing with the MNIST. Again, I mean, this is a simpler problem than the Celeb, but I wanted to present it because here it's also very intuitive. You know, if you have the digit 2, you block out the middle. It's equally likely to be 3 and 7. It's reflected in your mean and variance, but you can also do a classifier, like an independent classifier on the samples, which tells you now something like this image. Something like this image here is equally likely to be a seven and a nine, or this next image is most likely to be a four, or it can be seven or a nine. So you're doing probabilistic supervised learning in this setting, or I should say probabilistic classification. All right, so I'm almost at the end of my talk, so I just want to wrap up with a summary and then discuss some open questions briefly. So we presented a method for conditional sampling that the sort of highlight is that it's model-agnostic. So it lets you do probabilistic supervised learning in a data-driven way. You can solve Bayesian inverse problems like the Darcy flow example that I showed, or you can do likelihood inference with it as well. It takes the measured transport viewpoint towards conditioning, and it has a fairly straightforward implementation in that you can just grab existing code for GANs and then just add monotonistic. For GANs, and then just add monotonicity constraints on top of it. In terms of what it can and cannot do, when do you want to use this? You need large training sets, there's no free lunch, so you're going to need a lot of data. But it can work well possibly with inverse problems that have unknown or black box forward maps, or they have intractable likelihoods. When not to use this, if you have limited training data, it's not going to work. If you have very expensive forward maps, because you have to generate the training data. Because you have to generate the training data for the inverse problem. In that case, this is going to cost quite a bit. And also, if you have highly concentrated measures, so if your target measure is living on almost on a low-dimensional manifold, this can struggle quite a bit because the monotonistic constraint is, it becomes hard to satisfy. Open questions, there's a lot of them. There's a lot of questions regarding approximation theory. You might already be wondering. Theory: You might already be wondering, there's a big gap between our theory and what we implement. So, and you would be right. So, there are questions about how good all the neural networks are, what optimization, what function class should you use to approximate the maps. I briefly mentioned the coercivity, like what's the tail behavior of the map? What if my target is heavy-tailed that references a Gaussian? You run into issues, instability is there. Convergence rates are all. Convergence rates are open. How good is the average monotonistic set penalty, and when can it sort of cause problems? Those are all open questions. In terms of convergence and consistency analysis, also, you know, we don't know anything about sort of consistency of the minimizers in the large limits. So basically, if you want to prove, for example, that if you had infinite amounts of training and reference data. And reference data. So, as you approximate the expected values better and better, how good do you actually approximate the map? What's the convergence rate? Those are open. You know, choice of architecture, of course, choice of the cost function. We tried Wasserstein GAN. We also tried GAN. They all kind of work quite well. We picked Wassersting GAN GP because it worked a little bit better. But we also had a lot of success with LS scan, which is least squares. With LS scan, which is least square scan. So all of these seem to work well, and it's not clear which one is the better one for a given particular problem. And then in terms of inverse problems, there are a lot of interesting questions. One is how does all of this compare to MCMC? Because they operate quite differently. Like MGAN requires you to run the forward map a priori and train offline, but MCMC requires you to run the algorithm every time new data comes along. Every time new data comes along, so they're not even exactly the same methods, they're kind of quite different. When you get to sort of non-Gaussian priors, complicated posteriors, we don't quite know yet how well it performs. We have to sort of stress test it. And also there's the question of sort of experimental design and active learning and so on, which is sort of can you somehow generate your training data in an optimal way to find this map as opposed to just IIT? Find this map as opposed to just IID samples. All right, with that, I'm going to stop here and leave time for questions, maybe. Yeah, thanks a lot, Bemnet, for this excellent talk. I'm sure there are a few questions from the audience also. If you're uncomfortable to speak up because this gets this live streamed, you can type them in the chat window. I'll read them for you if you like. Who wants to? Who wants to kick it off? Otherwise, it's my chance. Oh, no, Peng. Peng, you will. Yeah, thank you. Very nice talk, better. So I was wondering, like, you know, you have this monotonic neural nets. Yeah. And you can also sort of like construct it with some other basis, like the polynomials and kernels by a suitable structure. You can preserve the monotonicity. I wonder if, like, do you have some comments on? Do you have some comments on using these different classes of bases or structures? Like, in particular, what is the advantage to use in neuroness? Yeah, that's a good question. I agree with you in that, you know, it's even beyond monotonicity. It's the fact that, let me go back to the theory. So you want surjective maps, right? So you can even use like an invertible function, right? So one approach. Right, so one approach would be even you can take this to be like the flow map of an ODE even, right? And you can make it be a deformorphism, for example. So those are very interesting questions. I've been thinking about them. In terms of what is maybe the benefit of the neural nets compared to like polynomials or kernels, Yusuf actually, in a lot of his other work, they use polynomials and RBFs. They use polynomials and RBFs and so on. So they work well, but we found that it doesn't work so well if you have like a high-dimensional problem, like the select data set that I showed. So for the really high-dimensional problems, the neural networks seem to work quite well. There's more sort of practical reasons behind it as well, which is that sort of in is also sort of the training of the networks as well. So this we can solve. As well. So, this we can solve using algorithms that people have developed for GANs and neural networks. So, we do sort of SGD and so on. I don't know how well all of that works if you have polynomials. Another sort of difference, I mean, this is not so much the neural network, maybe, but rather the cost function. But I'll just mention this as a comment is because if you look at Yusuf's earlier work or you look at normalizing flows, they typically have a KL. Have a KL divergence here, which requires them to compute like gradients of log determinants and things like that. So we don't need to do that at all, just because, but that's really a choice of the cost function. It's not really the neural networks. So that was just a long-winded answer to your question. If the neural networks worked well, yeah, I see. So essentially, like high-dimensional problems, maybe neural networks have some advantage over there. Yeah. Advantage over there, yeah, yeah. It's on our list, though. We really want to compare and see really do you actually need like a complicated neural network or not. In case of the cell-up data set, this one, we did actually need like a big network, like a proper sort of, and it took a bit of time for Nick to actually tune the network to get this to work because you it is a very high-dimensional problem, right? Um, so this one I Um, so this one, I mean, I don't know if something like polynomials would work with this. Yeah, so I just um follow up. Uh, we have some like recent work on projected kernels. So, essentially, in high dimensions, you can project the kernels into low dimensions by detecting their dominant information from the images, for instance, or from their inverse problems. And then you work in the low-dimensional subspace for the kernels. And there, you can somehow add to the added personal dimensionality. Yeah, yeah. I do. Yeah, yeah. Sure, yeah, that would be very interesting. I'll look that up. Sure. Okay, who wants to go next? Thomas? Yeah, I mean, I have a rather naive question because I'm not from the area, but so and maybe the reason is just a question of dimension. But if you want optimal transport maps and you go back to the theory, I mean, you could. And you go back to the theory, I mean, you could look at the Mange and Per equation, right? And you could get convex solutions from the Mange and Per equation, and gradients of those solutions would give you monotone maps. So is it just that the Monge and Per equation is too hard to solve for the dimensions that you're interested in, or am I missing something? No, you're correct again. Again, I go back to the surjectivity, right? I mean, it's just a matter of how do you want to impose Just a matter of how do you want to impose this, right? So, if you want to get a monotone map, there are many ways you can do it, right? So, the Monjam-Père, you can try solving that as well. I guess, and we thought about that too. The only part about that, and this may be my ignorance because I'm not an expert in optimal transport, but the only part that concerns me is whether or not you can parametrize the map this way. So, can you actually get from Montjean-Père a map that is parametrized? Montjam-Perre map that is parametrized this way. The reason is that if this k doesn't show up inside here, then the inverse of the top part will show up here. So it will become, imagine this k wasn't here. So then the push forward would be s of k inverse of x. And you don't want to invert that high-dimensional map. That's going to be difficult to do. So, like, so if there are a way to restrict the solutions to Mange and Per further, that would give To Mange and Per further, that would give you triangular monitor maps, and it might be of interest, or is it still just too high-dimensional? I would say it's definitely of interest. I mean, we could try to, you know, if it's high-dimensional, we could, I mean, this sort of a, you know, from the practical perspective, you can just relax the problem, right? Like what we do with the average monotonistic constraint and so on. So there are, I think, if you had sort of the optimal transport formulation, there might. Optimal transport formulation, there are probably ways you can relax it to get a practical algorithm. Because the important thing is here, unlike in optimal transport, we don't really care too much whether you exactly sort of, you know, we're not trying to approximate the map so that we exactly push eta to nu. As long as this divergence is a small, for practical applications, it seems you're happy, right? It's fine. So the ways you can relax the problem, I don't think. Ways you can relax the problem. I don't think it would be too bad. We've actually thought about it too, yeah. Because I mean, the Benomo-Granier formulation already tells you that it's going to be a monotone map, right? So it's a very natural question to ask. We just haven't really looked at it closely yet. Well, thanks. Very nice talk. All right. I'll give the floor to Lars then. I also have a question. So let's make sure that I can have a chance to ask this to. Make sure that I can have a chance to ask this to the experts. So, you said, can you elaborate more about why not to use maximum likelihood training? Because I mean, you use the GAN and I think it adds some complexity, but probably you have a good reason for not just training it like a normalizing flow. So, the problem would be that if you do the normalizing flows, or if you do KL divergence here, the gradient, the log determinant. The log determinant of the Jacobian will show up, or something like that. And you don't want to compute that for this map because it's not triangular anymore. And even if it is triangular, that's going to be, that's a kind of an inconvenient term to deal with. But if you want to have a block triangular map, that's going to be difficult to estimate. And the block triangularity actually gains, I mean, I didn't have this in the talk, but the block triangularity gives you a lot more. Gives you a lot more flexibility. We have a numerical example of this. So there are settings where, okay, so let me step back for a second. So if you do a triangular map, right, then the definition of the map will depend on the ordering of the variables. Okay, so now you might be in settings where the wrong ordering of the variables can make the map a lot more difficult to estimate. So the block triangle map will. Estimate. So the block triangle map will get around that largely. So there's a lot of flexibility you're gaining here in terms of your approximation power, and it will make the KL divergence cost very hard to optimize because you cannot compute the Jacobian determinant anymore. And a follow-up: so, if I gave you a network that can compute k of x and the log determinant. And the log determinant, as well as s. And I think there I only need to have the log determinant. I keep the first argument fixed, and I only need the log determinant for the y, or do I need the log determinant for the whole thing? You need the log determinant for t itself, so for the whole thing. Okay. Yeah. But if I gave you a network like that, oh, wait, sorry, I'm confused about that because now I'm so s basically. So, S basically goes from Rm plus N to Rm, right? Rm. So, how do you define the log determinant of that mapping? Well, it's not just S. So, note that the divergence you minimize is between the push forward under T and the target, right? So, it's the log determinant of the map T itself. Oh, okay. The full map. It's not just S. But what you just said. But what you just suggested, that will work, if you could basically approximate the KL divergence in an efficient way, that would be totally fine. Because again, theoretically, all of these methods are sort of supported in the same way, and that if you can make this losses small and you're, and it doesn't really matter which loss it is, as long as it's divergence, you're fine. And you respect sort of the constraints. And you respect sort of the constraints, you will get eventually a good map that can condition. It's more a matter of sort of, okay, how do I now approximate the map that the choice of the divergence comes in? And I would probably have to work on the inverse of T rather than on the T, correct? You mean for the KL? Yeah, because I mean, that's typically how you do it in normalizing flows, right? I mean, you go from the complicated distribution to the simple prior. Yeah, yeah. Simple prior. Yeah, yeah, that happens too. I mean, you can do it. You can put the inverse over here, for example. You can do tricks like that. And Yusuf has them in his sort of UQ handbook paper. Yeah, but it always has, KL has this issue where no matter where you put T or T inverse, the reverse will show up at some point, right? And then you run into issues of inverting the maps. They're not too bad, though, because you can just have like Can just have like an extra regression step. So instead of really inverting the map, you can have another neural network that approximates the inverse. Yeah. I'm trying to avoid these things. So that's why I'm not sure. Okay. Great. Other questions for Ben then? Just a follow-up, a quick question. So, in the computation of the log determinant, if you have like either a neural network or some other maps, have you tried? neural network or some other maps. Have you tried to use first like a decompose the Jacobian by low-rank approximation and then a compute log determinant? That's it. For instance, the complexity just not a scale respective.