So our next speaker is Art Crena, who organizes the long and interesting connection to the project. So thank you, Tony. Let's just say my career would have not turned out as well possible. And I'll say more. But I have about an hour's worth of slides here. I have about an hour's worth of slides here. So channel by Santag and get through the wall. And get through them all. So, the technique that is the key to what I'm talking about today is a very simple technique that everybody knows for me all who are in the finite dimensions. It's called completing the square. And I learned that technique from Michael Brockin. I can't remember whether I learned it from his book. John, is it in his book? Completing the square for LQR? Oh, yeah, yeah. Oh, yeah. That's where I learned it there. Okay, so let me get started. I've heard it there. Okay, so let me get started. I just came back from Seville, Spain, where I gave a talk on constructive methods for PDE control along with Yaroslav Kristik and Rafael Vazquez. And their key technique is called backstepping. And I'm offering an alternative technique, which I call completing the square, LQR, completing the square. Newton square. So here's a list of topics that I've been able to use these techniques on. And I'm particularly interested in point actuation and point sensing. That's what makes the problem hard, because they're unbounded operators. If you go to Curtin and Dwart, which is the classic linear PDE textbook, they don't, they avoid unbounded operators like so I've given So I gave 21 hours lecture for this. I gave my eight hours. Here's my talks. Reveal a finite dimensional LQR for the heat equation in one spatial dimension, nonlinear non-quadratic regulation of a nonlinear reaction diffusion equation, LQG of the heat equation, H infinity control and estimation, boundary control of the wave equation, boundary control of the beam equation, and boundary control of the heat equation. Boundary control of the heat equation on a display. The last one is not quite completed, but there is enough material there. If you'd like a copy of my EECI slides, just email me. I'll be very happy to share them with you. Just remember where you got them. And, okay, so here's the wave equation, the simplest version of the wave equation. Our spatial vampires is 0 to 1. W is the transverse displacement of the string. We assume we can only displace in a plane. We can only displace in a plane. We make it into a vector, a first-order vector differential equation by the displacement and the velocity of the displacement. And then we write it as a first-order PDE system where A is this matrix differential operator that you see there. We have to add an indicator condition and two boundary conditions. The most common boundary condition. The most common boundary conditions are Neumann, as you see here. The string is fixed at each end. I'm pre-control right now. I'm not talking about control. I will in a minute. And the other one is Dierschrei. The string, I'm sorry, the spatial derivative of the string is zero at each end. That's Naughty. And Dirichlet. And the question is always, which one is the more reasonable? We'll get to that in a minute. The eigenvalue. The eigenvalues of the A are INÏ€ and the difference between the Dirichlet and the Neumann is the Neumann has rapid zero zero, so it's only mutually stable, whereas the Dirichlet is all the closed loop eigenvectors are stable. So we're going to put a Neumann boundary control action in. You might why do we do Neumann rather than Why do we do Neumann rather than Dierschreu? Here's the Dierschrei version. To understand the physics, we're going to discretize in the spatial variable x. So we're going to divide it up into n sub-intervals, and we'll take the endpoints there and evaluate and approximate z at each of those endpoints. To handle Neumann boundary conditions, we have to do what is standardly done, which is introduce two fictitious variables, a point to the left of zero. point to the left of zero and a point to the right of pun. And then we discretize the right side of the wave equation by the standard second difference formula and the Neumann boundary conditions. This is the zero Neumann boundary condition and this is the control Neumann boundary condition. What this allows us to do is to solve for the fictitious variables in terms of zero and unity. Of zero and unity. So we solve them, we plug them in, and here's the resulting equation that we get. So this n here, and this n squared yields an n. The string is assumed to be uniform, so the math of a segment of string is just the length of the string kind of constant. So we make the right-hand side into a math. Right-hand side into a mass, 1 over n, times the acceleration. So the right-hand side must be force. And so we see that the Neumann boundary control is just applying a force to the string. If we do that for Dirichlet, we don't need the fictitious points. We have the interior of the interval. We have this second-order difference equation in space. We multiply, and again, we multiply. We multiply, and again we multiply by a segment of string, the length of a segment of strings, and this is what we get. And you notice this n time here. As n goes to infinity, we take finer and finer subdivisions. Our force is becoming impulsive. So this is not physical. The Urshwei control of the string is not physical. But it is interesting mathematically. So here is my So here's my wave equation. I'm putting a coefficient in front of U. W is the transverse displacement. We can track not just the zero trajectory, but if we have any solution of the homogeneous equations, homogeneous boundary equations, then we can track a particular wave shape. And the problems are identical because we're doing linearity. Because we're doing linearity, because of linearity. So we want to stabilize this strain to zero, and we choose a continuous symmetric value function, Q, think LQR. What's a little different here is it's a function of X1 and X2. You might think of this as the I and J coordinates of a matrix. And the matrix is block matrix because of this structure. And we're going to require that this form, this Fredholm form, be non-negative definite. And then we're going to choose a constant R, the cost of the control. And this is our LQR running cost criteria. We're trying to minimize. Now, the standard approach to linear PDE approach is to look for the solution. approach is to look for the solution of the Ricati, look for a solution that is called the Ricati operator. What is different from what I'm doing here is I'm looking for a solution that is a Fred home quadratic form. That gives me a tremendous advantage, you'll see, over the old approach. Now let's assume there's a state trajectory for every initial condition that stabilizes. We can prove this after the fact, and we will. And we will. Then, by the fundamental theorem of calculus, this is the integral we get. The expact right-hand side can be expanded, integrated by parts. Why is integration by parts important? Remember, our boundary control term is coming in, and it comes in very cleanly when you integrate by parts. Your whole problem of unbounded operators disappears. It's just standard integration by parts. By parts. And what we do is we add this fancy description of zero after we've integrated it by parts several times. We add that to the criterion to be minimized. And then we say, can we make that into a perfect square? And the variable that allows us to do that is this K. So we're looking for K, and as you might guess, K is going to be the feedback game. Be the feedback game. And K, of course, is a row vector of two entries. I learned this completing the square technique from Roger, as I said. So when we plug that in, we get PDEs. These are Neumann PDEs, where gamma is just a constant. They're elliptic PDEs with quadratic nonlinearity on the right-hand side here. And they satisfy. And they satisfy homogeneous Neumann boundary conditions in both x1 and x2. Once we found p, then k is given by either one of these two formulas. P is symmetric in its two arguments, so these two formulas means the same thing. I'm going to try to massage this to make it look like finite-dimensional relativity. So I'm going to define a matrix differential operator, A sub I, but it depends. Operator A sub I, but it depends on which variable I'm differentiating with spectrum. I'm finding a B that has a delta function at the end point where the control action is. And then this is what the equation on the previous slide reduced to. And you can see the classic body equation. Remember, the analog of matrix multiplication in infinite dimensions is to multiply two functions and integrate them over some interval. So this is that. So this is that. And as I tried to say before, the key to my program, the success of my program, is to take the Fredholm approach rather than the matrix, the operator approach. So we'll choose Q so it has a cosine series. Notice Q only has diagonal terms. They're two indices M and N, and when M is different from M, I'm assuming that Q is so. But I don't assume the same thing for P. I allow off-diagonal terms. Now, in my early work, three or four years ago on this program, I mistakenly thought I could get away with diagonal P. Several referees disabused me of that notion in no uncertain terms. But what happens, you're going to see that it's essentially diagram. It is, for all practical purposes, a diagram of multiple. Practical purposes in diagrams. So we plug these in, these expansions, and by the way, why are we plugging these expansions in? Well, we're looking for a weak solution of the Riccati PVE that I showed a few minutes ago. So it's natural to take serious expansions rather than any other way to solve it. I mean, that's, and so. And so you can see the Riccati structure here. Delta MN is the Kronecker delta. We're using both deltas. And here's the feedback gate. So if we can solve this equation, we're in business. Infinite dimensional algebraic Riccati equation, we're going to solve it by policy iteration. So we're going to make it an initial guess, and the initial guess is the one I stumbled on before, which is Stumbled on before, which is to assume P is diagonal. Different frequencies don't interact. So we're going to assume Pm is zero if M is different from M. That simplifies the equations greatly. They're basically quadratic equations. And these are the Riccati Yeah these are the this reduces to the Riccati equations of a string of two by two systems. Of two by two systems, and here is the two by two systems. Here's the solution of those algebraic quadratic equations, and we take positive signs because we're talking about a cost, which we expect to be positive. So there's our approximation to the kernel of the optimal cost. And here's our initial approximation to the kernel of the optimal feedback. The kernel of the optimal feedback. And we can choose not to affect a particular frequency just by taking Q and N to 0, and then the corresponding P and N is 0. But the question is, are we really converging up here to a continuous function? That's critical. That is a critical point. And here's the theorem. Here's the theorem. Suppose Q has the above expansion where Q and N is greater than or equal to zero, and there exists a bound, little Q and an R, an index R, such that Q over n plus 1 oxygen bounds in the case of Q as n goes to infinity. So what we're requiring here is that the Qs for higher and higher frequencies are getting smaller and smaller. What's R? R is an integer you choose. But you'll see in a minute we have certain limits on what R we can choose. So if we take R to be 1, then the Q series converges by standard comparison. If we take R equals 2, the feedback gain series converges, which is kind of a little surprising, but we have to take R greater than 6 to get the P series. To get the P-series, the optimal cost, the kernel of the optimal cost series, to converge. Why is that important? Well, if the kernel of the optimal cost series converges to a continuous function, then we know for any initial condition, the infinite cost is bounded. And so that means that we've managed to move all the eigenvalues into the open button template. That's the critical. That's the critical reason why continuity of P is important. Without going mono or mono with the individual eigenvalues, you now know they're all on the left hand. Now, in the literature, this has never been done. I mean, Lasieka and Trigliani are kind of the experts on hyperbolic problems, and they do LQR only over a flat hyperbolic. Only over a finite horizon for that. And why? Well, maybe this will tell you why. So a standard LQR cost, if you're just thinking physically, is to take the kinetic and potential energy of the string, add the control energy, and take that as your cost. Then Q is the generalized function I times the direct delta. And by Parsevile, that means that Qnn is the identity. So Q doesn't. The identity. So Q doesn't go to zero. And remember, Q had to go to zero fast. Now that's a great intuition. Why does Q have to go to zero fast? Well, you've got an infinite number of modes. If you charge yourself a lot for failing to stabilize these modes as they go out to infinity, you're going to get an unbounded cost. So in order to get a bounded cost, you have to pay less and less attention to the higher modes. Attention to the higher modes. Then the iteration is this one here. This is policy iteration. You start out with an initial guess for the optimal cost. Based on that, you choose an optimal policy. Then the optimal policy leads to a new guess for the optimal cost. And you go back and forth. The nice thing about policy iteration is it's not increasing. It's non-increasing. And of course, everything is bounded below by zero, so a non-increasing bounded below sequence converges. You might say, well, this is a function. Well, do it for a particular initial condition. It converges. So it converges for every initial condition. So what about the closed-loop dynamics? Where are these eigenvalues that we move? So here is the eigenvalue equation, and what's different. Equation. And what's different here is the dynamics is the same. That A matrix, that Cal A matrix, is exactly the same. What has changed is we've taken the Neumann zero boundary condition at the point one and replaced it by this boundary condition. Boundary is not quite the right word because it's to distribute it over the eigenfunction, but it's linear. That's important. And so we're looking for this. So then the first component, when we put all this together, the first component satisfies the classical boundary condition for the Laplace operator. And since it satisfies a homogeneous boundary condition at zero, we know it has to be multiple of the cosine function. And the coefficient inside that multiple is eta x. And eta could be real or it could be. And eta could be real or eta could be complex. We'll see that eta is complex in the closed loop. Once we have eta, then the eigenvalues satisfy this relation, mu squared equals minus eta squared. So mu equals plus or minus i eta for the eigenvalues. Here's an eigenfunction for that eigenvalue, but the problem is it blows up. Mu is going, the absolute value of mu is going to infinity. Of mu is going to infinity, so we write it in this form. These are the eigenfunctions that we take. So let's do an example. So Q is a plain vanilla. You ask, what R should I take, 6.1, just over the middle. Iterated 10 times, find K10. So the policy iteration I get 10 times. I truncated all. I truncated all the sums from 0 to 10. Here are the eigenvalues of the closed loop. Notice that the first eigenvalue of the open loop was lambda equals 0. We moved it into the left hand. But notice as we move up in higher wave numbers, the eigenvalues are getting closer and closer to the complex imaginary axis. So we've moved all the eigenvalues, but they're converging to the open loop eigenvalues. This goes back to the remark when I said P is almost diagonally dominant. If the closed loop eigenvalues happen equal to the open loop eigenvalues, P would be die. But since they're very close in the asymptotic limit, P is nearly die. So here are the modes. This is the zeroth mode on the The modes. This is the zeroth mode on the left, the first mode, second mode, the third mode. You can see how the green is the imaginary part of the mode. And you can see it's flattening out. And eventually it's just going to be essentially zero. What about the difference between Neumann and Dirichlet? Here's the Neumann open book spectrum. Here's the Dirichlet open book spectrum. They're basically identical except for lambda zero, which has multiplicity. zero which has multiplicity two an algebraic multiplicity two geometric multiplicity here are the riccati equations for p and for neumann and girschweig the big difference the only difference really is you take the partial of p with respect to one of the variables for that instead of just p itself this is what gives Dirichwe its impulsive force It's an impulsive force, taking a partial group. It makes it more singular. The initial Neumann integrate converges if R is beta greater than 6. Or Dirichweight, we get away with R14. So that's the extra strength of Dirichweight. Here's the two-dimensional, for our initial iterate, this is the corresponding two-dimensional block structure. And for Neumann and for... Here for Neumann and for Dirichwei, and I want to call your attention to this end here, which is the impulsive force of Dirichwei as the wave milk goes up. Here are the closer Neumann eigenvalues. With Dirichwei, we do a lot better, but it's not physical. What about other equations? I mean, there's three standard sophomore level, junior level calculus equations, the heat, the wave, and the beaver. The heat, the wave, and the beam equations. So we've done the wave equation. The heat equation is actually much simpler because the eigenvalues are going to negative infinity quite fast. So the only constraint on Q now is that Q be bounded. And the beam equation is a little more difficult. Curtin and Zewart take it, but I think it's an unusual. Take it what I think is an unrealistic problem. The reason they take that problem, I think, is that you can write down in closed form what the eigenvalues and the eigenvalues are. So this is a beam resting on two posts, and you have a bending moment at one point. Much more interesting from the Air Force's point of view, Fred, would be to have a cannon-levered beam representing a plane wing. And I haven't done that yet, but it's on my to-do list. My to-do list. So F is the deflection. It's a fourth-order system, fourth-order space system. These are the boundary conditions for the beam on the post. There are a lot of other boundary conditions that are possible for the beam. Vector notation again, same matrix differential operator, except we have four here instead of two. And a minus sign. The boundary conditions. The boundary conditions, the initial conditions, the open loop eigenvalues you can see here are still on the imaginary axis, but they're moving out faster. They're moving out like n squared rather than like n. So we stabilize by LQR, and now we have to choose that exponent r to be greater than 8 in order for the p-series to converge. To converge. So we choose R greater than X. So you can't pay very much for the high frequency oscillations of the field. You just gotta live with them. So I'll close with my slogan. Think mathematically, act computationally. Thank you, and I'll try to answer any of your questions. Thanks a lot. So, any questions or comments for all? Maybe you analyze higher order domains for the public. Yes, we're doing the heat equation on a disk. My approach to research is baby steps. You always go to the next problem that you haven't solved yet. And the next problem is the heat equation on the disk. And there's some interesting. And there's some interesting things. You know, what do you mean by boundary control on the disk? Probably the most physically relevant one would be to have intervals on the disk where you heat. Those intervals, you could have a constant profile, heating profile over the interval, or you might have a delta light function on that. That's almost finished. That's almost finished. And then the extreme case. Extreme case is you shrink the intervals to zero, you let the weighing function go to infinity, so you get a delta function, and so now you have point controls. And the interesting thing about point control is a boundary control, point boundary control on the disk, is now you're in codimension two. So on this string I had boundary control, but it was only codimension one with respect to the string, point control. Point control. Codimension 2 means that the eigenfunctions are no longer eigen classical functions. They are generalized functions, delta functions come up. But all these details are to be worked out. I was just wondering whether you think there might be any, it might be useful to consider more specific pole placement beyond just getting them into the left-hand plane, and if so. them into the left half plane and if so whether uh you think that is that problem could be tractable at all yeah you're and the technique you want to uh use is backstepping that that is yourslav and his colleagues have done a wonderful job with backstepping and what in ba if you don't know backstepping you start out with a pd e system with some kind of control maybe boundary control or assuming control Or astrology control. And then you choose a nearby system that you know is stable, asymptotically stable. And you try to come up with a feedback transformation, change of coordinates, and a feedback that takes one system into the other. Now, you get to choose the other system, the target system. So that's poll based. Just a thought on the boundary control. Boundary control, you could think of changing the domain. In other words, shrinking the domain or expanding the domain. Instead of interesting, I haven't looked at this. Not to mine, but I'm not an expert on the all. I don't, you know, I followed Roger's advice. I changed fields from finite dimensions to infinite dimensions recently. So I'm not an expert on the literature. I can't speak to you. It looks to me more natural. Because it it looks to me more na quite natural. You have a spray that's a spray. So the shaping, like you just have motion that you yeah, that's an interesting thing. And the string equation that I showed is the linear version of the string equation. And the key assumption that makes it linear is that the tension is uniform along the string. So, but that's not true. That's not true. Where the string is highly bent, the tension is greater. And so, to go to a more realistic physical model, we would get not only transverse motion, but we get longitudinal motion along the string. And I think this connects with what you say. And that, again, is on my to-do list. Are there? I have a question. This is Krishna. Hi, Krishna. Hi. So the question has to do with So the question has to do with the formula you wrote down for the wave equation setup where in the closed loop setting after applying the optimal control, you obtain something that looked like a Neumann boundary condition but it had this sort of dependence on the whole interval 0 to 1. And can you say something more about this in the sense that does this correspond to something fit? Correspond to something physical for the weight equation that is analogous to having a non-uniform mass distribution or something like that? I don't think it does at all. I think this is feedback, full state feedback, as you see here. Oh, this is the icon function. Yeah, there's K0, the kernel, and then higher kernels. It's not physical. Control, you know, the wave equation with classical homogeneous Neumann boundary conditions is self-adjoint. And so the spectrum is real, the eigenvalues are orthogonal, we've got all sorts of nice things going on. The closed-loop spectrum. The closed-loop spectrum here is not, the closed-loop operator with these boundary conditions. I mean, the operator hasn't changed, but the space on which the operator is acting has changed, and it has to satisfy this integral boundary condition at 10. And that destroys the self-adjointness. And so the causal binding values don't have to be real, we saw that, and they don't have to be orthogonal to each other anymore. Foggo between each other, right? I see. I have more of a comment on Turklan's question. So the boundary control, I know some people of Ruslan Krishetnikov. He did, I think it was parallel waves of moving boundaries. Who? Who do you know? Ruslan Krushchetnikov. So he has some experiments to do as well. So I think it's not quite Not quite what was discussed here. It was a different kind of control, but I don't know. I mean, I think when Trifan asked the question, I was thinking of a banjo with the person moving their finger along the string, changing the length of the string. And that's a very interesting question. Maybe I could get the reference. So the other thing that we've got to, I mean, probably. I mean, probably in the position to think about this, is a lot of the things that are physical, like a banana, for example, you probably get into realms where PEEs are not the right model because they're very local and there is coupling between the equations. Well, there is a non-local version of the string equation due to. Equation due to who's the guy who does circuit theory? The sum of voltages around a voltage drop around a closed curve. Kirchhoff has a variation on the wave equation that is not local. Right, right. That's the kind of thing I've tried to use my techniques on Kirchhoff's equation with no success. Maybe the non-vocality is defeating me. But again, Kirchhoff's equation is equal to equation. But again, Kirchhoff's equation is an idealized equation. Again, he's assuming the tension is uniform along the whole string. I mean, a much more interesting string would be a string that is in three-dimensional space, the points on the string can move longitudinally in two dimensions. And that would be much more interesting. Okay, that's Okay, thanks very much. All right, so for the next half hour, we're going to have a more informal thing, just an honour of Roger. So I'm just going to open the floor to my remarks or anything I'd like to say about Roger. So let me just start off by saying a couple of things that are related to this. So, yes, my first introduction, I guess, is to Roger. I guess to Roger, the board was doing a couple of courses, not in linear systems, it was 203 and 209, I think. It was non-linear systems and stochastic control. And certainly it brings to mind some of the things that Roger showed in general. So first of all, there were the problem sets, and you were never sure in the problem sets whether they were actually solvable or not. So you could use problems might or might not be able to do these. So when you were struggling at night, you know, it's quite hard to get hopes that that was interesting. How long we can hope so that was interesting. One of the other things that I remember Roger saying is that there are no problems, there are only opportunities. So that's something that I've always kept in mind. And the other good thing is, it was always nice talking to Roger on the Roger, having all these great conversations about various things related to the class or otherwise. And that's something that continues for many years after that. And certainly something I just now just talked to Roger either at Um, either at various meetings or on the floor at many times. Um, okay, so thanks. Um, so yeah, I should say that in the background, um, we've got these very nice photographs that we provide by that project from the sun. So we'll just keep playing these in the background so they're a lot of nice. Doesn't play the slideshow. I think it is on slideshow. Yeah, no, it's not storage. It's currently already frozen. Frozen products. Let's see, make sure we put that in there. Oh, was this a ton of money? I think I selected only one photo. It's my guess. Oh no, they no one here. I'm not sure. No one here. What's the delay report? It's a few signals. So just a couple of other things then. So you know, it was great to know Roger both on a professional level and a personal level, and you can see many of these things. Yeah. I had a lot of fun working with Roger. Working with Roger just on various things, and it's related to the last two talks. For example, I'll work on gradient flows and integrable systems that are reflected both in Christian's talk and John's talk and the way they came together. So, that was always a lot of fun. And later on, there was work on point control, among other things. But as you can see, Roger, it was just a joy to be with in many ways, both professionally and personally, and you're seeing that here. That here. So, also, what I'm going to do is. Can I just add something with pictures of the grandchildren? Roger has two grandsons. One is Roger Ware Brockett, too, and the other is Roger something else Brockett. Roger Jensen Brockett. Right, that's right. There are lots of Roger Brockett still around. That's a good thing. So, Roger Jensen Brockett graduated and he's back in the West Coast now. Right, I think one of them played tennis, which I enjoyed, I remember that. Yeah, so Roger and his grandsons used to, and his grandchildren, used to go up to Maine all the time. It was a lot of fun to meet them as well. So one of the nice things also I'd like to mention, as this goes on, so John has been involved in setting up a memorial fund for Roger through the IEEE. So John. And John, maybe you can say it a little bit more, but if you just Google IEEE Roger Brockett Memorial Fund, we'll find a way to contribute to this fund, and I think the idea is to do something that represents Roger and perpetuity going forward, and I think possibly a Roger Brockett lecture. So if you just click So, if you just click on that site, you'll see a way to contribute. And Parallel has also played a big role in this. John, do you want to say anything from all of that? Yeah, so what Tony's written down, just if you Google that, then you'll be led to some page where you can donate. I want to thank people in the room who have donated, very generous, very kind, and the control system society. You know, Carol Ann's Carol Ann's idea is that she wanted to do something memorial through the IEEE for whatever reason. I would have assumed I, you know, Harvard or something like that. She was very clear she didn't want to give money to Harvard. She wanted to do something memorial through the IEEE. And it turns out I knew a lot of people at the foundation. So we set up this fund, and the Control System Society turns out to be. To society turns out to be