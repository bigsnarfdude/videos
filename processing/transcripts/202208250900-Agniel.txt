I try to give you an idea of what we've been working on with longitudinal and high-dimensional surrogates. And this is joint work with Layla and Boris and Rodolph, who may be on the line right now. I also, not everyone knows what Rand is. So, just to give you a brief overview, Rand is a public policy firm. The headquarters are in Santa Monica, right next to the beach. We have offices all To the beach. We have offices all over the country, and we're hiring right now. So, if you have an interest in doing work that affects public policy or you have really smart students who like that, then please let me know. Okay, I mean, it comes as no surprise to anybody here. I'll be talking to you about surrogate markers today. And thankfully, most everybody is already familiar with this front material. So, just to get you on the same page with the notation, S is going to be the surrogate marker, Y will be the S. S is going to be the surrogate marker, y will be the outcome. And we sort of want to know: is some function of s able to capture the effective interest? Is it sufficiently like the outcome? And what does that mean sufficiently like the outcome? And hopefully, we've had a number of people put these four frameworks from Joffrey and Green on slides already. And sort of in the spirit of Michael Hudgens, I'm going to try to make a defense of conditioning on. Try to make a defense of conditioning on the surrogate or a version of conditioning on the surrogate that I hope you find convincing. But let's just start out with some notation. Again, a lot of this is stuff we've seen before. A is the treatment. We may have some pre-treatment covariates X. We'll use the counterfactual notation with the little superscript A for indicating the counterfactual if treatment were set to value A and same for the surrogate. And then we have the relationship. And then we have the relationship between the observed data and the counterfactuals. This is all very usual stuff. And we'll call it the overall treatment effect delta. Okay, so the conditioning on the surrogate, as Gert told us about, and it's traditionally understood from Friedman in 92, is we fit two models, one where that doesn't include the surrogate and one that does include the surrogate. And then we estimate the proportion of treatment effect explained by comparing the coefficients. Coefficients for the treatment in these models. And this, you know, seems a little naive and not like something that we might do today. But I think that if we just update it a little bit, then we get something that's quite interesting. So instead of basing these effects on parameters in a model, we can just write them in terms of expectations of counterfactual quantities where the overall treatment effect looks how you might expect. Overall treatment effect looks how you might expect, but then the conditional one is we just condition on the surrogate and then we set the surrogate or we integrate the surrogate over some reference distribution. And I'll talk more about setting the reference distribution in a bit. So some benefits of this approach. I don't get to, it's in PDFs. I don't get to do the show you the different Bullets at different times. Oh well. So, some of the benefits of this approach, and we don't need to assume any parametric models. I think everybody here is probably pretty well studied at estimating counterfactual expectations without assuming they belong to a parametric model. We also don't need to specify a joint distribution on the potential outcomes, which I find to be quite nice. And everything here that I showed you is essentially a treatment effect. So we get to use. Essentially, a treatment effect. So, we get to use a lot of the intuition and the machinery that has been built up over many years in causal inference for estimating treatment effects, for estimating all the stuff we need. We also don't need to assume, as you do in mediation, that S is manipulable or it is a mediator or that we can actually write down the counterfactuals in terms of the surrogate. Finally, despite that stuff, I'll show you that if you are willing to believe that S is a mediator, then the Then the thing that we put in the numerator of the PTE has connections to the typical mediation quantities that we would otherwise estimate. Just to give you a little bit more on that, if you assume the typical mediation assumption, so that S lies on the causal pathway between the treatment and the outcome, you have no unmeasured confounding between the mediator and the outcome, and that you can write the counterfactuals in terms of the surrogate as well as the treatment. In terms of the surrogate as well as the treatment. Then, if you take the, if you do sort of like the most Friedman-like thing and just take the reference distribution to be the distribution of the surrogate that you have in the population that you're studying, then the numerator of the PTE, the difference between these two treatment effects, can be written as some combination of the conditional, natural, and direct effects. And this is like maybe a little bit more notation than I'd like to get into, but just it, and if you choose the references. And if you choose the reference distribution in different ways, then the connection to the individual indirect effects that you're, I'm sure, familiar with becomes even clearer. But all this is just to say that doing this thing that at first seems very naive is actually quite connected to the very smart mediation things that people typically do. So I kind of think of these as mediation light estimates. And if I was much better at Photoshop, I would have changed the gold text to say mediation. But I kind of think of these as like doing mediation. I kind of think of these as like doing mediation without requiring all the assumptions of mediation. Okay, so lots of people we've heard a ton about doing stuff like this in the context of a single surrogate marker, and Layla has done a bunch of non-parametric work at doing essentially these types of estimators in non-parametric settings. And this just shows that the outcome is maybe measured at month 10, but we would measure the surrogate at month five. But the types of questions that we've become interested The types of questions that we've become interested in is what happens if S is measured over time, and we'd like to capture the disease trajectory or the dynamics of the disease, or just use more information. And a lot of the surrogates that we've heard about this week have been things that could be or are routinely collected over time. But we can also think more broadly about the stuff that Boris, for example, talked about where we were trying to use gene expression data as a surrogate marker. A surrogate marker. There are also other situations that come up in the type of work that I typically do, where people are using things that clearly aren't mediators. In this example, you might imagine, can you do a pointer? Okay. The outcome might be some hard outcome like diabetes. And the surrogate that people will use is stuff that's routinely available in electronic health record or insurance claims. And those things aren't. Claims. And those things aren't really mediators in any way. They're sort of reflections of some underlying disease status. If you just change someone's electronic health record by saying giving them a diagnosis code of diabetes or an indication that they received glucose testing, that of course is not on the causal pathway to the outcome, but it could be quite a good surrogate. So what I'll talk about, that's sort of the front matter. I think that's one of the big takeaways I hope of this talk. It was just some basic. This talk is just some basic defense of conditioning on the surrogate as a sensible approach. And then I'll tell you, talk to you about two papers that we've written in this space, one that's specifically about longitudinal data and one that's much more general that we had high-dimensional data in mind, but can really be applied in any setting. I'll spend a little bit more time on the longitudinal one because I think that the math is. The math is and the sort of like intuition is a lot more interesting, but the second one might is much more general and might actually be more interesting to you. Okay, so let's talk about longitudinal surrogate markers. So there have been previous approaches. Gert mentioned one in the meta-analytic setting, I think. And then there has been a lot of work in joint models of longitudinal surrogates and timed event outcomes, but we'd prefer to do something that's not modeled. We'd prefer to do something that's not model-based and we say only have one set of study. So, I'll talk to you a little bit about assumptions. We have this sort of typical causal assumptions, and I'll just draw your attention to the fact that for this paper, we assume that we have randomization and we don't need pre-treatment covariates. There are some more technical things that you may be interested in. There's some sort of technical stuff about how the surrogate is. About how the surrogate is a realization of some smooth trajectory over time. And then we have conditions on the support of the surrogate under control and treatment being the same. This is actually quite a sticky issue for these types of estimators. I'll come back to it in a little bit. You can weaken this a little bit too, but not that much. Okay, so our estimator, we just put hats on everything. And because of the, I guess I should say. Of the, I guess, I should say, delta S is that thing the integrating these conditional expectations that I talked to you about before, and for this one, we integrate over the distribution of the surrogates in the control arm. Because of randomization and because of this choice, we actually only have to, most of the things that we need to plug in here, we can just do by averaging, which you're familiar with this in the for estimating the overall treatment effect. For estimating the overall treatment effect, randomization makes everything quite easy. And so, really, all we need to do is estimate this mean function in the treatment arm and then apply it to the control arm. So, that's basically what we have to do. And I'll just describe some approaches to doing that. Estimation. Okay, so what we do is a two-step process. One involved, the first step involves dimension reduction, and the second step involves kernel smoothing. And you might be asking, And you might be asking, why two steps? You only have to estimate one mean function. We all know how to do that. And the reason is it's difficult to correctly specify models. And we'd like to be as flexible or non-parametric as we possibly can. And kernel smoothing is great, but we can't just sort of directly throw it at a p-dimensional variable because of the cursive dimensionality. So we're going to try to use sort of functional regression methods to do some dimension reduction, and then that'll allow us to do kernel smoothing. And then that'll allow us to do kernel smoothing. So, just to give you a little bit of intuition about why this might be a good idea, let's say we have this mean function that's quadratic in a two-dimensional surrogate. And we can do a really bad job of estimating this thing. Let's say we just assume that it's linear in the two dimensions and fit this linear model. You can see down here that the estimate of the mean function is not very close. Is not very close to the true mean function. You know, it's just missing a lot, especially in the tails here. But so we've done a bad job, but we've gotten it down into a dimension where we can do a good job. So once we're in this dimension, even though this is not a good mean function, it's going to allow us to be able to smooth and figure out a pretty good mean function. So that's the intuition, and that's essentially what I'll describe for this first. Essentially, what I'll describe for this first approach, which involves functional regression. And we'll talk very briefly about two different types of functional models that you could use. And then the second approach is going to be a little bit different, but same sort of dimension reduction and then smoothing, but it's going to only use kernels. And we're going to sort of like change the kernel space that we're working in. So the functional models, the first one is the like most vanilla functional model out there. It's called Vanilla functional model out there. It's called the functional linear model, and this looks basically just like a regular linear model. But the difference is we have these, you know, this surrogate is measured over time on a grid, and this beta is now a smooth, some unknown smooth function. And you just use a sort of like GAM type penalty function to manage the smoothness of this beta. And there are sort of standard ways to do that. And then the other approaches. And then the other approach is a functional generalized additive model, which again is like a direct analog of the typical generalized additive model, where now we have some smooth function that is of the surrogate and the time. And you can do some sort of basis expansion and use penalty terms again to control the smoothness of this thing. So these are the things that we use to fit our initial models. And then once we get the predicted values from the models, we just plug them into. Values from the models, we just plug them into it, like the typical kernel smoother. And there are some technical conditions on the kernel smoother and the choice of the bandwidth, but I'm not gonna bore you with that right now. So the final estimator looks like this. You fit a regression model in the treatment arm, and you get the fitted values. Then you plug those into a kernel smoother to get new, better fitted values, and then you apply that to the control arm. This gives you the residual treatment effect. This gives you the residual treatment effect, you plug it into the PTE, and we're done. The second approach, again, we can't use, we'd like to use a kernel. We'd like to say we don't want to go through some functional model. We'd like to just use kernels, but we can't do it because of the cursive dimensionality. But because of the nature of the space that we're working in, you can kind of sidestep the cursive dimensionality by exploiting the topology of functional. Exploiting the topology of functional data. So it was really helpful that Shuki talked about functional PCA yesterday because this is exactly what we use. So for any functional data, basically that's smooth, you can decompose it into an overall mean function and then a linear combination of some orthogonal basis functions. This is an example from the HIV trial that we. HIV trial that we looked at, where we were looking at CD4 count over time. And each one of these lines here is a basis function. So this is the first one and it captures the most variability in the data. This is the second one, it captures the second most, and so on. So what does that get you? Typically, this is really good. So this is, I think, using only three basis functions. We can actually recreate the trajectories. The blue dots here are the observed. The blue dots here are the observed CD4 count trajectories. The red line is what we get from the doing the living only in the eigenfunction space. And you can see that we don't lose a lot. We can actually do quite a good job of recovering what's going on in the data. Different patients. Yeah, sorry. So, yeah, each of the facets, for those of you online, these are different patients. So, we can use these eigenfunctions to define what's called a semi-metric, which is like a distance function that has this property where just because the distance is zero doesn't imply that the two functions are the same. They just need to be the same in the function space that we care about. So, if they're not different, and basically what we do is we project these differences into the eigenfunction space, and that gives us this distance metric. So, if you're Distance metric. So, if you're very different from the other curve on parts of the function space that we don't care about, then it doesn't contribute to the distance function. So basically, you can take that semi metric and use it directly in a kernel smoother, where here the k indicates the number of basis functions, eigenfunctions that you're using. And then this just works like a regular smoother, just a regular kernel smooth that you're all familiar with. Just a regular kernel smooth that you're all familiar with. And there again are some technical conditions on the way that this kernel is constructed. But the thing to keep in mind here is that the bandwidth is related to the number of eigenfunctions that you choose to include. And this ends up being quite restrictive technically, but we found in simulations that you can violate the technical conditions and still do okay. So this the final kernel estimator looks like this. First we choose the semi-metal. This first, we choose the semi-metric. We choose how many dimensions essentially we're willing to live in, and this is the dimension reduction part. And then we just compute the kernel smoother and plug in to the control arm and then plug that into the PTE. So there are some asymptotics. We do get asymptotic normality under a bunch of technical conditions for these things, but they're not necessarily converging. That they're not necessarily converging to the true PTE. And whether or not they converge depends on the adequacy of the dimension reduction. So they're going to converge to something. It just, we're not sure how close it is to the truth. And we give some expressions for this asymptotic variance, but they're super ugly. So we just recommend doing the bootstrap. It seems to work quite well. Okay, so that is the longitudinal piece. The longitudinal piece, we propose this two-stage kernel smoothing approach, which takes advantage of this unique topology of the longitudinal nature of the data. We talk in the paper more about some stuff you may be interested in terms of what if the longitudinal data you have is sparse and there are a bunch of simulation results in an application to an AIDS clinical trial. Okay, now I'll, in the last 10 minutes, talk to you about a more Last 10 minutes, talk to you about a more general approach where instead of last time we chose the reference distribution to be the distribution of the surrogates under control, this time we'll choose the, again, the sort of like Friedman-esque, just take the distribution of the surrogates and the whole study in both arms. And then you get this thing that is actually like an average treatment effect that you're basically Treatment effect that you're basically just treating the surrogate as a pre-treatment covariate. So, up here, you see the form of the overall treatment effect that you know depends on these pre-treatment covariates and then integrates over them. And the form of the conditional one is the same. You just condition on the surrogate throughout. Yeah, just to clarify, as it is function and surrogate directory in the biomarker over follow-up? So, yes, but here it can be anything. It's actually my next slide. Be anything. It's actually my next slide. So, for the purposes of this one, you can put anything in here. It could be a single-dimensional surrogate. It could be a longitudinal surrogate marker. It could be a mixture of a bunch of things. It could be Boris's RNA-seq data. It could be whatever you want. So this one, just very general. Anything you want to throw in here, we can estimate this PTE and these expected values. And this can come from observational data too. So, for this one, we don't assume randomization like we did in the past, but we do assume that you collect the confounders that you need to control for the confounding due to treatment. And we require two versions of positivity. The first one is the typical one that you would typically see. Typically, see that says that the treatment group and the control group are similar enough in the covariate space that they have overlap in that space. And then the second one is again similar, but now it conditions on the surrogate. So this requires overlap in the surrogate space as well as the covariate space. And again, this is a pretty restrictive potentially requirement. This just means that the treatment can't have such a big effect. The treatment can't have such a big effect on the surrogate that these two distributions become quite different. And then since this is just an average treatment effect that we're trying to estimate, we can use, again, like all the machinery that people have built up for estimating average treatment effect over the years, where this is just identifying the average treatment effect, this conditional treatment effect on the surrogate. Treatment effect on the surrogate in terms of the adjusted IPW form that many of you have already probably seen. And we just have this thing that's pretty much like a propensity score that goes in different places here. And then this thing that looks like a mean function. And we can write out a similar expression for the overall treatment effect that takes out all the S's and leaves in the X's. And we'd like to, again, be as non-parametric. And we'd like to, again, be as non-parametric as possible. So, in order to allow us to use a variety of machine learning approaches, we use sample splitting, which has been recently popularized by under the name of double machine learning by Viktor Chonozhukov, but it's also been developed, I think, independently by Mark Vanderland and others. In our implementation of this, again, you can sort of do whatever you want, but in our implementation of this, we use oops. Implementation of this, we use the super learner, and we also look at a version that just uses the lasso to estimate those mean functions and propensity scores. Because of the sample splitting, the asymptotics are quite straightforward. We get this asymptotic normality result as long as we have a sort of double robustness thing going on. And so basically, this is the error for the mean function, this is the error for the This is the error for the propensity score, or whatever you want to call this thing that conditions conditions on the surrogate. Each of these things can be as slow as n to the minus quarter, which most machine learning methods off the shelf will give you. And so as long as that is true, then you get this rate that is low enough to ensure that we have the sort of like nice asymptotic properties that we would expect. So we need this for this one and then for the corresponding. This one, and then for the corresponding ones that don't include the S's. So, what's this look like overall? First, we split the samples, say, into two halves, just for the purposes of demonstration. We use the first part, call it half A, to estimate all the nuisance functions, the propensity scores, and the mean functions. Then we apply those to the other part to get the treatment effects, these things. These things. And then we flip the roles of them and do these two steps again. And then we average them and plug them in down here. And that's how it works. And this seems to do quite well. We have a number of simulations where we compare to other methods for doing high-dimensional mediation analysis. And this top one right here is the version of our method that uses the super learner, and this is the one that uses the lasso. The lasso. And you can see in the sort of like larger sample size case where we expect everybody to do pretty well, everybody does do pretty well, but our methods seem to have lower, smaller confidence intervals or smaller, at least empirical standard errors. And then in all the other settings, sometimes the other methods just seem to start to fail. And we have sort of worse performance. We have sort of worse performance, but at least we're in the right spots. And we find this across a number of different settings. This one is mostly linear, but we have some that look at even higher dimensions and more non-linearity, and we still seem to generally be able to adapt. Okay, so wrapping up, we tried to give some very flexible methods for studying a wide range of potential surrogate markers with. Range of potential surrogate markers with a lot of attention to the longitudinal and high-dimensional settings that we're interested in. I hope that maybe you'll consider the possibility that just the controlling for the surrogate, which I think in Gert's presentation seemed quite naive, might not be so naive. And in part because it has these connections to mediation and that maybe aren't appreciated as well as they could be. And again, because we're And again, because we're like defining all of this stuff in terms of treatment effects, I think that there's a lot of headroom here to be able to use what people know about treatment effects in terms of sensitivity analysis, in terms of conditional average treatment effects, et cetera. I just think that this makes a lot of things easy that might not otherwise be easy. And then, like I was saying before, when the treatment has a large effect on S, this is why you might not be interested. If the treatment has a really large effect on S, Has a really large effect on us, we're probably looking at a positivity violation for that second positivity assumption. And this is quite troubling to me. We're working on ways to maybe fix it up, but it's also possible that some of the other approaches, the principal stratification approach, for example, might not have this issue. And I'd be interested to hear from people who know better than me. So that's it. I'll remind you again, we're hiring at RAN. Send me your smartest students. And thanks again. And thanks again to everyone for listening. Great. Thank you for starting us off so well. I'm going to open it for a few questions. Yes. So I really like the paper. I just had one question. So, like, I usually think of Mott. So, this is about the population. I usually think of models, so this is the positivity assumption. I usually think of modeling like S conditional on A and X. You have like A conditional on S and X, and you can rewrite it like the other way around, right? But I'm just like, conceptually, that's confusing to me because it's like you're trying to model probability of receiving treatment conditional on surrogate, which probably happens afterwards. So I wonder what your thoughts are on that. Thoughts around that? Yeah, I don't know that I have a lot of intuition for how to interpret the value that you get because it is this sort of backwards thing, other than in terms of the positivity, like do we have overlap? And like you said, you can rewrite everything in terms of the S conditional on A distribution if you know the distribution of S. And so it's possible that you could develop more intuition for it. More intuition for it, but definitely, I just think of it in terms of this overlap. You know, are the two distributions similar enough for us to be able to extrapolate one through the other? Yeah, thanks. It was a really interesting talk. I got a question. I guess early on you mentioned, or I thought you said that you're trying to predict the counterfactual S trajectory. Actual S trajectory for people who are randomized to placebo who would get treatment. So you're doing, that's the prediction you're trying to do. And it seems like if you just use baseline covariates, that would be kind of difficult to predict an entire trajectory. And so I was wondering if you're using sort of time-varying covariates to predict the counterfactual trajectory and if that raises some concerns. So, yeah, we're actually not predicted. So, yeah, we're actually not predicting the trajectory that I maybe shouldn't have nodded when you said that. We're trying to predict the expected value of y given the trajectory in that group. So this we just need again, it sort of comes down to the same type of positivity. We need to that we're just going to We need to that we're just going to take this expected value that conditions on the S's and port it to a different distribution of the S's. And so it's actually a much easier problem than the one you're thinking of. Thanks. I think for sake of time, we'll leave other questions for later in the discussion. So our next speaker is Dean Fulman from NIH, and you're lucky to 