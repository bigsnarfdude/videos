Okay, and I'm going to explain the work that we have been doing, trying to apply explainability methods to molecular data. The motivation of this work is we wanted to apply machine learning models and sprint mobility methods to a mixed data to extract the relevant genes for a medical condition. We had worked with a With images and experienability before in H5, where a set of doctors in Spain gave us some radiographies of gris with fractures, but we didn't have warming boxes, so we used experiencing to detect the fracture. In this case, the evaluation of the experienced is not easy because the fracture usually is not obviously for us, but you can call a radiologist and ask him, Okay, is there a fracture there? and Okay, there are fractures there and they can validate the explainability. But with the mixed data, it's not always as simple because sometimes the conditions are rare or the genes are not explored, there are interactions. So this is the type of problem that we faced. We have around one thousand patients with around fifty thousand genes each patient. Each patient and a target that is a medical condition. And we have worked with breast cancer, TCGA, and PBTA brain tumors that David mentioned the other day. So we have this data and we train a method, a model, in this case it's an XGBoost, and we apply the spring magnetic method that in this case is Trisha. And what we see is the gems that are important for the model. The gems that are important for the model. At the right, for a single instance, the gems that contributed more for a selected target. In this case, I think it's the presence of breast cancer tumor. And if we select the important gems for all the patients, we see the plot of the right source. And we see that there's obviously in red are the In red are the sick patients, and in green, the healthy one. And we see that the distributions are obviously different in the main part of genes. But what we see once we have these when we have this this experiment, we repeat it several times, changing when the model is is good. I talked about the error is very low, we The error is very low. We fine-tune it the models using Peptuna. And what we saw is that if we repeat the experiment only changing the random seeds, so we change the spitting and the random seeds of the model and the explainability method, we achieve good models and sometimes top score models, but the selected gems are different. This makes sense because actually what we are evaluating. What we are evaluating is the model, not the data. So, when the genes are correlated, it's very possible that the model selects one gen, and ignores other one, it's important for the data, but as it's correlated with the selected one, it's not important for the model. And also, there's a big course of dimensionality, but we wanted to give something to decide because, in this case, well, all the models are good, it's not obvious to the. Are good, it's not obvious to decide which model we should take. We have more information. So, well, we tried several things to deal with this before the agent for the course of dimensionality. We selected the top most variable genes, like training a model for each one of the thresholds, and we could reduce from fifty thousand to five thousand without losing accuracy. accuracy and the problem that I mentioned about the high correlation between different genes, we make clusters of correlated genes to well to once we have a gene selected as important, like recover the ones that were correlated with those genes, but not with others. But still we have the same. When we apply this, we have the apply this we have the same variability with high random volumes and so we proposed a way to well we thought that it was it would be nice to have external information to select between these wood models and we made something a pipeline like that where we make well several runs changing the random seeds we Changing the random seats, we in this case it's also changing the train test splitting. And from each experiment, we take a set of gems and we put them in a knowledge graph. We take into account where they fall. And the kind of graphs that we are working with are like this, where the nodes are gems. The edges in this case are pathways. So if the churches pathway so if the if two genes are the genes are in the same pathway they are connected by an it but we are doing a multi we want to do also a multi-level graph where we also include information about diseases and phenotypes and yeah the the graph is very big but I only plotted the first neighbors of each gene and in red are the genes that were selected important as important so you can As important. So you can see that some of them fall very close, like one step ahead, and others not. This is on point. So what we want to do is do the network analysis of the genes, see which of them fall in the same community, and see which kind of information we can give together with the model matrix like performance. Metrics like performance, F-score, and so on that can be useful to select the model. We want to also see if we can find some information regarding the interactions of genes because for the moment we didn't. And also use EVMs that have been mentioned during the workshop. So I want to try to. And yeah, we use the external knowledge to discriminate between performing models. And also, if we have no knowledge, we can also detect if a model is good, but maybe it's relying on some gene that is more related with things that are very different to the current illness, like that, because sometimes we are working with rare diseases where there are not a lot of instances. This is modelist, but it's looking at very different things. And that's it. If you have any questions, I have suggested it's just the beginning when they are the banks of Tennedifais that are not stable at all. Is there a whole Is the whole task? The fifty thousand. Uh yeah, with the fifty thousand happens, but when I when we reduce to five thousand it also happens. These trees happen. Do you use the intergracian interventional way that is more true to the data or the one that is true to the model? Because big t-boost is tube leading, so I tried both and with both. You mean the one that looks at the training cell to see the conditional probabilities, or the one that looks at which items fall in the tree, right, to detect the conditional probabilities? Because one of them is more typical data, so it detects more the Um basically use or if you just so it on the line handle super hand it has the functions. So we are going to use maybe a face more like that, like to compare the randoms and have compared with like the document. That's what you said. That's just a tower that goes to the other top ten, but they are on the top ten. Okay. Questions? Okay. Thank you, Marta. Okay. Thank you, Martha.