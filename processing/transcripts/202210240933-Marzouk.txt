I think you can share your screen. Yeah. Should I connect to the video? Yeah, yeah, yeah. You can unlock this and then close it off scratch. You don't need to connect to the Zoom or not. No, because they're recording somewhere else. Do we try? Probably it's kind of dark. I think you can even see the button value in the future. Okay, we'll just arrange it for the copyright server. Oh, I just want to show you.  All right, so uh our next speaker today is. So our next speaker today is Professor Josef Mozov from MIT. He will talk about ideas to explore low-dimensional structure in non-linear based universe problem-based questions. Thank you, Krieg, for the introduction, and thanks to the organizers. This is my first BERS workshop in many, many years, and one of my first kind of workshop-y-style things in many years, so it's really a pleasure to come. So, I want to talk about inverse problems in the Bayesian setting, particularly. Inverse problems in the Bayesian setting, particularly nonlinear Bayesian inverse problems, and methods that exploit transport in order to solve these problems, in order to characterize the posterior in some way. And the particular thing that I want to highlight in this talk is ways of understanding and exploiting low-dimensional structure that arise both in sort of the posterior in various ways that we're going to describe, and how they're reflected then in the transport maps that you can use to characterize these posteriors as the push forward of some, you know, some. Forward with some, you know, some simple measure. So, did you want it to use this one to change the uh? It doesn't seem to work. Yeah, you need to have this one here. Oh, yeah, that's just a charger. You can go with this one. Let's go with this one here. Technically, it should work. Thank you. Perfect. Okay, great. Thank you. Okay, there we go. So, yeah, so inverse problems in the Bayesian setting. I think many of you are kind of familiar with this. The idea is: I have some parameters x, I have some observations y, and it is kind of simplest. This is a completely non-hierarchical, this is maybe the simplest possible setting. The object of interest is the posterior distribution of the parameters x condition on these observations y. And this is just a product of a likelihood and a prior. And you can think of the likelihood as where your forward model appears if you're a deterministic person. If you're a deterministic person, and prior is essentially how you sort of introduce regularization, although really I think about the idea as much broader, in the probabilistic setting. Essentially, this is prior information about the parameters conditioned on this observation, and then you get this mystery. So, you know, this kind of, you know, the Bayesian setting, I think, is pretty well established for inverse problems, but computationally, it gives rise to a lot of challenges. The parameters of interest x are typically high-dimensional. They may represent the discretization of the field. In principle, you can think Of the field. In principle, you could think about them as just functions, as infinite-dimensional quantities. The posterior is in general non-Gaussian. If it's Gaussian, then simpler methods can be brought to bear. And evaluations of the likelihood function, because they contain the forward model, might be expensive. So this is kind of the jumping off point, the standard starting point for Bayesian computation, where what I assume I can do is evaluate this posterior density on the left up to a normalizing constant. And, okay, oops, another direction. So the main idea of inference through transport is essentially. So the main idea of inference through transport is essentially to characterize this posterior, which I'm just going to write as pi from here on, as the transformation of some simple distribution. So simple distribution meaning something I can sample from. For instance, let it be a standard Gaussian. And if I could find a mapping t such that if x, random variable x, is distributed according to this reference distribution rho, then I would like t of x to be distributed as pi. And why is that nice? Because then I can, for instance, draw IID samples from pi simply by drawing IID samples from rho. And you'll notice I'm not being too specific. And you'll notice, I'm not being too specific yet about what the form of this map is. Is it an optimal transform map? Is it some other kind of map? All I really want to do from this perspective is have a good engine for sampling or otherwise characterizing the probability of any set under pi. I can really just look at its pre-image under t and look at its probability under rail. So I can really think of this as a change of variables for integration, if you want. Now, just a bit of notation: the subscript sharp denotes a push-forward, and the superscript sharp denotes a pullback. So applying T inverse to pi. So applying T inverse to pi in this case. So, you know, many, many choices of transport map. You might under, you know, under what conditions do such maps exist? Are they unique? So on and so forth. Rather than focusing on optimal transport in this talk, as a building block, I'm going to look at triangular maps. And triangular maps, I think, are sort of another canonical choice. They have a lot of interesting and useful properties. Essentially, you know, this given ordering, so they're unique up to ordering, but once you specify an ordering of the variables in your target distribution pipe, then there's a Target distribution pi, then there's a unique triangular and monotone map, which is of this form, for any absolutely continuous distribution. So, as long as there's no atoms in your distributions, you're fine. And computationally, there's nice things that you can do. The Jacobian determinant is easy to evaluate. Parameterization of these maps is somehow more straightforward because monotonicity corresponds to this single derivative being positive for each map component. And the other thing, which I won't talk about in this talk, but I think is really useful from the perspective of things like simulation-based inference or likelihood-free inference, where you just want. Your likelihood for inference, or you just want to learn these maps entirely from data and you don't have a Ford model, is that these maps fundamentally expose conditionals. Essentially, these maps, like lower components of these maps, represent conditionals of lower, of higher variables in my ordering on sort of lower variables that are earlier in the ordering. So they expose conditionals in a fundamental way. The other thing I'll note is that if you're from a machine learning perspective, autoregressive flows and autoregressive normalizing flows are all built on this kind of fundamental building block of triangular. Of fundamental building block of triangular maps. Now, a general thing you can do, so I won't get too much into parameterizations and things like that in this talk, I just want to highlight now using these maps at a high level, a general thing you might want to do is learn these maps variationally. So this is essentially variational Bayesian inference using maps. And you pick some objective, for instance, KL divergence, from the push forward of my reference under a given map to the target. And I can, given that the map is invertible by construction, I can sort of transform this KL divergence just to look like this. Scale divergence just to look like this. And this is a tractable problem as long as I have access to pi and its gradients, or in particular log pi and ideally its gradients. So this is classic variational Bayesian inference, but the approximating class of distributions is distributions that you can get by transforming simple reference distributions via some class of maps. And this expectation and this KL divergence are the spectral reference measure. You can do this deterministically with quadrature, you can do this with Monte Carlo. This avoids other things you might want to do in this. Avoid other things you might want to do in this setting. This is not MCMC, there's no important sampling. In general, this problem is non-convex, unless it so happens that pi is all concave, for the same reason that sometimes that sampling all concave distributions is easier. This is easier in that setting as well. But the key steps here at a high level are parametrized things, so choose some class of maps, and then solve this optimization problem. Now, doing this in high dimensions is really what I want to get to in this talk, because I want to talk about dimension deduction, and you might imagine parametrizing high-dimensional maps. Parametrizing high-dimensional maps. So let our parameters be in Rn. Then these are functions from Rn to Rn. They're triangular functions from Rn to Rn, but still that last component depends on inputs. And so this is sort of generically a difficult thing to do. And then there's trade-offs here between the expressiveness, whoops, and computational effort interactability. So, how rich of a class of maps would I like to use? That'll get me closer to the true posterior, but the computational effort requires. Exterior, but the computational effort required could be considerably larger. So, whoops. Pointer is very natural, so I'm not sure. I'm just going to use my computer. Right, so to make this practical, one ought to use some kind of notion of low-dimensional structure. Various things that we can do, I'll just highlight just dimensions and past work. These maps can be sparse under certain conditions, in particular when there's conditional and dependent. Conditions, in particular when there's conditional independence in the target distribution. They can also be low rank, which is what I'm going to talk about today, and this has to do with some sort of intrinsic low-dimensional structure in certain classes of Bayesian inverse problems. And also, they can be low-rank in sort of more complicated ways. There's a notion of low-rank conditional structure, which I also want to talk about today. But the underlying idea is I want to find and exploit some notion of structure in the posterior or in the relationship of the posterior to the prior and encode that structure in a class of maps. That structure in a class of maps so that I don't have to solve this generically high-dimensional optimization problem over some class of rectangular maps or other kind of functions. So, a particular kind of lowering structure that I want to talk about first is something we kind of colloquially call this lazy structure, but it's the following. So, suppose I have some orthogonal basis for Rn encoded in this matrix U. Let's consider a map that departs from the identity only in an R-dimensional subspace. So, and let tau here be a different. So, and let tau here be a diffeomorphism. This is a function from r little r to r little r, so it's lower dimensional. The map otherwise doesn't do anything, it just sort of is the identity in the complementary directions u perk. And this class of maps, which I'm going to call curly tr, given u, departs from the identity only on the subspace given by the first r columns of this matrix. This map is lazy in some way. It's only going to be doing a little bit of work in this r-dimensional subspace. Now, what classic Now, what class of densities does this kind of map correspond to? What you can show is that basically, maps of this lazy form correspond to densities that depart from the reference row only by essentially by the action or by, they're only modified by this function here that depends only on things in the York direction. So you could think of this here, this left here as a rich function, and there's a one-to-one correspondence between maps of this form and densities of this form. Of this form. So, this kind of end density of this form that departs in the reference only in R-dimensional subspace can be written as a transformation of that reference or in the variable in this form. So, what can we do with that? So this is, in fact, I could find maps of this form. If my maps and this maps were accurate in some way in terms of representing the posterior, then that would be a nice thing to use to represent the target. So, why would that kind of structure appear? So, this is the kind of structure that many people have thought about in the past decade for Bayesian English problems. It's essentially For Bayesian English problems. It's essentially what happens when your problem is quite opposed. If the posterior departs from the prior, only in low-dimensional subspace because of limited informativeness of the data, very smoothing forward model, limited numbers of observations, then you can show that the posterior essentially departs from the prior on this low-dimensional subspace. And this is formalized through a variety of work. I'll mention this paper with Olivia Zahn on certified dimension reduction, which essentially comes up with error bounds on these kinds of approximations, but also has antecedents. But also has antecedents and likelihood-informed subspaces, active subspaces, and many, many things. So, conceptually, if I draw a cartoon of this, here's my prior. And somehow, the Ford model and the data conspire to only change the prior, for instance, along one direction in this particular direction. And as a result, the map should depart from the identity only in that one direction. So, from I mean, most of the practical problem I can think of, you don't really have an explicit form of that most theory. I mean, you have a form. I mean you you have a form there, but it's you can only I mean hope to sample from and get a little sample from it. Well even sampling from the posterior can be hard. Even that is not triple. That's not a question that is with that pair row in the process. Right. So what we assume that we have and I'll get into some of the details later is the ability to evaluate the prior density and to evaluate the likelihood function. And the likelihood function means I need to know something about my noise distribution, I need to be able to vote my forward call. To be able to out and afford all. That's all we assume. I don't necessarily assume, the whole goal of this is in some sense to sample from the posterior. And I don't have access to the normalizing constant of the posterior. That's just as hard as integrating. So I don't assume access to any of those things. So how to find a good subspace? Essentially, it comes down to computing a diagnostic matrix of this form. So this is essentially the log of the ratio of the, you could think of this as a score ratio, it's sort of score of pi minus score of rho. Of pi minus score of rho. And an outer, so this is a vector here. Outer product of these vectors gives me a matrix that's n by n, if x is n by n. Take the expectation, and I have this n by n diagnostic matrix. And what you can show is that if you sort of look at, take an eigen decomposition of this matrix and construct my basis such that u corresponds to these eigenvectors, then there exists a map, this is kind of the best map in this approximation space, such that the KL divergence between the posterior The KL divergence between the posterior and its best approximation in this space of lazy maps is bounded by the leftover eigenvalues of this matrix. Strictly bounded in this one half. There's no extra constants here. And that comes from the fact that rho, I should say clearly, is assumed to be here a standard Gaussian. So what that can result, what can you do? Well, essentially, is that I can come up with a good approximation if the spectrum of this matrix H pi decays quickly. And what is this optimal map here, T? This T star is something you try to get from be taken. This t-star is something you try to get computationally, what essentially corresponds to is the map that corresponds to a ridge approximation of the likelihood where I've integrated out all the inactive directions. So having fixed something in the R-dimensional subspace, I've taken essentially this conditional expectation. That'll be the optimal profile function, and this map T star corresponds to that optimal profile function. So this tells us the best that we can do. This is an upper bound on that forward-tailed divergence. It comes from this diagnostic. Now, so the other thing that Now, so the other thing that this trace diagnostic that we followed is actually kind of useful even more broadly. Suppose I find a map. This is an approximate map in general, because I've approximated a bunch of things numerically. Now let me consider the pullback of my target under this map, so t pullback pi, and look at its departure or how its score differs from rho by computing this diagnostic matrix. If I were done, so if the map were perfect, then t pullback pi would be rho. These things would be equal, this whole diagnostic matrix would be. Things would be equal, this whole diagnostic matrix would be zero, and it has zero KL divergence. Insofar as it's not, it gives me an upper bound on the remaining error in KL divergence simply by computing the trace of this matrix. And the one thing I'll point out, so what's required to compute this matrix, or this matrix on the previous slide, essentially I need access to gradient log of rho, or gradient log of pi. So I do not need the normalizing constant. The normalizing constant disappears. That's one of the beauties of these score gates methods. And that would be a fair question, David. Not be a fair question here, but so this would depend on your parameterization, right? The unknowns, the X that you choose for a particular problem. It it can. I mean, so in some sense, the eigen decomposition can give you information that's kind of it does depend on the parameterization specifically, but in some sense, like under grid refinement and things like that, the eigen decomposition also parameters maybe eigens. Yeah, great. Thank you. Yeah. And okay, so this is kind of a certain construction. Okay, so this is kind of a certain construction. I'm going to show you this construction in action in a little bit. But we might ask: okay, what if these eigenvalues don't decay quickly? What if for budgets I'm limited to maps that only depart from the subspace from identity in a low-dimensional subspace, so small r? So the next idea that I want to put forward is the idea that you can start to compose these maps in a greedy way. So let the map, let this be the first map that I find. It doesn't entirely do the job. It doesn't entirely transform the reference to the posterior. Let me now To the posterior. Let me now clean up with the next map and another map that follows it and start to build this composition in a greedy construction. So let me outline what such an algorithm would look like, and we call this kind of deeply lazy maps, because it's essentially constructing, in some sense, an architecture, a composition of maps that only act on low-dimensional subspaces. So we begin with our target, begin with our reference, and say some budget that gives us the size of the first map, R1. Compute this diagnostic matrix, construct a lazy map in that subspace. In that subspace. Now pull back. That map will not have entirely Gaussianized the target. What's left over is this residual distribution. This is the pullback apply under T1. And let that be your new target. Rho is still the same. The residual distribution is given here. Compute the diagnostic matrix. Now find the most important directions that account for the difference between pi, between rho and that updated target. Construct another lazy map that finds its place in the composition. You're essentially composing to the right. And if you keep on going, so And if you keep on going, so a generic iteration essentially looks like this: you're always building a lazy map to this pullback that comes from the previous L minus one maps applied to the target, and then you can stop when this trace diagnostic, when this precursor is there. So what is this doing kind of conceptually? I realize this algorithm is kind of backwards, but imagine here's the inputs. I have a lazy map that acts in R directions, then there's a rotation, the new change of basis, and then I have a lazy map that acts in a new set of r directions, and r could be different. A new set of R directions, and R could be different at any stage, and you're proceeding this way. And essentially, you should get to the output. So the end. So this is some sense step by step in a greedy fashion. So this is not sort of globally optimal, but in a greedy fashion, I'm constructing subspaces on which I choose to act in order to transform my distribution to get closer and closer to the target. And just to give you a kind of visual example of this in two dimensions, suppose this is my target here. It departs from the standard Gaussian reference in all directions, in both directions. In all directions, in both directions. And suppose I just constrict, sort of constrain myself to use only one-dimensional maps. I could solve this problem fully if I use two-dimensional maps. But one-dimensional maps essentially I'm going to find a single direction in which to act. And this is the sequence of pullbacks, and you can see they get closer and closer to these Gaussians. Now, how can I do this for kind of slightly more bigger problems? So this is kind of the classical spatial statistics problem, the Law-Gaussian-Cox process observed at these points. These are realizations. Observed at these points. These are realizations from the post carrier. And the problem is naively 4096 dimensional, just kind of discretizing pixels. And I'm going to fix ranks. And you can see what happens to this trace diagnostic as I compose more and more maps. And the choice I have is, well, you know, should each map only act in one dimension, or maybe three dimensions, or five dimensions? And you see if each map is itself more expressive, then you can converge more quickly. But nonetheless, they all sort of go on their way. And this is essentially looking at the Way. And this is essentially looking at the eigenvalues of that diagnostic matrix. So this is kind of like the power, the energy left in the spectrum. You see with each iteration, I think this is with, I'll forget what lazy rink this is with, but you sort of start to flatten this matrix and this spectrum, and it also starts to go down. Computed from samples, right? Dias, I mean. Yeah, yeah, exactly. Yeah. We assume we can evaluate the gradient on samples, the gradient of log pi on samples. The gradient of log pi on samples. This is going to be the mentioned before, where you have the upper bound and the. Yeah, this trace agnostic here is precisely the upper bound on the forward KL divergence. And okay, so then we can also do this for everyone's kind of listening to everyone in UQ, their favorite sort of elliptic PD, the Hayesian integrance problem, and similar things. And I think, just in the interest of time, maybe I'll kind of scoop through the details. Scoop through the details. So, you know, a large number of parameters, large number of observations. And here, just for computational constraints, we're letting the max be polynomials of degree up to two, lazy rings four. There are a lot of choices one can make. But you can see again, this kind of similar kind of convergence. And what you can do with the map at the end where you choose to stop is then just draw realizations to the posterior. So this essentially involves evaluating that composition of maps on standard Gaussian samples of dimension 2601. Of dimension 2601. And these are the realizations of the field that you've got. You have the full measurement inside of it. Yeah, we're measuring inside here of sort of 81 unstructured. In some sense, I have to say, the less you measure, the easier this is. Well, because the effective dimension is. Exactly. That's precisely true. So the lower the effective dimension, the quicker this thing converges. The more you observe, the harder. Exactly, yeah. So we just kind of did that in a practice. And then you can also take the same idea. You don't have to apply for patient inverse problems. Same idea, you don't have to apply to patient inverse problems, you can apply this to kind of your favorite machine learning models for which you might otherwise use like a normalizing flow. So, and here we're just comparing this is using inverse auto-aggressive flow. But the point is that you can wrap the same kind of dimension reduction around any other way that you choose to parameterize maps. I tend to like polynomials and these rectified representations. Ricardo has developed a lot of those. But if you instead prefer neural networks, you can still perform that kind of dimension reduction at each stage of the composition and normalizing flow. The composition and normalizing flow. And you can see, in general, for instance, for training a Bayesian neural network, pretty kind of challenging problem. Using the G3 means three lazy iterations of the approach, you can significantly improve actually by orders of magnitude both this trace diagnostic and other things, by all the things like that. Okay, so one other, I'll mention kind of two quick things. You know, in general, there's, I think, there's trade-offs, or the choices that one can make between solar. Create offs are the choices that one can make between solving the problem variationally, like building these maps, and then sampling. And so I don't want to say that, you know, sampling, in some sense, you could forget about sampling altogether, just use these variational Bayesian methods, stop when you reach a error that you're comfortable with, and then stop. And that's something we could do. You can evaluate different diagnostics, like this error bound, so on and so forth. If you don't like the error, you can always add another layer, enrich the map in a given layer. There's many kind of algorithmic torsions here. But one that's kind of distinct and I think different is that you can. That's kind of distinct and I think different is that you can always work on the pullback measure. So, maybe the one take-home idea from this talk should be: if you use transport methods to solve these problems, you always have access to the density of the pullback if you opt to a normalizing constant and gradients of the log density and things like that. So, you can work on this pullback with a different class of scheme if you wish. You could, for instance, say I'm going to take three steps, I'm going to precondition the distribution effectively by transforming it using very simple transformations, and then Simple transformations and then apply some asymptotically exact scheme like MCMC after the fact. So there is kind of trade-offs that one can do. And this goes back to a paper that Matt Karano and I wrote called Transport Map Accelerated MCMC, where essentially the idea, there you can even learn the map on the fly as you do sampling. But just to kind of give you an example for this Bayesian logistic aggression example that I've collapsed on the previous slide, if I sort of just use Hamiltonian Monte Carlo on the target naively, I could get an effective sample size of about 1%. An effective sample size of about 1%, or about much, much less than 1%. Versus, if I work on the pullback and the pullback is approximate, then it can get an effective sample size about 1%, so it's significantly larger. On this elliptic PDE, you might want to use something like PCN, pre-precondition, Craig Nicholson. And if you did it directly on the target for that particular problem, in part because it's well observed, you get a sort of very miserable acceptance rate, about 0.4%, and an ESS that's not computable. Whereas if you do it on the pullback, you can significantly increase. If you do it on the pullback, you can significantly increase your acceptance rate in your ESS. So you could also think about this as a way of preconditioning sampling schemes, and that really comes down to computational trade-offs. Maybe the last thing that I'll mention, should it stop, maybe I have like four minutes or two minutes or two minutes, okay. So we talked about this kind of lowering structure and iteratively exploiting it. Now, another class of, so you know, many problems might not naturally exhibit that structure, at least in one way. For instance, highly informative. In one layer. For instance, highly informative data, full-dimensional updates, inferior posterior. There might be other kinds of structure that's worth exploiting. For instance, like if you have perfectly local data, then the log likelihood action will have some sparsity. Maybe one should sort of think about that. What if the sparsity, however, is in a different basis from the car? So this is one sort of ensatz for structure, but I don't claim that this covers all problems. Now, there has been some recent work on looking at sort of hierarchical low-rank structure in related problems, for instance, like just for large Gaussians and looking for Like just for large Gaussians and looking at hierarchical approximations of these coferencies. And there's also this interesting recent paper looking at hierarchical matrix approximations of the Hessians, Hessians of the log likelihood or Hessians of the log target in Bayesian inverse problems. And so one thing we've been thinking about is, you know, can you, is there an analog of that in a probabilistic sense? Is there sort of an analog of, for instance, like hierarchical, off-diagonal, little rank matrices in a probabilistic sense? And this kind of, you know, I'll just maybe have time to give you the idea. You know, I'll just maybe have time to give you the idea. Here, let's consider sort of an idealized best-case scenario. So, suppose I had a posterior that factored perfectly. So, I have random variables x1 and x2, and generically, the map should look like this in this kind of block triangular form, but if x1 and x2 are actually independent, then that would be great, because instead of learning one, you know, n plus, n1 plus n2 dimensional map, I sort of learn, you know, two maps that are each of the smaller proportions. I fill all these questions. That would be wonderful, but in general, why should you do ever? But in general, why should you ever expect that to be the case? If instead you said, okay, what if x1 and x2 were approximately independent? What does it mean to be approximately independent? Well, the hope is that I could perhaps capture leftover dependence of x2, with some low-dimensional summary of x1. So maybe more formally, what it means is this. I have, in general, like this triangular structure corresponds to the marginal of x1, and then the marginal of x2 condition on x1. That's a generic factorization. That's a generic factorization. But if instead of conditioning on x1, I could condition on some low-dimensional summary function s1 applied to x1, where s1's dimension, output dimension, is much, much smaller than that of x1, then that would be nice because effectively I could build this map t2 in fewer input variables than it would have manually. Because essentially now it only needs to depend on something that's the dimension of x2 and a summary of everything that came before, in a lower dimensional summary. So this kind of motivates the question: well, how could I find? Of motivates the question: well, how could I find such a kind of low-dimensional summary? Well, one thing we think about, because we like to be simple, is just start with linear summaries. This is a good place to start. What essentially corresponds to finding some basis Vs, the transform X1 to give me a lower-dimensional summary that could try to capture leftover dependence between X1 and X2. I mean, worst case, Vs would become the identity, in which case the reverse nothing, but if there is reduction to be had, then perhaps you can find such a Vs. And then essentially, you need to learn a transform map of this form. Transform map of this form. So it's not one that departs from the identity only in the subspace, but it's one for which later variables, their conditioning is somehow simplified. Like the thing that you're conditioning on itself is going to be approximated by, has a low-rank approximation setting. Lower dimensional. So how could you find such a thing? Again, we're going to appeal to a similar kind of bound. So suppose I have this true joint distribution and then some approximation of the joint distribution, chi-tilde, which has... Distribution pi tilde, which has this summary embedded in it. What you can do, now here we require pi essentially to satisfy log Slobo of inequality, so that involves control on the tails, things like that. But essentially, you can upper bound the KL divergence between the true distribution and its approximation given any basis here vs, using this quantity, which again involves, this now involves gradients of log pi, this involves essentially second derivatives of log pi. So this you can think about this as taking the Hessian of log pi. Hessian of log pi, looking at its off-diagonal component, this off-diagonal block, x1, x2, and looking at its expected projection in expectation over pi. Now, this upper bound here is nice because you can actually minimize it. And it actually can minimize the upper bound. So if I take the summary basis to be the r leading eigenvectors of this matrix, that choice minimizes this upper bound, and essentially you have this kind of error control, but now this comes to mechanical control because it depends on the log cell that comes in the pipe. Because it depends on the log cell that comes in the pi. But essentially, if these eigenvalues decay quickly, then I may have something to say about. That should be the case when you have all of those independent eigenvalues, these things will be zero. But insofar as they're not independent, then you can sort of capture the essential aspects of structure. And I'm going to maybe kind of give you a picture. So here, let me skip a bunch of details. Let me okay. Let me skip a bunch of details. Let's just consider this is an elliptic PDF problem, and we're the pixel basis. So basically, each x is just a sort of ordering here, and the ordering is essentially like this. And let's consider a pixel right in the middle of the domain and say, okay, if I had to build a triangular map, generically, kind of naively, I'd have to build a map that depends on all the inputs above it, sort of on this upper left eye. But what is actually a summary of its dependence? So let's take a picture in the center of the domain. You can actually look at Center of the domain. You can actually look at the summary ranks. So this is sort of eigenvalues of that diagnostic matrix, and you can see they decay by a couple of order of magnitude very quickly. But the mode shapes are themselves pretty interesting. So if the pixel member is in the center, and I say, okay, in the posterior, how should the value of my field, my permeability field, or Lark permeability at that pixel, depend on neighbors that are behind me in the ordering? And can I summarize that? Well, it turns out that these first three summaries have a lot. These first three summaries have a lot of local structure, that they kind of sort of extend over a little bit, like this. And these are the kind of summary bases you can get just by solving the eigenproblem, and then you can encode that into the map. And so localization and sparsity. For a different kind of summary, you essentially highlight the interface, but that's kind of time for that. So yeah, so essentially, you know, this is a new onslots for maps. This is work in progress. We now need to actually compute maps in this onslaughts. But this is kind of, I think, one. This is kind of, I think, you know, one individual. So, this is one particular algorithmic choice. We picked a pixel and tried to summarize everything behind it. You could immediately start imagining applying these things hierarchically, blocking up the domain and summarizing blocks and then refining and then sort of doing things like that. And that really then is kind of the link, sort of the least inspiration to things like H matrices and so on and so forth. A little bit of code, Finnish mentioned code. We can talk about that flying. So let me just conclude. The idea I want to get across today is just The idea I want to get across today is just to think about how to exploit low-dimensional structure in variational inference, Braysian inverse problems, and Bayesian inference more broadly, where the engine of inference is transport, is the construction of these concourt maps. And there's many, many varieties of a low-dimensional structure. Here we just focused on two, low-rank updates, and then this low-rank conditional structure. And maybe the other key take-home message is that one can try to seek this low-dimensional structure iteratively, because this is enabled by access to a pullback density, a residual distribution, which is a really natural. A residual distribution, which is a really natural thing. And you have transport, and then you can start doing compositions and things like that. Or you can always think about these methods as preconditioners for asymptotical exactness. So that's it. Thank you for your attention. Thank you for much more time to talk. So I'm just wondering, so does the order of the hierarchical construction matter? Hierarchical case structure matter? Very much. Yeah. Yeah. In the hierarchical case, ordering absolutely matters. And how to pick a good ordering is, in general, a good question. So right, the ordering here, let me kind of point to this guy. The ordering essentially says that everything ahead of me in the ordering, I don't even need to worry about summarizing because I've decided to construct this triangular map of this particular ordering, which already says I only really need to care about certain conditionals. Again, it's telescoping factorization. And it's telescoping factorization of the joint distribution over all these pixels. I've said, okay, there's only a certain order. Now, how to think of good ordering is in general a little bit challenging. You can think about this as like, but actually, let me say the following. Where ordering shows up in this scheme is essentially in step one. Here. So what you would like to do to make this scheme kind of most efficient, you'd like to begin with a basis where things are kind of close. The basis where things are kind of close as close to independent as you could muster. And then, because the conditioning essentially cleans up what's left over. So, if by some magic I had a basis in which things were actually independent, then there'd be no work to do in the remaining steps. But that gives at least kind of a heuristic for how should I pick an ordering to make this scheme efficient. Well, let me pick something that kind of promotes more independence. And it's not just even a question of ordering. It's really a question of rotation. It's really a question of basis and then ordering within that basis. As far as how to do that. Basically. As far as how to do that, all I had is kind of, you know, in some sense, you pick the things that you can. So different spatial orderings, or maybe I take a Laplace approximation and look at eigenvectors of that. Maybe I look at the low-ranked diagnostic matrix in the first part of the talk and use that to construct these bases. I don't have a really good choice because, in general, that's that's not a good idea. same idea would I mean the general philosophy is that you search low-lack structure not on the phase space like the density by field we search low-lack structure in the push-forward maps yeah so will that I think if you apply this technique into this parallel parallel triangle map like if there's somebody define or restrict yourself in another type of push for remarks for example in machine learning the you know the after encoding they may have like very new structure yeah well the same thing Yeah. Well, the same thing happened. It absolutely applies. I mean, actually, the only so the triangularity is actually not essential to anything in this kind of iterative lowering thing. In the lowering conditioning, we do use triangularity, but in this iterative part here, essentially, this map tau can be anything. Need not to triangular. And so we've applied this to things like normalizing flows that include permutations, things like that, within tau. So they're totally not triangular within tau. So that is generic. So that that is generic. Any more questions? Let's thank the speaker again. I'm back at 10:30. Very nice, very nice. Very nice, very nice. Very soon. I forgot one. December. December. Yeah, that's right. That's right. I'll talk about something different for Harry. So you can talk about my next check. That's right. Right now, right now. I've been following your work for a long time. Never fail to get into any of this. It's very interesting. They are a lot of applications, they build this. I mean, really there's a lot of actual stuff. Great, great. Thank you so much for having me. That was great. Again, I'll see you in the next coming. And you can only stay for one day. I know it, yeah. So I talked to them here. I was seeing whether it was. Okay, great. Yeah. She knows you have a Facebook guest. Yeah. I think she's there. Super slow. I did something else in our case. I don't think I had another mindset. Oh, yes. I was like, oh yeah, it's not going to be a good idea. I don't know. You can ask me that question. I've never had a minute to talk about it. Yeah, probably not a good brain. Yeah, it's where we want the bottom of the bottom. But also, yeah, about the HPR, it's like. Yeah, when I was here last time, we were changing like the audience. So only the word that's really fun is all those really like. Oh yeah, so I think I'll do it on it. That's a very good idea.