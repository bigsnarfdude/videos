So, yeah, as you know, introduced, I will talk about hypergraphs today. And in particular, we proposed a latent space model for modeling hypergraphs. And this is a joint work with Xiang Shi Yu Yi. She was a former postdoc. Now she's at the University of Wisconsin in computer science. So this So, this slide is about network data. And here, I just want to emphasize that network data is a little different from the usual multivariate data we typically deal with. Multivariate data, what we have is usually like the n by p data matrix. N is the number of the observations, p is the number of variables. We use these variables to describe each observation. But network data is about relationships, and so it's different from unbyte. So it's different from an N by P data matrix. There are usually two very basic elements with network data. One is nodes, the other is links among these nodes. So these nodes usually represent individuals, and the link usually represents some interaction or relationship between two nodes. This slide is just to show that network data are ubiquitous. Are UV critters. So it's very common everywhere in many fields, many domains. And the nodes and the links, they just represent different things in different domains or different fields. So for example, in social network, nodes might be individuals and the link might be friendship among individuals. And in biological networks, nodes might be neurons and links might be interaction or communication between neurons. In information network, In information network, those might be web pages and links might be hyperlinks connecting different web pages, etc. But here, what I want to say is that for most of the work on network analysis, they have been mainly focusing on so-called dyadic relationships. So, these are the relationships that only involve two nodes. For example, like a friendship network, and two people, whether they are friends or not. Whether they are friends or not. That's a dynamic relationship. But in practice, in many of the real world scenarios, what's more common might be the so-called polyethylene relationship. This is the kind of relationship that involves more than two nodes. So for example, if we think about, say, cluster relationships, we often think when we think of, when we talk about the cluster network or cluster relationship, we tend to think about two people, whether they have co-written a paper or not. Whether they have a career on a paper or not, that's a dyadic relationship. But if you really think about the class relationship, it's actually like a polyadic, because what makes people to be collaess with each other? That's through a paper. When you have a paper, you have usually maybe two co-authors or maybe more than two co-authors. So all these co-authors for that paper, they are all collaxes with each other. So that is a relationship that involves more than two, it could be involved more than two classes. That's a polyadic. classes that's a polyadic relationship that's not really like a classroom network so that's not really not like a dyadic relationship and what people usually do like you know in dealing with this polyadic relationship is that they project it into a dyadic relationship so when we think about the co-author network when we analyze that or work with that we usually just concei con create a binary or dyadic network in which each node is uh is a author and if two people have a co-authored paper. And if two people have co-hard a paper, then they use a link to connect them. So that's the projection of this polyatic relationship into a dynamic network. But when we do that, we actually lose information, right? Because if you work with that dyadic network, you actually don't know, although you know which pair of people have costed the papers or not, but you don't actually know which maybe three or four people whether they have cholesterol paper or not. So we wish to work with this polyaddack you know data. This polyadic data indirectly rather than work with the projected polyadic relationship into this polystyrene network. So that's the idea here. So in terms of representing this polyadic relationship, a natural tool is a so-called hypergraph. So in a hypergraph, it also consists of two components. One is the set of nodes. So in a classroom relationships, you can think about Relationships: you can think about this is a set of all the classes. So, each cluster, each author is a node. And another component is this hyper-edge. So, in the classroom relationship, you can think of each paper as a hyper-edge. And in the hypergraph, each hyper-edge is usually just a subset of these nodes. So, in our co-author relationship, you know, example, each paper is a hyper-edge, and each paper will consist of several authors. Consists of several authors. And so that with each hyper ed or each paper consists of basically a subset of the nodes or a subset of the authors. So that's the hyper edge and nodes and hypergraph. That's the kind of relationship, that kind of data we work with. And also here, I want to point out that, so now in terms of the size of hyper x, which will be the number of That's each paper, and the size of the paper will be the number of authors in that paper. And that size, the size of the hypertext can be basically anything from zero to the total number of answers. And it's not restricted to two, which is the case for dynamic network. Here's an example, just illustrate an example to show the comparison between The comparison between dyadic network and the hypergraph. On the left, this is an example of a dyadic network where each node is, you can think of each colored circle is the node, and all these relationships are dyadic. So basically, each link or each edge only involves two nodes. And on the right, this is an example of the hypergraph. So here again, you know, each circle is a node, and each sort of parenthesis is a hyper hyperlink. Hyper add or hyperlink, and you can see now if you think about this is a co-author relationship, then each node is the author, and each parenthesis is a paper. These are the three authors involved in this paper, these are the two authors involved in this paper, etc. And then each paper or each hyper edge, or each paper is like a hyper edge involving potentially more than two authors. And this is the kind of data, or this is the kind of problem we will deal with. In terms of analyzing or dealing with hypergraphs, there are a lot of actual work in the literature. One type of work, these are the work mainly done by computer scientists or physicists or people outside of statistics. They have worked on node clustering, or they have worked on hypergraphic embedding. But most of these algorithms, they are heuristic and they lack statistical models or principles. And in terms of like you know, modeling. In terms of like you know modeling hypergraphs, much of the effort has been focusing on the so-called K-uniform hypergraph. This is the kind of hypergraph which requires that the size of all the hyper-edge to be the same. They are equal to k. So this is sort of like requiring when we look at these different papers, this requires that all the papers they will have the same number of answers. All the papers, they all have exactly k answers. So this is obviously very restrictive. And for work that's not restrictive to say the K-uniform hypergraph, again, there are a lot of work focusing on just node clustering, including Tracy has done some work in this area focusing on node clustering. And there's another type of work which is not restricted to non-uniform hypergraph, but they require the hyper edge to be hereditary. This is sort of in the sense that if a hyper edge has appeared, then this requires that. Has appeared, then this requires that all subsets of that hyperad have to also appear. So, for example, if we have a paper which has three authors, then this requires that all pairs of those three, all you know, subset of two authors of that, of those three authors, they will have co-written papers. This is also obviously a very strong restriction. Another type of work, sort of not limited, okay, uniform hyper. Okay, uniform hypergraphs, not limited to hereditary, you know, hypergraphs. But they model like sort of they don't allow for multiplicity of hyper edge. So they only require that each hyper edge can only appear once. So if you have three authors who have written a paper, they can only write one paper. They cannot write multiple papers. This is again like a very restrictive assumption or very restrictive condition. So our goal is that we wish to hear to That we wish to, you know, here to build a model for hypergraphs, and this is a latent space model. And we don't want our model to be restricted to k-uniform hypergraph. And also, we don't want to be restricted by this hereditary constraint. And also, we want to allow for multiple hyper-edge in the sense that the same set of people can write multiple paper rather than they can only write one paper. Another little feature about Another feature about our model is that this is motivated by observations for many of the real world hypergraphs, is that this is driven by sort of what we call like a diversity rather than similarity for these different nodes. So for example, when we think about the, say, we have an online post and we assign some tag for this online post, so this text will consist of hyper edge, right? So each online post. Hyper edge, right? So each online post is a hyper edge. Each tag is a node. Then these texts assigned to this post will be the hyper edge. And when we assign these texts to a post, usually we will assign text with different meanings to this online post. So these texts, these texts, they are very different. So that's diversity rather than similarity. Another consideration is that, say, when we think about an engineering project, we try to build a house, then we will have Build a house, then we will have people with different expertise to build a house. We will be people who are expert at designing, we would either people who are experts with electricity or people expert in plumbing, et cetera. So we need people with different expertise to form a team to build a house. And that's also diversity rather than similarity. So there are basically a lot of real-world problems in which these hyper-edge, they are driven by diversity among these nodes. Among these nodes rather than semi-particular among these nodes. So, this is another motivation for our model. And also, you know, in real world hypergraphs is that we'll see like a different node, they may have a different popularity in the sense that some notes appear in hyper X more frequently than others. So, we want to accommodate that kind of variation in our model. Okay, so to fix the notation. You know, fix the notation. Here we use this V to denote the set of all the nodes, that's the set of all the authors, and NV will be the number of nodes, so that'll be the total number of authors. And we use this E to denote the collection of hyper edge, and each little E is a hyper edge. So each hyper edge is a subset of the set of nodes. So each little E will be a paper, and this little E will consist of several others, which is a subset of the total. Is a subset of the total set of others. And here also, so NE denotes the total number of hyper-handed emphasize that in this set of hyper-edged, we could have duplicates. So we will allow the same set of authors to write more than one paper, right? Each paper will be a hyper-edge. So we could have duplicated the hyper edge in this set of and also, as I mentioned, And also, as I mentioned, we will use a latent space model to model hypergraphs. And so here, I is the index for the node. So each node will be associated with two quantities. One is the latency position, Vi, that's a D-dimensional vector. The other is this positive real number, alpha I. So that will model the popularity of this node I in terms of how often this node appears in hyper X. Node appears in hyper edge, or how you know, likely a node appears in a hyper edge. And then here we'll define another vector. This is a Vi delta. That's a concatenation of this Vi and alpha i. So this Wi is the standard basis. So that is an undimensional vector. And it's the nvidimensional vector. That's only the total number of nodes. So i's element is equal to one, all other elements are equal to zero. So this is Elements are equal to zero. So, this is an Nv by one dimensional vector. Vi is a D-dimensional vector. So, after I concatenate these two, I get a D plus NV dimensional vector. And here's an illustration. So, here, suppose we have three nodes, and each node has an associated latent position, V1, V2, V3. And also, each node has an associated WI, which is the standard basis. And then, you know, we multiply the corresponding standard basis with the square root of alpha i, then we have these three sort of. I then we have these three sort of you know vectors and then we can calculate V with the corresponding Wi. So each row is the V tilde. So we have three V tilde, one for each of the nodes. And in terms of the model, here's the key part. So here's, we use this big E to denote just a generic random hyper edge. Little E is a real particular realization for a hyper edge. So we model So we model like the we define this is we try to define a distribution on the on the on this big E. So here we define that make the probability for this you know a random hyper edge to be equal to a particular hyper edge to be proportional to this this volume, this square of the volume of a parallelote. So what is this parallel to? Because given this E this E consists of several nodes, right? And each node has a corresponding V i tilde. node has a corresponding Vi tilde. So if we look at all the Vi tilde from this little E, they form a parallel to model this probability to be proportional to the square root to the square of the volume of all the vectors sort of that consist of this hyper edge. That's the key component. Here's like an illustrative example. Suppose we have a hyper edge which has three nodes, so ijk. nodes, so ijk, and these three nodes they have their corresponding v tilde, and these three v tilde they form a, you know, it's not very clear, but this there's this you know parallel tope here. So this probability for this for this particular hyper edge will be the proportional to the square of the volume of this parallel tope. So that's the key sort of assumption for this model. And here, the nice thing about you know having this particular form is that many of the things will have closed form formula. So everything Have closed form formula. So, everything, many of the computation can be simplified. So, if we also create this nd by nd matrix, each element is the inner product of the corresponding vi tilde and v j tilde. So we have this nd by nd matrix. Then you can show that the square of the volume of that parallel to formed by the nodes from a particular hyper edge E, and that should equal to the determinant of a submit. The determinant of a submatrix of this L matrix. So the submatrix is indexed by the nodes in this hyper edge. So this L E, that's a, so for example, if we have a hyper edge with three nodes, then this L E will be a three by three matrix, that's a sub matrix of this L, and the square of the volume of the parallelotope will be equal to the determinant of that start matrix. So we can compute the square of the volume it goes from, and more than that. goes from. And more than that, if you look at the summation of this over all possible subsets of V, right? So this is the, if we have N V, no, this is a summation over two to the power of N V possible subsets. And this summation also has a closed formula. So that's very important. Because this is a discrete distribution. In general, there are many two to the power of n v possibilities. In general, we won't be able to really compute this. But in this case, under this particular model, This particular model, this actually has a closed formula, which is the determinant of L plus L is this matrix plus the identity matrix. And then so these two combined implies that the probability for a particular hyper edge will be just equal to the ratio of this determinant and this sort of summation of overall possibilities. And also based on our definition of V tilde, it's not difficult to show. It's not decoded to show that this L matrix can be written just in terms of the V and also the L. So that normal actually is actually not identifiable, but if we require, if we constrain that all the V i's, they have equal lines. So all the V i's, they all have the equal lines, and then we can show that under this constraint, the model, the V and the alpha, they are identifiable up to some, you know, transform. To some, you know, transform sort of like you know, also a normal transformation, and also some flips. So, this is the model identical variability conditions. And some remarks. One is that, so we know like now this the probability for a particular hyper edge is related proportional to the square of the volume of the parallel to and also if we recall our definition in terms of v tilde, then we can see that. We can see that the square of the volume of that parallel to two things. One is determined by the relative separation or angles among these V tilde. The other is determined by the length of this V tilde. But because of this definition of V tilde, and these V's, they will have the same lens. So the length of the V tilde is pretty much determined by alpha. By alpha, and the separate, and because these w they are all orthogonal to each other, because these w's, they are standard basis. So, these different wi's, they are orthogonal to each other. So, the separation or the relative angle among these different V tilde, they are pretty much determined by the Vi's. So, sort of like this V and this alpha, they capture two different things about this latent representation of different nodes. So, the V's will capture the orientation or the direction of the node, and the alpha will capture. Node and the alpha will capture sort of the length or the popularity of these different nodes. And here's again, like just a simple example to illustrate the point. But suppose we use beta to denote this common length from these different VI's. And if we have a hyper edge, which just consists of two nodes, I and I prime, then you can show this is equal to this quantity. So you can see that, so when these alphas, they are large, then this probability is. Then this probability is large. Or, like when this V and the V, this V and the V i Tilda, when they are all orthogonal to each other, so the cosine will be small, then this probability is going to be large. So, basically, when these v's, they are very separated from each other, then we are more likely to have a larger probability. And when these alphas are large, we are also having a large probability. So, just to illustrate that these V and alpha, they captured different properties about the Properties about the of these nodes. Some further remark. One is that, as I mentioned, much of the existing work on hypergraph, they require, so this is the difference between our model and many of the existing models for hypergraph, is that they model each hyper edge, whether each hyper edge appears or not, as a Bernoulli random variable, right? And they are independent. And they are independent of different types of edge. But here, our model is very different from these models in the sense that we only have one distribution. It's not like we have one distribution for each hyper edge. Here we only have like one distribution, and this distribution is on all possible hyper edge. So we can consider that as a, so this is a discrete distribution. We have a, you know, a finite number of possible hyper edges. And we have just one distribution on all these possible hyper edges. That's different. That's different. And we model that these hyper edges we observe are IID realizations from this single distribution of all possible hyper edges. So that's a very big difference from existing models for hyper edge. Another thing I want to mention is that our model actually can be considered as a specially structured example of a more general model, which is referred to as a determinantal point process. And this process is a distribution over, so given a set of points, and this distribution is used to model the distribution over the power set of a point set. The power set of a point set is all possible subsets of this point set. And in general, as long as this L, you know, this is for a generic determinant of point process, as long as this L is a positive semi-definite matrix, then it's a valid, you know. Matrix, then it's a validated distribution for the power set of this point set. And here we sort of put a special unique structure to this L, where we require this L to be equal to this thing, which we have, you know, a component which models the direction of these nodes, another component to model the popularity of these nodes. And this reduces the number of parameters from a generic to DPP. In terms of like you know parameter estimation, we'll use maximum likelihood estimation. And here also we try to separate the direction of the nodes and also the length of V. So we use the beta to denote this common length of these different nodes V and we use this big V to denote just the direction. So this the rows of the big V are normalized so they are all equal to one. So this big V captures the direction of different nodes and Of different nodes. And also, you know, assuming these different hyper-edged, they are ideal derivations from this distribution we just defined. Then we can write out the log likelihood. Then we can use microlock likelihood to estimate these parameters, which includes big V, beta, and also this alpha. And also, you know, we can show under certain mod under certain regularity conditions, our estimates are consistent. Estimates are consistent up to rotation and the sun flip. And also, we can show a synchronic normality for our estimated L hat, our kilometer after rotation and the sun space. And in terms of numerical studies, here we again, when we do the evaluation, we try to separate the evaluation for the direction and the lens. So we will separate the V hat and the beta hat and separately, and we will also separate. And we also separate, we also evaluate the alpha hat, which tries to model the popularity of different nodes, and also the altitude. We have tried many different settings here, just quickly talk about the two-step settings. One is this is a setting where the true Vs they are uniformly distributed on this sort of lower dimensional sphere. So if D is equal to 2, this is just a circle, and if D is a Just a circle, and if d is equal to three, this is the surface of the sphere. So, one example where these v they are uniformly distributed on the d minus one-dimensional sphere. And the alphas, so most of the alphas, they are small, but there are also a few alphas, they are very large. So, there's a variation in terms of the popularity of different nodes. And here's, for example, like a particular example of generated V's uniformly distributed on the Distributed on the on the on the on the circle, and this is a you know the distribution of the generated alphas. So, most of the alphas are small, but there are a few alphas large. There's some variation among the values of the alpha. And these are just the distribution of the cardinality of different hyper edges for particular realization. And these are the alphas values again for a particular realization. And then, here, what we see is that when we increase. That then, like, when we increase the number of hyper edges, we do observe that the errors for this V hat and beta hat, alpha hat, and L hat, they decrease so that we speak our consistency result. And here's a check of the authentic normality result that basically agrees with our athematic normality out. And here's another setting where these V stars, the true V, they are no longer random, uniformly distributed. Random uniformly distributed on the lower dimension, the sphere, but now they have a cluster structure. So they are generated from a mixture of the one mice fisher distribution. So this one mice fisher distribution, you can consider that as the analog of the Gaussian distribution, but this is defined on the sphere. So these are the three centers of the mixture components, and this is sort of like the scale parameter. This is sort of like the scale parameter, analog of the scale parameter distribution. And so, here is a particular real life set of these, you know, photo, a mixture of the feature distribution on the sphere when these equals three. Here are the estimated like a V hat. So, what we did here is that, so we, you know, we took the estimated V hat and we run like the Arch-Means algorithm for the A k-means algorithm for the spherical data to see whether we can cluster them correctly. And here, this is in terms of accuracy and the IMI index. So, but when he increases, we see that our accuracy in terms of clustering decreases. So, in the end, just going to quickly talk about a particular real data example. So, here we looked at this what's cooking data. So, here, What's cooking data? So here, the nodes are the ingredients in the recipe, and each recipe will be considered as a hyper-edge. Each recipe consists of several ingredients. And so that's hyper-edge. And also, in particular, here we focused on the Chinese cuisines. The reason we focused on the Chinese cuisines is that in Chinese recipes, usually you don't put the sort of similar ingredients. Very rarely you will see like, you know, you have similar ingredients in the know you have similar ingredients in the particular in one chinese recipe for example you don't put two different kinds of meat in one recipe right usually your your your your recipe either maybe they have chicken or they have beef or they have pork but you very rarely see people put the most chicken and the beef in one you know chinese recipe so there's this diversity so that's why we know we focus on this chinese cuisine to figure that fits with our model assumption better and we did some you know pre-processing of these data Some you know, pre-processing of this data, etc. So, in the end, we have about 2,600 recipes, about 900 ingredients. And then we sort of fit our model to this data. And so we look at it, then we use the D equal to 3. So these are the embedded sort of V's for each ingredient. And this is sort of like a look for this project. Look from for this projection from the North Pole, and all these ingredients are with sort of a latitude higher than 85 degrees. And this is like sort of like a zoomed out, a zoomed out view of this, where like we only plotted the ingredients embedded at the latitude lower than 85 degrees. So here we sort of like see some clusters. This is a cluster corresponding to the hybrid. The carbohydrates, like ingredients, such as rice, quantum wrappers, noodles, etc. And this is the cluster corresponding to the protein staff, like different pork, chicken, etc. And we also sort of focused on ingredient, about the 300 ingredients that have appeared a 10 or more times. And we get this embedded ingredient, then we apply a k-means algorithm for the spherical data. Algorithm for the spherical data, and we identify the sort of eight clusters. And again, you know, some of these clusters they correspond to, for example, we have two clusters corresponding to proteins, again, different beef, pork, chicken, also again, different beef, pork, chicken. And there's another other cluster corresponding to the carbohydrates, you know, rice, the noodles, you know, dumpling wrappers, etc. And there are also like other clusters corresponding to different uh sort of seedings, etcetera. Sort of see the essential. So, just to conclude, what we did, we have proposed a latent space model for hypergraphs. And some important features about our model is that it doesn't require a uniform assumption. So, it will accommodate hyper-edged with different cardinality. And also, it doesn't require this hereditary assumption. And it also allows for multiple. And it also allows for multiple hyper edge, it allows you to duplicate in our observed hyper edge. So, these are important features about our model. And also, this model is driven by diversity rather than similarity. That's different from many network models. And we try to argue that there are certain types of hypergraphs which do have diversity feature or property. And also, we have a model parameter to accommodate the variation in terms of. Accommodate the variation in terms of the popularity among different nodes. And we have established consistency and asymptotic optimality for our MLE estimate of the model. Yeah, so that's all I want to say, and thank you.