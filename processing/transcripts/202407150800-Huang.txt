First of all, I want to thank Denise as a wonderful leader for this whole thing. And I'm very excited to learn because, except the thing I know a little bit, other things I know very little. So I'm very excited to learn with you guys. My name is Whitney Huan and I'm from Clinton University. And down below, you can see a lot of logo. And also, you can see the pins right here, which is helping us make this happen. And there's quite a few logo here. And I'm very happy. And I'm very happy to say three out of five I actually associate with Canada, which I like Canada. So I spent one year as a postdoc. Later, I will talk about with both Dr. Adam Monaghan and also Francis Reyer at Uvick. And also at that time, I was a postdoc with SAMCI, which was NSF Institute in US, but also CAMC, in which John was the duplicate director. Deputy Director. And also, this is the this was another NSF big research network called StADMO that bring me to this area in which I have the opportunity to work on the statistical problem associated with climate. All right, so I'm still pretty early in my career and the fifth year assistant professor of applies. Professor of Applied Statistics and Data Science at Clunson University. So, Clunson, most of you probably don't know where that is, so that's in the southeast of the United States, pretty close to Atlanta. I was actually born in Laramie, Wyoming. It's actually closer to here than South Carolina, but I grew up in Taiwan. I actually did my undergrad in mechanical engineering. So I'm sort of engineer back then, but I realized I'm very bad at engineering, so I cannot finish any kind of experiments. Any kind of experiments almost I fail all the time. But what I know is, I know if I know what is supposed to be for the result, I can actually add some random noise to make it look like experimental results. So I guess that's why I decided to move to statistics and I did my PhD at Purdue University. And right after that, I did, as I just mentioned, the Sensi and Kency postdoc before I moved to Clans. All right. All right. And very quickly, so today's shortcut, today and tomorrow, my short talk will be more focused on the equine value analysis, in which you will see pretty quickly. In general, I also work on spatial and spatial temporal statistics. And also the third bullet is also related to extreme, but also has to do with computer simulation. So, in a sense, that at least from the staff community, people also code it as uncertainty quantification. So, they also So, they also made me qualify to be the first shortcore series speaker because I saw that's one thing called antification. Although I will mainly talk about a trend value analysis. All right, so let me check this one. All right. So here's the agenda for today. So you can see a lot of things. Today we have been, we probably will stop by here. So I'm trying to be a little bit slow to finish the basic idea about the univary train value fallacies. And for tomorrow, I will get into slightly more complicated issue about when you deal with multiple When you deal with multiple variables and also a little bit about spatial extreme. But let me start with the, because I saw only two, if I remember correctly, two stasotations. So let me try my best to give you some idea about the tiptoe statistical modeling, the contact, the workflow, and then we are going to move to the trend value analysis problem. And I have to acknowledge that this part, actually, I still have a lot of material. Actually, I steal a lot of material from Julie Bissak, who did the show course last year. So I forgot to ask. How many of you attended the show course last year? A few of them. So you can, you will see sounds like quite familiar because actually Julie is very nice to share with me her latest code so I can incorporate over there because I find that's very, very useful. So I just incorporate that here. So the first of all, I guess the first question is. I guess the first question is: What is the goal of this statistical modeling? So, really, the big picture is a lot of time we have some kind of process in mind, but we don't have the complete information about that process. But what we have is the data. And a lot of times, the data give you some incomplete information about the process of interest. And the idea is how to use that incomplete information to learn about the To learn about the process of interest. So, typically in the STED 101, the term you will learn is like the data and the population. Here, for the purpose of this, like summer school, I like to use the process. The process could be the spatial, spatial, temporal process. All right. And we want to do a few things. So, first of all, we have the data. We want to use the data to give us some idea about the main characteristic of the process. Of the process, and then because the data is incomplete, so how to make it a bit more complete? So, maybe we want to make some prediction, uh, we also want to do some kind of simulation maybe for the downstream analysis. But I guess always keep in mind because we most likely have incomplete information and we want to use the incomplete information to fill the gap, so there will be some uncertainty associated with it. So, a big part of the statistical modeling is we also want to communicate. To communicate the associated uncertainty when we try to do this process to move from the data to the processing. All right. And as you can see, there are several purposes. So first of all, we want to come up with a comprehensive description about the data in which to say something about the process of interest. We want to provide uncertainty quantification. We want to, so some situation we have very little data, so we want to So, we want to make use of the data most efficiently to say about the process, but also we want to acknowledge if we have very limited data, it will associate with quite big uncertainty. And a few things you want to do. So, prediction is definitely one thing, but the other thing is so-called the data fusion. So, the idea is that you might have several different sorts of data for the same process, and then you want to combine them in a way such that you can learn. Then, in a way, such that you can learn the process better. And the last part a little bit has to do with the so-called uncertain quantification, which means you will have a different mechanism to generate the data. And that mechanism is allotted due to the computer simulation, which encodes the physics, the first principle of physiology, and to simulate the data. But then the idea is that that source of data might have different kinds. And that source of data might have different kinds of bias and also different issues. So, for example, you might want to so-called build an emulator, or you want to do again the forward propagation to do the downstream analysis. All right. So, again, so the statistical modeling, the main points, we want to build a model just like every, most of the scientific data points, but we are going to build a statistical model, which the mathematical representation to describe how the data would generate. Describe how the data would generate from the other line process. All right. So, for the statistician, the data is very important because a lot of time we build the data to describe, build a model to describe the data. And as I mentioned, there are several different source of data. So those are just two common ones, but actually there are some others. For example, the data could come from numerical simulation based on typically very sophisticated partial differential systems. Sophisticated partial differential, the system of partial differential equation. For example, like a global circulation model, regional climate model, or other model to describe the geophysical process. So the physics is sort of building based on our current understanding, but it's not perfect in the sense that, okay, so here are actually two examples. Again, I steal from Julie, but I assume this geographical region is close to the Region is close to the Great Lake. And so I think those are the windfields, but those are two examples. So they are computer simulation output. So if you look at this spatial region, they're sort of complete in a spatial domain, but subject to the spatial resolution, which has to do with the approximation and discretization here. And you can see this one encodes much detailed information because this one has a higher spatial resolution and supposedly. And supposedly, this can capture the underlying process a little bit better. And this is an example of observation, although this one, so there are different types of observation. One could be station observation, but this place, I believe, I think Adam might know this a little bit better. Yeah, so this is kind of NS. So that what is faster for you? So, that one's faster for it. Yeah, it's a scatterometer observation of surface winds. I'll talk about that on Wednesday. Yeah, very good. That's well covered. So, one thing about the observation is, first of all, the sort of the observation could be sparse, both in space and time, and it could be sort of irregularly sampled, unlike this one, because this sort of complete in a sense, because you basically do the simulation. Do the simulation, you cover all the space and subject to the high resolution. All right, so we could have, even for the same process, you can have several different sorts of data. For example, there's some difference between the spatial resolution or even the spatial temporal resolution for the simulation. For the observation, it will also depend on the sampling density in space and time. And again, so the data fusion I just mentioned is that how to combine those. I just mentioned how to combine those together to make a better inference about online processes. All right. So a couple of challenges here. I would just quickly mention some of those. So in terms of data, we need to be a little bit careful about the data because there are some properties we want to preserve. We want to keep in mind when we do the statistical modeling. For example, a lot of quantities there, like fossil. So, a lot of quantity is there, like fasting quantity. So, when you try to build a statistical model, you want to keep that in mind. So, you don't want to estimate the precipitation is like a negative 200 or something like that. So, that's one thing. And some type could be categorical. So, the weather regime. Sometimes it could be integer. So, for example, if you look at like extreme events like a hurricane. So, for example, this year you might have like 10 hurricanes, next year, 15 hurricane, and so on. And this is for wind. And this is for wind, wind is actually an interesting species because you have a better field. So that means you are going to describe wind in terms of the wind speed and also the direction. So you could have a directional data. Let's go a little bit crazy. You could have direction data. All right. Of course, amount of data. So nowadays, a lot of people talk about the big data. That means you have a huge amount of data. You have a huge amount of data. Or small data, because sometimes, if you care about extreme events, by definition, you won't have a lot of data in the typical case. So amount of data, if you have large data, you could have a computational challenge. For example, the computation or even visualization or even load data could be a challenge. And too little data could lead to the other issue. And in a lot of cases, sometimes you have seemingly large data sets. You have seemingly large data set, but if you care about very specific events, you might also lead to the small data regime. Okay. And also, most of the time, we will focus on spatial temporal process, and the spatial temporal process can be quite complicated. So, here is just one example. But in general, I think to appreciate that fact, you probably want to pull out any of those, like a spatial temporal process, and try to make animation, and you will see the complexity of those spatial temporal processes. Spatial temporal process. Okay. And also, when building the model, people also want to keep in mind several things. The social uncertainty error, for example, for the measurement, it could have some kind of measurement error and some kind could be non-trivial. And also for the model simulation, you will need to worry about the fidelities in terms of physical fidelity or the spatial resolution and all that. Solution and all that. All right. Okay. So, here, so to make things a little bit more concrete, so let's say, let me give you the typical workflow of the statistical modeling with a very, very simple example. Okay, so the first part is actually quite crucial, the problem formulation. And usually, this part statistician cannot do this alone because they have to collaborate with domain scientists to figure out the meaningful. Main scientists to figure out the meaningful scientific question and then to collect appropriate data. So, data can come in from different sources. But once we got the data, so the first thing is quite important. And I will actually ask you guys to do some R exercise or the computing exercise using the platform you like to do, the exploratory data analysis. And this part I think is very important because I saw a lot of our students very quickly go to the statistical modeling. Go to the statistical modeling without looking at the data very carefully. So, exploratory data analysis is actually a very, very important first step. And once you look at data and depend on problems, so let's say you have some kind of time series, you want to do an appropriate statistical model. So, this is probably the simplest time series model one can use, which is called the autoregressive process of order one. So, that means this process only at present time only depends on the previous. Type only depends on the previous time, the process value at previous time, and add some randomness. Okay, so this would be an example of a statistical model, but to finish that, you also need to figure out some details of example, do we have any constraint of this one to make sure this is a well-defined or stationary process? And what is the probability or statistical distribution you are going to assume for the error term? For the error term, so those kinds of details. But once you have the statistical model mass, you want to match the model with the data you have. So that means we need to do a few things. So first of all, we have this model, but we still need to learn this quantity and this quantity. So we need to carry out the estimation and typically involve some sort of optimization. So, for example, the maximum likelihood. Example: The maximum likelihood estimate is one popular commonly used approach. So, basically, we're going to solve some optimization problem and to get the parameter estimation. And the optimization not only give us the parameter estimation, but also would give us the uncertainty. And that part is important because imagine if you have a huge data set or you have a very little data set, it is possible that you end up with the same parameter estimate, but they will have a very. Parameter estimate, but they will have very, very different absurdity. So, we want to also probably account for that. So, estimation and also inference. So, those are all we are going to estimate those parameters and also come up with some plausible range for those parameters. Another important thing is once you fit a model, that's not the end of the story because you want to build some confidence about the model you are going to use. So, you need to examine whether the model assumption we made is reasonable. Assumption we made is reasonable, and also whether the model fits the data well. So, there are a few things you can do in which you, at least for the R section, you will see I use the QQ plot a lot. And lastly, once we build a model, we can do the prediction. Okay, we can do, for example, in this case, if we want to predict the net time point, we have this parameter estimate, so we can plug that in as a point prediction. As a point prediction. But at the same time, we also want to account for the uncertainty by creating the so-called prediction interval and so on. We can also do simulation. In this case, that's quite easy. And there must be even a better way to do it. But in tune, you can think about can do this kind of thing recursively. All right. So that is more general background on the statistical modeling. So question? Yeah, I noticed the lower index for absolute changes. index for epsilon changes for the construction of a statistical model your epsilon is uh depending on the time t plus one right oh that's a great question so typically in this we typically assume that this one is sort of time invariance uh what about the next one which one epsilon i instead of i plus one uh bottom right bottom right the stiff indexing no no it's uh a so No, no, it's software. It's not consistent with XI plug logo. Oh, they're IID, so that actually. Oh, yeah, yeah, yeah. Oh, but yeah, you get it. Well, so yeah. But that should be I plus one. So yeah, so in so sometimes people can be a little bit sloppy, but if I want to make it consistent, you are correct, it should be I plus one. Thank you. Thank you. Any other question? All right. So, one thing I want to acknowledge is that in this short course, you will mostly see examples from precipitation. But in our session, I will provide a lot of wind-speed data. And my excuse is that the method is sort of general. So you can use that to deal with different processes. But because Brio, I use precept, which is easier to tell the story. Because it's easier to tell the story, but in the R session, if you really want to work with the wind speed, I provide a lot of wind speed data. You can work with observation at several different locations. You can also work with the data from very high quality wind tower and also computer model output. All right. So anyone from any of you from US? This must be some degree. Do you from Texas? Do you from Texas? No. Yeah, but so Hurricane Harvey. So some of you might remember this event. So the Hurricane Harvey actually made landfall. Okay, so I forgot. So I think it should be 2017 and late August. So this Hurricane Harvey made landfall here. It's very strong hurricane. But what happened is this hurricane stayed here for an unusually long period of time. And therefore, it produced a huge amount of production. It produced a huge amount of precipitation. So, at some location in the vicinity of Houston, you actually received that amount of rainfall within, I think, within five or six day periods. In order to make a contrast, I have a question for you guys. I don't know how many of you are local here from Kelowna. But my question is: can you make a guess about the Can you make a guess about the average annual precipitation in Kelowna? So, to be more specific, because the data I got is from the Kelowna Airport. So, anyone want to make a guess what is the annual average total precipitation in terms of millimeter here in Kelowna? It's displayed. It's displayed. You can see that. It's not your. It's not showing very well, okay. Yeah, so it's like a 300 millimeter per year. You can see that's a huge difference. But even for this place, in which they have a lot of rainfall, this is almost, it's almost like 80 or 90 percent of the typical total annual precipitation. So this indeed a very, very severe extreme event and they lead to very serious consequences. There are other examples, so for example, the heat wave. Of example, the heat wave, the Euro Heat wave, which lead to huge damage to the human society and also the storm surge. So there are more examples. I won't tell you all of those. This is a little bit outdated, but every year, I think the NOAA, every year NOVA will make a map like this, but this is 2021, so you can see there's a lot of Wi-Fi. But I guess in the past few years, you also have a very Past new year, you also have a very extreme hot temperature, especially for the Pacific Northwest. So, a lot of time, so for example, if I look at this area, a lot of extreme has to do with water in sun. So it could be heavy precipitation or it could be the flooding due to a hurricane and so on. So, let's take a precipitation, for example. So, a lot of the extreme it requires us to quantify the sort of worst. Quantify the sort of worst-case scenario. So, for example, what will be the so-called once in 50 years event in terms of the total amount of water that will dump into a particular location? Right. So a lot of then it requires extrapolation or leaf or at least require us to push all the way to the far-tail of the probability distribution. Okay. Of course, there are some other. Of course, there are some other. So, I already sort of alluded to the first one. So, for example, in terms of statistical modeling for extreme, we want to ask what would be the magnitude of the extreme event, for example, once in 100 years rainfall. And the other problem is we might want to know how the extreme varies across different locations. And maybe we want to ask with increasing CO2 emission, what would the Emission, what would the temperature exchange change from present to future? So, for example, the future once in 100 years event could be today's one in 3000 years event. So, what will be that change? Okay. All right, so that is sort of the background. And let's move to the theory. But the theory will be very light, and I will use a lot of illustration. So, and also. And also came out with this conceptual picture. So, think about this is a probability distribution of a variable of entry. So, think about this, maybe the precipitation amount, excluding those zero precipitation, or this could be wind speed. It looks somewhat like wind speed, although the tail may be a little bit too low. All right, so the first question I have for you guys is. is I believe all of you have you take the stat course in your college most of you right and do you still remember anything you learned from that course most of things uh probably answer is no but if you say yes mostly if you still remember that mostly you have some kind of random variable and this random variable would describe how those are data would arise and let me explain this a little bit so this curve this is This is a little bit. So, this curve, this is so-called probability density curve, and the height of density tells you what is the likelihood that you will have the data in that region. So, you can see there's a high peak right here. So, that means for this particular random variable, you will have, you will expect to see much more observation in this region. Whereas here, you have a very, very low density. So, therefore, you have less chance to get oscillation right here. Okay, so for your study. Okay, so for your stat 101, you pretty much focus right here. So if you still remember, we do a lot like do the statistical inference about the population mean, which is somewhere right here based on the observed data. So that's what I call the ordinary statistics. A lot of times we care about the bulk part of the distribution. And then from this, we do have some series which is called the central limit theory. And then, so because this series, so that's So, because this series, so that's why in your stat 101, you see the normal distribution a lot of time, much, much more often compared to other probability distribution. Okay, but extreme value modeling, yeah, actually we care about this part. So in a typical statistical context, people will say this is sort of outlier. So they typically want to have a procedure to less affect by those outliers in terms of inference, the middle part. Inference the middle part, but you give me about the statistical inference for this part. Okay, now the question is: do we have any theory? And what distributions suggest by the theory that help us to describe the Tail distribution? Okay. Is this clear? Okay, good. So let's move on. Okay, so let's start with, again, so let's start with a very simple situation with some assumption. Situation with some assumption in which later I will talk about the potential consequent of violation. So, think about this guy. It's a probability distribution. So, we assume here f1 all the way to x and those are the so-called random variable, but each of them will follow this particular probability distribution. And now, we want to learn about the tail distribution. The tail distribution. But remember, in statistics, we don't get to see this curve. What we want to do is we want to use the data right here. You can see those. So those are the data. So that's all you got. You want to use this one to reconstruct this one. In particular, you care about this one. Okay. All right. So the first thing to do, and if you go back to look at the data, so which data points like most relate to this puzzle, it will be. Relate to this part, so it would probably be this guy. And what is this guy? So, this is the maxima, the so-called sample maxima. So, this is your random sample, and this is the maximum value. So, the one way to start is to look at, to investigate the probability distribution for this guy, the sample maxima, that's the random variable, and to move from that to see whether we can come out with some theory and the corresponding distribution. All right. All right. Okay. So this is probability 101 because this one does not have nothing to do with the data, but it's just the probability construction. So let's say we have a sequence of random variables. They are IID, which means they are independent and identically distributed. And we define this guy as the sample maxima, which is the maximum value among those n random variables. And then we want to know about probability distribution. Distribution. So, first of all, the maxima, the sample maxima, is less than or equal to a certain value. That means every single random variable has to be less than or equal to that because this guy is the maximum. Okay, so we have this one. And from this one to this one, that is because we assume those random variables, they are independent. So, therefore, this joint probability will become a series of product individual probability. Part of the individual probability. Okay. And from the last part, because we use the identically distributed, because each of them is just, so this capital F is the CDF for the original random variable. So we have capital F raised to the power of n, and this is the little x value. Okay, so now we got this one. In most of case, this one, in the limiting case, this one is not that useful because if you hold. Not that useful because if you want to play the game to let the n goes to infinity, you basically have this so-called degenerate distribution, which is not that useful in terms of statistical modeling perspective. But now I want to remind you about the central limit series. And again, anyone still remember central limit theory? Although this is a mathematical description, but you could, I guess, I could add. Uh, you could, I guess, I could ask you to try to uh describe that in word, but basically, the central limit series says that uh, the sum of the random variable, so it can either do sum or you can do the mean. And I guess the mean version is the one you see most. So that sort of explains why a lot of how you can use the normal distribution to infer about the population mean, because this series says that the partial sum of the random variable and Partial sum of the random variable. And if you do this rescaling under quite general condition, it will converge to this standard normal distribution. Okay. But as you can see here, if we do the partial sum right here, we still need to do some scaling here. Okay. So that suggests that scaling might be a key step to attain a non-degenerate distribution. Remember, in the previous case, we have a degenerate distribution, which is not that useful. Which is not that useful. Okay, so this is the central limit theory. The question is: We have this one, and we know the limiting distribution, but this one is generally. But can we do similar things such that this one is a non-degenerate distribution so that we can use that to do the statistical model? I'm sorry? UNC model variance? Oh, good question. So I skipped that because I assumed. So the mu right here. So, the mu right here is the population mean for the underlying random variable, and the sigma is the population standard deviation for that underlying random variable. So, in your standpoint, you also have different versions because you could have like this one replaced by, if it sample, sample standard deviation, so that you have a student T distribution, for example. Sorry, can you remind me non-degenerate? Which one? A non-degenerate distribution. Non-generate distribution. Which means it is a distribution. So what I mean by degenerate distribution is something like this. So the probability distribution. So this one is generate in the sense that it will concentrate at a single point. So it's not a probability anymore. So it will always converge to the upper or the middle point. Yeah. So if you want to draw the graph, so this is a non-degenerate distribution, but the degenerate distribution will be just a delta function. Of the delta function. And in this graph, what does n represent? I'm sorry. Oh, yeah. So in this graph, n represents the sample size. So it's probably a little bit hard to see from here that we have like a 100 in this particular case. So n is the sample size. Question? The single linear team is valid for a very high end. So in reality, we might not have a lot of samples. Samples out of accurate GPUs? That's a good question. So, we will actually see some sample. So, central limit series require typically the end required for the central limit theory. Of course, it also depends on what this guy looks like. But in general, it's not to be you'll use a C1 example. Which is right. Okay. So, I guess one issue for the central limits. Issue for the central limit series. And a lot of people, when they do their stat 101, it's probably they still have a hard time to understand the central limit series. So I decided to build this animation in the hope that you will appreciate that a little bit better. So remember the statement about the central limit series says the sampling distribution. So first of all, sampling distribution is a very difficult concept to grasp. The sampling distribution of the sample mean when n N sufficiently large will approach to a normal distribution. Okay, but it says under fairly general condition, it will say the population distribution does not necessarily have to be normal. So in this case, I have this simulated population distribution, which is an exponential distribution. And you probably agree with me that this one does not look like a normal distribution, right? So this is our population distribution, but again, we want to learn about. Again, we want to learn about the population mean by looking at those random samples. Okay. But in reality, you don't get to see the sampling distribution because you just have a lot of time, you just have a single sample and you want to use this single sample to learn about this one. And the sample mean right here, which you just do the calculation, you take the averages. So this will be the sample mean for the data you have, just one point. That's no sample. Have just one point, there's no sampling distribution. So, sampling distribution is sort of conceptual tool. But in a simulation, you can actually try to get an understanding about what is the sampling distribution, because here we have the true population distribution. So we can do the simulation, the so-called multi-car simulation, many, many times. This is the first random sample in which we have n equal to 100. We have 100 observations right here. This is the first sample. This is the first sample. I calculate the sample mean and I put it there. But in a simulation, unlike in the real world, we can do that a few times. So the second time, I still simulate from the same population distribution. I calculate the sample mean and put it data. Okay, if we do this process many, many times, so now you can have some idea about the sampling distribution. distribution. All right, again keep in mind n is 100. And let's see how well the central limit approximation work. Those are the data from our simulation in terms of the sample mean. This black curve is actually the true sampling distribution for that. So what I mean by that is this one is true for this particular population distribution and for this n. And for this n. So, in this case, you can actually exactly derive this true density curve. So, what is the blue dashed line? The distribution, right? Very close. So, this is actually, so I achieved a little bit. So, this is just the normal distribution. So, what I did is basically I treat those as the data point and I just fit the normal distribution. So, I have the mean, I have the standard deviation, I just apply this. Standard deviation, I just plot this curve. And you can see this one, this one, they match reasonably well with n equal to 100. Okay. So the reason I do that, because I will do that again, but I'm going to do that for the sample maximum. Okay. Oops. Yeah, sure. Sure. The sample mean right here. So this is the underlying probability distribution. And we can draw a random sample. So this is the random sample in which we have this population distribution. So you have value right here, you have value between. Value right here, you have a couple value between like 0 to 10, and so on. So, those are the sample value. And the sample mean or the sample average basically take an average of those of those numbers. So, that would be the sample mean. So, if you take an average, they'll be right here. And I will just show it over there. So, this is one of the sample mean. But if you do this sampling many, many times, each time you have the same population distribution, but you will have. Same population distribution, but you will have slightly different sample value and you will have slightly different sample mean. And then this is show you a collection of sample mean in which we can use that to learn about the so-called sampling distribution of sample mean. Why do you have different samples? Yeah, because the randomness. So this is the probability distribution. So they have the same probability distributions, but every time you want to draw a sample from this, Want to draw a sample from this, you won't get exactly the same sample value. So each part you will have a different sample value. So you will have different sample mean. And then if you put that together, you will have an idea about what is the distribution, the sampling distribution of sample mean. So different samples with the same distribution? Yeah, but different samples. Different realized value, different realized samples. So hopefully. So, I'm going to go ahead and do a little bit of a test. Oh, okay. Good. All right. So, back to the previous question. So, for the central limit series to work, the rescaling is the key. So, the question is: can we find Can we find a non-degenerate distribution with a sample maximum? Okay. And the answer is again under quite general condition, we do have a limiting distribution for this guy. And that limiting distribution is no longer a normal distribution, but instead it's called the generalized extreme value distribution with three parameters. Again, we have a location and scale parameter. Have a location and scale parameter. So they have a slightly different definition. In general, you can think about this is the location sort of represents sort of the center of that distribution. And the scale still represents the spreadness of that distribution. This parameter, the shape parameter, tell you the tail behavior. So whether you have a very, very long tail in which you could have a very, very large value, or you have an exponentially decayed tail, or you could have a bonded tail. Or you could have a bond detail in which you have a finite upper limit in which you cannot. All right. So let's look at the simulation again. So I keep the situation the same. So we still have a population distribution, which is this exponential distribution. Every time I draw a sample, but now I look at the sample maxima instead of sample mean. Instead of sample mean, okay, so if that series provides a good approximation after we run the simulation, many many times we collect those sample maxima, this sampling distribution should be well approximated by a so-called geometric distribution. Okay, so let's play that. So, again, so in the exercise, I will also encourage. Encourage you to also play with not just doing the data analysis but also do some multi-colour simulation to enhance your understanding. Okay, so now here we go. So we do that 120 pi. I think that's good enough. So we have those sample maxima. We have 120 sample maxima. This is the corresponding histogram. We still have this black curve. And this is the theoretical sample maxima distribution again under this. Maxima distribution again under this particular situation, we have the exponential distribution. We have n equal to 100. In this case, we know this is actually the true sampling distribution. We have the red dashed line and we have the blue dashed line. So can you make a guess which one, which distribution produced this red one, and which distribution produced the blue one? Okay, so with the blue one, if you look at this blue one, it's look symmetric. So this is actually a normal distribution. So, what I did is I take those as the data and fit the normal distribution and plot the fitted density curve. The reason I show you this is just tell you that the normal distribution is very useful, but not in this case. If you look at the red one, you can see the red one match with this black one very, very closely. And what is that red one? And what is that red one again? When I do this, I assume I know nothing about this one. I simply just use those points and fit the so-called generalized equinval distribution to get the location, scale, shape parameter, and plot the fitter distribution. And you can see this one match with the theoretical one very, very close. Question? The right-hand side, is that the distribution for the next one? Is that the distribution for the maximum or the least scale? Ah, very good question. So, this one just for the maximum, and later it's a very good question. So, I will come back to that later because in reality, so in theory, okay, so I probably won't bother you by that. So, in theory, you will do this rescaling. You can actually derive those rescaling sequence if you know the population distribution, but in reality, you don't know. Distribution, but in reality, you don't land. So, what you will do is that you are going to observe those two sequences: one for the location parameter, the other one for the scale parameter. Yeah, so to answer your question here, this, when I fit that, I just use those maxima and then to fit a generalized e-turn value distribution. What did you say the blue dashed line was? The blue dash line was that blue one is I just uh I just fit the normal distribution to those sample maximum. And the reason to do that is I'm going to show the normal distribution is not appropriate in this case. It's not appropriate to describe the sampling distribution for the sample maximum. A question. What is scaling that you did before getting on? Get a knowledge. What is scaling? Okay, that's a good question. So, what happened is: so, if you think about the maximum value, if you increase the sample size, this will always go to the data direction. So, if you let the n goes to infinity, this one will either go to infinity or it will go to the upper limiting point. So, that's just a single point. The reason we want to do the scaling is we want to sort of stabilize that. To sort of stabilize that so that we still have a probability distribution. But in this case, here, basically, we say we are going to observe those scaling functions for this particular n into the location parameter and scale parameter. So the scaling is something we need to work on that in terms of theory. But in practice, when we work on that, we basically not going to figure out what is the scaling function, but rather, function but rather based on the n we are going to work with we are going to directly estimate the location prime to n scale prime yeah maybe related to this question so the entire one is the black curve so it depends on two parameters like the location and the the table right you mean this one yeah so um yeah how do we fix that how do we mask that so you mean this plan You mean this black curve? Yeah. So in this black curve, we know what is the sample maximum for this particular distribution. So for exponential distribution, we know what is the sample max, the probability distribution for the sample max for the exponential with the lambda parameter and also with this n. So therefore, we can actually derive this one. But what I'm trying to say. But what I'm trying to say is that this distribution is not exactly generalized eternal value distribution. So, this distribution is actually two parameter. This is three parameter. But without knowing this knowledge, we can still use the GED to do a pretty good approximation. So, the idea is very similar to the central limit series, in which for most of the time, we don't need to worry too much about online population distribution, but we still can. Population distribution, but we still can use the normal distribution to describe the sample mean. So that kind of universality, which made this series useful for describing the e-tribal. Another question? Yes. Yeah, but Unice, I think you are first. I just wanted to make a comment. So if one uses a normal distribution, one consequence of that, it would actually End up estimating lower values, then because it's in like the left tail, you are exactly right. And usually using the density, it's a little bit hard to describe that. But if you look at the corresponding quantile functional plot, you can see very clearly that if you are going to use this one, let's say to do the decision making or let me if you were really When you were really underestimated, Upper Tail. That's a very good point. Although, usually, when you look at the density, it's a bit hard to see that. You can sort of see that, but you need to turn your head a little bit. But if you look at the quantile part, it will be much easier to see. Question? Is the shape of the right and kind of the background dependent on the content? Yeah, that's right. Yeah. Yeah, so if you want, I would encourage you to do different. Want, I would encourage you to do different exercise. So, because a lot of the base on assuming the central size is reasonably large, so you can definitely do another exercise by lacking the end to be much smaller. And it will be interesting to see to what extent this theory will break down. Or maybe you can make it a bit larger to say whether it will give you even better approximation. Yeah, but yeah, this, so those multi-colour simulations, they are useful. Simulation, they are useful to make things a little bit concrete. But the price you need to pay is that you need to choose a particular setting. Although you can do this kind of simulation for many different situations like N from 20, 50, 100, 200, and so on, you can also play the game by changing this distribution. I cheat a little bit in the sense that this exponential distribution actually converges to the GV much quicker than other things. So if you want to break the series, If you want to break the series, you can also try our distribution. Yeah. All right. Okay. So we have the so what we learned so far in the very simple summary is that we know the generalized extreme value distribution might be useful to model the extreme, for example, for the quad maximum. But the question you might have is: what is so special about the generalized extreme value distribution? And you might have the same question. And you might have the same question: what is so special about normal distribution? So the reasoning here is that the maximum, I'm sorry, the generalized HM value distribution is the only so-called max stable distribution. So what does that max stable actually mean? So max stable follow, if the distribution function g is max stable, you will follow, you will, this one will be satisfied. Uh, you will this one will be set, but what does that actually mean? So, if you remember in the very beginning, when I talked about the probability framework, when I derived the CDF, the probability distribution for the max sample maxima, we're going to raise this power. So, think about this is the random variable where we take a sample maxima among a random variable. And this is the original one. So, what it means is this. mean is this distribution is sort of a close under the maximization. So when you keep doing the maxima, maxima, increase the sample size, the location and the scale will change, and most likely it will increase for most case in the location parameter. But the shape parameter, which describes the tau behavior, so whether you have a heavy tail distribution or you have exponential tau distribution or you have a bounded tau distribution, it won't change. So that's what it means. Change. So that's what I mean. So the generalized equation value distribution is the only max stable distribution. So that's why this actually played such a spatial role when you look at a simple maximum. Yeah. So you were AK is scale, you know, BK is location. Yes. Okay, and this equality holds for all k? Yes, that's for the, yeah, that's for those integer rounds. But yeah, that's precise equality not. Yeah. And precise equality not yeah yeah that that's the very definition for the max but uh max stable distribution yeah yeah also depend on like uh okay so I won't yeah so that's true any other question so if there's no more question I think this is the end of the series uh what is the time okay The time. Let me speed out a little bit. So, the so now we have that probability distributional result. So, in a sense, so let me do it very quickly to go back to fill in the blank. So, here, if we look at a tau distribution, we have the 8 trend value theory. And in terms of distribution, if we look at a sample magma, we have a generalized 8 trend value distribution. General I say trend value distribution. All right, but the next question is: now we have a series, but how are we going to use the series to do the data analysis? For example, if this is the data we have, how to use the data to learn about the tail distribution? So that is the statistical part. But before that, let me talk a little bit about this. So typically, when people do the trend value modeling, they are not going to give you the gene. They are not going to give you the geomass atrium value distribution because that distribution by itself is not easy to communicate. For example, if you want to say what is the atrium precipitation that could arise, you won't give them a distribution, but you want to give a summary. So the summary, you might want to give a so-called return level with the associate return period. But essentially, this is just a certain quantile, and typically it's a high quantile value. Typically, it's a high quantile value. And that's the high quantile value corresponding to this generalized trend value distribution. So you have this equation, and then you can actually derive this form. So basically, the one over p return level, the expression is actually right here. Again, you can see it depends on the location, scale, and shape parameter. It also depends on sort of the extreme length you are being looked at. To look at. And this is just a very quick graphic illustration to tell you that the shape functor actually will make a big difference. So, one thing I forgot to mention previously is here. So, here are three examples of the so-called generalized eternal value distribution. They have exactly the same location parameter, which is zero. They have the same scale parameter, which is one, but they have a different shape parameter. This one, I forgot what it is, but it must be a magnet value. It must be a magnet value, so therefore you have a finite upper bound. For this black one, the shade parameter is equal to zero, so you have this exponential decay tau. And for the red one, I believe the shade parameter is probably 0.2. So therefore, you have much, much heavier Ta, even they have the same location and sphere parameter. And the difference would be much clearer if you look at this sort of a quantile plot. Sort of a quantile plot, although this axis is not the percentile, it's on our scale. So you can see the shape functor zero. If you have a shape parameter positive, you have much, much higher quantile values. All right, so next question is: how are we going to make use of the service to do the data analysis? So, suppose now we have the data. So, suppose now we have the data. So, let's say we have a time series data. Let's say the daily wind speed here in Kelowna. And we want to use the data to infer what will be the ones in 20 years event in terms of wind speed. So, how are we going to do that? So, first of all, in practice, what we are going to do is this. Again, you can see this, the scaling right here, the limitation, and scale parameter. This is the theory. When we do things in practice, we are not going to use this, we are not going to figure out this one and this one, but rather we are going to put this one, this one to here, here and here. And then basically we just treat this as the location parameter and this as the scale parameter. And we let the data to tell us what will be the plausible BN and AN for that given N. Okay. Right. Right. So we are going to, once we have the data, we want to fit the genome ICG value distribution to learn about this parameter, this parameter, and this parameter, and also account for the estimation uncertainty. But once we have done that, we can perform the follow-up inference like estimate the return level and so on. All right. The maximum likelihood estimation, I believe, I think Julie probably covered that last year, but I also recognize not all of you attended report last year. But the idea is, first of all, we need to have some replication or sample point in order for us to fit a probability distribution. Because, for example, if you want to fit a normal distribution, you cannot just use a single point. You need to have a separate point. not just use a single point. You need to have several data points in order to do that. But the tricky part here is you are not going to, in this case, you are not going to use the entire data set from the original data because you need to extract the sample maxima first and then to put them together to get the sequence of number in order to fit the data. So what happened later, you will see is that we will have a so-called blocks maxima approach to extract those N1, N2 all the way to Those N1, N2, all the way to NK, each of them is corresponding to a block maxima. So we have a replication of those block maxima, and we are going to use that. And this is the likelihood function. So basically, think about it as an objective function, but it's a statistical motivated objective function. They help you to learn about the parameter. And also, based on the likelihood theory, you can work with this to learn about associated estimation uncertainty. Associate estimation uncertainty. Okay. All right. So again, the other thing I mentioned is to account for the uncertainty. So for example, use the limited data to learn about one in 50 years event. You will probably induce a lot of uncertainty. And typically, one way to communicate uncertainty is using the simple confident interval. In a trend-value modeling, there are two approaches. Modeling, there are two approaches to do that. One of them is so-called the delta method. So basically, you have some sort of a normal approximation to help you to derive the confident interval. This approach is very easy to compute because there's a closed form. But the drawback is you always produce symmetric confident interval, which may not be realistic. Okay, so I make this statement. You are welcome to produce. Are welcome to produce your own simulation to sort of verify that? The other one is so-called profile likelihood approach, which is more sensible in a sense that it can accommodate the n symmetric confident interval. And again, you will see some example later. But you need to, the drawback is you need to do that numerically. Okay. Let me start with this example. Okay. So Okay, so this is the Clemson David presentation data. The data source is U.S. Historical Climatology Network, and this station is actually not that far from the place I live. This is the daily accumulated precipitation time series start from 1970 all the way to 2014. And a project, the unit is an inch, but you can probably do the conversion. Inch, but you can probably do the conversion very quickly. So, here you have like a five inch per day. Even not, so I would say this is very, very heavy precipitation, but you can probably do the conversion later on. So, occasionally, you have a very, very heavy precipitation. Most of the time, you don't have that much precipitation, even more than often you have a zero precipitation. Okay, so the question is: here is the data we have. So, how can we use this particular data set to estimate, let's say, one in 50? To estimate, let's say, one in 50 years return level and also provide uncertainty under the assumption that we assume we have a stationary distribution here. All right. Okay, so first of all, you need to get those N1, N2 all the way to NK in order to perform the parameter estimation. And remember, the generalized H value is for the sample maxima. It's not for every sample value. So, first of all, we Sample value. So, first of all, we need to get a whole bunch of sample maximum. Okay, so the one approach is as following. So, we are going to divide the type service into non-overlapping block right here. In this case, each block is one year. So, basically, you have one year of data, you take the sample maxima, the second year of data, you take the sample maxima. That's what you got. So, the sample maximum. That's what you got. So, the simple maximum first year, second year, third year, and so on. Okay, so now when you do the modeling, you are going to only use those data, but you are going to throw away the rest of the data in order to estimate the cal distribution. Okay. Those data point, and if you turn your head like a 90-degree contour clockwise, you can see those. This is the histogram, but those are 65 annium magma. And this blue curve is the fair. And this blue curve is the fitted generalized equin value distribution. And from here, you will say, okay, it seems to fit reasonably well. But we want to have a better look. So this is so-called the quantile quantile plot. From the x-axis, so this is the model quantile based on the federated geometry distribution. On the y-axis, you have the empirical quantile from those 65 annual maxima. And your maximum. And the way you are going to assess that is if those points follow along this one line very closely, you will see the model, the probability distribution fit the data reasonably well. You can see some deviation. A natural question is, should we worry about this? Okay, so again, multi-colour simulation could also be useful to help you to assess whether you should worry too much about this. And one way to do this. And one way to do this is so-called the parametric roostra. Basically, you have the fitted model, you use the fitted model to simulate a whole bunch of the synthetic data. And from that synthetic data, you know your model is exactly correct. But when you plot this, you will still see those fluctuations. So you can use that as a way to assess whether this is reasonable. You can even do that multi-colour simulation to construct sort of a confident band here. All right. Can you go back to the previous figure? This one? Yeah. So if I select a big block size, the distribution will change radically. That's a good question. So what happened is if this block size is already bigger now, bigger block size in Siri issue work. The consequence is that if, let's say here, you use instead of one year block, you have five year block. You will have a five-year block, and then you will have very, very few block maximum. So that means you have much fewer data points. So that means you will have a lot, much larger estimation of certainty. So this is the classical bias and balance trade-out in which bigger block size series should work better, but you will have much fewer data. Is there something that tells you that if I shrink my block size, it will Block size, it will converge the same distribution regardless. Uh, so or is that not there's some empirical way you can test that? So, if you go back to the definition of max stable process, so that means if the block size is already big enough, increasing the block size, the shape parameter should be a constant. But still, when you do that in practice, there will also be some uncertainty. So, that could provide a way to. So, that could provide a way to assess whether the block size is already small enough, but also big enough so the serial will work if the data point is sufficiently large. So, in practice, that's a little bit difficult to do unless you have a very, very long time series, in which I did this experiment before. So, I will be happy to talk with you about that. So, basically, I have a thousand years of equilibrium simulation routes. So, I can use that to do such experiment to afterwards. Do such an experiment to ask the question whether one year block size is already big enough. So I can do one year block, five year block, 10 year block, and to monitor whether the shape parameter is stabilized. And does it have to be non-overlapping blocks? It does not. You could do the overlapping, but you need to have some way to deal with the dependence introduced by having the overlapping block. But some people are actually doing that and show they might have some. And show they might have some improvement in terms of efficiency. Question: What if instead of using the blocks, will the distribution change much if you just take the top like 50 values instead of for every single one? Wow, that's great. So it seems like you already invent your own eternal value analysis because that would be the NAS approach. So that's the approach is that instead of do the blocking and take the blocks maximum, I can do the thresholding and using those value above the threshold. Using those values above the threshold and use the same theory to fit a related distribution. So that is essentially you just described. But that couple of versions, one version is the next one I'm going to talk about is instead of using those block maxima, which can be wasteful of the data, you can choose the high threshold and retain those values above the threshold in a fit detail distribution. The other version is as following. For each year, within each block, you can choose like a Within each block, you can choose like a top three order statistic or top five order statistic. The series will be very similar to this one, so the distribution will also relate to the GEV in some sense. So that would be the other approach. Yeah, but again, the top, let's say the top order statistic, at some point you cannot take too many top order statistics. Again, that's also some kind of selection. Okay. All right. Here we go. So I think we have a very good group of people. So they really think about the alternative approach. So in this case, we have 65 years of data. It's almost like a 20,000 data point. But here we only use 65 data points to learn about extremes. So it seems like we do not use the data very efficiently. Efficiently. The other approach, which is called the threshold exceeding approach, is instead of do the blocking and take the blocks maximum, you can select high enough threshold. So we will come back to select a high enough threshold to retain those values above the threshold and fit the theoretically justified probability distribution to that. And then based on this, we can proceed with the estimation. All right, so this slide I will go very quickly and I will skip some steps. Again, so the first two, so this, so we make the same assumption. We say the sample maxima can be approximated by a generalized trend value distribution, if that is the case. And then if we choose a sufficiently large threshold, we have this conditional distribution. From here to here is just a very definition. Just a very definition of the conditional distribution, except I multiply by the central size for both numerator and denominator. And the reason I do that is this one will go to this one. The idea behind that is something related to so-called point process. So basically, this is, but in the end, so basically this one and this one look very much like this, the thing inside of John's extra value distribution. So basically, this. So basically, this guy will get this one, and this guy will get this one. And the online idea is based on some point process algorithm. So if you got this formula, and then so from here to here, we have n large enough. So we have this approximation. And if we rearrange that, we have this formula. And but this one happened to be so-called the survival function, which is sort of like the Which is sort of like the one minus the CDF for the so-called generalized sparetile distribution. Okay, but the bottom line in terms of practice, if we still have that extreme value series work, if we choose sufficiently high thresholds, we can model those threshold accidents as a generalized Pareto distribution. Again, we have another distribution, but that distribution relates to a trim value distribution in the following. Distribution in the following way. So, first of all, we need to choose the threshold. After we choose the threshold, we have the scale and the shape parameter. The shape parameter represents the same quantity. So the shape parameter will still describe the tail behavior. If you have a shape parameter positive, you have a heavy tail. If you have shape parameter zero, you have exponential tail. If you have shape parameter negative, you have bonded tail. And the scale parameter for the generalized. Scale parameter for the generalized breakfast distribution relates to the GEV scale parameter in this way. So you have the scale from the GEV, you have the fair parameters, this is the threshold you choose, and this is the location parameter from the general ICT value distribution. Okay, the implication is the alternative approach to model the TEL distribution is that you first choose a high enough threshold. Use a high enough threshold, we can those values and fit a generalized peripheral distribution. Okay, the one question I haven't really addressed, and that is something it's not just unique to the generalized Parato modeling. It's also true for block maxima, as people already raised the question, what is the block size? Can we make it larger, make it smaller? So that's also a tuning going on. Although in the environmental pi-series, because you have the very natural Because you have the very natural annual cycle. So typically, people would just use the annual blocks. In principle, you can make the block larger, but typically, you don't have that many data to do that. Okay, but for the threshold, you see that approach, you have more degree of freedom to choose the threshold. Okay. So, in practice, what people would do is they have some - hard approach to select a threshold, and which is not necessarily easy. But again, it's also Very easy. But again, it's also involved at bias and variance trade-offs. So, again, if the threshold is very, very high, the series is probably going to work, but you probably don't have a lot of data in which it does not achieve the goal to retain more data point. If you have the threshold too low, you might introduce the modeling bias. So, typically, people look at this file, so-called the mean residual life plot. And the idea is here: you choose seven. Here, you choose several different thresholds, and for each threshold you choose, you just calculate the average value for those points exceed the threshold. So, basically, for different thresholds, for each given threshold, you will have a so-called the mean residual line value. So, if you do that for many, many thresholds, you will come up with this curve. Okay, and sort of the smallest threshold that you want to use is what you want to find a point, right? Want to find a point right here such that this curve starts to behave like a linear function. Okay. This is the one I choose, but imagine if I ask other people, if I ask Adam, he may choose another threshold. I asked Denise, she might choose another threshold. So choosing the threshold is not necessarily easy. And sometimes you choose different thresholds, the result might be sensitive to that. Okay, but next I'm going to show you how the modeling will work. If we're happy with this threshold of Z per se, so we choose that threshold, we return, oh, one minute. We return, again, we will do the maximum likelihood estimation. What's that? Okay. We already fit that, but we want to see how well the model fit the data. Well, the model fits the data, which is right here. And then in the end, we can also do the inference. So we have threshold. The histogram of threshold is even. This is the fitted generalized paradox distribution. This is the estimate 1 in 50 years return level. So one thing I forgot to mention, so you have two different ways to express the uncertainty. One is the delta method in which the integral is always symmetric. That means the here. That means the here to here and here to here, they're always the same. Whereas the delta method, you will see the n-symmetric confidence interval, you will have a slightly longer interval to this side. Okay, so I think I run out of time, so I won't okay, I would just leave it here. Yeah, yeah, yeah. I had one, but I'm guessing you answered it in the last third. But I was, it was part of it was you had this big assumption of the I and E. Is it flexible enough to deal with this? That's a great question. So that's those slide. I skipped. I'll look at your slides. Yeah, so one way I can do is. I can do is I'll be happy to talk with you offline and alternatives that I can probably make a video to cover those. How about that? Yeah, but very, very quickly. So there are two things. So because you could have a violation of the independent assumption or identical distributed assumption, or maybe both. So the first front So the first front of the slide, where is that? Okay, so yeah, and it's very likely in the real world when you look at those environmental type series, you will have a violation because you will expect to have a local temporal dependence. You could have a lot of long-term trend and a seasonal variation. In this case, basically, we say we still have a stationary distribution, but we might have some temporal dependence. So, what would be the consequence? Consequent and ensure the block max mask approach, although it did not use the data very efficiently, based quite robust again to that because even though you have a station advertiser, when you do the blocking, those are typically the short-range temporal dependence become negligible. So that is okay. But if you want to do the threatened approach, you need to somehow deal with that. And that is actually if you have seen the reference paper. You have seen the reference paper I include, they spend quite a bit of time to deal with this. So, when you do the threshold incedent, very likely you will violate an independent assumption and the likelihood is not exactly correct. So, how to account for that? One way is so-called the class wing. So, remove the dependence for those thresholds in them. The other one is directly model that. So, there are at least two approaches, but actually that's a third approach. So, ensure. approach so ensure step uh uh temporal dependent is more issued with this approach not this approach uh non-identical distribution uh the solution typically is more uh pragmatic is basically try to build some sort of a triangle value regression model for example if you have a seasonality you want to see how the location scale and shape parameter vary as a function of like a seasonality Like a seasonality, or if you have some other potential predictor, like a CO2 concentration, and all that. So, the typical approach is to extend the model to accommodate a non-identical distribution. Yeah, so which is that here? Yep. Is any question about estimating the uncertainty or modeling the uncertainty on the G? The uncertainty on the G here, the generalized Pareto pollution parameters. So, what we've talked about is very kind of frequent estimates. That's right. Can this all be cast in a Bayesian framework and are there advantages? Yeah, so yeah, because so I own basically I present a frequentest approach, so you can definitely do that in the Bayesian framework, and it will be an advantage if you do have a pretty If you do have a pretty good knowledge to help you to specify the prior distribution, yeah. And a lot of time, so I'm not Bayesian, but I can see a lot of situation that the Bayesian framework could allow you to do things a little bit more natural. For example, to come up with like a predicted distribution to fully account for the parameter estimation uncertainty. Price specification. Price specification in this case is actually, in my opinion, people should pay much more attention than other cases because here, in both cases, we are sort of data limit. So you cannot do a very like non-informative file and pretend that nothing happens. Yeah, so one thing is that if you are Bayesian, you want to be a good Bayesian, but you can still proceed. I'm wondering if there are any questions from the Zoom audience. From the Zoom audience, any question from the online audience? Maybe I should ask, can you guys actually hear? Can I actually see the slide? Yes, we can hear and see the slides. Okay, great. But you guys don't have a question? I would assume so. Any other question? You might have a question at some point, but I guess I would very much encourage you to actually not only so I so for the precipitation analysis, you can actually reproduce analysis, you can also modify that to put. You can also modify that to perform your own chain value analysis for the win speed. Okay, so in the afternoon, I probably will prepare another slide to explain the exercise, but I will say something, what are some exercises you can do? Yeah. Sounds good. Thank you very much.