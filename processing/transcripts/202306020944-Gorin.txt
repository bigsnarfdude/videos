Thank you. Since my talk is the last one, it's time for Laverus. Thank you letters. So thanks, Tima, for giving us an excuse to go here. It is an amazing place. Thanks to all the organizers, including Firas, for making this happen. Thanks for all of you who managed to stay till my last talk. That's always good. Okay, so Phil yesterday already mentioned a little bit about six vertex model and Bit about the six vortex model and its connections to the topics of this conference, but I will start from a slightly different representation for this model, which is maybe historically how it first started. So, in this other representation, you know, the initial point is that to take a grid of atoms of hydrogen and oxygen, so that you have, you should think about oxygens being in the vertices and hydrogens being on the edges of the grid. And then, what you try to do And then what you try to do, you try to take some final domain in this brief, and then you try to match all the atoms of hydrogen and oxygens into molecules of water. So you should have like twice as much hydrogens than oxygens for that, and it's subject to some other control constraints. And then you, you know, that's one possible matching in this domain drawn in red, that's one possible matching into molecules. Molecules. This is called square ice model because you're kind of creating ice molecules on the square lattice, that's why it's square ice. Although it might seem very simplistic, but somehow real-world ice has some features similar to that. So the first rigorous mathematical article to demonstrate that, that was from Lee, from 50 years ago, there he computed some quantities for. Some quantities for this planar model and then compare to the experimental quantity for the three-dimensional ice, something called residual entropy. And somehow it wasn't exact mesh, but it was close enough that it seemed to be a good prediction. Okay, so you can say that's 50 years ago, that's quite a long time. More recently, actually, people were trying to find these really square eyes, like really planar things. And there was a paper of a group of Alexander Of Alexander Game. So, this is the person who got Nobel Prize for inventing graphene. So, between two layers of graphene, they were trying to get the layer of these square ice. And now, they reported that they have found it. It was a paper in Nature, like less than ten years ago. Although later, there were some comments which were claiming that maybe there was some contamination, and what they observed were not really ice. Wasn't really ice, but it was some maybe salt. So, you know, I'm not sure. It's kind of a little bit unclear at this point, but you know, at least people keep trying to get these square ice in the nature. Okay, so what is our interest? So first, you know, another name for this model. Another name for this model is six vertex model. Why is it six vertex? Well, that's because if you look at, you know, local configuration, how it looks like, if you have oxygens connected to hydrogens. Oxygens connected to hydrogens, and there are six ways to connect oxygen to two out of the four hydrogens. And these are these six possible local configurations. That's why it's called six-point model. Okay, so we will be studying Gibbs measures for this model. So what does it mean? So it means that we will assign six weights, A1, A2, B1, B2, C1, C2, to these six types of surfaces. And then given a finite domain, we will look at the Gibbs probability map. We will look at the Gibbs probability measure on configurations of this domain. So we will weight, we will count how many are there vertices of each of these six types, and we will weight the configuration on an A1, the number of vertices of this type, A2, number of vertices of this type, etc. Then we will normalize by the partition margin to make it a probability measure, and then we will be trying to understand something about this probability measure. Well, the simplest, well, the first remark is that Is that you start thinking that there are six parameters here, right? Because there are six leads. But actually, there are not six parameters, but only two parameters, though it takes some time to figure this out. And that is basically because there's quite a number of conservation laws. Well, the simplest conservation law is, of course, that the n the number of molecules is fixed, right? Just equal to the number of oxygens. So if your domain is fixed, the number of molecules is fixed. And this means that if you multiply all six parameters, All six parameters by the same number, then the probability is actually the same. But there are actually three more conservation laws which are a little bit more delicate, but you can find them. And by applying all these four conservation laws, you can figure out that actually the probability distribution like this one, it actually depends only on two parameters. You can write them as B1, B two divided by A one, I two and C one, C two divided by A one, I two. So they're just two parametric family of Gibbs measures, and we want to study those Gibbs measures in various domains. In various domains. Well, simplest example that you can have in mind, adjust if all parameters are equal to one and adjust the uniform measures. You just pick a domain, and you pick a configuration of the model, and this domain you form it at random, and you try to say something about this. But this is related to what we heard yesterday. There are still different types of errors. It will appear, it will appear. Don't worry, it's coming. So, so far in this representation, we study asymptotic properties. And in this representation, we study asymptotic properties of these Gibbs measures. So, really, we will take a domain to be huge. We want to try to understand what's happening. Okay, you might ask which domains? You know, you can draw very complicated domains on the plane, so we will not go into that. We're actually going to start with the simplest possible domain, because it turns out to be very interesting. So, the simplest possible domain on the plane, well, that's a square, right? So, you take n times n squared. Well, you need to be a little bit more careful, you need to explain what's exactly happening at the border of this square. Happening at the border of this square, and the simplest situation is so-called domain boundary conditions. So, you will say that you cut this square in such a way that there are hydrogens on the left, hydrogens on the right, and on the bottom and above there is kind of hydrogen, oxygen, hydrogen, oxygen, kind of internal atheism on this. So, in my picture, this is like one, two, three, four, five times five squared. And you know, this is one possible perfect measure, one possible configuration, but you know, of course, there are many others. Of course, there are many others. So that is our setup. So we have n times n squared. Also, for simplicity, I will take symmetric weights. As I mentioned, I will show only two parameters out of six. So there is no loss of generality by declaring that the weights are symmetric, which means that these A1 and A2 will be the same. Just A, B1, B2 will be the same. This B, C1, C2 will be the same. This is C. So now you have just three parameters, A and B and C. Still up to multiplying all of them by the same constant. Them by the same constant. And so that's our question for today's talk. So you take a random ABC random configuration and n times n square, and you want to understand how does it look like when n is huge. It will turn out that an important parameter will play a role, and the answer will depend on that. So this parameter is historically called delta. This is a squared plus b squared minus c squared divided by 2ab. This is an expression which is similar, in high school, I called, you know, in trigonometry, there is a Hossine theorem. Phenometry, there is a Hossi theorem, I think, something. There is expressions of this type entering to this theorem. Okay, so let's start from some simulations. So there is a simulation which David Keating, our apology of medicine, made for me. So there is a picture for the value a equals to 1, b equals to 1, c square root of 8, delta is equal to minus 3. And the dominant is quite large, so this is 200. So what is shown here? So, what is shown here? Well, I show only C-type vertices. So, this, remember, these are the different types of vertices. I always show you C-types, which are horizontal molecules and vertical molecules. Okay, so you see that something interesting is happening. There is some shape appearing on the screen. Somehow C-type vertices, they like to be inside some region. In the very middle of this region, you know, it's kind of fully only C-type vertices, and here it. Purchases, and here, you know, the combination of C-type purchases and not really C-type purchases. I should mention that rigorously, almost nothing is known about this picture. It's definitely a very interesting picture, but you know, we can't really. It's in a square, right? It's inside a square, right? So all that stuff is like zero in the corner. Well, right, so you know, these are C-type vertices, and there are four other types, right? So there are four other types, they will be in only one, two, three, four, they will be occupied at the corner. Inside, there also will be some number in only why there is some number. Also, there will be some number, you know, y, there is some number of these four types. Right, C-type vertices are there, you need to determine the rest, right, right, right. So, so that is efficiently just some kimonetorics, which you can figure out. Okay, very interesting picture, nothing new, but you know, it is what it is. So, picture for different values of parameters, you might enjoy it. So, a equals to 2, b equals to 1, c equals to 2, different value of delta. 2, different value of data. So previously it was minus 3, now it's close to 0. Data is equal to 1/4. So you see that the picture is different. Some similar picture again, there is some similar features. Again, there is some region where C-type vertices love to live, but another region in the middle disappear, where they're kind of densely bad. So that's a picture which kind of conjectured you should be there, but again, nothing is proven. About this picture, maybe slightly more is known, but still no full description. Still, no full description of what is that. Okay, so we still can't do some analysis of this picture, but you know, for my talk, I will tell you one result about this picture. Namely, we will be looking at boundary limits here. So, we will not analyze what's happening in the middle of the picture, but we will analyze what's happening near the boundaries. So, you know, there are four boundaries for the size of the square, so in this picture, we'll look at. The square, so in this picture, we are looking at these four points, in this picture, we are looking at these four points, and we try to understand what happens locally near the boundary as the size of the domain goes to infinity. You can think that in a sense these are most important points, right? Because really your configuration interacts with boundaries only through these points, right? That's the only way how it feels the boundary. So somehow this boundary limits should define everything. There are a lot of symmetries in this model because you can rotate square, you can flip it. Because you can rotate square, you can flip it. So we'll only be looking at the bottom boundaries because of these symmetries. But again, there is no loss of generality in trading. Okay, so let's go to the third theorem about these boundary limits. This is a joint work with Carl Lichty, the result we should obtain this year. So for now, let's take delta smaller than one, again, delta this parameter a squared plus b squared minus c squared divided by twice ab. And we look at the bottom part of the picture. And we look at the bottom part of the picture. That previous simulations look at C-type molecules, so I'm still concentrating only on C-type molecules. And moreover, I will be looking at only the horizontal molecules, so this one shown in red. So you see, like if you look at the bottom layer and you try to understand what are the molecules there, then you can quickly convince yourself that, you know, it starts with these corner molecules like here, then there is one horizontal molecule, and then it goes with the corner molecules The corner of molecules opposite direction. It's just kubernatural. There is no other way to make a molecule. In this kubernetorial special, you can, if you go up, you can convince yourself that in layer number k, there will be at most k of these horizontal molecules. So you can try to keep track of the positions of these horizontal molecules. One here, two here, three there, etc. But for each configuration, there's just an inequality. So you see that in this layer, You see that in this layer, you know, there is actually only one horizontal molecule rather than maximum or model 3. So the first theorem is that actually in layer K, asymptotically, you will have exactly K horizontal molecules as size of your system goes to infinity. And for these horizontal molecules, K of those, you can get an exact distribution, and that is an object of random AT theory, namely the joint law of these horizontal molecules after you subtract. Molecules after you subtract like some constant multiplied by n, so there is some centering that is sits somewhere here, it's like you know, the position of this point or that point. And after you rescale by some other constant multiplied by square root of m, this will converge to the eigenvalues of the matrix so-called Gaussian unit transsex, which is just, you know, you take a complex matrix with IID complex normal random variables, and you make Hermitian matrix out of it by adding. Make your Mission matrix out of it by adding to its complex conjugate and dividing by two. And you look at the eigenvalues of this matrix, and somehow miraculously, in this documentorial problem, you get this eigenvellial distribution asymptotically on each level k. Now, as we already had an answer question, so I could emphasize again that horizontal molecules actually uniquely fix everything. So, from knowing, for example, what's happening with these molecules like okay, you can reconstruct everything which is happening below. Everything we just happened in below, and in particular, we can extend our theorem to the statement that the joint distribution of everything from k pointer and below, so maybe you know, from k level and below, so maybe you know layer two and layer one together, this is good with an order to so-called GUE pointers process, which is a joint distribution of eigenvalues of GUE matrix and its corners. So, kind of, you know, if you want, you can take this all these cases simultaneously in this theorem, and there is no loss of generality, this theorem would still Normal loss of generality theorem will still hold it. Okay, so there are some previous results which I should mention here. So there is a really new result when you look at all values of data, so all values of parameters. So previous results, they dealt with two special cases. So if data is equal to zero, so if a squared plus z squared is equal to c squared, then there's a bijection between this model and model of tilings or timer model. And for tilings, I'm mentioning what. Model and quotanic remains more true, much you know, many other tools. And you know, in this way, Kurt Jochens and Eric Nordenstau analyzed it 15 years ago at this value and prove a similar theorem. And also at this kind of combinatorial point when all weights are equal and you are looking at the uniform measure. Also, there was a previous result, actually, in my own paper with Bretta Panova seven years ago, which was related to the fact that there are some shoe functions which govern the picture at these values of parameters, and this allows you to do asymptotic analysis. You to do a symptomatic analysis. Okay, so that's then the smaller than one. Now, you know, there are these constants here. So I think for completeness and to demonstrate that Eric should have proved something, I will show you these constants. So when absolute value of delta is smaller than 1, when delta is from minus 1 than 1, then all these constants are encoded in various trigonometric functions. So it's typical in this situation to write the weights in terms of signs of two parameters, gamma and t, and then you write also the And then you write also the answers in terms of the sincing. Various elementary functions, you know, cotangent, tangent, sine, cosine, and we get an explicit expression in terms of all these things. I don't want to bother you with these details of this formula, just that it exists. Well, data smaller than minus one, that's more complicated situation, which kind of was already hinted by the fact that the picture is more complicated. And really in this situation, people tend to encode parameters, A, B, C, in terms of the hypergeometric signs and cosines. And cosines, and you know, the answers for these constants, okay, for this shift, it's kind of easy actually, still cotangents. And then, you know, more complicated functions appear. Unfortunately, you can't write it in elementary functions anymore. You need to also use Jacobi elliptic theta functions, all four of them. And these are the scaling constants in terms of these elliptic functions. You don't need to know that. I think it's more interesting that there exists some constants that have to rescaling. This is some constants, and after rescaling, you know, we get these Gaussian initiatives. Okay, so we have, you know, it's very important that we have data smaller than one here. So, you know, the question that you might ask here, is it just a technical restriction or is it something conceptual and that the behavior of the model really changes when this parameter of data is larger than one? It turns out that indeed that's an important restriction, it's not just technical. So here's a simulation again from David Keating from other situations. Now that is equal to three halves. Now, that is equal to 3 halves. So A is equal to 3, B is equal to 1, C is equal to 1. And you see, the picture is completely different. We no longer see this huge array of C-type vertices. Now, somehow they only concentrate along the diagonal, and there are some small defects around this diagonal. So, really, the system is completely different when the entity is larger than 1. It doesn't prevent us from still studying it, right? It's a different system, but we can see. Starting it, right? It's a different system, but we can still. What? That's weird line determined. Yeah, yeah, no, it's a little bit. Well, then, yeah, but what will happen is that here you will see just one type of vertices. They're all the same. And here you will see another type of source, all the same. So it's kind of very boring outside this one. You don't see the other two types? Well, you will see a little bit of those. You know, there are these defects and they are near those. You will see a little bit of all types. But kind of dominant uh contribution will be just of two types out of Will be just of two types out of the okay. So, well, we still can have an asymptotic theorem here. And this asymptotic theorem actually will be much closer to the topic of our conference. Because you can already see that maybe along this line, okay, maybe there's some interacting particle systems. And there is an interacting particle system exactly along this line. So that's our next theorem. So for delta larger than one, for this picture I took A larger than B, but it doesn't matter, just by symmetry you can get a smaller. Just by symmetry, you can get smaller than B or not. Then the configuration near the corner, like if you zoom in here, converges actually to the model which we saw yesterday in the field's talk, to the model called Stochastic 6 Vertex model. Now let me, before defining the Stochastic 6 vertex model, can you on the next slide, let me emphasize that there is no rescaling here. That's a big difference. So previously we needed to rescale center by something proportional to n, and rescale by square root of n to get the. And the rescale y squared of fan to get the limit. Now there is completely no rescaling. We just, you know, zoom in there, no rescaling, and we see our limit. Now, what are the stochastic 60 vertex models? So that's different types of weights that you can introduce to your model. So previously, you know, I was telling you that we took symmetric weights, A, B, C. Now, for the stochastic 6-vertex model, you make them isometric and you make them very isometric. So you take A1 and A2 actually equal to 1, and then you. To 1, and then you take B1 plus C1 equal to 1, and B2 plus C2 also equals to 1. So you kind of start to interpreting those as some probabilities, because these are positive numbers, which sum up to 1. So these are the weights for the stochastic 6-vertex model. Now, if you compute this parameter of delta again, then just the fact that all is positive implies that the actual delta is larger than one. So this object stochastic 6-vertext model does not exist for delta smaller than one. Exists, but that is smaller than one. And that's one of the reasons why it appears only larger things. It's really larger than this. You know, why is it called stochastic again? So the stochastic, because it's really model of the quadrant, will be defined by so-called certain local sampling algorithm. We'll just sample vertices one by one, which is something which you typically kind of do for general domains. Like if you have a domain, well, it's kind of hard if you think, how do you even, how would those pictures drawn, which I showed you in simulations? Actually, it takes some efforts to find a good idea. It actually takes some efforts to find a good algorithm to draw these pictures. Now, here for the stochastic case, it's actually a very easy algorithm. So, we will start from the quadrant, kind of corner. That's a corner of this picture that we want it to have. So, it has these hydrogens on the left, oxygens on the hydrogens on the bottom, and it will sample vertices sequentially in this direction. So, we kind of will be growing from the corner and in this direction. So, how do we sample it? Okay, we looked. Do we assemble it? Okay, we looked at the types. First, we looked at the type of the corner. So, in the corner, okay, we have three hydrogens, one oxygen. So, actually, only two options here. It's either this molecule or this molecule. And our stochastic weights will luckily arrange exactly in such a way that these two things come up with complementary weights, which you can then interpret just as probabilities. So, you flip a coin with these probabilities and you sample this vertex according to these probabilities. Okay, so you did it. This pro here, this okay. So you did it now. You move further to the next one again. Okay, just another coin which you flip here. Great. Move here, another coin which you flip. You're up here, a slightly different situation now because everything needs to be matched. Now you notice that here there are actually no choices, you know, just hydrogens. You know, these two hydrogens need to be connected to the oxygen. Again, this matches our weights because, luckily, for this vertex, we chose probability equal to one. So this kind of matches the fact that we have no. This kind of matches the fact that we have no questions asked here. And you continue. So always, you know, either you will be choosing between two configurations with probabilities like B1, 1 minus B1, or B2 minus B2, or everything will be coming with probability 1. Here, for example, you need to choose between these two configurations that probabilities B2, 1 minus B2. And you continue. And this way you sample entire quadrant. Entire quadrant with these stofastic waves. So that's what's called stofastic six-verted model. Now we can connect nicely to this interaction particle system which Phil was shown yesterday. This is just a bijection actually. So you just can replace your molecules by configurations of lines. So this first one goes to empty configuration, no lines. Second one goes to densely packed configuration, like there are two lines which touch each other. Like there are two lines which touch each other. This one goes to the horizontal line, this one goes to the vertical line, this one goes to one corner, and this goes to another corner. So with C-type vertex, this is a corner. In this way, your configuration of the molecules turns into configuration of paths, which are allowed to touch each other. And then the stochastic weights, well, these are just saying to you that when path moves, you know, you're kind of making a decision whether it will turn or not. And this decision comes, you know, with probabilities. You know, either B1's B. Either B1, C1, or B2, C2, depending on whether the path was moving up or path was moving to the right. And if you spend some time staring at this model, you can realize that there's a discrete time version of the ASIP asymmetric simple exclusion process, the point of view which also was used by Peel yesterday. Now, what is known about this object, about Stochastic 60 or X model? Like, what was known before our print? Well, that was actually first introduced on Taurus in paper of Boer and Shepon, and they demonstrated. Of Boy and Shuan, and they demonstrated somehow heuristically that this should be in KPZ universality class. That was their prediction by using this model on Taurus. Kind of noticing that there is a way even to create a model which will have these local update rules. Now, in the quadrant, like I need something more recent, we had an article with Alexei Baradin and Ivan Corwin, where we look at the situation when B1 is larger than B2 and proved some low-flight. And proved some low-fludge numbers and some trace-button fluctuations for this particle system. Now, why is B1 larger than B2 important? Well, that's because, now look here. You know, kind of paths enter from the left, and they do not enter from below. So, if B1 is greater than B2, then this means that you have a larger probability for moving to the right rather than for moving up. And that's kind of good, because then you think that your system kind of unpacks, right? So that the paths kind of take. Right, so then the paths kind of tend to go far away to the right, and you expect that there is something non-trivial happening there. And that's exactly the situation which we worked. So there was a tracy video there. Now there was, afterward, there were two other papers. Well, first we had one point result and was extended to two point results by a paper of Yuki Dimitrov. And then there were two other interesting limits in the situation when actually, you know, B1 starts to be close to B2. Starts to be close to be true. So it turns out that you can get KPZ equation out of this. That was a paper of Colorin-Bossel Shannon sign. And it turns out that there is another stochastic object called stochastic telegraph equation, which you can also get out of this by properly rescuing everything that's interesting. But kind of the main object of interest before today was actually really the situation B1 greater than B2. So when all kind of unpacks and gives you non-trivial limit shapes, etc. limit shapes, etc. Now the thing which will be relevant actually for my limit, that's an opposite situation. This is kind of stationary regime when B1 is smaller than B2. So when B1 is smaller than B2, then it means that you move to the right with smaller probability than you move up. So in the end you kind of, the paths do not want to escape away and pests kind of tend to stay close to the diagonal. But that's precisely what we saw in the picture, right? We saw that this action is happening all the close to the diagonal, so that's really All the close to the diagonal, so that's really a relevant regime from the point of view of these stofized six workers. Any questions about the definition of this model? Hill's talk, he took a scaling limit to get to A7. Was this just about getting to? Right, so in terms of this B1 and B2, that is just B1 going very close to 0 and B2 going very close to 0, so that you cannot have mostly only corners, and sometimes you have these kind of defects coming from these lines. kind of defects coming from these lines. That's the limit of A set. How can you describe this as A set? Well, so again, so think about one vertex, right? Maybe here. So you sample this vertex. Now there are two paths incoming and two paths outcoming, right? And okay, there are four options here. Either there are both Four options here. Either there are both paths in Kaimen, or there are no paths in Kimen, or there is one path, or there is, you know, one path on the other side. And you should compare it with ACEP with the configuration of two adjacent particles, of two adjacent, you know, either particles or holes at two latest sites, right? Now, in ASIP, you know, these two things they can swap, right? Particle can move to the right, or particle may move to the right, and left. And this comes with two intensities. But similarly here, like, you know, this particle can either move here or move here, and this particle can move. Here or move here, and this particle can either move here or move here. And again, this comes with two probabilities rather than intensities. Right, so you think about you should think about time kind of going horizontally, right? Time in ASEP goes in that direction, really. Okay. Other questions? Okay, so now we know the definition, and now that's a theorem in full. So there is a formula, some quadratic, we show some quadratic equation, which produces for you Which produces for you the weights of the stochastic sticky vertex model out of the original symmetric weights. And then the theorem tells that you can convergence to this particular situation as you know spice of the system goes to infinity. How many are ending up with that? What? Why is it? Well, actually, you know, what's happening is that you can. You can, you know, remember that measure actually is dependent only on two parameters, right? So you can say that, okay, you can always take any parameters and try to convert it into stochastic weights. And you will have a quadratic equation here, and this quadratic equation will actually have two solutions, but there will be no way to distinguish between those. So you really, you know, it might be this one, B1 and B2, or other way around, B2, B1, right? And you really, we needed to choose by hands that that's a choice, you know, that is B1 smaller than B2. And that's, well, that's what. And that's well, that's what it is. So you said that in this case you have a drift to the left, right? But should I? Well, right, drift to the left. So kind of, well, you kind of have drift to the left, but also there are paths entering on the right, which kind of creates some pressure. And because of that, you know, if you stay near the, so kind of the path just keep making turns. So the situation here it will be just turns and turns and turns. Here it will be just no paths at all. And only in small vicinity of the diagonal that's where some action happens. Diagonal, that's where some action happens. Essentially, you're pushing, the diagonal is pushing in particles all the time. So if in the original one you had two parameters, and the stochastic one, you still have two parameters. Yeah, still two parameters. Then isn't the stochastic a subset of the whole thing, or is it? Right, perfect, of course, yes. So so you can always convert the weights for data larger than one into stochastic weights. One into stochastic reads. I see, but stochastic reads. But only hold data larger than one. But of course, the difference is that, you know, this stochastic thing which you've got here in the limit, it has no boundary condition on one side, right? Originally in the system vertex model, you want to take boundary conditions everywhere. Now, in this limited object, you don't take boundary conditions everywhere. It's only on these two borders. But there you kind of allow the system to propagate how it likes. So if you want, if you now condition also on what's happening after large time, that will be true. Happening after large time, that will be true in a sticks-vertex model with the three thing, but the data marginal models. Okay, that makes sense. And the delta less than one, you can just sometimes convert it? No, never. It just if and only if. You can convert to the stochastic weights if and only if delta is larger than one. Okay, so there are several conjectures that I want to state here. First, well, we believe that for delta smaller than one, the behavior which we find Than one, the behavior which we found is actually generic behavior. So we believe that for any domains near the boundaries, you will only get the zero square root of n rescaled and the GUE as an asymptotic object. So that's the first conjecture. So, you know, so far we proved for squares, but there is a similar result in the world of Bosnia tilings, which we proved to be the more ugly one, where there it says that really for any type of. Says that really for any type of nominates, you have this behavior. So, we believe that there is universality, at least in the situation when delta is smaller than one. In the situation when delta is larger than one, that's actually an open question, what can happen? Because I showed you that here for this particular square boundary conditions, we have conversions to this stationary version of the Stofastic 6 vertex model. But there is an article of articles of Ibn Dimitrov and Mark Pachinovsky, which deal with some issues. Which deal with some infinite domains, so not finite, but we do, but some infinite domains, but they you kind of can interpret those infinite domains as having data larger than one, and there they have GUE as their answer. So it seems that both objects can appear, but maybe there's an indication that there's even more other objects, and we don't really know what's the full range of scaling events which are possible at that quarter in front. Right, right, right. Right, right, right. So it's always the same. Yeah, it's the same object, only rescaling changes. But I mean, you could also get GUE, right, by looking. Everything has GUE in it, right? Right, so the delta smaller than one, right? So this parameter is kind of fixed, right? You fix the model with some ABC weights. So when these weights satisfy that delta is smaller than one, and there is a GUE from asymptotics. And when delta is larger than one, then And when data is larger than 1, then okay, for some domains you can find GUE, for some other domains you can find stochastic 6 vertex. But this isn't a question that now we're looking at stochastic 6 vertex and now we scale and be able to repo that. Well, you can also ask it like that, but if you wanted to state it in terms of stochastic 6 vertex, then you want to put boundary conditions on all sides. Because really, this is a question about the models in finite domains, right? So you kind of condition on having really this domain, not just, you know, some. Having really this domain, not just some part of the boundary of this kind of function. Well, what you can notice is that clearly there is a discontinuity at delta equals to 1. So naturally you can ask yourself, okay, what's happening when delta is approximately 1? And the answer is that at this point we have no idea. So, you know, we don't have any, our methods completely break down. So we use some observables, which just disappear at delta equal to 1. But I can give you just one result to indicate that probably there is an interesting one. That's probably there is an interesting world of limits there. So let's look at the very degenerate case of the six-virus model. Namely, we will take parameter C to zero. So these were these corners, or these were horizontal or vertical molecules. So take this parameter to zero. So in other words, you know, in this query, you just minimize the number of horizontal or vertical molecules. There are still well-defined configurations in the limit, and if you spend 10 minutes. Spend 10 minutes staring at it, you can convince yourself that these configurations, well, there will be actually one resultant molecules per line. That's kind of the minimal thing you can get. And their configuration will just include back permutation. And now what remains out of the A and B parameters? Well, that's a probability measure on these permutations, and the so-called Mallow's measure. So the probability of permutation will be proportional to some parameter B squared over A squared raised to the number of inversions in this permutation. In this permutation. So that's the example of permutation corresponding to this configuration. And you know, you can count inversion, you know, that's what. And this measure, you know, it's easy to get all kinds of limits as an implicit measure. So for instance, you know, here's a simple proposition that, you know, if you rescale your parameters so that as things grow, B and A start being close to each other, then there is a limit in which positions of these molecules will converge to some sort of... Molecules will converge to some truncated exponential distribution with a parameter theta, which can be anything there. So, this shows, okay, there are more limits. And I think it will be extremely interesting if somebody can find all the limits. Here, theta goes to one. Don't have time to speak about the proofs. I will just say that exactly interability is very important. Here is some explicit computation known as the Easter-V-in-Charm determinant, which plays an important role in our proofs. We don't have time for that. So let me just conclude. So we were studying the six-vertext model in n tax n square with the main wall boundary conditions. We found that if data is smaller than one, then there is a unique asymptotic object there, eigenvalues of Gaussian units sample. And for data larger than one, we found this station-stefastic 60 vertex model there. And we expect that for data closer than one, there are more limits, but it's still an open question how to find these limits. And that's it. Thank you very much. Any questions? The Benelux measure and this kind of truncated exponential transition comes up in blocking measures of exponential time. Blocking measures for what? For ASINs. It's possible, because, you know, we are kind of almost in the same regime, right? Because I told you that for data larger than one, really, you know, we have conversions to stochastic 60 or 20. You know, we have conversions to stochastic 60-vertext model, which is a discrete version of ASIP, right? And it is also data larger than 1, because if c is equal to 0, then your thing becomes a squared plus b squared plus b. It's necessary larger than 1, then we will scale it close to. So probably, you know, in ACEP, you know, this parameter, a or b in asep, that's again kind of ratio of the two intensities. So I would think that, you know, maybe when this ratio converges to one, you also get similar things. So probably it's kind of the same world, right? Because this kind of Because this kind of stochastic 60 vertex one on the stationary regime is kind of analog of these blocking measures for TASEP, right? For ASEP. Because really it kind of stays close to the diagonal. It's kind of this blocking measure, right? That's the only interesting feature is that you become reversible measure. Probably it's the same. We don't use anything of that, but I would think that, you know, really I come to treat this stochastic 60 word small as just a description. Fast 6 works model, just a discrete version of ACIP. So, whatever is true for ACIP, I expect to be true for this model as well. Now, the only difference is that this model has more. On top of ACIP, you can find other behavior, but all the behaviors you have on ACIP, I would expect that you can find Mr. Classic 600. I might have another question. So, I guess from the square ice model, it is a sample from the Gibbs measure. Somehow, I guess if we take the But somehow I guess if we take the limit to the stochastic sphertex model, I guess with the southwest boundary, it seems like it's sampled locally. Is there like intuition or behind this? Well, I can tell you a posteriori. Like if you already know that it should be stochastic six vertex model and deliver. And you can convince yourself that's what it should be because you take a finite domain and you change the weights to turn it into stochastic six vertex. Because I saw that there are just two permetic memory of weights, so you can always change the weights. Ephemera of weights, so you can always change the weights. So you kind of demand that you start with stochastic 6 vertex, but then the stochastic 6 vertex, so it's kind of local, but you know, what is about boundary condition? Well, we kind of still condition on the fact that we need to return back. All paths should return back when they go up, right? Because that's our boundary condition, right? So that's all paths should eventually go up. But now, if you believe that, okay, there is this. But now, if you believe that, okay, there is this stochastic six vertex in this kind of blocking situation, you can say, okay, it probably mixes after some time. And then, if it mixes, then what's happening after a long time should be kind of independent on what's happening there. And then the fact that you're kind of conditioned on coming back, well, probably shouldn't matter for what's happening at the beginning. And this way you can say, okay, this locality that kind of survives. So, you know, locality is not affected by the fact that we kind of conditioned on something. Kind of conditioned on something happening after a very, very long time. But interestingly, you know, that will work only for data larger than one sense. Smaller than one, I can, there is no way to save locality in the moment. Further questions? Yes. Uh so um what kind of limit process do you see, like if you don't look at K t row but you know go microscopy to limit Microscopically? So if you go microscopically down, then this is GV corners process. So this is just. Yeah. So if you look at the joint of these and these, this is eigenvector of the 2 times 2 matrix and then eigenvalue of the 1 times 1 corner of this matrix, so just matrix out. Similar there. If you go macroscopically far up, that's where I have no idea, because that really starts to be a question about, you know, we're proving that this picture is like. You know, we're proving that this picture is like that, and that is very open. I have no idea how to do it. Thank you. This is the last talk of the conference, and I think we can all agree on one thing: that it's been a very productive and enjoyable five days. It's gone by too quickly for me, actually. Let's give the organizers a special round. There's a special bad thing.