Okay, uh yeah, thanks. So this is more of like a sociological talk than a research talk, I guess, but it's on some stuff that I've been thinking about a lot over the last several years and would like help thinking about. So this is, yeah, work with a lot of people over the past and then in particular some undergrads and when units where we've been having a lot of fun working on some of, you know, like. Working on some of actual research projects. Yeah, so the setting is the following. We've already seen this estimator in the past, but let me just put it up here again. So we have a random vector where each entry is mean zero, and then the covariances are all, oh, this should say, yeah, this is wrong. It should say they're zero if they're the off-diaponal entries and one if they're the on-diaponal entries. And so if you have this, then This, then in expectation, these quadratic forms give you an estimator for the trace of whatever matrix is in the middle. And so this is colloquially called Hutchingson's trace estimator. And I think it's like, to me at least, one of the coolest things in randomized and regular linear algebra because I can understand it easily. I feel like all these other methods are very complicated, but this one, you know, the whole thing is there in one line. Yeah. So, So, what I really got interested in is why this is called Hutchingson's trace estimator. And in particular, this is the abstract from the Hutchingson 1989 paper, which everybody cites. And in it, this doesn't leave there? Yeah, they referenced this other paper by Gerard from like two years earlier. And in that paper, they have a very similar estimator. They're using a different distribution for the entries of V, but like. Distribution for the entries of V, but like in effect, it's almost identical. Yeah, so this is what sort of motivated most of what I'm going to talk about now. And I think it's still sort of like an interesting question of just like, how did this happen? Yeah, and so if people are more familiar with the history from back in that time, I'd be interested in talking about that. Okay, so people started using the estimator in the 1980s. Using the estimator in the 1990s. And then, at least from the numerical linear algebra and theoretical computer science perspective, I think people started studying concentration after, you know, like the 2010s. So even the early works, like Hutchinson has a pretty detailed analysis of the variance of the estimator. I think Gerard's paper also has that. And you can write down the variance, and so from like Chebyshev's inequality or something, you can get some sort of concentration. But often, what we would like. Concentration, but often what we would like is a bound of this form where those question marks are maybe something that depends exponentially on epsilon. So when epsilon gets bigger, the probability of failure gets like exponentially smaller. So here are sort of like a bit of the history. I think this Aberon Toledo paper is sort of the first paper where they proved like strong concentration inequalities, and then it was improved in 2014, and then subsequently over the last several years, really. Subsequently, over the last several years, really, there's been some new work using like sub-gamma concentration inequalities or handset right inequalities to get even more fine-grained control over the tails. I thought this was interesting. They put Hofding in it to analyze this and to show exponential concentration, but I think there's some issues with their proof. So, like Hofding, you need bounded random variables on some interval almost surely. And they were estimating those intervals after they had the random variables. Intervals after they had the random variables. So there are some issues, but at the very least, people are thinking about concentrations stronger than just these variance bounds. Sense basically the outset. Okay, so some of my research recently has been trying to understand how to use these algorithms for studying quantum systems. And yeah, one of the sort of big examples of where people care about. Of big examples of where people care about computing the trace of matrices is computing the trace of matrix functions of Hamiltonians in quantum mechanics. And so you can use this to study properties about quantum systems using classical techniques. This is my understanding of stuff at a high level. So feel free to interrupt me if I'm saying something wrong. I'm not entirely sure that this is the right way of thinking about things. But in quantum mechanics, self-enjoy. Mechanics, like self-adjoint operators are called observables, and all observables have that form. And then we can view the state of our quantum system as a vector, more generally, as a sort of matrix. And when we observe, you know, our observable, what we get is one of the eigenvalues of the matrix. Like quantum mechanics is inherently random. And so when we do some sort of measurement, we're not always going to get the same thing. And so we'll get out one of the eigenvalues. We'll get out one of the eigenvalues, and the probability of getting the eigenvalue depends on the state of our system, how aligned it is with the sort of eigenvectors of the observable matrix. And so from that, you can sort of imagine if we did this experiment a lot of times, we get some sort of distribution, and we can ask what's the expectation value of that distribution. And so, if we just take like the value that we got out times the probability of getting that value and sum over all of the probability. Getting that value and sum over all of the possibilities, then we get this expectation, which we can write as either like this quadratic form or as a trace of a matrix times, like of our observable matrix times this other matrix. More generally, we could have our system described by a density matrix. So, I think, although I'm not entirely sure that you can informally think of this as that the system could be in any of the The system could be in any of these different states with some probability. And so now, when we measure our observable in this state, then we can think of first like taking a conditional expectation over which of the b's we're in, and then doing this other expectation for 4. So, again, what we sort of get is like this trace of our observable time-sum density matrix. And so, yeah. And so, yeah, some of the work that I've been doing is trying to understand how to compute this efficiently when A is some matrix function of some other matrix. Yeah, so where does this concept of typicality come in? So, a natural sort of state to look at would just be the identity state, so the outer product of any independent orthogonal set. And the reason for that is you're just kind of not prescribing any sort of bias. Having any sort of bias in one direction is the other. So if you want to just study the observable, this is like a very natural thing to do. I think there's like philosophical arguments in physics, at least in the past, about whether you should or can't do something like this, but at least from my perspective, it seems reasonable. And so if we take just a random state, a single vector from the unit sphere, then the expectation over the random state is the The expectation over the randomness in our choice of the vector, not over like the randomness for quantum mechanics, gives us exactly this quantum expectation value that corresponds to measuring our observable in this sort of like uniform state. Okay, so this is essentially the idea of quantum typicality, where if you just look at a random state, it sort of gives you the same thing as if you looked at this sort of uniform state. Do they consider other, like, because I guess like these could be fixed or random? Do they consider other distributions, like something but with some noise around it? I don't think, well, yeah, so this typicality stuff, they do, this is like the most basic form of typicality, and what they actually care about is something like a lot more complicated than this. And I'm not, you know, I spent a long time trying to understand. I've spent a long time trying to understand exactly what they're getting at in terms of linear algebra. I'm not really sure what it is, but at the very least, for this, I think they're less concerned with the particular distribution of there and more just with the idea that a random state could somehow be representative of this state of all of the so people wanted to start understanding this. Wanted to start understanding this more theoretically. I think even from the beginning of quantum mechanics, people realized some of these ideas that, like, when we have these random states, there's some kind of concentration or that they'll be an unbiased estimator for what we're interested in. So, there's some papers by Schr√∂dinger von Neumann way back at the beginning of quantum mechanics where they outline these ideas of typicality. Then, in physics, at least. Then, in physics, at least, they kind of went out of favor a bit in the middle of the century, and then more recently started to come back where people realized that, in fact, what was in those original papers was right, and we can prove it. So, yeah. In particular, I think what's very interesting is that in these papers, they show typicality, so that they show that these, when you measure in the random state, When you measure in the random state, that's the same as measuring in this identity state with pretty high probability. And they have some Gaussian concentration inequalities, maybe five years before what we would normally think of as the first of these kinds of bounds. But I think they're less concerned with constants, so they kind of prove this one result. They're like, it concentrates, and then they're done. They didn't really come back to it. Okay, this idea again is like very old and so its use in algorithms is also old. There's a recent paper. Yeah, there's a recent paper here called Random State Technology, and it's a survey of the use of typicality in algorithms. So, like using random states to initialize your algorithm essentially and get some sort of estimator for. Some sort of estimator for what you hope to actually measure. I'd say, even in the context of trace estimation, a lot of these papers are basically implicitly doing trace estimation. They don't actually write down the trace explicitly with a matrix, but if you look, it's a sum of stuff multiplied by these terms of your random vector, basically. So it's pretty much the same. And the problems they're solving are very similar to the ones that people are using trace estimation for now. Trace estimation for now. So I think this paper, the oldest one they could find, was this one from 1975. I've also not really been able to find algorithmic work on these estimators, at least in physics, prior to this, although I'm sure that people had these sort of ideas for a long time. Yeah, and then recently there's been a lot of work to improve the variance of these estimators. Of these estimators. So, in particular, for most reasonable distributions of v, the variance is closely related to like the Frobenius northern matrix. It can vary a bit depending on your matrix and the estimator, but like that's a rough estimate. And so, a simple idea is the following. We can decompose our matrix into two pieces: one, which is A tilde, which we're going to try and compute deterministically or like exactly, and then the other bit, which is the residual, which we might hope to. might hope to estimate using this estimator. And if the norm of this second term is much smaller than the norm of the original matrix, then our variance of this algorithm will be reduced. Again, assuming we can compute the first term exactly. Okay, so there's this recent paper, Hodge Plus Plus, where they show that you can kind of do this in a certain way and improve the estimate of just like the standard estimator, which is the Monte Carlo method necessarily has a 1 over epsilon squared dependent to 1 over epsilon. Square dependent to one over epsilon. I think that was sort of, it became very popular because it gives a very clean analysis as well as like lower bounds for this one over epsilon dependence. And then related to this, it's actually quite interesting. This idea also is quite old. So this is that original paper by Girard in 1987. And in it, I mean, I can't read the French, so I'm not exactly sure what it says. But like, this looks pretty much like the same idea, right? This looks pretty much like the same idea, right? Like they're splitting A into B and A minus B, and then using this variance reduction technique, assuming you can compute this B. So this is the extent of what they did in that paper, basically. So it's like this idea of splitting into something where you know the trace and then something where it has a lower variance is, I mean, this is really the first paper in the numerical analysis literature that is doing these kinds of trace estimation things. And even here, Of trace estimation things. And even here, this idea was there. Similarly, in the physics literature, I think this has always been sort of understood, informally at least. And in particular, this is a survey paper on something called the kernel polynomial method. Stuff in this paper keeps getting rediscovered. And yeah, so that's quite interesting. They also, in an offhand comment, basically suggest that you can do this kind of variance reduction by deflating the matrix. Okay, so here's maybe a bit of a summary with some key references. The columns give the different communities, and so in particular, note that like computational physics and these theoretical physicists carrying a typicality are actually in separate lines of research, and they also seem fairly unaware of each other's work until very recently. And then I would classify like NLA and maybe TCS as sort of the same thing. So, yeah, in numerical linear algebra, we have sort of starting in the late 1980s and early 1990s, these trace estimators coming out. From the outset, the variances were understood, and then concentration was proved later, and then lower approximation and analyses of these sort of like methods were very recent. I think in physics, at least in computational physics, they don't care about proving their algorithms work, and so they're kind of known all about. Algorithms work, and so they've kind of known all of these things for a long time, but maybe it's just like they know that they're doing it, they know everybody's doing this, but don't really make too big of a deal of it. And then this typicality stuff, I think, is really coming in from the beginning of quantum mechanics. And in particular, they have like mathematical proofs for concentration in at least 2006. So, I mean, I said that I would give opinion. Opinions of my own in my abstract, but I didn't really come to anything. So, what I wanted to do is just ask and see if the audience had any thoughts on these questions. So, I still don't know why it's called Hutchings' estimator. I wasn't able to discern that. It's interesting to ask, like, could these lines of research have intersected earlier than the last several years? And would that have made any difference in terms of the development of algorithms? And then, also, just like what other things are we And also, just like what other things are we overlooking in the literature, which is probably already solved and we might be spending time on now when we don't need to. So, I'd be curious to hear people if we have time. Otherwise, if we don't, we can close. Thank you, Tyler. We do have time for some questions or some answers to Tyler. I don't have an answer, but I can add another. I remembered this and then I sort of forgot to look it up, but I can add another paper there listed on this. So, Yang Le Kun, who's like this big deep learning guy, has a paper from 1990, I think I use Spectra, using random, essentially randomized trace estimation. He's working with the theoretical physicist, but it feels very disconnected from like, I don't think he referenced Hutchinson's work or anything like that. So, we can also add machine learning to the communities that came from. Yeah, so sorry, I was a minute late, so maybe you already covered this in the beginning of your talk, but sort of what's a reasonable set of ground rules about how much access you have to the matrix A? Oh, yeah. So in these, what you usually assume is just matrix vector access. So I pass in a vector to some black box and get out A times my vector. Are there any other interesting ones? Are there any other interesting things besides that? Well, I think people have also studied where you pass in one or two vectors and you get like a quadratic form like u transpose a z out or something. And people have proved lower bounds in both settings. I think other people know better than me, but I think when you pass in the two vectors and get out a scalar, the epsilon squared dependence is the right one. And then if you pass it as one vector, it's one of them epsilon. Go on to that because I like matrix and oral models. So if you talk to different people. So, if you talk to different people in different domains, you'll end up hearing people explain to you what access they have to their matrix, and then they want you to make a matrix vector oracle for that model. So there's a few different flavors of this. One of them is like, you're allowed to submit, and they all roughly, the ones that I've seen have the rough flavor of like, it's still a matrix vector oracle, but you're not allowed to submit an arbitrary vector X. Now the vector X has to have some structure. It has to be at least this sparse, or it at least have lacronic or structure, or something like that. Chronic or structure or something like that? Any speculation about why it's called Hutchinson? To your first point, actually, I don't know if Avron and Toledo, they might have just called it stochastic trace estimators or problem. They call it all Hutchinson's. Well, okay, yeah. So, like, I was trying to look a little bit last night. So, at some point, people are calling Hutchinson's estimator the ones with plus minus ones, and other ones the ones where it's normal. ones the ones where it's normal but at some point that kind of got lost i feel like but that might be a very recent sin like that might be yeah like i don't know i can't remember but they even yeah i mean even then if you look at like the number of citations on the hutchinson paper it's nearly a thousand i think and the number of papers on the the number of citations on the charred paper is like much like 50 or something i think so it's like yeah even if people were sort of precise at some point like the hutchington one was the one that sort of deemed possible One was the one that sort of deemed popularity. At least in our community. In physics, I think there's other influential differences. There's a paper by Steeling, which did all of this. I think one thing is people who went to grad school in my generation had Joel's surveys and Scheinin's book. So we tend to think of plus-minus one Gaussians as the same thing, because it's all sub-Gaussian. And then I wonder if at the time it was less obvious that there were. At the time, it was less obvious that these were so similar to analyze the Gaussian case in plus minus one. I thought also part of the point was that the plus minus one of the variance is like a constant smaller or something. Yeah, it's not quite that, but it is better in some ways. You basically don't pay for the diagonal of the matrix. Yeah, so there is a difference in the variance. Do you have any approach to tackle the third question? What else? Yeah, the third one I think I think about just reading. And the third one, I think if people just read the papers they're citing, I'm sure for these Hutchinson things, people just like see, oh, this person cited Hutchinson in 1889. We're going to cite it. If you read the paper, you don't have to go very far to realize there's other stuff on this. And likewise, like that kernel polynomial review. If people read that in detail, they would have realized that a ton of problems that people thought they were solving were already just standard knowledge. Yeah, you're going to have it. Yeah, you're good. Anything else? Anything?