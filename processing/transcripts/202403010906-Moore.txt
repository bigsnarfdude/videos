Let's get going. Okay, everyone. It's a it's it's always a huge treat to have Chris Moore speaking, especially on talk word. So uh Chris, take away. Thank you. So some of you are the world experts on this problem, some of you are not. I'll try to be a bit pedagogical. You know, if I'm going really slow, do some rude gesture like this, you know. Rude gesture like this. All right. So this talk is about this thing called tensor PCA or the spiked tensor model. And what does that mean? It means you observe a noisy tensor which is some sql-to-noise ratio times the outer product of some vector with itself, p. Of some vector with itself, p times. P is maybe the arity of the tensor, the number of indices it has. And if you're not used to tensors, I'll try to write this out a little bit more. Plus a noise tensor, which we'll be guessed. So for illustration purposes, I will generally take p to be 3. And so what this means is that for each ij and k, yijk is lambda times pi. is lambda times Vi, VK, VK plus Wyv. So that's what this notation up here means. And I mean, you know, this looks, if P were 2, this would just be the spiked matrix problem, where we would have lambda times the outer product of V with itself, in the usual sense, in matrices, plus a random matrix with Gaussian noise. So, n here will be the number of dimensions. All of these things live in from 1 to n. And let me tell you what scaling I'm going to use here. It's not about how to write intergestion from brain. Puzzle. Puzzle. Okay, good. And, you know, we need a prior for V, and I'm going to take V to be uniform on the sphere. To be uniform on the sphere of radius squared n, and we need a distribution for our noise. And I'm going to take all of these entries to be independent and normal, except, and if you're an expert on Gaussian random matrices, you know this, on the quote diagonals where some of these indices are the same, the variance is a bit bigger in order to make the whole thing invariant under orthogonal rotations. So, like, if you So, like, if you know about GOE Gaussian orthogonal ensemble matrices, you know that the variance is twice as big on the diagonal. And usually, you don't need to worry about this, but so. Okay, and so we have the usual problems. Detection slash distinguishing slash hypothesis testing which is: can you reject the null model? Where y is just the noise, and there is, in fact, no spike or signal. That's that. And then there's reconstruction or estimation or recovery, which is, you know, estimate v. And what I mean by this is find some vector w such that Such that the inner product squared of w and v is bigger than zero. So that we have some angle between w and v bounded below 90 degrees. Good. So this has become kind of a classic problem, and it came from, I'm not sure. People seem to credit Richard and Montanari with writing it down. It down. But it's also closely related to a model of your Bristolian physics because since these things are Gaussian, what's minus the log likelihood of getting y from b? Well, since this is Gaussian, it's just going to be one half times the different squared. The difference squared between these two things, which is going to be, oh, and then when you multiply this out, there'll be this term, it'll be y squared, there'll be v by itself raised to the 2p, or rather it's like squared raised to the p. And what will matter after we remove those constants? What do you call removing a constant of a constant? Removing a constant with an equality size. Anyway, after you remove a constant, you'll get the inner product of y and v tensor with itself p times, which is lambda times the sum over, for instance, if p is 3, all ijk of y ijk pi pj pk. Oops, like zi vj vk. So again, I'm not sure sort of if people have seen stuff like this before, but if there were only two of these, we would call this an Isaac spin glass. But there are three of them, or in general, P of them. And physicists call this the P-spin model. And it's of deep and abiding interest. And specifically, we would call it the spherical P-Spin model because rather than Spherical piecemeal model because rather than the computer plus or minus one, there are vectors that live on the spherical. And so lots of people have studied that. So specifically, though, this is an example of a planted spin glass, a planted P-spin model, because this thing here, which physicists would call the coupling tensor, is itself correlated with a particular V. Correlated with a particular V that, in some sense, is the ground truth or the planted state. So, thinking physically, if we sampled from the Gibbs distribution with this definition of energy, you know, this is this call minus the probability of the energy. We like to minimize energy instead of maximizing things because rocks fall down. I know it's early, but I'm trying to get you to smile. Smile. So, you know, one might hope that, depending on how strong this signal is, again, lambda is the signal-to-noise ratio, one might hope that if we could sample from that Gibbs distribution, that a typical state would be fairly close to V. The maximum likelihood estimator we would call the round state, that is not going to be equal to V, but it might be close to it. Might be close to it, and a typical draw from the Gibbs distribution, or if you prefer, from the posterior, that's the same thing, would be a good estimator of V. But this will probably depend on this thing, which is like an inverse temperature. And so, you know, the hotter the system is, the smaller this is. And so there's this classic physics picture that you've heard from people like Florant or maybe Linda and others that when the system is That when the system is too hot, V over here doesn't really have much effect on the energy landscape. As you cool down the system and increase the signal-to-noise ratio, there starts to be sort of a dip here. But this doesn't necessarily, you know, since this is like on a logarithmic scale, this is like the log likelihood, the probability. The probability goes down exponentially with the energy. This isn't necessarily going to show up when m is large because most of the Gibbs distribution would still be here around things that are not correlated with V at all. And at some point, what we call the free energy of the stuff near V crosses and gets lower than the stuff around generic vectors that are not correlated with V. And now most of the posterior. And now, most of the posterior distribution or Gibbs distribution would be concentrated around things that are close to being. But at this point, things still might be algorithmically hard because you still have this energy barrier to get over. If you start here with some Monte Carlo algorithm, then it might take you exponential time to hop over here and find the good states. If you're running a message passing algorithm for many different initial conditions, there might be an exponentially small fraction of initial conditions for believing. Of initial conditions for belief propagation or whatever, which will lead here instead of there, and so on. So, this is the physics picture of this hard state, this hard regime. And then finally, when lambda gets really large, somehow it looks like this, there's no longer a barrier, and then lots of algorithms, including various local search algorithms and perhaps low-degree polynomials, will be able to find the accurate state. Yeah, it's okay. So, right. Good. So, all right. So, first of all, if p is 2, again, that's the matrix case, we know everything we want to about this problem. And because the noise matrix itself has Matrix itself has this classic semicircle distribution of eigenvalues, the Wigner distribution, and which, by the way, was originally used to model the eigenvalue spacings in atomic nuclei. Anyway, there's my story there. And so what we know is that when lambda crosses in this particular scaling one over root n, then suddenly, even though And then, suddenly, a new eigenvalue appears above this bulk, a discrete eigenvalue, and the eigenvector associated with it, w, is indeed a good estimator of v. And we even know exactly how this accuracy increases as we go above that lambda, that critical value of lambda from zero to one. One. Okay, so for the matrix case, we know everything. For the tensor case, the situation is more complicated. And in particular, there is a statistical computational gap, which is exactly this conjectured cardinal gene that I was describing before in physics terms. And that is that when lambda is below n to the minus p minus 1 over 2, again, use the. Minus 1 over 2, again, use the array of the tensor. Then, below here, by which I mean kind of significantly below here, the problem is information theoretically impossible. And then there are now multiple algorithms known, polynomial time algorithms, which succeed when lambda is significantly above n to the minus p over 4. So various spectral. So, various spectral algorithms, which I'll allude to. And then, in between here is this, between, in this gap is where this conjecture card machine is. Okay, how many of you already knew everything I just said? Okay, that's good. Good. We still enjoy hearing it. Yeah. Oh, good. Okay. I prefer when you say it. All right, so notice these are the same when p equals 2. They're both n to the minus 1 half. Because they're both n to the minus a half. And yeah, but for a larger p, there seems to be this gap. So I'm going to write, so conjectured threshold, algorithmic threshold for the easy-hard regime algorithmic threshold is where lambda goes as n to the minus p over 4. I will mention, and I hope I get to connect to this, that something really beautiful happens below this threshold. So there are, what we know below this threshold is that we have low degree hardness, that no, you know, no constant degree or logarithmic degree polynomial will do a good job at detection, for instance. Detection, for instance. But you can still solve the problem, you just need order, as you go up here, order n to the epsilon, where epsilon increases as we go below here. And that gives us algorithms with running time about 2 to the n to the epsilon. So this is very pretty. You know, you have like exhaustive search will work down here, 2 to the n. Polynomial time will work down. To the n, polynomial time will work down here, and between there's this interpolating set of sub-exponential time algorithms. And this, yeah. By the way, since this is a board talk, I'm going to take the excuse of not carefully writing out all the references. If you feel slighted, just make some sort of gesture and I will call upon you to, you know, I so reviewing our. So, reviewing our reviewing one paper this morning, I believe this was shown by Patrick Prolu et al., and then again by Hawkman, Sarter, and Xi, and then again by maybe one of your papers, and again by my paper with Alex and Poppett. I think it was like quite for a little, and then we kind of got into Taish at the same time with SOS algorithms. And then later, like. Then later, like, analyses come up, like, little degree. Yeah. Okay, good. Yeah. Okay, so, I mean, this algorithm that I described here before, just find the highest eigenvalue you can and look at the corresponding eigenvector. Well, we call that PCA, right? That's what that algorithm is. So let's just do the same thing on tensors. Indeed, this is why this is called tensor PCA. What's the eigenvector? What's the eigenvector of a tensor, though? What is an eigenvalue of a tensor? So there are a number of definitions, but none of them are right for our purposes. So what are we going to do? What do you mean by remote for your purpose? For my purposes. What about the vector that maximizes the key form off the singular vector? Why would you like that one? That's a thing. That's a thing. I mean, that might work. But that's the MLE. Yeah, I mean, that's the MLE. Yeah, you could call that an eigenvector if you want, but there are typically, as I learned from Tim, there are typically many of these and not just one. And I don't think we generally know how to find it. There's this notion of tensor power iteration. We definitely don't know how to find it. Wait a second. You're saying there's more than one optimizing. So it's in and out. So there's one optimizer. So, there's one optimizer. There's many, there's like exponentially many sort of critical points of an eigenvalue type optimization problem generally, not for the maximum eigenvalue, but for arbitrary eigenvalue. So it's not like a nice spectral. But there is a maximum answer. Yeah, and it would be good if we could compute it, right? Okay, but if you want efficient algorithms, I agree, it's not good for you. A spectral. I mean, yeah, that's a thing. We can call it an eigen thing. thing we can call it an eigen thing I don't want to just go yeah yeah yeah let's we're good okay so so since we don't know what the I'm gonna erase this before you erase it I'll just mention that in hard division because there is we do have plant leaking compartments precisely this generating model ah even even with these non-sparse vectors yeah yeah for some reason nobody knows this yeah Knows this. Yeah. Oh, that includes me. That's my kid. Can you send me that? Yeah, it's in this like 200-page, 20-page. Well, that explains why no one knows it. Yeah, fair enough. Yeah, well, please send that to Provident Providence. Yes. So where are you going? Are you going to display this some exponential diagnosis? Maybe. I'm going to take you somewhere beautiful. And and I so the first thing I'm going to do is reprove this using a radically new method, actually radically old. And it's simple and beautiful, which is good because I want to do beautiful things and I'm only capable of doing simple things. So I think you'll like it. Now, um Tim and Alex, by the way, this is joint work with Tim Triniski and Alex Mongol. Work with Synchronisky and Alex Mongol. Sorry, I should have said that a little earlier. And so I'm going to reprove that, but we hope that our methods can prove things that we don't already know as well. Although, as you all know, I mean, new proofs of old things are often very valuable, even though the computer science community doesn't always take that long. That's a publication strategy. Anyway, okay. All right. Good. So that's the piece in mind. Good, so that's the P-Spin model. Okay, so one thing we can do is flatten the tensor into a matrix or squash it. Okay? So like you could take this, you could take, you know, y, i, j, k and say, oh, I hereby interpret i as an input to this tensor and j and k as an output, and this is now an n by n squared dimensional matrix. My n-squared dimensional matrix, and I can multiply it by its transpose and look at the eigenvalues, or equivalently look at the singular values and singular vectors of this matrix. Multiplying by its transpose would look like that. And this works. And there's a generalization of this called tensor unfolding, which was proposed by Richard and Montanari, and then you guys proved that it works. You guys proved that it works, right? I think that was Sam and David had dropped the tool. Okay. All right. There was like HSS and HSSS and HSSSS. When they added me, we did something else. It was like about getting linear time audit. All right, okay, okay. So yeah, you can do this. So you can say, I love linear algebra so much, and I'm so committed to algorithms involving eigenvalues and eigenvalues. Algorithms involving eigenvalues and eigenvectors of matrices, that I insist on turning this tensor into a matrix. Okay, sounds a bit harsh, but yeah, you could do that. And one nice thing that was used, and this was your paper, was this interesting thing, where we put two copies of the tensor here. And now we define a matrix, an n squared by n squared matrix, that looks like this. And what this means is a sort of This means is a sort of partial inner product where we sum over this index internally. Now, I want to point out to you that there's a disease ravaging our community where people say things like, oh, this is a vector. And, oh, by default, my vectors are row vectors. And so this is, now it's a column vector. So this is an inner product. And this is an outer product. This is worse than type 1 and type 2. Worse than type one and type two, right? This is crazy. Moreover, how in this awful notation? Who does this? I think many of us have been guilty for time. How in this horrible notation would you express this anyway? You'd say, well, take the sort of transpose of the take the, you know, take in take instead of y times y tensor times y transpose take y transpose times y and then turn it 90 degrees no you can't do no can't do it um so this is symptomatic also of this this flattening approach so it's just wrong and um you know why should we take a beautiful tensor with its independent Its indices floating freely and squash it into this. I mean, it's like, you know, it's like taking an octopus at dash. Good, good, good. So you already see here what I'm starting to do, which is introduce this graphical or diagrammatic picture of functions of a tensor. Functions of a tensor. So in this picture, let's say this is a matrix. Here's the trace of the matrix. You know what I mean? What's M transpose? Let's say, good point. Let's say M symmetric. Here is the trace of M squared, which of course is N times the second moment of the times the second moment of the eigenvalues. Here is the trace of m cubed, and so on. And you know what I mean. Am I supposed to know this already or are you making this up? I mean... This is a new notation or? No, no, no. What this means... Okay, what this means, this is a tensor network. It's a little one. And all the tensors of degree 2 because it's just a matrix. But what this means is you put an index on each edge. You put an index on each edge, and then each vertex corresponds to that entry of this matrix. And then whenever you have this closed edge here, you sum over that index. So this is the tricks of MQ. Does this only work for some edge matrix? Do we have directly? I just for some work for it. Very good. Very good. If these matrices were not symmetric, I would have to tell you kind of which court each of these edges is coming into. But notice my tensors are permutation symmetric by intent. Oh, I forgot to say, sorry, I forgot to say this noise tensor is symmetrized so that if you permute, you use indices, it stays the same. And then this is fairly permutation symmetric. Permutation symmetric. Also, if I had complex numbers living in my matrices, I would need to put arrows on the edges because the inner product of two complex vectors, you know, you have to tell me which thing is being conjugated when I sum. Okay, so, but these are all real vectors, and all my tensors are permutation symmetric, and so I don't need to tell you. And so, I don't need to tell you either of those things. I can stick to undirected graphs. Now, I could wave around words like monoidal category, but that would be rude. But let me just point out, right, how many of you have received, or worse, have given this exercise to your students? Be honest. All right, but you were given it, right? There is nothing to prove. There is nothing to prove because this is A and B happening in parallel and then C and D happening in parallel, which is the same thing as A and A C and B G happening in par happening in parallel. Happening in parallel. It is only our pitiful one-dimensional brains trying to crush this object into a one-dimensional string of symbols that would make us think that there is anything here to prove. There is something that these diagrams use, like commutativity or also state points. Because it's a monoidal category, yes. Yeah, so for instance, you know, doing A first and then B on a And then B on a separate set of variables is the same as going the other way around. Yeah. That's all you need. That's the only problem. Well, this is like A tensor the identity, and this is the identity tensor B. Do you give that as an exercise? No. Well, I mean, do you want to? Adam loves exercise. All right, so now, what sorts of things could we do to a tensor? To a tensor. Well, what about this? So I put a copy of Y at each of these places. I put, you know, indices here, I, J, K, L, M, N. I know I was using M for something else. Who cares? And then again, what does this mean? It means Y sub I J K, Y sub K M T Y sub K. Y sub, I don't know if I really need to write this all out, but as n i l t. And then we sum over all of these indices. And by the way, if you really want to impress people, you can say, well, let's use the Einstein summation convention, which says whenever an index appears twice, meaning that those are the two endpoints of an edge, we automatically sum over it. Then you could say further, well, because these vectors are Well, because these vectors are real, there's really no need to distinguish contravariant and covariant indices as we would in general relativity. And people will think you're really, really smart. Okay, so this is a quantity. It is a quadratic polynomial in the entries of the tensor. Overhead. It is quadratic because each term involves the product of four entries of the tensor. Entries of the tensor because this diagram has four vertices. Quartic. Oh, quartic. Sorry. Yes. Yeah, there's some weird historical reason why we... Anyway, yeah. Yes, Quartic. Thank you. There's a weird historical reason why we use Quartic instead. No, why we use Quartic? Well, I guess, never mind. Alright. So yes, it's Quartic. Sorry, I'm still a little punchy. Alright. Good. Now, so these are. Now, so these are spectral moments of the matrix M. We might call this, you know, we might call 1 over m times the third power of the trace, the trace of the third power of the matrix. We might call that the third moment of its spectrum. We might even call that M3. Well, little m and big m don't have anything to do with each other, like little m since remote. Sorry, this M stands for moment and the other M stood for matrix. All right. Well, you know, one slogan that we had when we were starting to write this paper was, well, we don't know what the spectrum of a tensor is, but we know what its spectral moments are. Okay? I mean, these are diagrams of matrices, so let's put diagrams at the center. So let's put diagrams at the center of our point of view. And then this thing could be called like the moment of this tensor corresponding to K4, this free regular graph. And maybe it's really these diagrams, these graphical moments that matter. Okay. I guess I should use the rest of the board at some point. Okay, so let me point some. Let me point something out. You all know that the trace of a matrix and also a trace of the matrix powers is basis invariant. So you know that if you take a orthogonal matrix Q, this is the orthogonal group of n by n matrices. That's not the usual big O in computer science. Hopefully you'll be able to tell the difference by context. You know, if you know that if you send a If you send a matrix to its conjugate, changing the basis and the inverse of an orthogonal matrix as this transpose, you know that none of these quantities will change. Well, a similar thing is true for tensors. So first of all, what do we mean by conjugating a tensor? Well, this conjugate here looks kind of like, I mean, this orthogonal matrix is not symmetric. So let's... Symmetric. So let's call it this, you know, with directed edges to indicate which index of Q we're applying here and which one we're applying there. Then reversing the edge is exactly the transpose. And notice that if you took this matrix and rather than viewing it as a linear operator from one vector to another, you viewed it as an n-dimensional vector. This is the same as putting This is the same as putting q tensor q on both of these indices. So to change the basis for a tensor, it means attaching a copy of q heading outward from every vertex. Well, what happens though when that meets this one? When that meets this one, they annihilate. Because this one is now Q transpose. Just as you would get this cancellation between QmQ transpose times QmQ transpose, all the inner pairs of QMQ transposes would become the identity. So in fact, just like these things, any diagram like this, any one of these graph moments, is invariant. Is invariant under orthogonal basis changes. Now here's a lovely fact. All polynomials which are invariant under orthogonal basis changes of a given degree are linear combinations of graph moments whose size is less than or equal to that degree. So this may or not be obvious. I can actually I can actually prove it to you if you like, and proving it to you would not be a bad thing because it illustrates an operator that will come in handy later. It's actually a very old result. It goes back, you know, it's one of these things called invariant theory. And it goes back to like Harman Weil and lots of classic stuff from the early and mid-20th century. So it's intuitive, but I mean. So it's intuitive, but I mean, you know, you probably already know that the only basis invariant polynomials of a matrix are things like this and their products. They're things like, oh, let's have the trace squared times the trace of the third power. So this is a product of moments of eigenvalues, so it's a function only of the spectrum, and indeed all polynomials which are rotationally invariant are. Uh, rotation invariant are linear combinations of such things. Yes? I'm a bit confused now. So when you do a change of basis to a matrix, it means like you're doing some orthogonal transformation to the space. But now you're applying it to a tensor. So what does that mean? But what do those arrows mean to avoid? Well, I mean, so you can view a tensor, just as we can view a matrix this way or vectorize it that way, I mean, you know, you can view a tensor as many different kinds of. As many different kinds of operators. You could view it as a cubic operator from vectors to scalars, numbers. You could view it, as we did before, as an n squared times n or n times n squared matrix. You can even view the whole thing as an n cubed dimensional vector. It doesn't care, right? It doesn't care how you view it. It's the same tensor. And And so then this basis change really does rotate the vectors that would be coming in along here. So if you define the function, you know, t times w xy, where these are vectors, then we really would be rotating the w xy. That's always just like a shorthand for putting another vertex and putting a keyboard here? Not another. Well, I mean, yes, you could call this. Well, I mean, yes, you could call this a little vertex of degree two because it's a matrix. So if you view it as one n cube dimensional vector, this rotates that one vector instead of? But with a product rotation, you would rotate it by q tensor, q tensor, q instead of one big and q dimensional. Also, if you rotate a rank one tensor, right, that's just rotate, like if you rotate d tensor p like this, that's just the rotation of d. That's just the rotation of the tensor V, right? And then extending wheel linearity, nothing other bit. Yes? So it's just to interpret what you just said. So, in principle, you can also have this fighter of graph moments, right? Nothing restricted. Just look at before. But you're saying that if it's orthogonally invariant, it's like degree to like, it's a graph, not a hypergraph. Very good. I'm glad you asked that. You could ask, well, what about something where some index appears? Where some index appears three times. No such thing is basis independent. So it's really these things that the indices are connected to each other pairwise that are. Okay, good. So now that I've told you that, by the way, let me just give you sort of one. I'm not sure if I should do this or not, but. Not, but there are multiple ways to see this. One is to think about this. Actually, let me not go there because I only have 45 minutes left. Okay, so good. So now, so what are we hoping to do? We're hoping to say, well, we're going to reason about low-degree polynomials. We're going to reason about low-degree polynomials using these diagrams. So now that we have this characterization of low-degree polynomials, oh, okay, sorry, first. This is a spherically symmetric prior. The noise is also orthogonally invariant. Therefore, I claim that among the optimal algorithms for this or this, you know, the best test statistics for detection, there are There are, you know, if there is a polynomial algorithm with a certain performance, there is also an orthogonally invariant polynomial with the same performance, just average over all possible rotations. By the way, what does a reconstruction algorithm look like? It looks like a tensor network with one edge hanging off, like this, for instance. After all, what is power iteration? After all, what is power iteration? It's something like this. If this is the thing you want to power up, like t times t transpose, I know I wasn't going to use that kind of horrible language, but anyway, so then you plug in some v0 vector here, and you take high powers, and you get out some estimator of w there. So, in other words, power iteration is still in this family, except maybe this question of what's the initial vector. vector. But by the way, you could do this, which some people call a partial trace of the tensor. So this would start you out with the vector which is the sum over i of t i i j, and then you could power that up. That might be a good idea. And by the way, the thing we had before with this, this is like powers of this eight, this, this tensor. This tensor, this m squared by n squared matrix is this railroad track type diagram. Okay, good. So now what do we want to know about these graphical moments? Well, we have this structured model, which I'll call P, and we have this null model we're trying to reject, which I'll call Q. That's just the pure noise model. And as we already have heard, what we want is to understand some. What we want is to understand something like: Is the difference in one of these graph moments, the difference between the structured null model large compared to the standard deviation of that same quantity in the null model? And if so, then Chevy Sub's algorithm tells us that it will be a good statistic for distinguishing the two because it will be like way out here. Here or in expectation. Only with one type of capability or that, right? Only one of the two types that we always mention together. Right. One-sided error. Yeah, one-sided error. That's good. So this is the kind of thing we want to understand. So let's look at our friend the K-4 moment. And let's, as a warm-up exercise, As a warm-up exercise, let's compute its expectation in the null model. Okay, so again, that's where all of these vertices are just noise. And well, the thing about the independent normal variables is that they have mean zero. And so if you imagine expanding out this sum over all tuples of indices and then taking And then taking the expectation over the Gaussian measure, the only terms in this sum which will make a non-zero contribution are those where the same indices, where each index of w appears an even number of times. So let me draw this a bit bigger. Here is one such family of terms. Let's put i here and Let's put i here and also put i there. And let's put j here and also put j there. And I'm going to fake you out. Let's put k there and l there. Well, this is w ijk, and this is also wijk. And both of these are ijl, ijl. So this family of terms gives us the expectation of wijk squared, wijl squared, and in expectation each of these things is just one, and so the expectation is the number of such tuples. And because I have four different things here, then the number of such tuples is essentially n to the fourth. But you can see that this is based on a kind of perfect matching of k4, and there are three such perfect matchings. So, in fact, in the limit that n goes to infinity, the expectation in, we have computed that the expectation of m sub k4, sorry, of w is 3 times n to the fourth, plus smaller terms. Plus smaller terms. That's a little mysterious. And you might be, you're already. The smaller terms come from like carbon, I mean, picking other labeling. Exactly, with even fewer, with three or two or even just one distinct index. So that's just like L is equal to K or something all. Yes, exactly. It's the restriction of this sum over six different things to the ones where L equals To the ones where L equals, yeah, what you said. Okay, good. Now, so you're probably already thinking, oh, you know, this looks like some kind of funky maximum matching problem. Indeed, it's a sort of edge coloring problem where you want to maximize, you know, it's an edge coloring problem where every vertex must have a partner with the same neighborhood of edge colors around it. It and where you want to maximize the number of different colors on the graph. Well, so what's the leading term here? So think of the assignment of distinct indices to these edges as colors, colorings of the edges of the graph. Because each entry has to appear an even number of times, every vertex must have a partner with the same colors of the edges. Colors of the edges incident to it. But to maximize this power, you want to have as many different colors in the graph as possible. So this doesn't appear to be in the usual compendia of empty-complete problems, but Tim showed that it is empty-hard, given as a function of the graph, even just to compute that exponent. And by the way, we conjecture that this coefficient in front of it is sharp p. Coefficient in front of it is sharp p-hard because it looks like it's something about counting maximum matchings, but we don't know that yet. That's fun. But it's also a sign that maybe something's wrong. Maybe we don't actually want to use this object quite like this if we're going to have all sorts of really hard problems involved in even computing expectations and variances. So now, to give you a hint of what might happen next. What might happen next? Let's suppose again f is this graph moment, this was the expectation. What about the second moment? Well, a beautiful fact about these tensor diagrams, again, is that the expectation of the square of this K4 moment is the expectation of this moment. You know, with just two disjoint copies of the graph. That's nice. Now, so what could happen? Now, so what could happen here? Now I have a larger graph, and I again need to label the edges so that every vertex has a partner. Well, there might be a bunch of ways to do this, but one of them is to match each vertex with its counterpart on the other side. And yes, I know this particular graph has a bunch of automorphisms, so that will just be some constant. And so, but now if we do this, this can be ijk, this can also be ijk. This can also be ijk. So we just copy the indices over to the other side. And this tells us that this expectation will be at least n to the sixth, because I have six edges that I can label independently and just copy those indices over to the other side. But again, things are complicated because for some graphs, there might also be contributions to the second. Also, be contributions to the second moment where some of these partner matchings are internal to the graph, and some of them cross over. And there are even graphs where the leading term comes from some weird mixed thing like that. So that's a little frustrating. So now, how can we fix these problems? So it's a fun combinatorial problem, but it also looks complicated. Is there a way to modify these moments? To modify these moments so that it's easier to control their expectations and variances in the planted model and the null model? Well, the answer is yes. And we noticed that, and this was not really original with us. A number of people, including Alex in a recent paper on tensor decomposition, have said, well, let's restrict this sum to where. To where these indices are distinct. Okay? Did I do this like I think people probably did this in 1850? Okay. You know, could you? I love citing 19th century papers, so if you could send me some. Okay, yeah, I made it in 1850. Sorry. Yeah, I made up the 1850. Sorry, I'm not trying to steal any of these thunder. Self-avoiding logs. Yeah, it's self-avoiding logs. I mean, when you look at the definition of graph matrices from these SOS lowerbounds, papers, they also have to do this. Okay, good. So this is not a new idea. And by the way, one of the things that you all did in this paper was you said we need to project away the symmetric subspace. And that's the same as saying that this index has to be different than this one. Almost in a way that we'll expect in a moment. Okay, so notice what does this do by requiring that these things be distinct? Well, first of all, in the null model, the expectation will be zero because there's no way for any entry of the noise to repeat anywhere except with this diagram, which we call Frobenius. Which we call a Frobenius, the plural is Frobenii. High-energy physicists call this the melon. Fewer syllables, but less fun, I think, to say. So here, the problem is that even if these are distinct, well, I would have yijk times yijk. So this is a mildly modified version of the Frobenius squared. Okay. But anyway, Okay. But anyway, for any other diagram, or any diagram which is not merely a disjoint union of copies of this, the expectation is zero. That's good. Also, what does it do to the second moment? It prevents any internal matchings, so that the only matchings are from vertices on one copy to vertices on the other copy. And so that means that it will be exactly, and by the way, if you think about it, And by the way, if you think about it, such a mapping defines an automorphism between the two. That gives us some nice things. It tells us that, for instance, in expectation in the null model, oh, that's why I called it Q, right? Okay. In expectation in the null model, it's not my conjugation matrix. Then if you have something K sub G with, you know, let's say G. You know, let's say G has A vertices and B edges, and since G is P regular, this is B is PA over 2, then in that case, the second moment here will be some constant times n to the b, the number of edges. Some other nice things happen, which is that the covariances, if g and h are non-isomorphic, and again, let's ignore the Frobeniuses for now, this is zero. So this is starting to look attractive because now we have, if you define an inner product by taking the expectation of the product of two functions in the null model. Functions in the null model. Now we have what looks like an orthogonal basis for polynomials. But there's a fatal flaw. Well, not a fatal flaw. Somebody tell me what the problem is. Lost basis invariant. Exactly. Saying that the indices are distinct is not a basis invariant thing to say. Saying that a matrix is zero on the diagonal is not a basis invariant thing. On the diagonal is not a basis invariant thing to say, for instance. Well, let's just symmetrize it then. So, if we define this distinct index thing, we like to write it, we're open to other notations, we like to write it with a little exclamation mark. So, this is the distinct index moment. Well, now we define kappa g of y. g of y as the expectation over all orthogonal matrices of the distinct index moment of, you know, this is our notation for the tensor conjugated by Q, the copy of Q out all its indices. Well, this is now automatically invariant, but what happened to these other nice properties? I'm running out of time. It's very sad. Out of time, it's very sad. So, let me try to tell you a couple of things before I. Okay, okay. You don't five seven minutes. Okay, then I'll go back to my plan of telling you everything. Yeah. Okay. So it turns out that this symmetrization does something beautiful, which does not. Which does not upset these properties too much. So, what does it do? So, let me take a graph like this, or actually a smaller one. By the way, I should be saying multigraphs. We have no objection to multigraphs. They can be useful. Consider this little thing, for instance. Let me write this as two matrices. Two matrices up here, and then combined with an object like that, bear with me, because I mean what's happening is what ought to happen. Let's view this as an n to the sixth dimensional vector. What is this? It is the n to the sixth dimensional vector where it's the sum over all, you know, e of. know EI tensor EJ tensor blah blah blah up to well let's call these I1 I2 up to I6 and it's the sum of all of these such that I1 equals I2 I5 equals I6 and I3 equals I4. Okay, so we call this a matching vector. It's just a sort of vectorized indicator function for being consistent with this matching. If you take the inner product of this with this, If you take the inner product of this with this, which just means gluing these two things together and putting these up there and contracting, you get that. Why do you get that? Why do you get that? Because... Oh, I see, I see. Sorry. Sorry. I see. I see. Yeah, yeah. Yeah, okay. Now, suppose we put a random orthogonal matrix, the same Orthogonal matrix, the same random orthogonal matrix on all of these wires. This is our symmetrization. So now we get to use a little bit of the representation theory of the orthogonal group, which is always a good thing. So what happens here is if I take the expectation over this, over again, I'll see. Over again, that's the orthogonal group. I get a beautiful object, which is a linear combination of things that, you know, that look like this. For instance, is that a good example? That's not a good example. Okay? Things that look like this. In other words, this is, okay, so this is actually an outer product, if you will, of one matching vector with a different matching vector. Matching vector with a different matching vector. We're going to replace the row of cubes with this. Right. So what you get is a sum of a whole bunch of these. And for each term in that sum, what this does when you put it in here is, let's actually put that one in. It rewires the diagram. But the nice thing is that the coefficients of this sum, which the coefficients are called the Which the coefficients are called the Weingert function are dominated on the diagonal. So the more you rewire, specifically the farther away you are in the number of swaps between one matching and the other, the smaller the coefficient is. So the dominant term comes from the unrewired thing. You know, the thing you had before, by the way, this is also. By the way, this is also how you can prove that any orthogonally symmetric function is a linear combination of diagrams. And so these, basically, these things which we call free cumulants, for a reason I'll mention in a moment, are not so wildly different from these distinct index matrices. I guess another way you could put that is that if you projected these distinct index, sorry, distinct index moments into the space of Into the space of symmetric things, a lot of their normal elements survive. Bot means in L2. Yeah. Okay, why do we call these free cumulants? Because of connection with free probability theory for matrices. So I maybe don't really have time to say. I maybe don't really have time to tell you about this from scratch, but free probability theory is the study of: suppose I have a matrix A with a known spectrum, okay, a symmetric matrix, and I have another matrix B with a known spectrum. Both are n by n matrices, and n is large. And then I want to know something about, well, what spectrum will I get if I add these matrices together? Will I get if I add these matrices together? Well, I mean, if they share the same eigenvectors, then we just add up the eigenvalues. But in general, we know that's not true. The eigenspaces of B will be misaligned, will be incoherent with those of A. So one way to view this is, well, suppose that I first took a random orthogonal matrix and conjugated B by that random orthogonal matrix to spin it around so that in some sense we know as little as possible about its eigenspaces. Possible about its eigensteness. And then I looked at, then the marvelous thing is that when n is large, this object has a typical spectrum, which is a function of the spectrum of A and B. And that is called the free convolution of the spectra. And there are these things called free cumulants, and there's non-crossing partitions, and there's all sorts of beautiful stuff. There are things called free cumulants with the property that Cumulants, but the property that they are additive. Okay, so this is meant to be imitating classical statistics. You know, your classical cumulants of a distribution are additive when you take the convolution, when you take the sum of those two random variables, taking the convolution of those two distributions. Here, this kind of interesting non-abelian sum where you spin things around in a high-dimensional space randomly, these are the analogous. These are the analogous cumulants that are out of it. Our cumulants have this property even in finite n. And in fact, this property holds exactly, which is rather nice. And there's a limited amount of work on kind of finite-dimensional versions of free probability. There are papers by Spielman and others. So we think there are connections with those ideas. But, um, okay, and I'm not quite done. Any question? Yes. In exchange for my question, giving them a long time. Thank you. This fact about. Twice as much time as it takes for you to ask and me to answer. We'll bargain afterwards. This additive property for the accumulants, I guess your exact additive property of the accumulant comes from the fact that you have required non-like indices to be indistinct. To be distinct. It's the world for matrices as well. Yes. Okay. In fact, it seems to be less widely known than it ought to be that if you define a sort of distinct trace of a matrix as a sum of distinct indices like this. Sorry, a distinct index matrix power like this. Like this, which is exactly a kind of self-avoiding walk that asymptotically when n gets in the limit of large n and we divide by n for scaling, this gives exactly the free cumulative, in this case the third order free cumulative. Okay, so you just like short-circuited the limit by just requiring systems? I mean, I don't have time to show you, but there's a beautiful proof involving those matching. A beautiful proof involving those matchings that this is exactly additive. So it's equal to the free cumulants to leading order. The free cumulants are a kind of asymptotic object about a limiting distribution of eigenvalues. So it's those to leading order, but it gives us kind of a finite answer. Right, these other not maximum subquality. Okay, good. So now let's go back. So now let's go back to our low-degree calculation. Now we have 40 pages of machinery, now we'll have a two-page proof. So let's start by asking about a single one of these cumulants. First of all, what is its expectation in the planted model? Well, I just Well, I just told you that if you add one thing to another thing, and they might as well be randomly rotated with respect to each other, the cumulative is additive. I also told you that in the null model, where we just have the noise, the expectation is zero. That was true even before we symmetrized. So the only non-zero contribution to this comes from the all-spike term. This comes from the all-spike term. What's the all-spike term? Well, what does this mean? It means that you have an outgoing copy of this vector V coming out of each edge, V V, V V V, and so on. When these meet, you get V inner V, or its inner product squared, which is Or its inner product squared, which is n. So again, if g has a vertices and b edges, this is up to a small correction term because, yes, the indices are distinct. n to the b times, oh, we also get a copy of lambda at each vertex, that scaling factor. So it's lambda to the a times m to the b. What's the variance in the What's the variance in the null model? What's the second moment since the expectation is zero? Well, again, we have this disjoint unit of two copies of this, except now because of symmetrization, they do get rewired to some extent. But the leading order term comes from the identity rewiring, which doesn't rewire them at all. And so I get, again, essentially. Again, essentially, this n to the b because each of the b edges can have a different index. Are you still with me? Almost there. Okay. So for a single diagram, the question then is, is this mean much larger than the square root of this variance? Ah, but b is pa over 2. So let's divide both sides by n to the b, and then b is p over 2, so this is minus pa over 4. Let's take the ath root of both sides. Boom. So when I mean you use that additive property of the three part code. By saying that in expectation, the this is actually overkill in the case of Gaussian noise, but by saying that the expectation of a cumulant of this is the expectation of this plus the expectation of this, their cumulants, and the expectation of the cumulant here is zero. There's a little tweak, a little centering trick that you have to do for if you. trick that you have to do for if your graph contains some of these. Interesting. Okay, alright. Okay, so good. Now, this thing for ordinary graph moments was actually already pointed out by Urfelli, Rivasso, and Tambazusti. Rivasso is a physicist, and this was in a AAAI paper a few years ago. But we want to control all polynomials, which are linear combinations. So, doing this, showing you the single diagram. This showing you the single diagram doesn't work, it is inspiring, but doesn't help. So turns out that the best thing you can do is have a, if I allow you diagrams of up to size A, A here is the degree of our polynomials, because that's the number of vertices. It turns out that pretty much the best thing you can do is have a sum of all Of all the graph cumulants of that size, all p-regular graphs of that size. Why is this a good idea? Well, you know, these things are also roughly independent, roughly uncovered. So, you know, by averaging over a bunch of them, we can hope to reduce the variance. And when you do that, let's say we have, let's say we give them each equal weight, so the expectation will be the same. So, the expectation will be the same. We have to think about the lowest eigenvalue of this Grand matrix, which is again related to that Weingarti function. We can control that when n is sufficiently large. And what we have on the bottom is, again, n to the square root of, well, what's the variance going to be? If we average them, it will be this divided by... This divided by the number of them, right? So the number of random regular graphs of size A. And so this will then be multiplied by the square root of, sorry, divided by the square root of the number of random regular graphs of size A. When A is bigger than a constant and you work out the combinatorics of the number of p-regular graphs of size A, you get exactly the same interpolation of sub-exponential time algorithms that we've already gotten several times in the past. We discovered this the day before yesterday. So, just to conclude, I mean, you know, this whole landscape picture. You know, this whole landscape picture that I started out with, I know that for P-SPIN models, it might be more about getting started rather than there really being a barrier. But I think there is thought to be a barrier. This is why, for instance, approximate message passing does not work all the way down to this point. It's believed that tensor power iteration does not work all the way down to that point. These are sub-optile algorithms, which is why Alex and Ahmed and I wrote our cookie sheet paper. Cakushi paper to reunite statistical physics and computer science. But there's nothing in here about a landscape hump, which is why I'm so interested in the Franz Parisi paper, because that does connect, start to connect low-degree hardness with the shape of the landscape. But it seems like this ends up being just this combinatorial thing about the graph being deregular. And I think that's really nice. So I don't know. So, I don't know how many other expressions like that can be explained in this combinatorial way. All of this is, yes, only for spherically symmetric priors and spherically symmetric related models. So, yes, there is a big distributional assumption here. On the other hand, there are a lot of priors where we think this is the out where there is also a low-degree threshold here. And maybe we could work that, do something like that with that too, with more fancier diagrams. Thank you very much. Diagram. Thank you very much. Alright, so here's a proposal. So instead of running to checkout and hustling back here,