And unfortunately, we have something in 1D, and I will keep coming back to issues or challenges for higher dimensions. So first I want to talk about what I consider as congestion. We've had some congestion talks before, but I have some pictures of describing different scenarios. Okay, so this is the Shibuya scrap. This is the Shibuya scramble crossing where so many people are walking around, they're trying to dodge each other. I'm at that corner, you're at that corner, we want to go the opposite way, but we don't want to bump into each other. And here in this middle picture, we have the growth of a tumor. So at some point, it starts off in this very small volume, but as it's growing, as it's expanding, okay, it doesn't just stay confined to that volume. It hits a sort of maximum density saturation. Of maximum density saturation, and I will come back to this term of sort of a maximal density, and it expands. Okay, so it grows outward. And on a completely different, but I guess relatable topic, this is a picture of what I suppose the 405 looked like in 2018 in LA near Thanksgiving. So you see this sea of cars all trying to, everybody's trying to get their turkey, and it's a nightmare. And it's a nightmare to drive through. So, what am I trying to argue that all of these have in common? Well, I want to say that congestion sort of describes all of these effects. And what do I mean by that? Well, let's maybe start off with this sort of traffic here. When I mentioned maximal density, you want to think about density of cars as number of cars per unit area. Okay, so obviously we don't want the cars to collide with each other. And how do I express this in terms of a density? I want to say that the density of cars should always be less than or equal to, let's say, one car per square meter. I don't want to consider a model in which I have a million cars per square meter. That's very bad. That's how accidents happen. So what I would like to do is I would like to model this using ODEs. This is using ODEs and then eventually a PDE, subject to this density constraint. And that's the content of this talk. So I want to start off. I'm going to confuse one dimension and higher space dimensions, but let's just say that you have n particles. They live on the real line. So maybe they're n cars. They live in one lane. They are evolving under some velocity. Are evolving under some velocity given by a potential phi. So normally, if I don't bother prescribing any constraints on them bumping into each other, I would just say xi dot equals negative gradient of phi xi. Okay, so that's a very standard gradient flow. But I'm including this purple term. And okay, I'm not going to describe too much what they are, but what you should think about them or how I interpret them is that they are Lagrange multiples. I interpret them is that they are Lagrange multipliers. And I include them because I want them not to overlap. Okay, so in the same way, I don't want a million cars per square meter. These Lagrange multipliers are making sure that there's only one car per square meter or something like that. Okay, so that's the role that these lambda i's are playing. And then down here, these are in the language of optimization, the complementary slackness conditions or the KKT condition. conditions or the KKT condition. So here I'm saying that the particles are ordered in such a way. So xi plus one is the position of the i plus one particle, xi is the position of the ith particle. And I'm assuming that all of them have a common radius of r. So I want them all to be at least two r apart from each other. And then these the other two things are, as I mentioned, the complementary slackness. So, this is what's happening, and I will refer to this as the microscopic dynamics. So, when I have n particles. On the other side, if I sort of zoom out, okay, so I was on the helicopter that sort of looked at the freeway and Thanksgiving around 2018. I look at this sea of cars, and instead of keeping track of each individual car and their trajectories, I'm going. Car and their trajectories, I'm going to keep track of the density of all of these particles. And that's this equation that I'm writing here. And I'll refer to this PDE along with these conditions as a sort of macroscopic PDE. So I'm thinking about rho as the density of my particles, but I'm also thinking that this models the same sort of congestion behavior in the slide that I previously drove. Okay, so normally, if I didn't care about congestion, if I allowed for, you know, For possibly a concentration of my density, then I would drop this purple term. So I would have dtρ equals divergence rho gradient phi, very much mirroring what's happening at the microscopic scale. But the point is that I have this now plus Laplacian of the pressure in order to enforce that rho is going to be less than or equal to one. And as well, you should interpret p and this other thing. These are complementary. This other thing, these are complementary slackness conditions. I think some people in the literature don't, they don't really like that terminology, so I'll refer to this as the saturation condition. So this P is, we call it a pressure, but it's similar to the lambdas from the previous slide. It's playing the role of a Lagrange multiplier to make sure that the density stays bounded, bounded by one in particular. Okay, so those are the two models that I'm considering. The two models that I'm considering. And what I would like to know is: in what sense, when I send the number of particles to infinity for the microscopic model, do I get this? And what does that even mean? So that's referred to as the mean field limit. And I will be rigorously justifying that. So as an overview, in the next couple of slides, I'm just going to talk about sort of where this fits into a bunch of different areas. A bunch of different areas, each having been influenced in different ways from optimal transport, which is quite interesting. Then I'll talk about, after the literature stuff, I'll talk about what we were able to prove. And sections three and four really are, I'll try to keep it high level without getting into the details, but this section three and section four, I'll try to just describe the Describe the method without getting too much into the details, I hope. If you're intimidated by this number 36, please don't be. I intend to skip a non-trivial number of those slides. Okay, so one thing that I want to mention is, let's go. Okay, so within this sort of differential equation method of modeling congestion, we've already seen top. We've already seen talks with the word congestion in them, and here this is a different sort of, if you like, perspective. In the literature, usually congestion takes the form of two different types. There's soft congestion or hard congestion. So you can forget this formula if it intimidates you, but what I just want to emphasize is that we are considering what's called hard congestion with these differential equation models. So, hard, what that means is that What that means is that I'm imposing a penalty of plus infinity to some sort of energy function if the particles overlap. Okay, so I am strictly forbidding that particles could ever get closer than a certain width from each other. On the other hand, this notion of soft congestion is saying, well, okay, we're going to allow possibly particles to overlap with each other, but if they do, they're going to incur some very large penalty. Some very large penalty. And that's one way you can add that is here. I took the xi dot equals negative gradient phi xi, which was coming from the previous, from the from the model that I wrote down originally. And you can add this interaction term here, where this k is such that if the distance between the particles is less than two times the radius, it's something very, very large. And because you want to think of it as imposing a large penalty if Imposing a large penalty if they overlap. But this does not forbid. So I'm not talking about soft congestion, I'm talking about hard congestion. Okay, now I'll talk a little bit about the gradient flow structures. So I think we're all familiar with sort of optimal transport and Wasserstein gradient flows, but I just want to quickly do a comparison between what's happening in the unconstrained dynamics so that I can better set up what's happening presently with. What's happening presently with the constraint dynamics? Okay, so I grab my smooth potential pi, and I consider this energy which sends probability measures with bounded second moment to the integral of pi d rho. This is very classical. And then, of course, as you all know, the two-Wasserstein gradient flow of this is just the macroscopic equation that I wrote without the Laplacian pressure term. And of course, we can interpret this as the two-Wasserstein gradient. Interpret this as the two-Wasserstein gradient flow of the energy. And we know a lot of nice things about that. So this goes back to the AGS book, Ambrogio Gigli-Savare, auto in 2001. Okay, but now one way of thinking about the PDE that I wrote is that we take this functional e, and what we instead do is we define what I call here e bar. So e bar is going to be e provided that Provided that rho is bounded above by one, which was precisely one of the density, the maximal density constraint that I had earlier. If rho, the L infinity norm of rho is ever strictly bigger than one, then I assign an infinite penalty. And I'm going to ask what is the two-buserstein gradient flow of this. And as it turns out, based on these references, so this is N1 camera. In Mon Kim, Anthony Melee, and Yijing Wu, you get stuff like this. And there's a lot of literature on this as well. I'm going to just briefly go over this because I think this was very well explained by Alpar in his talk. You can also think about what I'm going to be talking about as the incompressible limit, so the m to infinity limit of the porous medium equation. So he already gave a much better overview of this. A much better overview of this. So, yeah, refer to, I guess, his talk for information about this. Right, okay, so that's the sort of gradient. Yeah, sorry. Sorry, using what using the same energy. What do you mean using the same? So, I'm not doing porous medium at all. doing porous medium at all i'm or or if you like i'm i'm thinking of porous medium as m equals infinity so i've already taken the limit i'm i'm not taking i'm not sending any limit uh okay so what is uh mean field limit i informally described it as you know the the rigorous process which connects what's happening at the microscopic dynamics when i send the number of particles to infinity okay and i just want to go through uh what that And I just want to go through what that connection is all about. So I have n particles, they're subject to this sort of ODE. And I want to know when I send n to infinity, how do I get this macroscopic EDE? And the connection is really in using, classically, uses this gadget, the empirical measure. So what you do, let's say phi is smooth and everything. So you take the solutions x. So you take the solutions xi corresponding to their initial condition. You're going to construct the empirical measure. So this is 1 over n, and then the superposition of the Dirac's charging mass at each of the positions of the particles. And then this measure exactly solves this continuity equation where the initial condition is the same thing, but it's charging mass at the initial particle. At the initial particle positions. And the point is that this, using this empirical measure, is classically the bridge between the microscopic dynamics and the sort of macroscopic dynamics. But that only explains, if you like, how to solve a continuity equation with empirical measures as the initial condition. If, for example, If, for example, you wanted to consider, let's say, dt rho equals divergence rho gradient phi, where now my initial condition, what did I call it, row superscript. If this was not an empirical measure and I wanted to still get the same sort of formalism where I send n to infinity and recover this, how are you going to do that? Well, this slide is going to tell you. Slide is going to tell you how. So, let me try and explain sort of what's going on here. The first block that I have here is, if you like, just a very weak form of the law of large. I have a probability measure. What I can do is, if I take IID samples of this, then I will know that when I construct what I called rho n0, the empirical measure charging mass at each of these II. Charging mass at each of these IID samples, then when I take n to infinity, I know that the one-Wasserstein distance between the empirical measure and this is going to go to zero. So the empirical measure is going to well approximate this. What does that do for me? Well, if I think about what's happening at a later time, then this very classical result by Debruhshin says that the von-Wasserstein metric of the distance between the empirical measures. The distance between the empirical measures at time t with the true solution of this, it's bounded something like that. Okay, so if you like, putting these two things together is what's typically done for these sort of mean field webinars. I should also say, if there are any questions, please just interrupt. All right, so now I want to get closer to the discussion for congestion. For congestion. So in 2011, Maury and Vanel, and then that Maury et al. is Maury Vanel plus Rubnev Trupin and Santambrogio. They published a bunch of papers establishing the well-posedness of the microscopic system. And in some sense, you can think of this as almost the starting point. So we could not quite use this very. We could not quite use this directly as a black box. We had to give a little bit of an improvement. But, anyways, the point that I want to emphasize here is that they have a well-posedness theory for these XIs with the Lagrange multiplier. I'm highlighting the 2R, the radius, because in the mean field limit, when I send the number of particles to infinity, there has to be a certain scaling between the radius, the particle radius. Between the radius, the particle radius, and the number of particles. Because if I just kept the radius constant, then the particles will just fill up the entire space. So it wouldn't really be, it wouldn't really lead me to anything exciting. I've stated this here. I've written it with a bit of confusing notation. So I've written it in a very one-dimensional setting because that's what we're going to be focused on. To be focused on, but what they were able to do works much generally in higher dimensions and for general velocity. So, not just coming from a potential, but they also considered interactions. We can also handle interactions, but I am not, for the purposes of keeping the slides compact, I'm not going to consider this. I'm only going to think about the velocity equals minus gradient of i, but it can still work if you have sort of interactions. All right, so. All right, so what is our main result? This is an intimidating slide. I'm going to stay here for a while, so don't worry. Let me just try and guide you through what's sort of going on. So, okay, what we were able to prove is the following. So, we have a smooth potential, phi. So, the second derivative of the potential is bounded above, and we have this sort of coercivity assumption on phi. So, if you like, it's everyone's favorite convex. Everyone's favorite convex potential. It's x squared over 2. So, what am I saying here? Construct the row n's. So, these are the empirical measures, charging mass at the particle positions. So, particle positions that we know exist from the Maury capers. And I'm also going to construct this discretized pressure, P sub N. That formula, just think that it's P swap. Formula, just think that it's piecewise constant. Okay, so on each, maybe I'll write it down here. So on each, on each piece, it's equal to lambda i when x is between xi and xi plus y. Sorry about that. Let me. So it's piecewise constant. And so I have the empirical measure, which I'm claiming is some sort of approximation. I'm claiming is some sort of approximation of the density, and then I have this piecewise constant discretization of the pressure. What is the result? The result is I can construct, so I have two sequences, and as an analyst, whenever you have sequences, of course, the thing is, is there a limit? Is there a sub-sequential limit? So we can show that there exists a pair rho, p, which live in these spaces with some amount of regularity, such that Of regularity such that these two discretizations rho n and pn converge to rho, p. I'll tell you the notion of convergence in the next slide. I didn't want to bother filling up this slide with the details, but these discretizations converge to the pair rho p. Okay, fine, so that's the compactness that we always expect when I talk about a sequence of functions. But more interestingly, the pair rho, p, it will be the unique. P, it will be the unique distributional solution to the macroscopic PDE. So I've just written it again down here, and I've changed a few gradients to partial x's, just to emphasize that this is a one-dimensional result. And we know that this is unique due to DiMarino and Mazzaros. Let me also point out that the entire sequence converges because of the uniqueness result. So the entire sequence. So, the entire sequence, rho n and pn. So, rho n will converge to rho, pn will converge. Okay, I will mention this again in the next slide, but I want to stay here just to highlight that I'm not telling you something. There's something missing. What is missing? Well, in the macroscopic equation, okay, if you look at what I have in magenta, I really like colors, by the way, so that's why I have this. So, I have So, I have in the saturation conditions that rho is less than or equal to one, as well as this complementary slackness. What I should have mentioned when I first wrote the macroscopic equations is that these are pointwise constraints, pointwise almost everywhere, but they're still point. But if you notice, the discretization I've wrote here, this is just a measure. Rho n is just a sum of empiricals. I'm somehow telling you that rho n. I'm somehow telling you that rho n, which is just a measure, converges to rho, which apparently is not only a genuine function, but it enjoys some sort of L infinity bound. So there is something mysterious happening. How can this sequence of measures converge to something which is L infinity bounded? Similarly, this Pn, this pressure is piecewise constant, but somehow this pressure. I'm this pressure will live at each time, or at almost every time, the pressure lives spatially in an H1 space. Okay, so this piecewise constant thing, how does it have sort of H1 regularity? There is something going on here, and that's what I think makes this really exciting. Because, you know, normally, just taking limits of this, you should only expect that ρ will converge to some sort of measure and Pn will converge to me. To some sort of measure, and Pn will converge to some sort of L2 function. But somehow I'm gaining some regularity at the limits. Yes, question. I think in an earlier slide, if I remember correctly, you wrote the pressure, like the pressure was, if the pressure gradient was inside, it was the x rho times the. Yeah, that's a very good question. That took me a long time to figure out as well. So the point is, so P lives in H1. H1, or so I'm pick a time and then p the pressure at each time is going to be an h1. So what do we know? We know that the partial of p is zero on the set where p is equal to zero. Okay, so if you combine this, if you combine this with the saturation condition, what you can show is, well, dxp okay, so if dx So if dx, this is equal to exactly the other way that you mentioned, precisely because of this regularity that I'm claiming with the saturation. But that's a good question that it's written like this. And I think from now on, I will write it as the Laplacian instead of putting it with the rho. Okay, I should also mention that the precise scaling as n goes to infinity with the particle radius. With the particle radius, is that r times n remains order one implies this. In the voice, it is, you know, what get uh so when uh when rho if okay um let's say uh when row is less than one when row is less than one p has to be When rho is less than one, p has to be equal to zero. Okay. And then this is a zero equals zero. But if p is equal to rho is equal to one, then rho is equal to one. And yeah. So thanks. I mean, I was also very much confused with this when I started learning about it. So yeah, I appreciate in particular Inwan's patience for explaining these things to me. Okay, so this slide, it's really just me writing down everything that I said. Writing down everything that I said. Well, the second half is. The first part, let me just mention that I made an assumption that the initial density should be compactly supported. I believe it is more technical than conceptual. I'm not saying that it's trivial to just remove it. It will certainly make things nicer, but I believe it's just a technical assumption. As I mentioned again, let me just repeat. I'm using the particular scaling. I'm using the particular scaling that's twice the radius, or every particle diameter is 1 over n. And if you change this scaling, then instead of getting rho less than 1, you'll get rho less than some different constant or something like that. And then as I promised, I have the notion of convergence from the limits. So rho n converges to rho in the narrow topology of probability measures uniformly in time. The Pn. Time, the Pn will converge weakly to P in L2. But again, just to emphasize and repeat the challenges about this and why I found this interesting, the discretizations rho and and Pn are less regular than their limits. Rho is not only a function, it is a function which enjoys an L infinity bound. And somehow, you know, just narrow, that's the convergence in measures. Convergence in measures, that alone is not enough to guarantee L infinity bounds on rho. Similarly, I claim that the limiting pressure is in H1 in space, but for the piecewise constant discretization that I wrote, there's no way that can ever be H1, right? Every spatial derivative will hit a Dirac. So that's a regularity issue on the limits, but also in the initial condition. The initial condition, okay. What I want to do is I want to go back to somehow this story in the analog of the mean field limits. Okay, what I wanted to do, remember the x i's and the lambda i's came from the microscopic ODEs. So they came from xi dots equals negative phi prime xi minus n lambda i minus lambda i plus one, together with a bunch of assumptions. Together with a bunch of assumptions, but in particular, I wanted to consider this equation where the initial condition, the initial particle positions, are also separated by a minimum distance of 2r, right? And I mentioned that in this Dobrucian stability for mean field limits and this law of large numbers, what you could do in the classical case, in the unconstrained case, is you could take your initial condition and you can take IID samples of it. ID samples of it, you cannot do the same thing in this case, right? Because if I take my row zero, so my row zero, it's initially going to look something like, so here is one, row zero could look something like this, and it could have a plateau, but it'll be something like that. If I take IIE samples of it, it is possible that I'll sample this is X10. sample this is x10 and I'll sample x20 such that their width, the distance between them, is less than twice the radius. Okay, so I cannot, if I just do an IID sample, I cannot even initialize the microscopic dynamics. Okay, so what is the punchline of this? What is the story that I'm trying to sell you? The story that I'm trying to sell you is that you should look at the quantile function. At the quantile function of rho, or another way of thinking about that, the pseudo-inverse of rho's cumulative distribution function. So, I'll also refer to this perspective as a Lagrangian way of looking at it. So, Lagrangian, because we're really going to track in some sense the particle trajectories in order to recover the Eulerian formulation where you're just looking at where max gets sent to. But the main, but the really the But the really the answer to both of these things and what makes all of this work is that we're looking at the quantile function of rho. Yes, question and then you project, you know what you're trying to confirm? Ah, sampling and then project. So what do you mean by project? You sample and then you make you you Yeah, I then I think that's uh that problem yeah I'm pretty sure because they also have as you said projections I wouldn't know off the top of my head but okay so let me try and sort of explain to you what what's going on with the the quantile function so here Here is a definition. Maybe some of you know this, but for me, I want to, it was sort of my first time learning about these things. But given a probability measure, we define its quantile function as a function from 0, 1 to the real line, given by this. And this is where the compact support assumption of the initial condition creeps in. So, okay, a few technical things. At s equals zero. At s equals zero, you could rightly say that the quantile function should be equal to minus infinity, because if that value is zero, then I can just take little x negative infinity. But I don't really want to think about the quantile function like that. I want to think that the quantile function, when evaluated at s equals zero, it corresponds to the smallest, the inf of the support of rho. Whereas if I take s equals one, so I'm looking at the first x value for which the max. X value for which the mass of rho exceeds one. I want to think about that as the supremum of the support of row. So in this case, in this picture, if this is my row, I'm thinking about that as x row 0 0 and this part here as x row 0 1. Okay, so now in the next slide, I'm going to do formal calculations. Calculations. I'm going to ask myself: let's say that I have a solution rho, p to the macroscopic equation. I want to know what is the quantile function of rho going to solve. And I also want to know what is the Lagrangian counterpart, which I'll call capital lambda. So this capital lambda, I want to think of it as a Lagrangian pressure. I want to know, given that rho and p solve the macroscopic equation, what do capital? macroscopic equation, what do you capital X, capital lambda solve? Okay. Thanks. So in 1D, yeah, so in 1D, this I understand, but in higher dimensions, I and this just speaks to my own ignorance. I don't understand. Higher dimensional analogs. understand higher dimensional analogs of this. Okay, so as I mentioned, so let's pretend that we have a solution rho, p of the macroscopic equation. I want to know what does capital X, the quantile function of rho, satisfy. So the first thing that's kind of nice about this problem is that because I'm assuming that rho is an actual function, so its density is non-atomic, it doesn't have to be a density. Non-atomic. It doesn't have any Diracs because it has to satisfy an L infinity bound. So from the definition of the quantile function or the pseudo-inverse, formally I can just differentiate and get this equality. And then if I define the Lagrangian pressure, capital lambda, so it's going to be, it's going to take on the same value as my pressure, but it's moving according to the trajectory of. According to the trajectory of capital X. So this capital X and this capital Lambda, when I recast the saturation conditions in terms of capital X and capital Lambda, I'm going to get that the partial in S of capital X will be bigger than one. That's corresponding to rho less than one. Pressure being positive, so the Lagrangian pressure, because it's the same value, it's also going to be bigger than or equal to one. And then the saturation condition here. Saturation condition here. When you make use of both these two formulas and just substitute things, you're going to get this thing. Okay, so this is how you can translate the constraints. As for the PDE itself, we're going to use a formal calculation. So here, I think I'm doing okay-ish for time, but I'll just mention that this is the equation that capital X. capital X and capital Lambda jointly satisfy. So how do you get this? Well, it's really, so this is still just a formal computation. Again, we're pretending that we already have rho and p. Eventually I will get to how they are constructed. But the point is that based on the definition of the pseudo inverse of the of rho's CDF, we have little s equals this integral. And then when you take the time derivative And then, when you take the time derivative of both sides after elementary use of a Leibniz's rule, you're going to come across zero equals this thing down here. So the point is either rho is equal to zero or this is equal to zero on the support of rho, and that's going to recover this equation. Modulo, the fact that I have an x gradient of p, and I'm telling you that that's the same as the s derivative of calculus. S derivative of capital lambda. And that is a consequence of the saturation condition, very much related to the question that was asked by Hector. So the S gradient of capital lambda is really equal to the X gradient of the Eulerian pressure. So this, if you like, is how you could go from rho, p to x, lambda. So we can go from comma lambda. So we can go from the macroscopic equation to the microscopic equation. On the other side of the story, we can also do the reverse. So given rho, we can find capital X, which I've sort of abused and can be implicitly calculated from this formula down here, because rho is a function. So this is a consequence of the pseudo-inverse formula. But also, if you give me an x. You give me an x, which is a legitimate distribution function, then I can construct a corresponding row by taking the push forward of the uniform measure on the uniform on the unit interval. And that's how I would construct the density. Okay, so the point is that we can go back and forth between the macroscopic PDE and the equation that I had on the previous slide through this correspondence. Sorry. Sorry. So, one thing that I also wanted to mention that I didn't put in the slides was that here, optimal transport also enters in a very sort of deep way. This sort of relationship between the density and the pseudo-inverse, it gives rise to an isometry between the space of probability measures and these sort of function spaces where these pseudo-inverses live. So, I'll just Inverse is lip. So I'll just write that down. So let's say you have row i equals x i push forward. Oh, sorry. Let's take yellow. So let me write over. Row i equals x i push forward uniform measure. So let's say that I have two functions x1 and x2, where if you push forward the uniform measure, I get two probability measures. You know, two probability measures. What I can tell you is that a very classical result is that the P-Wasserstein distance between row one and row two is exactly equal to the LP distance between the functions x1 and x2. And that is used in the background. So that is a classical result which you can find in Villany's book or Or optimal coupling. Okay, so let's say that these xi's are the quantile functions of the row i's and and i'll be working primarily with the quantile functions anyways but yeah thanks yeah yeah yeah right you only have one side of the inequality just from that uh okay so um Uh, okay, so um, this was already mentioned a little bit earlier, but I can answer the first question of sampling using the quantile function because instead of just sampling IID from my initial condition, once I grab the quantile function of my initial condition, I can take my samples to be evaluation of the quantile function at i over n. And this is what's going to allow me to satisfy to me to satisfy, to initialize the microscopic particle locations, which will respect the constraint. And as well, I'll know that this converges to zero. But yeah, as was discussed, you could also just do the IID sampling and then the projection. But this, I kind of like it because it's somewhat explicit, tells you from the quantile function that you just evaluate at a few points. Okay, but what is the next thing that I want to do? The next thing that I want to do is, so Want to do is um so I want to solve this equation and then I want to take solutions of this to build discretizations of the macroscopic equation. Okay, and how I'm going to solve this equation is I have my initial data, it's all these xi zeros, and I'm just going to feed them into the JKO scheme. So, this might look a little bit different than the usual JKO scheme, but what I want to just emphasize is that. want to just emphasize is that it's the JKO scheme in disguise with a bit of a convex constraint. So I take an initial vector with all of my particle positions. How I'm going to update it is I'm going to say, okay, I'm going to update it based on this minimizing movement scheme where I want to minimize this thing in curly brackets. So the Kn, this set here, this is just the set of vectors which make Which makes, which respects the non-overlapping condition that the particle positions remain separated by the width that I want it to. This sum over here, really what you should think about it is it's really acting like this integral of phi times integrated against the measure rho. So it's really acting like the e rho functional that I had near the beginning of my talk. Near the beginning of my talk. Except here, if you like, the row is overall empirical measures. And this Euclidean distance between the vector x and its previous iterate, it's really just the two-Wasserstein distance in disguise, because I'm looking at all of these vectors and I want to, so I have all of these xk, this xk vector, I want to find the vector of x, which that distance, yeah, as I mentioned, is just the two-basserstein distance in disguise. In disguise. But here I'm trying to again sell you the Lagrangian perspective or the Lagrangian view of this. This is just, this is the usual JTO, but just written with Lagrangian glasses, if you like. Okay, so you go through this minimization process, and I'm not going to talk about how you take the time interpolations. When you send tau to zero, just like in the usual JKO scale, Just like in the usual JKO scheme, you're going to recover an actual curve, which will enjoy some regularity. Also, because I'm minimizing over a subset of Rn, I'm going to pick up Lagrange multipliers here. So this, if you like, it's a repetition of the Maury and others paper, except I'm also appending what we did with the N1. What we did with N1 and ANT1, because so you know, we're providing an alternative construction of solutions to this constrained equation. But because of the power of the JKO scheme, as you all know, we get a little bit more than just what is in this abstract paper. So here in this 2011 paper, they're able to construct solutions, but it's done using a very weak form of convex analysis. A very weak form of convex analysis with proximal cones. And more precisely, they don't have any estimates to send the number of particles to infinity. But with the JKO scheme, we get the solutions that Maury and Venel have, and we are able to prove that there are certain estimates enjoyed by these xi's and lambda i's in order to help send capital N to infinity. So this was going to. So, this is what's happening when the time step goes to zero and we recover their solution. And as well, we get some more estimates. So, I want to just sort of go back to comparing when you have a finite number of particles, and I really want to draw the analogy between that and the equation that I formally derived satisfied by the quantile. Derived satisfied by the quantile function. And really, if you look at this a little bit, you should already see some similarities. If I treat the index i as a variable, so rescaled by dividing with 1 over n, then these finite differences, these n times lambda i minus lambda i minus 1, this really looks a lot like a derivative on the pressure. The pressure. But so that's exactly how I'm going to think about this discrete equation. And it's going to be my philosophy in constructing the discretized quantile function and the Lagrangian pressure. So I'm getting closer to resolving the issues that I mentioned in the theorem that I talked about. So what we're going to do is we're going to, we have all of these little xi, little lambda i's. We're going to grab them and We're going to grab them and we're going to define capital Xn to be and capital lambda n to be the piecewise constant interpolation between all of these xi's and lambda i's and uh the when we put a bar over it it'll be a linear interpolation between these little x i's and lambda i's okay so you know at the moment if you've taken any course on numerical analysis this is really not uh exciting i'm just thinking about two different interpolations Thinking about two different interpolations. But where these are really important is because, as I mentioned, the rho n, the empirical measures, and the pn, the piecewise constant pressures, there is no way that they can enjoy any of the regularity of the limits. So the rho is not even, the empirical measures are not functions. And this Pn, it's piecewise constant. So how can I ever expect that its derivative or its gradient will ever be? Derivative or its gradient will ever be an L2. But the point is that these discretizations are related to capital Xn and capital lambda n because we're using the piecewise constant interpolations to construct them. So the empirical measure comes from taking the push forward of the piecewise constant interpolation. And the Pn, if you like, it's a piecewise constant interpolation of the pressure. So what we're instead going to do. So, what we're instead going to do is we're going to say, okay, instead of taking these things which morally came from piecewise constant interpolations, let's take the piecewise linear interpolations. Let's use that instead. So in particular, I'm going to define the row n with a bar on top. It's going to be the push forward through the uniform measure of bar xn. So because bar xn is linear or piecewise linear. For piecewise linear, I'm going to get this quantity over here, which is a genuine function. And moreover, with the constraint that xi plus 1 minus xi is bigger than 1 over n, this will satisfy that this is less than or equal to 1. So this is an actual function. And moreover, with the bar pn, I'm going to take the linear interpolation. And there I can actually take derivatives now and not pick up the rax. Derivatives now and not pick up your acts. Okay, so that's sort of the trick that we saw that made everything work. We consider instead of the usual discretizations coming from piecewise constant interpolations, we're going to look at using the piecewise linear interpolations. And the punchline is that, yeah, when you take this, then things are going to work. Then things are going to work out. You're still not going to throw away the old interpolations. So the rho npn, it's going to solve exactly the equation that we want. But as for the saturation condition, we're instead now going to use the bar row n. So here I have this sort of almost because one thing I want to point out is that if we're trying to send capital N to infinity, trying to send capital N to infinity, this P n and this bar rho n, they, so the bar rho n, because it's a function, it might have, well, though when I take the product, these are both weakly convergent sequences. So I'm trying to take the product of weakly convergent sequences. That's not really going to work. So instead, what I'm going to do is I'm going to just, if I replace the PN with a PN bar, that is something which has better regularity, and I can show that it's. And I can show that it's vanishingly small as n goes to infinity. And the moral of the story is that you, because when you use both of these interpolations, you're going to get the nice compactness for the things coming from the linear interpolation. So that's what these first two bullet points are talking about. But the rho n and the rho n bar, they're still going to remain close to each other. So rho n will still converge to. Rho n will still converge to whatever bar rho n converged to, but because bar rho n is more regular, its limit will have more regularity. So that's that's sort of telling us how we got somehow extra regularity, even though the rho n's were empirical measures, and similarly for the p n's. So I'm going to quickly skip. Yeah, so here is a restatement, if you like, of that of the Of that, of the main result. And now, the new thing is that really I'm including, I've told you how to use the quantile function to get these sort of piecewise linear interpolations. And that hopefully convinces you that this closes the theorem. So of course, there are a lot of behind the scenes details, but again, the point I want to emphasize is using the quantile function to get these piecewise linear interpolations. These piecewise linear interpolations. So I won't have time to go into this, but let me just summarize what I've already done so far. I took little xi, little lambda i as solutions to this. We construct these four interpolations or these four discretizations, which we believe are approximations to the macroscopic density and pressure. And then when we send n to infinity, we recover rho p. But you can also ask, But you can also ask, what if I send n to infinity starting from there and then do the conversion? And the answer is we were able to do that as well. So if you started from xi, lambda i, you can send n to infinity to get capital X, capital lambda. And then based on that formula, going from capital X lambda to rho P, you'll get the macroscopic equation. So we are able to do both. I'm not going to get into that. So yeah, as I mentioned, a lot of So, yeah, as I mentioned, a lot of slides that I'm skipping. But so let me just conclude with a summary of a few things that I've said. So we can also include interaction terms, which I mentioned a little bit briefly. I didn't want to show it because it's a bit cumbersome. So if you have this interaction, there's a missing row somewhere over there. So there's a row missing, that's a typo, then we know how to incorporate that into the equation. incorporate that into the equation for the capital X. And similarly, also for the finite number of particles. I also want to mention that uniqueness is known on both the macroscopic equation and this sort of microscopic level. So I already mentioned DiMarino and Messaros, they proved uniqueness and the macroscopic equation. And we, if you like, sort of did the parallel, but for the X equation. Some other things I would like to. Some other things I would like to explore a little bit later. I already mentioned that I feel the compact support assumption of the initial condition is technical, but I think there is a little bit of work involved in lifting that. And as well, other things to extend this are with sort of cross-fusion. So if you have two different species or two different densities of species, can you play the same game with this? I haven't talked about. I haven't talked about internal energies or another way of thinking about that. I haven't spoken about non-linear diffusions being incorporated into this. That is another thing to consider. This equation was somehow on all of R. So the density rho, the support of it could expand forever. So one thing is to think about boundary conditions. And last and certainly not least, what happens in higher dimensions. So that is a very, very tricky. Is very, very tricky on, I would say, a technical level and a mathematical level. There's certainly a lot that is not known. But yeah, with that, I would like to thank you for your attention. And yeah. Thank you, Jeremy. It was a very nice talk. So I didn't understand. So when you construct the capital X, it's like you're sweeping out from left to right until you get the S mass. I didn't understand when you start with the interval 0, 1 and do uniform, and then you get the XI. Why do you get the constraint on the length? If your initial probability. know if if your initial probability distribution is very has a vertical uh edge what's what's going on or you need to take the capital N sufficiently large what's I got lost okay um I what do you mean by a vertical edge so if my initial condition let's say it's something close to a delta well I still want it to be pointwise less than one okay and it and it has to be Okay. And it has to go on. Sorry? No, no, no. So I want it to be less than one, but its mass, all of this area, should still be equal to one. So I don't want to think about that. Oh, the particles are, is that what you're saying? Yeah, white, wait, wait. Is that what you're saying? Yeah, why I didn't get why the particles would would be separated, no, with the shows that you make. So you take a uniform in 0, 1, and apply the inverse of the accumulated. Yeah, sorry. Many? Okay. Right. So I guess I So I guess my interpretation is you're asking why this separation. So one way of thinking about this is that if you have, let's say, so the integral from xi to xi plus one of row zero, you know that row zero is always less than or equal to one. So this is less than replacing that with one. But moreover, you can write this as integral of minus infinity. As uh integral of minus infinity x i see sorry yeah and does this have any relation? I mean to see that I want to mind is that that connection that there is with the conservation laws and Hamiston Jacobi know that you do this integral you can only do in one dimension or is this closer to kind of a biochi transform is I don't know so the conservation laws we looked oh no um Oh no, we looked a little bit at sort of these pressureless Euler equations and what, so like sticky particle systems coming from like kind of Boltzmann equation. And yeah, I should have also mentioned, yeah, so this idea of using the sort of push-forward pseudo-inverse of a cumulative distribution function, it has also been applied to what I said, so like sticky particles. What I said before, so like sticky particles and other stuff. I'm not quite certain about this Hamilton trophy that appears to go fail and about the paradigm connection between observational laws and coming together for each connection. Yeah, yeah, sorry, second one. At the discrete particle level, it's not really about dynamics. Yeah. This is just because we have to find a competitive way to put the delta message with the constraint that it has to be between 0 and 1. Right? Yeah, yeah. So in this lemma, these were not randomly chosen, but what came up is that you could take IID samples, but then just move them around. Samples, but then just move them around in such a way that they are separated. Yeah, thank you for a great talk. I just want to maybe think about this more geometrically because we talked about this, but maybe now that I saw the talk. So the inverse coherent function is the optimal transport map from uniform to your density. But going into the tangent, you're going to have a density, right? So instead of the density, you think about the transport map. So if you're okay with W2 instead of W1 and you don't need exact isometry but just the pound, then you can do it in higher dimension tangent space. This is the linearized operator. So what was sorry, could you repeat what you were saying about W2 and W1? Oh, no, I it's it's not that important. No, it's not that important. It's mostly about linear or less optimal transfer, it only works in W. So if you're okay with a bound in W2 and you don't need exactly something, but just a bound of the process, then you can do the same idea. So instead of the quantum function, oh, that's important. I will have to look into that. Thanks. One thing that I really have a hard time understanding is so I Time understanding is so I in so the discretization of these transport maps. So here the emphasis was that the empirical measures were coming from the piecewise constant discretizations of the transport maps. And we saw that the push forward of this would be empirical measures. And that is unacceptable, or that's not good enough. So we needed a piecewise linear interpolation of that. So I guess my technical question. My technical question would be: given the transport map in higher dimensions, how can I get something piecewise linear? Is there a piecewise linear discretization of a higher dimensional transport? I think there's something that could be done. So I mentioned this before, like we don't do empirical measures for the measurement of oil cells and there is a similar result, a recent result. A recent result actually says we do this approximation instead of empirical approximation, people also convert the same convergence. So I can show you some reference things. I think there's some. Yeah, yeah, yeah. I'd really like to know, yeah, because higher dimensions is it boggles my mind. One minute. One minute. I owe Matt one minute. Okay, yeah, let's make our figure again. We'll look at it in a second.