And a very interesting subject, machine learning detects terminal simulators. Thank you so much for allowing me to speak. It's really pretty. Yeah, so today I'm going to tell you about something I did in my PhD, about applying machine learning methods to answer questions in my adult. Yeah, so the structure of the talk is: I'm just going to talk about machine learning as a totally pure math for like five minutes to kind of just kind of Five minutes to kind of just kind of put this into a context. Then I'm going to introduce the maths: so, toric funnel varieties and terminal similarities, which is the question that we are interested in. Then we're going to build a model to predict terminal similarities and talk about some consequences. In between here, maybe I'll share my screen and show some code because I was asked to show some code so we can do a small demo, but it's not going to be as nice as before because it's just about clicking a button. Clicking button. Yeah, and all of this is doing work with Tom and Al, who are my supervisors. Great, so it's a silly slide, but what is machine learning? So it's an umbrella term. Now everyone thinks strategy PT. I'm thinking a much easier problem. So in general, it's an umbrella term for algorithms that process data and learn patterns within data. And the more you let it look at the data, the more it's going to do better a certain task. The task can be different. And in our case, we will only look at the data. So, all our data set is going to have a certain number of samples. So, n here is going to be very big, like the millions. And each sample is going to have a certain number of features, x1 to xk, and then there's going to be a target that we are trying to predict in some way. So, we're doing supervised learning. This is not the only type of data that machine learning cares about. There's pictures, for example, are not like this. Pictures, for example, are not like this, and like graph data, but in our case, we're only going to notice something that looks like this. And our aim is to basically use machine learning as a function approximator. So we're trying to approximate the function that takes in the features and gives us the targets. So this is possibly the simplest setup that we could work with. Cool. Yeah, and so I wanted to kind of just summarize what the workflow is. And I'm going to go through this specific example that is potentially applicable to other problems as well. Other problems as well. So he starts by formulating a mathematical question that is amenable to these types of tools. With that, I mostly mean that I need to be able to have lots and lots of data because machine learning works better than humans when it's processing like huge data sets that I cannot look at all at the same time. So, once we have a question that makes sense, then we need to collect this data. Maybe this is already available somewhere. There's already a lot of mathematical data sets in places. For example, there's a Places. For example, there's like mathematical physics people that are obsessed with ecological data and they're doing a lot of stuff in it. Or maybe you're generating it yourself. Here, potentially, your question slightly changes because you need to be able to actually get your hand on data pretty fast. So maybe you have to restrict to a certain class, so you have to impose an extra condition. Then you want to build a model. What model you use depends on the task that you want. Maybe you're doing classification, maybe doing regression, depending on whether your target is controlled. Regression depending on whether your target is continuous or discrete, or potentially you're doing supervised or unsupervised learning, depending on whether you have labels or not, or you're doing clustering analysis. So, there's a lot of choice here. Specifically, we're going to do classification using a neural network, and then we want to use the model to actually do smarts. So, there's some examples about using machine learning, like a high-accuracy machine learning model, to Model to aid in conjecturing and improving theorems. There was the big deep mind paper where they used like a neural network that was able to predict some constants in map theory from some other constants. And then they did some kind of like saliency analysis, so seeing which one mattered the most, and then they actually formulated something when they wanted to prove that. So there's examples like that. In this talk, I kind of just want to talk mostly about using the classifier itself, so using the model itself as an in-place of In place of expensive computational tools. So, in kind of like we have a model that's approximating a function, let's use it for something rather than print something else. So, in our case, our question is, when does a fun variety have terminal similarities? I'm going to explain all these words in the next slide. And we are going to work with type varieties because of this problem of we want to get loads of data and we want to get it systematically without a human actually having to look at them. Having to look at them. And then we build a small NLP. So, this is a multi-layer perceptron. I'll show an example when we get to the neural network. So, this is the kind of like most vanilla type of neural network that is able to detect terminal similarities in our data set. I say small because now with like large language models, anything that doesn't have like a billion parameters is small. But for this type of task, you don't actually need that big networks. Need that big networks. And then we use the model. So I'm mostly going to talk about using the model to sketch the landscape of these objects. So we're going to use it in place of computational routines. But as I kind of like off the back of doing this, we also went and proved the result. But it's kind of like related, but not really. Like knowing that maybe there was an easier way made us like then go and do something else. But they're not as related as maybe they were in like the deep mind they were. Like the deep microphone. Okay, so this is the kind of plan of what we're doing. Can you explain MLP? MLP stands for Multilayer Perceptor and it's the vanilla type of neural network. But I'm going to show you a very nice example where we can actually see how it works. Yes. You have to choose among the availables the available source. This MLP, how did you choose the Do you mean so? How we chose how we decided to use this is because we, okay, there is, I'm gonna answer two questions maybe we didn't ask. So within choosing like the neural network, so choosing fixing the architecture, there's a lot of parameters that you have to choose within it that we did by like kind of random research, just trying a lot of them and seeing which one was doing the optimization better. But then before that, choosing specifically this architecture over our individuals, we tried Over all the logs, we tried different things, and this was the thing that was working best. So we tried like decision trees and support vector machines and other things, and they didn't seem to be able to pick up. While the neural networks seem to be able to pick up these things, but then also there are more kind of advanced things that we didn't try because we got good results and then we stopped. Yeah, cool. Any other questions? How many layers and neurons? So it's three layer, 512, 700, 512. 12, 700, 512, something. So it's like, it's not huge in byte, though. And how much data? So it's going to be 10 million, 5 million for training and 5 million for testing. But we'll show in a second how to generate what the data looks like. And also I'll show a picture why I have so much data. Great. Anything else? Okay, so I'll give you a Okay, so I'll give a bit of like mathematical jazz hands to justify why we care about this. So, the context of this is a union model program, which for rational geometry aims at classifying algebraic varieties up to a small transformation. So, these are rational transformation. Philosophically, it is a composes variety into three basic pieces: funnel, calendar, and general type. And we specifically care about the funnel ones because. Because the other two types, Caladia and Gerala type, can be realized by imposing extra conditions inside the funnel. So we care about funeral varieties, and also there is hope to classify them because there's finitely many in the smooth case. So there's finitely many smooth funnels in each dimension, and we have a classification done up to dimension 3. So there's only P1 and the nature 1, there's 100 passive surfaces, there's 105 dimension 3, and it's unknown for the nature 4 and higher. So if the idea is to kind of So, if the idea is to kind of get a classification results within algebraic geometry, tackling like fun varieties makes sense because we can actually maybe. Unfortunately, in the context of the minimum order program, sneakness is not exactly the correct setup to deal with. We have to allow some kind of mild singularities. But this is, we're not, not whole hope is lost because there's still a kind of like finiteness result when. kind of like finiteness result when the singularity is of boundary complexity. So it still makes sense to classify them because there's going to still be finite many. And the type of singularities that we are looking at are finite varieties that are key factorial and terminal. And even less is known about the classification. So for the smoothness we have some classification result when we put these other two conditions and we relax smoothness to allow some singularities less is known. So the kind of like driving question is Driving question is sometimes checking whether something amidst this type of similarity is challenging. So, can we use machine learning to do this so we don't have to do it ourselves? Or we kind of like try to grind our intuition. Just quickly remind us, what were terminalities? So, terminal similarities are like the similarities that you cannot get rid of by doing your absence, potentially on your own too. To combination. But in the case of Tauric varieties, we're going to see exactly what they mean because it has a combinatory criterion. So that's going to make more sense. So this is general funo, and then we're going to switch to Taiwanic. Yes. If you give your network some variety, can it reliably predict or verify that it has a terminal singularity, or can it make a guess, an educated guess? This will have a terminal singularity. This will have a terminal singularity, this one doesn't. So you run a checking algorithm for this singularity on that one, and smoothness for the other? So. But presumably you would cut us. Yeah, no, but like the network is never going to be 100% accurate. The one that we produce is like 95%. So it's not always correct. It's always like a very good educated guess. But our idea was that if what I want to do, potentially. If what I want to do, potentially, is having loads of data to generate loads of data to then look at where I sit in space, and I need to do this extra check. I don't potentially care that 5% of the data I'm looking at is wrong, but I'm kind of looking at general pattern recognition and behavior. So in my case, I didn't care as much that it's not always correct. But yes, if, for example, there was some kind of weird behavior, like something that tells me that it is stamina, but it's not behavior as it should be, I can always run the actual. I can always run the actual check and then see whether the network was wrong or right. But yeah, the network is not 100%. But it could be used to do pre-selection. Yeah, yeah. So if you could use it potentially, like I could run the network and I only take the terminal ones and then I only check those or I only check the ones for which the network wasn't completely sure because the network doesn't give you a binary thing, 0, 1, it gives you a probability distribution. So it tells you, oh, I'm 50% certain this is something. And then it's like, okay, maybe this one I will check properly. Well, one of them. Maybe this one I will check properly while one of them is like, I'm completely sure, and then one can take it. It'll become clearer as we go along. Anything else? Cool. Okay, target comes in because I said we want to generate loads of data and this is hard. So we want to do we restrict to target varieties because they're coming after objects. And so we can actually generate. And so, we can actually generate lots of correctly labeled data because we have a systematic way of checking photometry in this case. It's just that the systematic way is very time-consuming. So, we change the question. So, by still is a question that is of interest because, in the torque world, checking whether something has terminal similarity works like lattice-boiled counts, and in high dimensional lattice-boiled counts kind of explodes. So, it's still of interest to have a kind of efficient. A kind of efficient method that doesn't do this. And to give you even more motivation of like actually what I was thinking about when I was saying this question, because this is kind of a little bit jazz hung, is in previous work we visualize weighted projective spectrum using some constants A and B. So if I'm given a Q finantaric variety, Q final means that it's a Q functor. Qfana means that it's a Q factor and terminal. Then we can compute two coefficients and kind of project each variety as a polygon in R2. And we use this to kind of like see some dramatic information. I'll show you a plot in the next thing. And these growth coefficients, these A and B constants, are growth coefficients of something called the regularized quantum period. So the regularized quantum period is a key invariant of one of our ideas. It's conjectured to be a complete. Funeral varieties is conjectured to be a complete invariant, but it's a very fine invariant. That is a power series where the coefficients start with one and zero, and then there are some multiple or some grommet invariant to back, so they're counting curves. So in our work previously, we were looking at these power series. They're supposed to contain a lot of information about a variety. And we noticed that the growth behavior of the CD is really controlled by two constants. So the result was that in the case So, the result was that in the case of a Q for the Fanovariety of dimension D, and C D has these coefficients of these power series, then if I'm looking at the logo there, then it grows linearly and it has a sub-leading logarithmic term, well, the coefficients is the dimension by the way too. So, and A and B are just kind of geometric constant whose value depends only on the weight data of my target final variety, so like my geometric data of my final variety. So, we use this in the context of weight approach. We use this in the context of weighted period space to visualize the kind of landscape where they sit in space just in our turn. And we got a very nice picture, which looks like this. So this is on the x-axis and the y-axis, there's A and B, so these two growth coefficients. And each point there corresponds to a weight projective space. They're colored by dimensions. You get a grading between 3 and 10. And you also get a lot of structures. There is a each patch has Each patch has like a bound underneath that we can make precise. So, this kind of like the belly or whatever this kind of hedgehog shape is is kind of precise. And then there's the number of like spikes it has is equal to the number of dimension of the patch. So, it's a very like geometric picture. And so, we got this with reactive spaces. And the question was: it'd be nice to do it for auditoric varieties. It'd be nice to generate loads of data and look up kind of plots that look like. Look up kind of plots that look like this. But in order to do it, we kind of have to check there are varieties of terminal. And in weight projective spaces, there is a very fast criterion to do it. In any kind of higher rank, you have to rely on the status point. It takes a lot of time. So the idea was, okay, I want to just kind of get landscape views of my PTAR rank 2 toilet varieties, for example. I do not want to do the terminality check because it's going to take ages. So, can we use machine learning to kind of bypass it? Learning to kind of like bypass it. So, this is the action motivation we have. I would give you more less. Okay, so let's talk about data. So, very third question is how much data are we using? So, we are probing this question on a data set of 10 million Q factorial toy varieties. Specifically, we're restricted to dimension A tempicarub 2. And the data is balanced between tabinoid and non-terbinal. So, there's 5 million that are tabino and 5 million that are non-terminal. There are tabina and five million that are not tabina. And why did we do this? So, why did we put so many restrictions around the data? So, the first restriction was toric instead of general, but we've discussed this. Toric variant is a nice gap combinatory of things, and that means that we can generate actually this data. Why are we in dimension 8, which is slightly random? That's because in low dimension, in low dimension, the problem is easier. So, smooth and terminal I equivalent to dimension two, for example. Equivalent to dimension 2, for example, but also we don't have a lot of data in small dimensions. So, in dimension 3, the equivalent of our data set only has 34 examples. So, as I said, when we want to do machine learning, we want to have big data sets that I cannot look at as a human. So, we kind of went in dimension 8 and probably overshot this lively to get enough data to do something interesting. And we're looking at Picabron 2 because, as I said, we know Picard Quad, the Wi-Fi Planetary Space case. We already have a kind of Space case. We already have a kind of rice criterion that detects terminality, so that is kind of solved. We look at Picard2 as the next case. So, this is a kind of toy example to see whether this is a methodology that makes sense. Just a small reminder. So, weighted projective spaces can be written down as a list of positive integers between A0 and AP. They give an axis of C star on C plus 1 that is specified like this. One that is specified like this, and then the weighted projective space is the geometric portion of this action. It is going to be a d-dimension of the weighted projective space. So, for example, here, of course, is a weighted projective space. So, we look and generate Picarum to Tariq Fano varieties in a similar way using weight matrices. So, our data points will be generated at 2 times D plus 2 matrices. D here is the dimension that I'm expecting my variety to be. To be. All of these entries are integers, and these describe an action of C star 2 on Cd plus 2, which is given by this. And then we can take the geometric quotient, which means that we have to take out some points where the orbits bound to behave. So specifically, we have to take out S plus and S minus, which are these two linear subspaces, and then take the quotient. And when S plus and S minus are these two-dimensional, then the quotient is generally a D-dimensional parifunction. Is generally a d-dimensional Paris kind of variety, we become two and we are not talking back. So when I'm thinking of varieties, I'm really just thinking about this matrix data. And silly example, P1 cross P1 is a torrent fun variety P car rank 2, and its weight matrix is just 10100101. So this is the kind of data I'm looking at. Okay. Then I keep saying that our varieties are nice and combinatorial, and that's because we can also associate where a gap. That's because we can also associate a gadget called the fan and a spanning polytope, which is the polytope obtained as the convex hole of the primitive generators of the rays. In our case, if I'm given a wave matrix A1 to An plus 2 and B1 Pn plus 2, then the vertices of P are just lattice points that satisfy this system. And I'm saying this because I need to do my check, my terminality check, really on the fun picture instead of the way. Really, on the fun picture instead of the weight picture. I need to put some at this point out in this polyto. Just one quick question about the indexing there. Yeah, oh you're going from P day. Oh, yeah, yeah, yes, yeah, yeah. I think potentially that M is a D in the previous slide. Sorry about that. Oh, good? Yes? For example, if I'm given a type variety like this: 10011113, then I can write. 1113, then I can write a polytope as the right-hand side, and the yellow polytope is the spinning polytope I'm caring about. Of course, this is not unique, I can find many of them that will satisfy the system. Why am I telling you this? Because my terminality check is done on this side of the picture. So we need to check that my variety is Q factorial and has the worst terminal similarities, and I use the polytope to do that. I use the polytope to do that. So to be Q factorial is equivalent to the polytope being simpler, and to be terminal is equivalent to having only the origin and the vertices as the lattice points inside my polycomp. So this is where the lattice point count kind of comes in. And yeah, as we said, terminality kind of involves this expensive lattice point calculation, so it's kind of annoying to do, especially in a high dimension. And we would like to say whether we can tell terminality from the weight data, so this kind of matrices. From the weight data, so this kind of matrices, and kind of bypass building the fun and doing this kind of other expensive computation. And this is exactly what's happening in the weight objective space. You just take the string of integers that defines a weight objective space, and then there's some combinatory criterion that just looks up some divisibility data, and then it tells you whether it's terminal or not. So we like to do the same. Okay, and examples. Previous example is not terminal because I have these two red dots. Because I have these two red dots that are not the origin of the vertices in the vertices words. So, this is just to give an example. Cool. I wanted to draw attention to a point of the fact that this can be formulated as an invariant machine learning problem, and that's because my objects are not unique. So, if I'm given a variety, I will have a lot of wave matrices that represent it, and specifically, these wave matrices are really. And specifically, these wave matrices are related to each other by two actions. So, if I'm given a wave matrix, I can always permute the columns, and that will leave the quotient the same. And I can also apply a GF2Z action on the left, and that will reparameterize the basis of the total. So I have these two actions that change the Moon matrix, my leave my answer the same. So you kind of have to think about them when you're thinking about what algorithm you might want to. Algorithm you might want to use. This is a kind of general problem. Mathematical beta has representatives that a lot of the time have symmetries between them, or their representation is not unique. And most of the kind of vanilla methods, machine learning don't take them into account, so the result will not be invariant or equivalent with respect to the action. So the way we bypass this is by doing a pre-processing step. So in order to produce an invariant model, instead of choosing the model to be invariant, Instead of choosing the model to be invariant by kind of geometric reasons, we always put our weight matrices into a standard form which just picks our preferred representative of each group orbit. And this is what it looks like. So the columns are singletry ordered, which gets rid of the SM action. And then they are always like in the positive quadrant. The first vector is on the x-axis, and the last vector is either on the y-axis. Last vector is either on the y-axis or slightly of it. So, in this way, whenever I feed a weight matrix into my machine learning algorithm, even if the machine learning algorithm is not invariant per se, it's not seeing any other representative over the orbits, so we don't actually have to deal with that. So, this is called fundamental human projection. It's not the only way to deal with this. So, machine learning people that do like GDL or something prefer to kind of build the group action within the network. Group action within the network, and this is done, for example, in SN. This is very well known, but groups like GMTZ are more tricky. There's architectural stuff like finite groups, but when your group becomes different, it becomes more tricky to kind of build it in. So really the pre-processing stuff would be interesting to see if we can actually build an architecture in NOSS. The parentheses. Okay, cool. So after this, we can actually. Cool, so after this, we can actually generate the data. So, what do we do? So, first of all, we choose a random status, true and false, because we want our data set to be balanced between terminal and non-terminal. And then we generate a 2x10 integer value matrix with the entries bounded by some value, so we're doing kind of uniform choice for each entry, and it's in standard form, so it's already been put in this preferred representative of the logit, and it's 2 by 10. or bit and it's 2 by 10 because if we go plus on my back I want things to be eight dimensional so I want 10 columns because it's plus 2. Okay then we construct the spanning polytope and we check terminality and q factoriality so we're going to sell this point count and then we're checking whether the polytope is simplicial and we keep the example only if the terminality check agrees with the predetermined status of like we've decided that this example should be yes or no. When do you do it? Why do you do it this way? Problem if you just build the example and then add two sets? There's no statistical problem per se, but depending on what bound you put on your weights, it's more likely or less likely to be terminal. So like in the bound, we put a bound of seven and it's 80% of the time your terminal and the lifetime, like it's quite likely for you to hit it. We didn't actually really know that she's not. So it's 50% true and 50% false. Then you have a statistical problem because one set has bigger entries than the other. Yes, but that's already like it's interesting in itself to kind of like if the answer is to have like you think it's better than you think it's a feature of a no but I think that's because if you think about it okay this is slightly one wavy but I'm gonna try if you have Going to try. If you have small entries of your rain matrix, it's correlated to having a small polytope, and a small polytope is more likely to be terminal because it's going to have less focus points. This is not necessarily always the case, but I think because of this kind of like Hammond correlation, then that's potentially why the likeliness. But doing so, you interfere with the machine learning, no? Well, because we build our Because we build a balanced data set, that's supposedly not doing that. But I think there is no no, but there is like a big question here about sampling techniques. And I think I'm not entirely sure whether there is a certain amount of noise you introduce when you sample and create the data set, and it's unsure whether that is what gets picked up by the network or not. But on dimension eight, terminal people. Terminal people. If I'm looking at dimension eight taric varieties of Picam 2, if I actually were to count them all with a certain bound, you have so much more than the actual sample that we're looking. So I have a feeling that even if we are doing a kind of like, we are making our sample balanced things, it's still kind of, we are not doing some kind of overdetermining, we are not over determining our sample by enumerating all of them and just. Like enumerating all of them, and I just that it's already seen all of them or something like this. Yeah, you're not overfitting. Sorry? Overfitting. Yeah, it does not overfit. Yeah, but now the point is this is a very good discussion to have because the sampling is really does affect a lot of what you're doing. Like I've done some similar stuff on like finite data sets because like maybe you already have classified all your examples and stuff and then machine learning tends to just pick out really To just pick out really trivial things that is just a feature of the fact that your data set is finite, that you've kind of already seen whatever can happen. But here, like the actual number of things that can happen is up in there, hundreds of millions. So we are far away from that. But yes. Efficiency question. In terms of, I mean, let's say you decide that from the beginning you want one million examples, 50% true, 50% false. Why would you not just generate Why would you not just generate examples, put the truth in the true box, false in the false box, until each box has at least 500,000? Yeah. And then you just cut off the first 500,000. That seems to half the time in which you would generate your examples. Yeah, I think I didn't really know before doing it how many like the percentage of each was, so I just without running a Was so I just without running an experiment, that this seemed like the correct way of like generating balanced data sets without actually generating loading and cutting or something like this. But yeah, I think there is like equivalent way of without bias generating balancing data sets, but I'm not this. It's not exactly the same, because that will give you exactly 50%. This will give you something, some normal distribution around 50% or anything like that. But to be honest, we do cut it so that it is 50-50, so it is the same. Then we insert, if it check, we check we do not already have it, and then we insert in the data. So this is cool. So we said we were using like kind of tabular data setup. So we use the entries of our way matrices as features and we just button them. So I have 20 entries, I have A1. them so I have 20 entries I have a1 to a 10 and then b1 to b10 and then my target is just a boolean of like true or false for the list time on accounts this is like binary classification it's the simplest thing we could do and we train the famous anonym taking platinum weight matrices on a balanced data set of five million and then he detects terminality for two factor fungi so pyramid two with 95% accuracy on plastic so we are training on five million then we are So, we are training on 5 million, then we are testing on 5 million that are completely unseen. And now I wanted to do some actual showing of things. So, I was actually going to like first kind of introduce and show an example of a multi-layer receptor because I feel like there were questions at the beginning, so maybe I'm not boring anymore. Can you just give us a when I give you this 2 by matrix, how many seconds or fractions? Uh how many seconds or fraction of a second it takes for you to check? And probably to run the MLPs very much. I have a nice okay Can I put a pin into discussing? Because I have a nice table with like timing data which are not infractionable seconds, but they're like kind of comparatively to like those things. Oh, maybe I should remember that. Oh, okay, wait. So like, okay, so if I'm So, like, okay, so if I'm checking just one sample and the original algorithm takes one, then the machine learning model takes it 450 times faster. But the point is the machine learning model is already being written, so that really is happy doing big parallelization over on the GPU and stuff. So, if I was checking like 10,000 things, of course, I could have written my original algorithm so that it's completely parallelizable and maybe using the GPU, but I currently don't have that. So, in that case, We don't have that. So, in that case, I can get up to 30,000 times faster. So, when later we're going to kind of explore questions related to landscape and things, I build 100 million examples, and it takes 120 CPU hours versus 300 CPU years that it would have taken using kind of like the exact method. So, this is different between an hour on the HPC and I don't know how many weeks. So, that's the kind of like general idea. But yeah, I don't have it in milliseconds, I'm sorry. Cool. Yeah, so I wanted to like, in case people are not as comfortable with neural networks, then I wanted to just give like a silly example. This is Playground TensorFlow, which I saw from like the three, blue, one, brown video that explains neural networks. And basically, kind of just showing what it's actually doing. So this is solving a very easy problem: I have two classes, blue and art. Two classes, blue and orange, and then I want to build something that is able to differentiate them. And I'm only putting in x1 and x2, so the two coordinates of my points. So what a neural network is doing is taking in two values, so my x1 and x2, and then it's, for now, I just put one neuron in my hidden layer, and then it has an output that either tells me that it's blue or it's orange. And for now, what is this map doing? What is this map doing? This is an affine linear map. So it's an inner map plus a bias and just goes into the one neuron, and then the next one is a sorting part. And if I just run this, this is now going to do many things. So what this is doing is, if you can see, there's a kind of shading of blue on the kind of left corner and a shading of orange on the right corner. And that's basically saying that the network is thinking, oh, down here, things are probably blue, and up here, things are probably orange, but it's very hard. Things are probably orange, but it's very unsure. And this is probably because the network is very small, so I can try and add more things. But this is also not learning very well. And the problem is that our data is clearly not linear, right? It's a non-linear boundary. And I'm now, what am I doing? I'm just doing a lot of composition of affine unimaps. So each one of them is going to learn some kind of affine linear boundary, but they're not going to be able to. Boundary, but they're not going to be able to put themselves together to do something nonlinear. So instead of just doing composition or finding a maps, I want to add some kind of non-linearity. So I add something called an activation function, which is this thing that we have. So for example, I can put value. So what this is doing, it's being applied to each neuron. So each edge is going to be on a five-linear map. Then on each neuron, I do some non-linearity and then I keep going. And now I'm able to. And now I'm able to, and this should be able, or maybe not. To be honest, I didn't want to add more data, I wanted to add more. So let's be up. So this works. Yay, okay. So this is saying that everything outside is orange, everything inside is blue, and things that are on this boundary are kind of unsure. And I find it kind of interesting because value, it's a piecewise linear function. So it's a 0 for x negative, and then it's the identity for x positive. Negative, and then it's the identity for x positive. And very nicely, my boundary is also piecewise linear. Polygonal. Sorry? It's symmetric and polygonal, actually. If I was choosing something like sigmoid, which is not, which is like smooth, then eventually kind of blends it and it becomes something smooth instead of being something piecepace. I thought this was nice because I never actually saw it. Yes, and what is actually Yes, and what is actually happening is that it's trying to optimize a loss function that is basically calculating how many of my samples it's misclassifying. So this is what this spiggle thing on top is that's going down. This is my loss. It's basically saying that I'm misclassifying less and less examples. And it's doing this by doing gradient descent on the loss. So it's calculating the steepest descent and then taking steps. And then taking steps. And in order to take steps, I need to tell how big my steps are. And this is what the learning rate is doing, which is this thing coming here. So if I reset this, if I take my learning rate to be very big, that means I'm going to take very big steps. That potentially means that I'm not going to be able to actually go anywhere because I'm jumping around and I'm not falling into it anymore. So this is the loss is exploded and this is clearly not going anywhere. But if I were But if I was taking my learning rate to be really small, then I'm probably optimizing something, but I'm going very, very slowly. And it's not actually, it's going to eventually get there, but it's going to take a very long time. So the choice of learning rate, for example, is something that really affects the learning. So I just wanted to put it so that we kind of have a visual understanding of what's happening before I show this. Before I show a little bit of code, I'm one. Okay? So, this is in Python in a Jupyter notebook. Is this, maybe I can make it clicker? So, if I define my neural network in PyTorch, so that means that this is the architecture that is looking at. Looking up. So it will always have an input layer that is the thing that is taking in our previous example was the X1 and X2. In our case, we have 20 features, so it's going to take 20, and then it's going to take whatever I tell you the first layer is. And then it's going to build a number of hidden layers. And this depends on my input, so it's going to take I define what layers are here. In this case, the default is 200, 200, 200, and then it's just going to decide that. And I have an output layer. And I have an upper layer which, because I have to take two classes, it's going to only have one learn, and it's going to do like whether it's one class or the other class. And I always need to add some kind of non-linearity in order to make sure that it can actually learn non-linear function. So in our case, we found that Deaky radio is the thing that worked the best. So this is instead of radio which is zero and value identity, it has a very small slope on the negative thing because it's trying to deal with like vanishing gradient problems. Deal with like vanishing gradient problems. And these are all kind of hyperparameters that we tried different activation functions and learning rates and things and saw which one worked the best before fixing an architecture and training on it. So once I define my architecture, I didn't find the forward method of my network, basically all that is doing is doing a pass. So it's going through the input, then going through the hidden layers, when I'm always composing by self.m, which is my activation function. Which is my activation function, and then at the end, instead of returning the output, I return the sigmoid of the output because then it transforms it into a probability distribution between 0 and 1. So in that case, I have 0, maybe it's my non-terminal class, and 1, it's my terminal class, and it's just saying which one I'm more likely on in the end. How many neurons in each layer? So we're going to have yes, so our Yes, so our network at M is going to have this architecture. So it's going to be 512, 768, 512. And that means that if I was looking at my previous picture, our input layer will have 20 features, and then it's going to have three layers, well, that's not the right one. Where it's 512, 768, and 512. And then it's going to end on one neuron at the end of. And this was done again by random research over the space of like different The space of like different hyperparameters. So I put a few of them and then I like power of two, so then that's what it happened. But like we tried for like one layer, like three or four or five, and then it was a balance between accuracy and actually also speed because the bigger your network is, the longer it's going to take to train it. Question? Something like something convolutional? Because you don't have like just Convolutional because you don't have like just a point in R20. Yeah. You have 10 vectors R2. I tried some deconvolution and it didn't really work, but eventually I wasn't using like a good kernel or something like this. I don't know if I've heard people dealing with very small matrices that have multiple meaning and then using graph neural networks. That to me doesn't make sense logically, but apparently works very well. But NGNNs are kind of convolutional networks. Combination apples. But um exactly. I mean, this is not like a calculator, so it doesn't care about the vectors. It's much more like us. It should appreciate the picture more. Yes, yes. And maybe the integer vertices inside the polytops or something. So if you could give it the picture, maybe it increases its precision. Oh, nice. So you're saying if I give it, like, I treat it completely as a picture problem, and you just give it a picture. Yeah, but the point is, I wouldn't, you know, had a I wouldn't in your head, am I giving the picture of the polytop? Because that's really what I don't want to do because I don't want to having to build a polytope without doing things. I mean, the picture of, as you suggested, just the picture of the two vectors, either the two vectors or the ten vectors, depending on how you want to do it, and literally the picture. So you just treat it as a picture and give that picture. And perhaps see if your machine is actually much better at that. Yeah. I think it's that. I think it's the potential, yes. I think when I treated it as a picture of like a 2 by 10 pixel, and each value is kind of like, if you think about each value in my matrix as a pixel value, and then using like a convolutional kernel that kind of like does things, that didn't work as well as I would have expected. So I was a bit confused. But potentially, again, that seems to miss the geometry a little bit, whereas the actual polygons might, especially if you mark. Might and especially if you mark the integer points as well, so that helps. Yeah, no, I think my point was that I don't want to actually have to produce the political picture because then once, oh yeah, I guess, yeah, I understand potentially. It would be interesting to try and see it. Okay, cool. So, what was I doing? Great. So, we've defined the network, and then we can generate a sample. So, here I'm describing a dimension, an upper bound. An upper bound in my entries of my weight matrix, and I'm specifying which model I'm using. So because I generate data that has a prescribed upper bound in the weights, then this is the model that was trained on that data. And I just run through the kind of, oh no, I didn't have the login. So, this is just generated a weight matrix as two rows, and it's telling me that it's not terminal, so it's false. And this is just done by doing actual maths. But now I can make it into a machine learning setup, so I can turn all my things into tensors. So you have to forgive the extra bucket because they always think everything is batches, so this is the batch of one. But anyhow, I turn it into 10. But anyhow, I turn it into tensor, I flatten it because we're not thinking of pictures, but we're thinking of just having a very long vector of twenty things. And also, I scale my data. So neural network prefer entries that are between like 0 and 1 or between minus 1 and 1. They like things that are small and kind of look more or less normally distributed. So this is a point where we lose the integer structure that we had. So this gets just down the scale. So, this gets just standard scaled. So, the mean gets translated to zero and the variance gets scaled to one. And I'm importing from the one that we have trained, the scalar that we have trained on our training data, and just applying it onto my vector. So, I'm scaling transform and making sure it tensor. And then I need to import the network that I trained. So, I specify some configs. So, this is my layers. I specify my learning rate, buff size, and momentum slope. But these are all about. Momentum and slope, but these are all about optimization, so these are not important. And then I initialize a network with the configuration, and then I load the weights, so I load the actual setup of the network from what we actually have trained. And then I insist that it's in evaluation mode because training neural networks have two modes. One is for training, and it's actually calculating all the gradients, and it has a lot overhead, and one is evaluation. Had and one is evaluation, which is just used for like actual prediction. So I want to do prediction, I insist in evaluation mode, and then I run through my example into the network and I get an output. And then I do a little bit of yoga just to get, instead of getting like a probability, getting like a 0, 1, where 0 is non-terminal and 1 is terminal. So if I run this, another warning, but this is telling me that it's not terminal because if the press is zero, not terminal because if the constant is zero I'm saying it with the has a very very small probability so if I'm between zero and one this is saying it's really really close to zero so we're very very sure and this was false to begin with so this is nice any questions about this before I go back to the slides so what if you actually divide it into three or whatever so less than a third probably between a third and a two thirds Probability between a third and a two-thirds, and two-thirds and one. So, if you do this, first of all, I mean, what is like the distribution like? And if you just looked at the two parts, which is more sure, what is the probability that it's actually right? So, I don't know. I think for some reason, okay, I haven't done this precisely, so I don't know the answer to your question, but for some reason, it seems to be that when it's predicted something is non-terminal, the probability is really, really low. Whereas when it's predicted something is terminal, the probability is more viable. The probability is more variable. So it seems like, and when it kind of when it's not trained very well, maybe I haven't seen enough training points and stuff, its default is predicting that everything is non-terminal. So it tends to just predict always zero, and then training apparently seems to kind of pull out the examples that are terminal from just predicting that everything is zero. But yeah, I haven't done the example that you're saying about, but it would be interesting to see whether like it is a, because I'm currently cutting it as you're going. Because I'm currently cutting it as the open file, right? And then I'm just saying everything less and everything more. Well, actually, maybe the actual thing is that I should cut it somewhere else. Any other questions? Okay, so just to yeah, so this is a learning curve that we produce. That we produce. So I've said that we have training on like 5 million samples and then testing on 5 million. So this seems a bit extreme, and this is mostly to justify that this was useful. So on the x-axis of this picture, we have how many training samples are used, from 1 million to 5 million. And on the accuracy, on the y-axis, we have the accuracy of our model. So when we are training on 1 million, I get an 88% training accuracy, and I get. Training accuracy, and I got less than 84% testing accuracy. So, this is telling me that it's overflifting a little bit and it's also not great. And everything was trained for like 100 epochs, which means that I'm kind of passing through the data 100 times. So, the more we pass through the data, the more it's going to overfit on the training data that it's given. And then, as we are increasing the training samples, the accuracy increases, and then the two lines kind of go next to each other, which means that it's over 50% less. And then we get to the top, which is Then we get to the top, which is almost 95%. Okay, and yeah, so I just want to talk about the consequence and tie back to the math that we had before. So off the back of this, we go and prove a new algorithm for target fan of varieties of Picaram2, which is 15 times faster than the original method and does exactly what we wanted to do, which was not building the fan and counting things by the combinatory criterion on the weight matrix data. I'm going to say though. Data. I'm gonna say though that this is completely unrelated to the neural network because the neural network is not seeing any integer kind of structure. Everything gets killed. So, this, well, instead, what we've done completely looks at like divisibility and nice integer properties. But it was kind of like just off the back of this nice result. But what actually we wanted to do is computing large amounts of data quickly. So, this allowed a kind of exploration of the landscape by generating large amounts of data, and we had the data. Large amount of data, and we had the table before saying that exploiting the fact that the machine learning model is nicely parallelized, I can put it on the GPU and it runs much faster, but also just in general, it's really only composing linear maps and slightly non-linear maps. This is much faster than doing any kind of lattice point count. Okay, so to conclude, we generated this kind of 100 million examples and then did some plots to see what they sit in space in R2. So, A and D here. In space in R2. So A and B here are again these kind of constants that we cared about, which are some growth coefficients of a very fine invariant of final varieties. And here they are colored by Fano index, which is our invariant, so it gives a nice grading. These two lines have a mathematical, like we can actually write down what they are, and they are precise. I know that all my varieties that have this specification will kind of lie in this kind of We kind of lie in this kind of code-shaped thing, which is similar to the way the period is space. We had a lower bound that was linear, and then everything was kind of to the right of a certain vertical line. And also, I guess some frequency. This is potentially where we go back to your point of like sampling is important. So, most of the examples that we are generating uniformly are random are kind of lying here, but then there's an extra like bit up here. This seems to be very common, but this is due to the fact that we're putting a bound for a uniform generation. Putting a bound for a uniform generation of our entries, and that means that we are more likely going to hit certain things that have some divisibility. So, if we were looking at a higher bound, that kind of like cluster of higher things will just get moved up and it's not actually real. But it's still interesting to see what they kind of sit in space. But you have to kind of keep in mind that your sampling is always important and not doing things. Okay, and we can also plot nice things on top of that. Plot nice things on top of it. So, in grey, we have the previous picture, just remove the colours, and on the left-hand side, we have product or wave regional spaces, which are picaround to tariffana varieties, and they exhibit the same gradient, and they're kind of in the same shape. And also, we have smooth, which are lying kind of on top. And this, we have some kind of like heuristic of like certain complexity of terminality should kind of make again some kind of gradient. Of make again some kind of gradient in a certain direction, of like the more complex your terminality, your singularities are, should be kind of lying in a certain direction. But yeah, it was just that's kind of interesting to like plot things and see what they look like. Okay, and I have like five minutes, so I'm going to conclude on mostly talking about limitations and the limitation of the approach. So there's a problem about out-of-sample performance. Out-of-sample performance. So, throughout this, I've been assuming that my wave matrices have coefficients which are bounded by something because I need to do some kind of uniform sampling of my entries. And this is problematic because once I get out of my bound, my network starts performing really badly. So, what am I showing you here? So, these are confusion matrices, which means that for each one of here, I have the predicted axis and the true I have the predicted axis and the true axis, and this is showing how many of predicted non-terminal are also truly non-terminal. So we want a model that is mostly black on the diagonal and very non-black on the non-diagonal. So this is saying that if I have a model that is trained with wave bound 7 and I test it on 10,000 samples, that I have an alpha bound of 5, so the upper bound is smaller than my upper bound of being trained. The company is doing very well. Been trained on one is doing very well, and then I can walk. This is at bottom six, and he's still doing well, and at bottom seven, he's still doing well. And from eight, nine, and ten, it starts kind of degenerating, and it degenerates to predicting that everything is not terminal. We can redo this then, and we can like retrain a model with wave bound 10, and we see exactly the same thing happening. So it's doing well when the wave bound is less, this is seven, eight, nine, ten, and then it degenerates higher up. So this is a, it raises up. It raises a question of what is the network approximating. So, is the network approximating a general behavior? And because neural networks are the way the neuronets are, the like-scale data and everything being between 0 and 1, once I kind of like go outside my sample, it freaks out and it doesn't know what to do. Or is he approximating a statement about small entry matrices? We are not entirely sure, but this is something to bear in mind about the fact that because. To bear in mind about the fact that because we're scaling things and the way neural networks work, it's not actually capturing exactly a mathematical thing, or maybe it's approximating it only within the certain bounds of our sampling criteria. Yeah, and other things that could be improved is it was more training data than we would like. So 10 million was quite painful to compute and it took roughly 30 CPUs or something like this. Yes, or something like this. So it was kind of more expensive. It would be nice to see if there's an architecture that maybe needs less data, maybe something that captures the kind of like pictorial thing or like uses the matrix not as a kind of vector, maybe can get more information out of the data that we have and this need less training. And also it'd be nice to, like next step, extract dramatic information from the sketches, so actually use the sketches as we have and do this in different contexts. This is in different contexts. So it'd be nice to either repeat the same thing or find a model that does not care about dimension and rank, because for now we have a specified architecture. So the entry needs to be this 20 thing vector. So that means if I change my rank on my dimension, I have something different and I cannot use the same network. So you can imagine like using attention or something and then kind of be more flexible about your entries and then you can actually do different ranks in different dimensions and then moving towards an unplugged variety is that. Moving towards unemployed varieties, that's kind of like more open ended up. It'd be nice to see what happens after that. And this is what I have to say. Any questions? Can you repeat how you always scale all your inputs? So the input is always scaled so that the mean is a zero and the variance is usually scaled up. mean is a zero and the variance is usually scaled at one. So it kind of looks an affine transformation. Yeah. Your input data was very simple. It was integer matrices. How do you think, or do you think it's accessible to put to have as input data really multivariate polynomials? For instance, when you ask, does this For instance, when you ask, does this system have a rational point? Or does it even have a solution? Because these things are very unlikely to happen. So I imagine this to be very difficult to generate reasonable data sets or to have the neural network detect these things. Do you think it's possible? No, I think it depends on the problem. I feel like I played around with the system of things, I think. And I think what I was doing was like kind of recording coefficients and the exponents of things and I was using that as my data. But that also feels like there's a lot of symmetries and things that you're not really taking into account because multiple polynomials will give you the same zero set. And then how you're really telling the network that these people are actually the same people and you should treat them in the same way is mostly maybe what will come to mind as the first concern. But then also just generating data in a sensible Generating data in a sensible way that kind of samples uniformly from some distribution. That's also the question of like what distribution you're sampling from, where do your objects live. But yeah, I think potentially there is scope to do it. I just, it requires a lot of questions about how you're presenting data and how you're vectorizing it in a meaningful way. And then again, because of scaling and things, how much of that meaning gets lost in the network, or maybe using the network to kind of like. Polynomial is just a spectrum of coefficients, so you could definitely enter it into the But a polynomial system is a subspace of functions, which is harder to oh I mean but you could just say I'm going to list the coefficients of degree V polynomials and then I have a fixed size of vectors I just keep doing that doesn't that doesn't give a good representation of a system though yeah I mean that is why I thought asking I mean it should probably be like a very intelligent person if it had Like a very intelligent person if it had this information. If it just looked at the polynomial without calculating, would it suspect whether it has a solution or not? But I shall if not, then probably this also can't. That looks like a human. Super. So with your poor out-of-sample performance, I mean, if you sort of compare that to more traditional statistical methods, or is it just data analysis? Methods, or is it just data analysis methods? It seems, it resembles what you would see if, for instance, you try to approximate an exponential function with polynomial functions or something like that. You get very poor extrapolation. So is there something that you could say in that respect? Is there a kind of classifier, kind of function that neural networks are more Networks are more able to learn? And are you actually looking at something else? And should you perhaps change something in your inputs to allow it to learn the right kind of function?