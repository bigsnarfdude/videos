Disadvantaged talk after the beautiful talk of early. So I want to talk about sketch-based community detection. This problem is collaboration with my former PhD students, Andre Bekus, who's now at the Air Force Research Lab. And my former student, also Mustafa Rahmani, was at Amazon Web Services. So the problem of community detection, as you know, is a problem in which you're given a graph. And your goal is to find communities or clusters in that graph. And communities means And communities means that you have clusters that have a denser, higher density of connection of edges within the clusters than between the clusters. Of course, there are many definitions of what a community is. So there's plenty of approaches that people have proposed for community detection, including more than I can definitely cover here. There's hierarchical approaches, spectral clustering-based approaches, even convex formulations, such as using matrix decomposition and other techniques. And other techniques. So, the motivation for this talk is that much of the work that you see, you suffer from questions of scalability. So, even if you have fast clustering algorithms that are polynomial, running polynomial time in the graph size, it becomes difficult when you have large graphs. The second issue is that of phase transitions. Is that if you look at many of the algorithms, the small clusters tend to be lost. You don't manage to find the small clusters or get. To find the small clusters where it gets overshadowed. And that's why you find sufficient conditions sometimes of these forms, lower bounds on the minimum cluster size, for example being square root n of the graph size. So in this talk, I'm going to talk about two things. The first thing, I want to try to address these limitations using the sketch-based approach, as you will see. The question we ask: can we reduce the complexity? Because you will see that there's a low-dimensional structure, of course, in this problem. You have a smaller number. In this problem, you have a smaller number of clusters than the graph size. So, can you leverage that so you can reduce complexity? The other thing is, can you improve the success probability when you have unbalanced graphs or graphs that are disproportionate in terms of the cluster sizes? And then, in the second part, I want to extend that to dynamic graphs. So, that sketch-based framework, we want to extend it when you have dynamic graphs that are changing over time. So, the communities themselves could be changing. And you could have evolutionary processes. And you could have evolutionary processes in the graphs, such as growth and shrinkage of these communities, or you could have communities that are merging, you could have things that are splitting, and so forth. So we're going to look at the stochastic block model. In the stochastic block model, you have, for example, this is the adjacency matrix. So the intra-cluster edges are generated with a Bernoulli P, Bernoulli probability T. And between the clusters, you generate the edges with Bernoulli Q. The edges with Bernoulli Q. And of course, P has to be greater than Q. And your goal in community detection is essentially to find permutations of the rows and the columns so that you can identify this block diagonal structure, in which you, of course, find, for example, deletions of edges over here within the clusters, and you also have additions of edges between the clusters. The number of communities is unknown. The number of communities is unknown, which is R. And as I mentioned, typically Q is going to be less than P. And what we're interested in is exact recovery. So we want the probability of the nodes being correctly labeled to go to one as the graph size increases. And we're going to be working, of course, in the regime where R is small. So because R is small, that gives us hope that, because there's a low dimensionality in the graph, so maybe you can do things much faster than some of the existing algorithms. So let me first give you at a high level. So, let me first give you at a high level how the sketch-based approach works, and then I'm going to talk about different guarantees and so on. So, you're given an adjacency matrix like this, maybe of size n by n. The first thing that you want to do is to obtain a sketch. So, you want to sample some nodes from the graph and obtain the subgraph that is induced by these nodes. That's the first step. And there could be, of course, different ways by which you can do sampling. I'm going to talk about different techniques, especially the first two. You can do uniform random sampling. You can do uniform random sampling, that's the simplest thing that you can do. You can do sampling based on the degree distribution or based on the degrees. There's other techniques as well. After you obtain the sketch, you want to do a community detection on the sketch, meaning you want to cluster the sketch. It's much smaller than the graph itself. Hopefully, we can only sample n prime, where m prime is much, much smaller than n. And of course, the community detection part or the cluster. That the community detection part or the clustering part is usually that part that's the bottleneck of the existing algorithms. For example, if you're working with the full graph, you would have something that is maybe polynomial. If you're doing test VD, for example, per iteration, you would have something that is polynomial in the size of a graph, maybe order Rn squared. But notice that now we're only going to be applying this to a sketch, to a much smaller graph. After you obtain the clusters within the sketch, of course, you're Clusters within the sketch, of course, your goal is to be able to cluster all the nodes in the graph, not just the nodes in the sketch. So, here we're going to have a retrieval step or a low-complexity operation by which you're able to cluster the nodes in the graph. That's at a high level how things are working. So, let me walk you through the different steps and give insight into how this is going to work. So, here is in sampling. For example, you're given a graph like this. Suppose that you're going to be doing uniform random sampling, so you sample n prime nodes, and then Sample n prime nodes and then you obtain a small sketch. Of course, you're sampling some nodes, you're sampling along the columns as well as along the rows because you obtain the subgraph that is induced by these nodes. Yes? Do you ever consider other sketching or techniques like smoothing the graph first or something? No, we're sampling nodes. In this case, we're sampling nodes, but you can sample the nodes in many different ways. So you can sample the nodes, for example, just uniformly at random. Example, just uniformly at random, or you can look at the degrees and sample in a way that is inversely proportional, for example, to the node degrees. One other technique that we're doing is what we call spatial random sampling, in which you take the adjacency matrix and you project it on the unit sphere, and then you obtain some random directions of the unit sphere, and then you try to find the columns that are closer to these random directions. But we're sampling nodes at the end. But would that be like linear combinations? That last case? That last case? No, it's not a linear combination. I was going to say something like people like, do you think? I was gonna sound like people like do things like kind of run some thing on the adjacency to get something that's like a weighted average of your neighbors or something. You know, like if you're two steps away, then you get like a one five or something. I'm just curious if yeah, we have not looked at this approach, but yeah. So in the next step, you want to do clustering, and you can really choose any algorithm of your choice to do the clustering. Here we're doing low rank plus parts decomposition or correlation clustering. Decomposition or correlation clustering, in which you try to find the partition of the sketch or the partition of this graph that minimizes the number of disagreements between the actual graph and the partition that you obtained. This is, of course, reminiscent of the low rank plus sparse decomposition, where this lower ang component captures the different clusters that you have here in the sketch. And the sparse component captures the edge additions as well as the edge deletions that you have within the communities and between the different communities. Again, this is only. Different communities. Again, this is only applied to the sketch, and you can also choose any algorithm of your choice to do that, to do that clustering of the sketch. And then you can do retrieval very simply. The idea is if this is the sketch, the partitions that you obtain for the sketch, then you have these characteristic vectors. Then you can look at the rest of the nodes. If you look at the rest of the nodes, all you want to do is to look at the node. How much, what's the degree of connectivity of that node to the cluster that you've identified in the sketch? And you choose the cluster. The sketch. And you choose the cluster that it's most connected to. So essentially, what you're doing is really finding an inner product between these characteristic vectors that correspond to the sketch that the clustered sketch that you've obtained with these columns AIs that correspond to this matrix. And of course, because some clusters could be smaller than others, then you want to normalize this by the size of the community in this case. So, for example, if this is very small, the size of this particular community, then this increases. Particular community, then this increases this value. And then you choose the maximum value. That's the retrieval step. So let me show you some results for this approach. So here we start with a simple case in which you have a graph that has two blocks that are equally sized, so there's a balance. We're only sampling 200 nodes, and we start from graphs that are of size 500 all the way up to 10,000. 10,000. And we're only sampling 200 nodes. These are the values of P and Q. And also we have rho, which is the observability, because you don't necessarily observe all the edges. So you observe them only with probability 70%. And what we're plotting here is the runtime as a function of the graph size. So if you look here, this is the approach that we have as a function when we do uniform random sampling. And these are some of the state of the art techniques that do full scale clustering. That does full-scale clustering. For example, the approach by Chen, this does a full-scale Lorang plus pars decomposition. Here I'm showing you the phase transition as well in the plane of N prime and N. And as you can see, for example, with a very small value of N prime, you're very successful. The white regions show the success in being able to cluster the full graph or to do community detection. And as you can see, as you increase n all the way from 500 to 5,000, you're going to be able to. To 5,000, you're successful, and here also the time is not increasing because the size of the sketch is pretty much not changing. A huge, a huge saving in time context. In terms of results, of course, there's lots of regimes that you can look at, but let me just give you a flavor of some of these results. So, for example, when the cluster sizes are linear with N and P and Q are constant, because the density gap, which is the difference between P and Q, is an important parameter in many of these problems. Parameters in many of these problems. So, in this regime, you see that the node sample complexity that we have of our approach is nearly dimension-free. It's polylogarithmic in n. What's log n between friends? So, n prime is really very small compared to the full scale approach in which you use all the nodes. And the per iteration complexity, because as I said, the bottleneck is the clustering step, you're only clustering a very small sketch, and the retrieval step is very, very fast. And the retrieval step is very, very fast because you don't need to do any clustering. It's order log to the 4n. So again, polylogarithmic n versus something that is order rn squared. So you see a huge saving in the computational complexity. But then a problem that I mentioned at the beginning is that some of the clusters could be very small. The graphs could be unbalanced. And if you're going to use something like uniform random sampling, then whatever imbalance that you have in the That you have in the original graph is also going to carry out to the sketch because the probability of sampling from the bigger cluster is going to be higher, much higher than the probability of sampling from the smaller clusters. And when you have these disproportionate cluster sizes, then you end up falling into problems of being able to cluster the graph well, and the small clusters are also going to be lost. And as I mentioned, that's why if you look at, for example, the work by Chen, which does the full-scale decomposition, they have a lower bound on the minimum cluster size, which is Lower bound on the minimum cluster size, which is order square root of mu. So, what do we do? How do we address this issue? So, the hope is that we can sample in a way such that we want to increase the size of the minimum cluster in the sketch. Ideally, I want to be able to sample more from the smaller clusters, so I can end up maybe with a balanced sketch, and based on that, I would be able to obtain better results in clustering. Right? So, the idea that we have here is that you can do sparsity-based sampling, what we call sparsity-based sampling, is that you want to sample in a manner that is inversely proportional to the node degrees. And to do that, it's quite simple. This is if you do uniform random sampling in a case like this, where you have two small clusters and one big cluster. Of course, the probability of sampling any node is 1 over n. So then you end up with a sketch that looks like that. But then, if instead you sample the node. You sample the nodes in a manner that is inversely proportional to one over the L0 norm of the columns of adjacency matrix, then this is going to be essentially inversely proportional to the size of the cluster. And you end up with a sketch that is balanced. And then you can improve the phase transitions of algorithms. We're not breaking the data processing inequality or anything. It's just that the full-scale algorithms are not robust to the imbalance that happens in the graph. If you look at sample results of this sparsity-based sampling approach, in this case, these are the parameters that we have in this problem. If you look at our approach in terms of runtime, as you increase the size of the graph, it's achieving significant speedups compared to the full-scale approaches. So, to give you a flavor of some of the great regions in which we can operate, look at the region where, for example, P is. Look at the region where, for example, P is constant, P is the connectivity, the edge connectivity within the cluster, and Q is between the cluster. So when Q goes to zero, meaning you are in the low inter-cluster connectivity regime, so it goes sufficiently fast, as maybe log n over n or something, then the best, the state-of-the-art result for full-scale clustering due to chi is that n minimum has to be order log n. So we achieve something very, very close. So we achieve something very, very close to this, while also being extremely fast. I mean, you're very close, it's almost polylogarithmic in N, but instead of having a node sample complexity that is N, we're only sampling a small number of nodes that is only polylogarithmic in N as well, and in turn you achieve these significant speedups in clustering the graphs as well. And as you can see, we're improving the requirements on this. On the size of the cluster as well, on the size of the smallest cluster. In the full case, for example, you have something like square root of it. Yes. Did I say you see you're sampling more low-degree nodes or high-degree nodes? You're sampling more the low-degree nodes. And in these regimes, is the degree of the node correlated with the membership degree or not? Well, it's an SBM model. So all the nodes are connected with probability. Within the cluster, they are connected with probability P. With nodes outside the cluster, they are connected with probability. Nodes outside the cluster, they are connected to the network. There are some regimes of P and Q where you can just look at the degrees, you'd be like, okay, all the big degree ones are probably the same P in low degree ones. Just by looking at the degrees. And there's some regimes where the degrees are informative because they're kind of, by design, technique you just sell. That's correct. And actually, this is why in this case, because we didn't want it to be an easy problem, we had two small clusters as opposed to one. So if you're only looking at the node degrees, you're not going to be able to identify. Then all degrees you are not going to be able to identify all the degrees. So you can't cheat just by looking at the degrees themselves. Exactly. Exactly. So that's why how we did it this way. Thank you. So I think that's because the unity sizes are unknown, right? Yes, the community sizes are also unknown. Okay, so this is again for the unbalanced case. Let me show you. This is the phase transition if you use uniform random sampling when you have two small clusters and one large cluster that is of side f. And one large cluster that is of size size n minus 2 n min, and the size of the graph is 5000. So this is the phase transition with uniform random sampling. This is the region of failure. This is n prime, the number of nodes that you sample or the size of the sketch. And this is the minimum cluster size. With SBS, the sparsity-based sampling, you can see that, for example, even when the minimum cluster size is 200, only 200 nodes would be sufficient for you to be able to cluster. For you to be able to cluster. And that's because you're able to sample more from the smaller clusters. And that's what we're showing here. This is the minimum cluster sample probability as a function of the minimum cluster size. If you use random sampling, as you can see, when the minimum cluster size is small, you're not sampling much from the small cluster. Whereas with using this SBS, you are able to approach the one over R. 1 over R is R is the total number of clusters. So if you have three clusters, ideally you want to sample one-third from each. You want to sample one-third from each one. So, here we're also showing the probability of success as a function of the minimum cluster size. Let me try to go a little bit quicker. So, we wanted to extend this idea to dynamic graphs, meaning graphs that are also changing over time. And of course, dynamic graphs appear in many, many applications. So, in dynamic graphs, you could have snapshots of the graphs, and the clusters themselves could be. The graphs and the clusters themselves could be changing over time. Nodes could be moving. For example, first we started looking at node-level events, and then we started looking at high-level cluster events, as I'm going to show you. So, in node-level events, you can have, for example, some node that moves from one community. This is one snapshot. In the next snapshot, the node moves from one community to another community. You can have a node that is deleted from the graph. It just disappears. You can have a node that Disappears. You can have a node that joins one of the existing communities. These are node-level type of events. You can have regeneration of some of the edges according to the SVM. So, for example, this generative model that we have, imagine a situation you have three clusters and we have all these different node events that could be happening. 20 nodes at each time step move change the cluster membership. And you can have also one node being deleted from the blue cluster, and that's why you see that as time passes by. You see that as time passes by, the blue cluster gets smaller. One node added to the red, and that's why you see that the red cluster is getting bigger and larger. And then you can also have 10 edges being regenerated. And is there a way to be able to solve these problems in an efficient manner as well by leveraging some of the sketch-based techniques that we talked about? So, as a first-level idea, why don't we do it snapshot by snapshot? Just look at every snapshot. You can have a graph, you can sample. You can have a graph, you can sample a small sketch from the graph, cluster the sketch, apply a low-complexity retrieval step so that you can obtain the clustering of the whole graph, and then do the same thing in the next step. But of course, you have to also match the clusters that you obtain in the first, at time t, t minus one, to the ones that you obtain at time t. And one way of doing this is, for example, to match the clusters using the Jacquard index, which tries to find the best matching. The best matching between the clustering that happens at time t minus one and the clusters that you have at time t. And then you can do it. And we call this approach the sketch-based independent clustering. But of course, this approach is not going to be leveraging the temporal smoothness that you might have in dynamic graphs. It's not leveraging information that you had from the previous snapshots. And that's why we have a different approach, what we call here an efficient single sketch clustering. Single sketch clustering. How much time do I have? So in this case, imagine that you have a sketch that you obtained in a previous time step and you clustered the graph. Then you can maintain the same sketch, but you want to update it. So when you go in the next step, you want to update that sketch. For example, imagine that in the next step, some of the nodes have left the graph. If some of these nodes have left the graph, then you need to delete them from the sketch if they exist in the sketch. So you delete them from the sketch, and then you. So, you delete them from the sketch and then you add new nodes from this graph using the clustering that you obtained in the previous step. As well as if other type of operations also happen, you can update the sketch and then you obtain the clustering and so on. So, you can maintain the same sketch but keep updating it using the clusterings from the previous step. In this case, you don't, I mean, automatically, you don't need to do something like a Jacquard index because it happens inherently during the process. And of course, the retrieve. And of course, the retrieval step is, as I told you, just look at the node, you see how much connectivity it has to the clusters that you obtained in the sketch using this simple operation. So it's a low complexity retrieval step. If you look at some of the results in terms of timing, this is as a function of the graph size. These are the two approaches that I mentioned. One of them is the independent, which treats every snapshot separately. And the other one is the one that uses the efficient single sketch. And this is first for the Sketch. And this is first for the simple case when things are not imbalanced. So we have only two clusters that are of equal size, and we have these different node events that are taking place. And we're comparing here also to another dynamic clustering algorithm, which is by DIN. I can tell you more about this approach at some point. This approach actually treats clusters as super nodes, but then as changes happen to the edges, it needs to start adding a lot of nodes to the graph. A lot of nodes to the graph, and then the graph size becomes big, and then the runtime also starts increasing. We tried this also on a situation where you don't have equal cluster sizes. So the clusters are not equal. So for example, you start with 600 for the red, 400 for the green, and 200 for the blue. And you have all these different node events. This is the estimation of the clusters that you obtain if you use the sketch-based independent that does uniform random sampling. If you use the Sampling. If you use the sparsity-based sampling, like I mentioned, it does much better, and actually, even better than the full graph, if you do the full graph standard independent. And what I'm plotting here is the normalized agreement between the clustering, the clustering that you obtained, the estimated clustering, and the actual clustering. And this is normalized by the size, by the number, by the number of communities that we have. So we looked at the effect of adding nodes, deleting nodes, and so on. We have very similar results. And so on, you have very similar results. But I want to go very quickly to show you the evolutionary processes that could also happen in a graph. Because you could have, for example, some clusters they could be shrinking over time. Some clusters could be growing over time. You could have a community that splits into two. Or you could have a merging that happens between communities. You can have a birth and death of communities as well. So are we able to capture high-level cluster events like these as well? Cluster events like these as well using the sketch-based approach. So we presented a benchmark. A benchmark is essentially a process by which the graphs, these different things could be happening, the birth and the death of communities, the merging and split, the growth and the shrinkage, and then you compare different algorithms, how they perform in these cases. So this is the full graph. If you look at the planted partitions, you see that you start here. This is the birth-death setting in which this is. Setting in which this is driven by some kind of a triangular wave, in which these are increasing. This cluster is increasing in size, this cluster is getting smaller and smaller to the point that it eventually dies. So this is the birth death. And then the opposite happens to the other cluster. This one is reborn, and this one at some point dies. In the merge split, you generate edges between the clusters when the edges, when the number of edges increases so much that you cannot anymore tell the difference between. You cannot anymore tell the difference between the communities, so they start merging. In the growth shrinkage, some nodes are moving from one community to the other, so one is getting bigger and the other one is getting smaller. So that's the planted partitions in the full graph. And then you can obtain also the sketch. The sketch can show you the planted partitions, but in a much smaller sketch of a much smaller size. And hopefully, you'll be able to capture all these different events that could be happening in the graph. In the graph. So, okay, so we have an algorithm. This is a high-level description of the algorithm, how it works. You start with a small sketch. You're given the different graph snapshots. You want to start by clustering the initial snapshot, but you do it only using a small sketch, by sampling the small set of nodes, uniformly at random. And you can use any type of algorithm, but you're only applying it to a small sketch. But you're only applying it to a small sketch. And then you want to do a retrieval to infer the membership of the other nodes. And then for every other time step, you want to update your sketch by including, you try to maintain a balanced sketch so that the size of the clusters within the sketch do not change much. And then there's a way to detect the birth in the sketch if there are any nodes that are being reborn. And then you infer the community. Reborn, and then you infer the community of the membership of new and move nodes. Then you want to split, detect, detect the split. We have a split indicator. The split indicator is we look actually at the second eigenvalue and compare it to the square root of the first largest eigenvalue. That gives you an indication of the fact that now the density gap is increasing. So there's a split that starts happening inside the communities. And then after that, if you And then, after that, if you know that there is a split that has happened, then you can also obtain small sketches of these small communities and cluster them quickly, also using a sketch. And then you can identify the membership of the rest of the nodes in the graph with respect to these. And then you detect also the merge. And then you build, at the end, you build an estimate of the full graph. So let me show you very quickly some of the results, for example, that we have for the growth, shrink, benchmark. Here we have four different communities. Here we have four different communities. One of them is growing in size here, and the other one is getting smaller. And then the opposite starts happening, and the same is another concurrent process, is also running at the same time. If you look at our approach, for example, in terms of runtime, and as a function of the size of the community inside the graph, you see that our approaches in terms of runtime is much, much faster. This is, by the way, logarithmic. And you can also see that in terms of And you can also see that in terms of agreement, the normalized agreement, this is our approach. Here we're plotting the normalized agreement as a function of time as well as a function of f. f is a parameter that controls how small during that evolutionary process the clusters how small they become the larger f is the harder the problem is and as you can see if when f gets larger you see that other algorithms one of them is bayesian you see that other algorithms start losing track of the small clusters this is the This is the birth-death benchmark. We have similar results for the mixed benchmark. This is the estimation of the clusters in the full graph based on what you estimate in the small sketch. This is what you get using other techniques. And in terms of complexity, our approach is essentially: it has a complexity that is almost n prime cubed, where n prime is the size of the sketch, much, much smaller than n versus order of n cubed. Versus order of NQ. And that's why you see the huge improvement in runtime. We actually have results also on analysis results, how to be able to detect these different events that could be happening in the communities. I'm just going to go over this. We have phase transitions. We verified that with theory as well as in simulations. I don't have time to talk about this. So probably I'll just wrap up. So we have a sketch-based application. So, we have a sketch-based approach for community detection, which improves both the computational complexity as well as the sampling complexity, and improves also the requirements on the size of the smallest cluster. And we give guarantees on the sampling computation complexity in different regimes, as well as the minimum cluster size that you need. And then we extended these to when you have evolutionary processes that are happening in dynamic graphs. We presented different benchmarks for these evolutionary processes. The birth death is actually very new, but that has not. Actually, very new that was not in the literature before in terms of dynamic graphs. And obtain sketch-based techniques to be able to detect these processes and produce the estimates of the communities. Any questions?