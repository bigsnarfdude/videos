Okay, so let's get started then. Good morning, everyone. And again, I apologize that I couldn't make it in person, but I'm glad to have this chance to at least present my work virtually and interact with everyone. So I would like to talk about our recent series of work along this line of spatial temporal point processes with deep kernels. This is joint work with my students. With my students, John Dong, my former student, Xixian Zhu, who's Carnegie Mellon, assistant professor, and Haoyun Wang, my student, and collaborators, Dr. George Mateo, who's one of the organizers. And his former student, Dr. Rodriguez Quartz, and my current collaborator, Xiu Yan Cheng, at Duke Math Department. Here's the outline of my talk. So I start with. So I start with motivation and then I'll start to talk about the model. And this model started to be developed as a tool to solve a particular problem. And I'll tell you a story and then I'll show the result of feeding the data we have in hand with this particular model. And then I'll talk about extension of this motivated by the developments. So I get a data from I get a data from Dr. George Matur. And so he reached out to me one day with email saying that he has some COVID-19 data. So this during COVID-19, about two years ago. And this is a COVID-19 data from the city called Cali, Colombia, which is the second largest city in the country. And so the interesting part of this data is that it has a very high resolution. It has a very high resolution. Usually, in the US, we have only aggregated data. For example, we know how many cases are diagnosed each day in each county in the US. But in this particular data, it is for individual cases, for each individual COVID-19 for about 26 weeks from March 15 to September 30, 2020. We know the We know the exact diagnosis date and exact geographic location of the residents. We know where this person is living. And then, so on the right-hand side, this shows a map of the city. And so, in total, we have about 38,611 cases. And so, given this high-resolution data, which is kind of rare to have, and we want to To have, and we want to say, can we make advantage of this high-resolution data and developing a model for the data so we can understand how COVID-19 is spread from location to location over time and also be able to interpret the results? And hopefully, this can be used to make some predictions as well. Okay, so that's motivating our approach of using spatial temporal point processes to model this high-resolution. To model this high-resolution individual case data. And so our goal is: since this data is very high-resolution, and we want to develop, we have a tons of data, right? We want to develop a stationary, go beyond a stationary model and develop a possibly not stationary, at least not homogeneous spatial temporal model. And so this means that we believe the spread. Believe the spread pattern of the disease can differ from location to location. And so, how can we capture the spatial variation of the spreading pattern? And eventually, can we also capture the non-stationary, non-homogeneous spatial temporal pattern? Means that the spreading pattern can also be different over time and also over space. And so, in order to develop this model, we Model, we also bring in this new approach, which is to parametrize the underlying kernel in the point process using deep neural networks. This can help us to harvest the strong representation power of deep neural networks to achieve our goal. And so, besides that, we also want to be able to take into account the certain location information of the city landmark. Information of the city landmarks. And for example, the landmarks could be the airports, the churches, and the markets. And those are places with high population densities. And so this landmark information can help us to figure out where the hotspots or high-intensity COVID-19 cases tend to be. And this is a visualization of the data I received from George. Of the data I received from George and his students. On the left-hand side is the case density by Comuna. I think Comuna is the concept of a county in the, you know, in their country in Colombia. And so, and for example, this particular county here has an airport. And so this is the case density estimated by kernel density estimation. And so besides that, we also since So, besides that, we also, since we have a temporal information and data, and this is a visualization of the number of confirmed cases from the beginning, which is March 29, 2020, you can see there are very few cases, just the initial onset. And as time goes on, there's more and more cases. And this is roughly speaking the peak of one wave. You know, there are multiple waves. This is actually one of the first waves. The peak of the first wave. The peak of the first wave in COVID-19, you can see the density is very high, and then the peak is gone, it's getting better. And so, clearly, there's going to be a temporal correlation and also spatial correlation because as we can imagine, that if you know, as time goes on, people start to spread COVID-19 cases and that caused temporal correlation. And we verify this indeed. We verify this indeed by looking at, for example, just looking at the aggregated number of confirmed cases in two communas. And it shows the partial autoregressive conditional function. And then you can see this decays over the legs, but there are pretty significant temporal correlations. And we also tested our assumption that the number of cases between Cases between the different communas or the counties can be correlated. So there's spatial correlation. And the spatial correlation seems to be decaying with distance as well. And so this is again a verification using data. You can see it plots the temporal correlations across time series for five arbitrary locations. And also these other ones. These other ones that are in their neighborhood, so we also plot this correlation coefficients with a decayed distance. Actually, this is the smallest distance, this is larger distance. As distance increases, we can see the correlation indeed drops. So this indicating there seems to be this drop of the correlation in the number of cases. And almost like the longer the distance, the weaker the correlation. The weaker the correlation there will be. So, given all of these observations, this motivated us to develop this model that we want to be able to capture the spatial temporal dependence pattern and also be able to utilize the pretty precise space and time information in the data point. And so, in order to utilize the spatial and the temporal The spatial and the temporal information in each individual data cases. We are going to consider the point process model. And since the temporal dependence is strong, and there's also the spatial dependence, we will use the Hux process, which is a self-exciting or self-dependent point process approach, which was introduced by Alan Hux in the 70s. So this workshop. So, this workshop is about point processes over space. So, I think, especially this is the last day, I think we have probably heard a lot about this. So, this is just a quick recap about this kind of point processes, especially self-exciting point process or known as the Hawkes process. So, consider the one-dimensional cases. The observations is a sequence of random events at times t1, t2, and so on. And we can define the We can define the measure of the law of the point process using conditional intensity function, roughly speaking, as the conditional probability of an event in a small interval given the past history. And so in our observation, we have a spatial temporal point process. So the observations is over space and time. The observational space is from some time horizon 0 to graph to t. Time horizon 0 to capital t to a two-dimensional spatial region. So that's our observational space. And the observations are indicated by an X, and it has at least the time and the location information. And for some other kind of data-related nature, sometimes we have the so-called marks as well. So it could be a mark process, for example, tells us the nature of each incident. Each incident. And the history as a filtration is basically information from all the past before time t. And so the conditional intensity function can also be understood as if a testing count, expected number of counts inside this very small region over time and space, condition on the past. So this is a, we can generalize the Hawkes process for. We can generalize the Hawke's process for spatial temporal setting. And so, in particular, the Hawke's process wants to be able to model the impact of the past observations over space on the current conditional intensity function. So, the conditional intensity function is a function of the time and location s, and usually it contains some deterministic part, thus lambda zero. This lambda zero can also depend on TNS. And the second part is going to be the data-dependent part that depends on the past observations. So this is a counting measure. And then this is basically a, I can think about as inner product or convolution. And so I write this in the most general form. And this part means the past incident happened at time tau before time t. Time tau before time t, and some location u, which can be different from s. How does this incident impact a future event at time t and location s? And so this impact of the past event at different location is going to be carried through this triggering kernel. And this triggering kernel is a function of the current location and the time and the past. And the time, and the past location and time of the observed observation. So it is a triggering kernel, we call it a triggering kernel. And so, in fact, there has been quite some study in the past about the spatial temporal self-exciting point processes. And one notable contribution is the so-called ETOS model, which was developed by Ogada and collaborators and students since the 80s. And students since the 80s. And so they basically parametrize this spatial temporal trigonometrus using a Gaussian diffusion kernel. So you can see that this trigonal kernel, which captures the impact of the past to the future, is decomposed into two parts. One part captures the decay over time of this triggering effect, and the second part captures the decay over the Captures the decay over space. So the larger the distance, the smaller the triggering effect of the past incident. And in particular, this also can depend on the t minus t prime, which is how far away the past event is relative to the current time t. And so this EDIS model was developed during that time for studying earthquakes, and it has become pretty popular ever since. And it has become pretty popular ever since then. So basically, they want to capture how the past earthquakes impact the current earthquake. And in that case, the earthquake also has a magnitude for each recorded incident. So that magnitude is additional mark. And so this kernel can be further modified to consider the mark information as well. But one particular feature is you can see this is shift invariant, and so the kernel. invariant. And so the kernel trigging impact only depends on excuse me, sorry. Only depend on how far away the past event is relative to the current time and also depend on how far away the past incident is relative to the current location. So it is shift invariant. So if you want to develop a non-stationary kernel, we have to go beyond this shift invariant setting and then Invariant setting and then just consider this kernel in the format that doesn't necessarily depend on the difference in the location. And so to simplify things for our COVID-19 study, start with, we're going to make the assumption that the temporal part is still shift invariant. And so I'm going to consider this particular parametrization of the kernel. So as I talk through, you can see the main idea throughout this line of work is Throughout this line of work, we want to find a good and a strong and also interpretable representation of this influence or trigging kernel that we can use to represent our data and model our data. And so we still consider this trigging kernel as a product of two kernels. One is the temporal part and the other part is the spatial part. And so this temporal part, to simplify, we still assume the decay. We still assume the decaying, we have seen that the Gaussian kernel. And so this controls, the decay speed is controlled by this parameter, sigma 0 squared. And then this multiplies the spatial kernel. And so in making this assumption, we're assuming the space and the time impact are decoupled from each other. And as I will show later on, that we can further generalize this. Further generalize this to also incorporate the kernel that can allow spatial-temporal interactions. Okay, so now we have this kind of non-stationary spatial temporal kernel, and we know how to model this time part. But what about the space part? And so our idea comes from the representation of the kernel in a different context, which is for the Different context, which is for the Gaussian process. So there is a highly cited work by Ecton, Swall, and Curry in the 90s for extending stationary Gaussian process to non-stationary Gaussian process. So basically the idea is to say that in studying and model of the Gaussian process, the kernel also plays a very important role. For example, for a stationary Gaussian process with the so-called carbon. Process with the so-called corollogram on the rhode, and usually it can be decomposed as the inner product of two kernels. And C is a distance between two locations. Once we have this decomposition, then we can decompose the Gaussian process as a spatial convolution of a white noise process with this kernel. But this is for stationary. So basically, this work. So basically, this work extends this to non-stage regression process by instead of having this kernel correlogram, which only depends on the difference of the two locations, but consider this is depending on both the path, you know, two locations. These two locations are S and S prime. And then we have a kernel associated with each location. And then you consider their inner product. And so this motivate us think about. And so this motivated us to think about maybe we can use a similar kind of approach to represent our space kernel for the considered non-stationarity over space. And in their consideration, they consider the Gaussian kernels. So the Gaussian kernel is parametrized by, in particular, the covariance matrix. And so that motivates our study. And if you think about what this covariance matrix looks like, it's basically usually can be. Basically, usually can be parametrized by three parameters. The A, A square represents the variance, and the B square the variance along the X and the Y axis over space. And the rho captures the dependence of the two directions. And so we can control the A and the B and the rho to vary over space. And that can give us a different kind of shape of the kernels. And this plot basically visualizes that. Basically, visualize that particular effect. You can see, for example, in this particular kernel, this particular part, the kernel is more isotropic, it doesn't have direction. And then we can vary the row, for example, over space, and it gradually changes the shaping here. So, we're going to take this approach and then use this for our purposes to represent the Wing process kernels. And so, in the prior work, they basically go ahead to take the kernel for the Gaussian process and represent it by taking samples. They call the NOT locations. But in our setting, the idea is to say that we still find a representation through these basis functions. In this case, the basis function is this underlying Gaussian-shaped kernel controlled by three parameters. kernel controlled by three parameters and then this parameter itself depends on the location. And so instead of taking samples, which is the excuse me, sorry, the old approach, and now we can combine this approach with neural networks. And the neural networks is a function that maps each location to all the parameters we need. And so since the neural network is known to be a good function approximation. Function approximation, and then we can fit the model and let the neural network do the job to figure out how the parameter changes over space, and that can give us a desired spatial variation pattern of our spatial kernel. And so we have tried two approaches to represent our kernel based on this idea. The first one is again taking The first one is again taking this kernel, and so we can take this, this is a conditional intensity function where space and time, take this kernel, and then this kernel we can represent as a mixture of the Gaussians. And each Gaussian component can be parametrized by these variance over space and time, over the two coordinates, and also over space location. Over space location S, and also the row S. So this requires three neural networks to represent each of the parameters over space. And now we can combine them together. And this example shows that. What do we mean? So for example, why do we consider a mixture of such Gaussian kernels? Because that gives us a better representation power further enhanced. And so we can see that on the top, this is one particular. One particular Gaussian kernel varies over space, and this lower one is another Gaussian kernel. When we combine them together, we can form a pretty complex shape of this influence kernel. And then this maps into, you know, by convolving this with the counting measure of our point process, this can give us a pretty complicated representation of the conditional intensity and how this varies over space and time. And this was a And this was a work done in these two, our prior works, and we used those to study the seismic activities in North California. And so this is the original ETIS model. And the other two plots are two results coming out of our deep kernel representation through this approach. And so when we show this to our collaborator in geophysics, they're quite happy to see that this kernel. To see that this kernel is able to capture this intensity here, indicating the higher chance or higher condition intensity function for future earthquakes. So our collaborator commented that this result makes a lot of sense to them because this pattern, you can see the stripe is along the coastal line and this one is along a known fault called St. Andreas Fault that is an area that has higher seismic Has higher seismic activities. And so we have, after we have posted this paper, which is published in the Machine Learning Conference and IEEE Transaction on Kernel Knowledge Discovery and Engineering, I think that's where George find us and sent us an email and they started collaboration and in particular working on the COVID-19 data. And so this is another visualization of what this deep kernel can offer. On the top is On the top is the kernel representation by superposition of these Gaussian kernels. And on the bottom is the traditional ETAS model by Ogata. And so you can see, number one, this kernel indeed can vary over space and time. And number two, as you can see, the ETAS model usually is whenever an earthquake happens, it place an isotropic ball and atom together. Ball and add them together. And versus this non-stationary space and time kernel, and you can see the shape of each earthquake can cause different impact. You can see the shape is different. And this turns out to be important, I think, in particular in the earthquake context because the impact of one earthquake is not necessarily isotropical. It should have direction due to the geographic structure of the Structure of the underlying structures. And so I want to explain a little more and also saying that besides representing this using A and B and a rho, the variance and the correlation coefficient, three parameters, there is another way to represent it which is equal, equivalent through the full set of representations. And so this is another visualization you can see if we fix the You can see if they fix the center, and then by varying this foci, the Q and alpha parameters, and we can create a series of continuously varying kernels. And that's quite interesting to us. And so when we study the COVID-19 data, we take this approach as well. And so, more generally, we can write this space kernel as. Space kernel as inner product of this basis function. So this basis function phi s is one for the current location, the other one is for the past location. And so this phi s is a weighted average of our fixed basis kernels. And in this study, we choose these basis kernels as the Gaussian kernels, again motivated by the power study, and then Study and then the Vs spatial kernel. If I choose the basic function to be a mixture of Gaussian this way, and the number of Gaussian we use here is this capital R, and then usually this R is chosen to be small, and we can choose this R by cross-validation. And in our study, we find out usually when this R is 2 or at most 3, we can have already very good results. Good result. And so when we choose the phi to be a mixture of Gaussian, and then we can observe that this inner product of the phi s can be simplified to be this expression. So this basically, you can see what this looks like. It's like we take a pair of Gaussians and then we define a new Gaussian kernel. This new Gaussian kernel depends on the sum, excuse me, sorry, the covers. Sorry, the coherence this past slide. The coherence matrix depends on the sum of the two coherence matrices coming from the past and the current locations. And out of the different modes, R1 and R2 are the different basis functions. And then we also can learn the weights and how you know weight we can, how much weight we can get to each one of these such kernels. And so this is one visualization. What does Visualization, what does this give to us? And essentially, like we said, through this kind of representation of the spatial kernel, this can give us pretty complicated spatial homogeneous shape of the kernel that captures how one incident impact can change, you know, this impact change over space. And with this kind of flexibility and strong representation power, we can fit the data and let the data tell us. Data and let the data tell us what does each event impact over spread over space. And so I mentioned that we also want to consider the landmarks, which are the locations of the airport, the city centers, the city hall, and the, for example, markets, supermarkets, which are the high concentration of the populations. And then to consider these landmarks, we can focus. Consider these landmarks, we can further introduce this component here, and that can based on the location of each city center. And then maybe this is the Gaussian kernel, for example, we can choose it to be centered at each landmark and decays over distance through some Gaussian-shaped covariance matrix, and then with some weights. And so, this is the final density condition function we want to be able to learn so that has. These parameters, gamma L, sigma L, and this kernel is parametrized by a mixture of Gaussians that I have presented before. And the Gaussian parameters into this kernel are parametrized by neural networks. And so how do we learn this model? We use maximum likelihood. And so this is an expression of the maximum likelihood likelihood function, which can be written down in closed form expression. And usually And usually, the computation of this integral part can be the most difficult one. And in our study, it turns out we can approximate this integral pretty well. And so we derive this almost loose-form expression. And so essentially, we can perform the computation pretty efficiently and using this expression. And so, since we are going to wrap So, since we are going to represent the parameters using neural networks, and there's a choice we have to make about the neural network architecture, so we did the study and find out a particular architecture, which is a feed-forward neural network, with 3216 as a number of nodes in the hidden layers, serve our purpose and have the best performance. And now, let's see how this performs on the real data. And so, we first And so we first fit this model using maximum likelihood, and again, that's solved using to solve maximum likelihood function. We can use green descent, which I don't include all the details here, but I think similar to the previous presentation, that we can use, you know, implementation is we can use the PyTorch, implement the, you know, forward backward. Or backward gradient descent approach for neural networks. So it can be built upon existing packages. We also release the code online. If you're interested, please contact me and we can share our code. And so this is a comparison of in-sample performance to see how our model is performing. And so we compared with many approaches, and these two are our approach. We consider compared with ETAS, which is this yellow. Which is this yellow, and so this AR3 is if we combine all the COVID-19 data into one time series, so because AR3 is only a time series model, one-dimensional, it cannot capture the spatial part. And so you can see among all the models, the approach, our approach, this black line, has the best much data fit to the overall data. All data and it also performs much better than ETAS, which is this yellow line. And this is in particular, quantify the performance. You see our approach with R equal to 3 using 3. Make sure Fialsis has the best likelihood function. And also, you know, have pretty good. We not only look at average, but also look at the quantile. So we have pretty good. Quantile, so we have a pretty good line here as well. And this AR3 seems to have the best 75% quantile in the maximum average error, but again, recording this is only a temporal model, so it cannot capture the spatial part of it. And so, since this kernel is not a black box, so one thing I want to highlight is that in the construction of the spatial. In the construction of the spatial non-homogeneous kernel, we still use the neural network as a representation of the underlying sigma. The sigma is basically, we can have one sigma that tells us, roughly speaking, the shape of the Gaussian kernel at each location. So we can actually pull out the parameter values of the Gaussian kernel at different locations. So you can see this can actually tell. So, you can see this can actually tell us some interesting patterns. And so, in the fitted model, we have the first, this is the first kernel, second kernel, and the third kernel. And we can see they seem to capture very interesting pattern and tell us how the COVID-19 cases are spread over space. And so, for example, this one, this is where the airport locates in Cali. And this particular mode seems to be capturing how the To capturing how the airport influence is triggering the subsequent cases over space. And so when I put this together, and this is the final spatial kernel, and so we consider in particular what this looks like for different locations. They are port, center for the community 15, and 18 and 1. And so you can see indeed this seems to be capturing how this each location is their impact. Location is their impact spreading over space, but this varies over space. And this is the visualization of what the landmark impacts look like. Again, since we have such a flexible model, this seems to capture the impact of different kind of city landmarks. And so this is the magnitude of these different landmarks. You can see that it looks like the Taohao have the highest impact, which kind of makes sense because this is where. Kind of makes sense because this is where people tend to gather and that makes the COVID-19 spread faster. And so, one usage of this model is once we have this conditional intensity function, we can use this to predict what is the chance of having a hotspot in different times. And so, we use this to verify our model and making hotspot prediction. And so we plot the cases against this condition tension. The cases against this condition-intensive function over time. You can see indeed, it seems like the hybrid, you know, this you can see that the dots, this incident, are indeed concentrated along the high intensities that we have predicted. Okay, so this is some quantification. So, in the remaining time, I'm probably going to take five minutes just to show some, I think, hopefully, extensions to this motivated by this approach. And so, so far, I have talked about. And so, so far, I have talked about a carefully constructed spatial temporal kernel where we treat the time part independent from the space part. They are decoupled and multiply each other, and we model the space part to be inhomogeneous. But now we thought about we can take a complete general approach where we can take this kernel and then just extend this kernel using some basis function, and then we can use new. We can use neural network to represent the basis function. But this time, we can take the time and location and even marks to be one couple, which is this X, instead of separate them. And so now we can use this to represent a completely general, time-dependent, non-homogeneous kernel. And so this could be interesting of capturing the fact that the impact can vary over time, like at the beginning of COVID-19, the middle, and till now. The middle and till now, the end of COVID-19, the impact of incident can be very different. And so, this is one example of what this non-stationary kernel may look like. You can see this varies over time, effect. And this can also be useful for studying crime activities. So, we did use this to study how crime incidences will impact the future. And so, how do we do this? We take this kernel, and now we can expand this using merge. And now we can expand this using Mercedes theorem, the most general representation of the kernel. Again, it takes the form of some product of the basis function, and you weigh them, and you combine them across different modes. And this R can be chosen to be a small number, then we have a low-rank kernel. And again, we can represent these basis functions using neural networks, deep kernels, and then we can combine them through this Mercedes theorem representation to have our. Mercedes theorem representation to have our kernel. So similar idea as before. And then we can estimate again through maximum likelihood and then it's going to be take this particular form. But this time for this integral part, we may have to use numerical integration because now we don't have these nice Gaussian kernels where we can calculate this integral in an almost closed form expression. And however, this representation indeed can offer a lot of performance gain. A lot of performance gain compared to the non-state this kernel compared to the stationary kernel. And we can even say something about the estimation in terms of the uniqueness when we're getting close to a true kernel. Usually it's very difficult to see something about conformance guarantee of neural network, but in this case we can see that the functional optimization problem is locally convex. So at least this representation is hopeful to help us to find. It's hopeful to help us to find a particular kernel if the kernel, such a good kernel, exists. And so we also tested that: do we overfit? Because one criticism usually for neural network model is that this tend to overfit the data. And so we try this fitting on this exponential kernel, and it does a reasonable job. So you can see this is the fitting using, if we know the stationary kernel is exponential, and if we don't. And if we don't know this is our kernel disneural networks, it can still do a reasonable job. I think I'm running out of time, but I just want to show that this can achieve a really good performance compared to many state-of-the-art. And one last thing I want to just mention that we can also go beyond spatial temporal, but also study the network setting. And so, in particular, for example, if your data has, you know, Has you know lives on you know straight on the map, so it's better to model the data as observation or point process over a network. And so, again, this is a collaboration ongoing work with Jorge Matuos. And so, we're studying the crime data in Valencia. And so, we know the location of each crime incident, but they're along the street. So, we want to be able to model using the street network. And so we can. Network. And so we can, again, using this kernel, deep kernel approach, but we decompose this into space and a graph kernels. And then from there, we can actually even achieve better performance by using the graph structure. All right, so with that, I would like to conclude my talk. And so basically, I present a series of work along the line of deep kernel representation for point processes. The goal is to say we want to be able to To say we want to be able to harvest the strong representation power of neural networks, but still be able to have some interpretability. So it is not a pure black box model because we can still see something using the basis function to tell us how the spreading pattern is happening. And so here's a list of references. And most of the work presented here has been published in GRSSC, this paper, and the technical part can also be found. The technical part can also be found in this ICLR 2022-2023, and there are two work in progress. All right, that's all I want to say. Thanks a lot for your attention. Let me finish and see if there's any questions. I'll be happy to answer. Thank you. Thank you very much. Thank you all.