Etc. And of course, the first slide already has a typo, but anyhow. So thank you very much. So what I am trying to do is kind of give an overview of. Let's see if I can go to the full screen. All right. So what I want to do is to give an overview of a very Give an overview of a very simple geometric situation, which is a generalization of symplectic geometry. And then you can do quite a number of the same things that you do in symplectic geometry, but there are differences actually. So you can do the same things, but then you obtain different results because the hypotheses are different. And then the geometry, I will be a little bit more, I will be talking about maps, but there is a corresponding. Be talking about maps, but there is a corresponding theory also for vector fields, which is basically that you have a map and you have a symplectic form, and then it multiplies the symplectic form by a constant factor. So the basic idea is that taking this very simple geometry, then you can do something which mixes analysis, numerics. Analysis, numerics, and geometry, and then global results. And then the things are a little bit surprising because you can do many interesting things. So one of the themes is that you can prove theorems, right? And the theorems are a little bit different. And there are surprises, like in KIM is one of the things that I have been involved the most, but then there are other things like global. Other things like global Melnikov theories and so on. I mean, there are several big theories in dynamical systems, like, for example, there is KAM, normally hyperbolic, and Melniko theory, and then these geometric factors interact very well with all of them. And then one of the things that I should say is that when you have this lambda, I mean, imagine that, for example, lambda is. Imagine that, for example, lambda is smaller than one. If lambda is smaller than one, the symplectic form getting reduced implies that the system is dissipative. This is an extremely singular perturbation theory, because if you have a volume preserving, you get Poincaré recurrence, etc., and then all the other recurrence is going to be much smaller if you have some dissipation of volume. Some dissipation of volume. Okay, so it's a singular perturbation theory, but on the other hand, the geometry, if you look at it, and so the results are going to be extremely different. Sorry? I don't know if somebody is a question or not. One minute, Rafael. Sorry, one mute. It's okay. It's okay. It's okay, okay. Okay, so let me try to tell a little bit. So, I will be doing things like I will be talking about periodic orbits, and then to be honest, I think I will not be able to talk about several of the other things. I mean, this is a very interesting work of Stefano Maró and Alfonso Sorrentino, that they discovered that many of the things that you do in symplectic things like the Aubrey matter theory, then can be generalized. It can be generalized to this context. And I mean, I talk about the soft overmatter theory, which is invariant measures and weak KAM, but there is a hard over-matter theory, so to speak, like COVID and the existence of minimizers and homogenization, and that seems to be still open. Okay, so conformally symplectic systems is a system, as I mentioned before, that the push forward is multiple of Is multiple of the symplectic form, right? So these things have appeared in many circumstances, and it's a very simple geometry, but it makes connections with several applications. For example, it was studied before anything. I mean, I learned about all this from Agustin Baniaga because I mean, he was using this and developing the shift theory, etc. Another Etc. Another reason that I got interested in this is when you have mechanical systems in which the friction is proportional to the velocity, and this is something that is the things that we use into all the differential equations. I mean, it's an assumption which is satisfied in certain things. If you think about cars or planes, the friction is proportional to a power of the velocity, and that's not conformally symplectic. Another interesting thing is. Another interesting thing is when this is a very thing that happens in financial mathematics, right? So when you take, for example, several transactions and then you discount them with the inflation, right? So you sort of try to do things in present value. The present value has these exponential factors, and if you do the Exponential factors, and if you do the Euler-Lagrange equations for these things, and then you assume the usual Legendre condition, then you obtain these models. And these models have been studied by many people here, like Renato Ituriaga and several other people. And there is a very interesting theory. So, the other thing is quasi-periodic orbits. I have these slides, but you know, everybody here knows it already. So, a quasi-periodic orbit. It already so a quasi-periodic orbit is things that can be expressed as a sum of a finite number of frequencies. In this case, I was putting the frequencies, right? And then you can write them. The geometric way of looking at them is that there is an embedding of the torus, and then the internal phases are marching like a rigid rotation, these things that I was putting J omega, and then there is this function, and then if you have a prediction. This function, and then if you have a periodic orbit, that's why sometimes, of course, people in the audience know it very well, but I wanted to send a notation, which is that quasi-periodic orbits is the same as invariant tori, in which the motion is conjugate to a rotation, so that the internal phases are marching along with like a rigid rotation, and then you look at them, and this is the embedding. So, this is the way that I think about quasi-periodic orbit. I think about quasi-periodic orbits that there is like a reference torus floating around in whatever platonic space, and then these things get embedded. And then the motion in the real space, in the real torus, corresponds through the map, through the embedding K into the rotation. So there is a rigid rotation in the reference plane, and then this gets translated into the Gets translated into the dynamics, etc. So that's why sometimes we talk about invariant torus and we omit the fact that the dynamics in the torus is a rotation. So, but it's kind of understood. I mean, it's a standard practice. So, this is the story. And another thing that I am going to talk about is this business of a posteriori theorems, which is an interesting idea from numerical analysis. So, when you have a functional equation and you try to A functional equation, and you try to solve. I mean, and we are, and before, as I mentioned, the invariance equation is the equation that we are going to be interested in, right? So, if you are an analyst, right, then they give you the F and they give you the omega and then you have to find the K. So, that's the way that we look at it as an invariance equation. So, you have to find a quasi-periodic solution is basically the same as finding the K once they give you A. In the k, once they give you f and they give you t. So, there is so once you want to solve functional equations, there is a very nice kind of theorems which is loved by the numerical analyst, which is the following. So, how do you know that there exists a solution? Well, you take, you find that there is an approximate solution that, when you plug it in into the equation, produces a small number. Then you check some condition numbers. Then you check some condition numbers telling you that this function is not totally crazy, that is not produced by using axiom of choice, et cetera, right? And then, if you have that the error is smaller than the condition numbers, then you know that there exists a true solution and that the true solution is close to the approximate solution. So, for example, the usual contraction mapping theorem is one theorem of this kind. So, if you have something which is small and then you Which is small, and then you have several. If you have something that is a very close to be an approximate fixed point, then you obtain these things over here. And then, in several cases, then you can obtain also the uniqueness. And the thing that I want to, I mean, I don't want to sort of you to pay too much attention to the slides, but in this case, there is one thing which is subtle, which is I was putting here one norm and then I was putting another norm in there, and that's very Norman there. And that's very typical in KAM theory that the assumptions have some regularity, and then the conclusions have some other regularity, which is sometimes a little bit less. So this is not, I mean, of course, this is just not a theorem. This is a format of theorems, and we will try to adhere to this format of theorems, right? Of course, it's very hard in order to. In order to have a theorem, I have to specify what is f and what is m, and then this function a, right? But I will try to do the theorems in this, that all of them fit in this format, that I have to give the function curly f, I have to give them, and I have to give the a, and then I have to explain the norms, right? So this is a format, and we will use it as a template and so on. And this is extremely useful because. Extremely useful because one of the things is that one of these, there is one very important thing that I have to sort of tell you that in order to conclude that there exists a solution of the equation, you don't need to justify how you got the approximate solution. The only thing that you have to verify is the assumptions of the theorem, which is the estimates on the invariance error and in the non-degeneracy conditions. In the non-degeneracy conditions. And this is extremely powerful because it allows you to justify things that are, in principle, not rigorous. For example, you could take as your approximate solution a polynomial of degree 50, and then you say, how did you get the polynomial of degree 50? It doesn't matter. I mean, you could be using dirty numerics, you could be writing automatically in your sleep or something like that. In your sleep, or something like that, or asymptotic expansions, and this is something which will come up to be a little bit interesting later. I mean, you can take asymptotic expansions that don't converge and then take them as the jumping off point for one of these a posteriori theorems, and then you obtain justifications of the asymptotic expansions. And of course, in the numerics, you can also justify the numerics a posteriori so. So, in this talk, I mean, we will be talking always when the functional is something like this, right? It's the invariance equation which tells you that the motion is the same as the rotation. So, in ergodic theory, people say that the rotation is a factor, right? Or other people say that it's semi-conjugate. It's semi-conjugate to a rotation, which is a notation that I dislike because conjugacy is symmetric, semi-conjugacy is not symmetric. All right, so there is also something that I want to make propaganda a little bit, which is that once you try to do quasi-periodic orbits, then we don't require that the system is close to integrable. So the only thing that you need is that you have these approximate solutions, etc., and then this can be very And then this can be very far away from integrable systems. And then, also, as you will see a little bit later, you don't need global assumptions on the map like twist properties, etc. You only need these condition numbers, which are typically formed about the derivatives of the guess, etc. And there are several other techniques that I will sort of try to describe, etc. I mean, there are certain novelties, even from the theoretical point of view. Even from the theoretical point of view, like in the case of Wiscottorai, you don't, when we discuss Wisconsin, you don't need that the stable and unstable bundles are trivial, which is very interesting because this is an assumption that used to be in most of the theoretical papers. But on the other hand, these non-trivial, stable, and unstable bundles appear naturally for parameters, and we have found them all the time. Parameters, and we have found them all the time in the three-body problem. Okay, so let me tell you: the limit of zero dissipation is very subtle. And the subtlety comes to be, I mean, you know, if you have something which is volume preserving, then you start seeing lots of recurrence, right? If things are bounded, if you have a volume-preserving thing in a compact manifold, then you start seeing lots of recurrence. Then you start seeing lots of recurrence. If you start seeing lots of dissipation, if you have dissipation, I mean, if the symplectic form gets multiplied by a factor smaller than one, the volume gets reduced, and then you have several other things. So it's a subtle phenomenon, and then you have in KAM theory, you need to adjust parameters, and in KAM theory, you need to choose initial conditions. So one of the things Initial conditions. So, one of the things that one has to do is to write a formalism in which the parameters play the same role as the initial conditions, and that's a lot of fun too. And also, there are several surprises. Like, for example, when the map is dissipative, there are no Birkov invariants. That's one of the things. So, the tori are always conjugate to a linear rotation in a neighborhood and so on. And then, this creates quite a number of challenges. Challenges. So you cannot talk about twist KAM, twist theorems or anything like that, or you cannot do anything about periodic orbits. And also, I mean, if you try to do perturbation expansions, it's a very interesting thing, which because you can ignore the fact that it's a singular perturbation, so you can do expansions, which is what applied mathematicians have been always doing. And then you do these expansions, you close your eyes. And then you do these expansions, you close your eyes and you do it, et cetera. And then there is some results about the fact that this series is algebraic. And this seems to be, I mean, I was very happy to hear the talk of Professor Zhang yesterday because there seems to be some relations. Okay, so the proof of the KAM Tori of the KAM theorem is basically it's also very related to the numerics. As I told you, the As I told you, the things that if you have an a posteriority theorem, they tell you that your numerics are correct. But also, they tell you that the numeric, I mean, as it turns out, in this case, then you obtain a very fantastic method, which is very practical. And it has several features that I am going to try to make here. I mean, you will see it a little bit later. So, the method is going to be quadratically convergent. So, if you have k figures right after K figures right after you run one step of the algorithm, then you get two k figures right. Then there is also something which is interesting: that is, if you are trying to find quasi-periodic orbits, then you don't need to work with functions which are more dimension than the dimensions of the quasi-syperiodic orbits. And this is, and they you don't have no linear constraint in the data. So, this is very important because, I mean, from the point of view of mean from the point of view of numerics, because if you try to, there is something which is, I mean, if you have to deal with functions of dimension D, the cost of dealing with them grows exponentially with D and with unfortunately with a very large factor typically, right? If you want to discretize with a very small precision, like 100, and you want to get a function which is of dimension d, then you have to use 100 to the d, which is really. To the D, which is really prohibitive, and 100 is not a very high accuracy anyway. So, this, for example, is a big advantage with methods based on transformation theory, because in transformation theory, you have to do transformations which are of the dimension of the phase space. And with these methods that we are going to be describing, then you are having just to look at all these things. And then we will see a little bit later. Later, that there is low operation counts, and there is also something which is conceptual, and I will sort of describe it a little bit later: is that there is something that we used to call accurate strategies, which is that if you have a method which is based on this iterative procedure, is guaranteed to reach the boundary of existence. So, if there is a KM Toros, et cetera, which is in the connected component of the Component of the domain of existence, then you can get it. So, and then also I should say that these numerics, if you have the posterior theorem, they are also the base of something that people call computer-assisted proofs. Because if you evaluate the error with taking, if you evaluate this epsilon that I was putting and the condition numbers taking care of everything, including the random error and the truncation, then The truncation, then this becomes a computer-assisted proof. Anyhow, so this is the kind of the propaganda, and there are several other properties that you can get automatically from the things. So, for example, it's automatic that you get differentiability with respect to parameters, that you can put a strap the regularity. And the idea is very simple: if you have a regularity, say CR. You have a regularity, say CR, then you truncate a polynomial. And if the truncation is enough, then this satisfies the result for analyticity. So that's a little bit, etc. So let me try to tell you a little bit about, I am going to try to give the details of the theorems, etc. And if you give the theorems, as I told you, if you try to fit the things into this aposteriary format, I have to give. I have to give you what are the spaces and what are the functionals. The functionals I have told you. The spaces, this is the spaces that we use typically in KAM theory. There are several schools about what do you use, but we decided to follow the original papers of so you take analytic functions that are defined on a torus and then you give them the topology of the supremum. The topology of the supremum, there is one small technicality that you assume that they are continuous to the boundary, but that's the same. And this makes a Vanach spaces. And we also discovered that there is a very interesting spaces, which are these spaces that I mean, we I mean, I learned about them from, so they are basically Sobolef spaces, and they are so they are Sovolef spaces, and they can be continued with the thing. So if you want. Can be continued with the thing. So, if you want to do finance regularity, for reasons that I will explain a little bit later, Sovolef regularity is much better than CR. And the reason is very simple. I mean, the reason is very simple. If you are doing KAM at some point, typically you have to use Fourier analysis. And if you have to use Fourier analysis, the relation between Fourier analysis and Sobolev norms is very clear. The relation between The relation between Sovolev norms and CR is more complicated. Anyhow, so the theorems, and also, I mean, everybody here knows that if you want to do this KAM, it's a very subtle theorem. And then number theory comes into play, right? So, and this is the definition of number theory is very standard. And I think everybody in the audience knows it. So, I just want to put it so that the things that It so that the things that you remember is that the is the constant of the Diophantins and then tau is the exponent. So that's basically the thing that you need to know, right? So that everybody here knows the definition. So I'm just using to set the notation. Okay, so let me try to tell a little bit about the algorithm, etc. So maybe I will state the theorem and I don't want that, I mean, the interesting. I don't want that. I mean, the interesting thing is doing the theorem, et cetera. And then the theorem will be based on dealing with something like that. So what are the assumptions? Well, the assumptions are, I think I have already mentioned that omega is Diophantine. You have a family because we need to adjust parameters, et cetera, right? Then you assume that there is an approximate solution, and then there is a non-degenerate C assumption. So the non-degenerate assumption. Assumption: so the non-degenerate assumption I put it over here is formula five, but I just want to call attention, so don't try to look at it, etc. But the only thing that I want to say is that this formula is a very explicit D by D formula. It's a very explicit D by D formula. It's a matrix, right? And all these things that I was putting S and et cetera, they are just things that you obtain from the initial guess by taking derivatives and doing algebraic. And are doing algebraic operations. There is no global assumption on the theorem. And of course, if this gets to be, this determinant gets to be 10 to the minus 2 million, then you will need that the initial condition is extremely accurate. So this size of the initial condition affects this size of five affects what are Affects what are the smallness assumptions. It's a condition number. And then over here, then you need a little bit of extra assumptions, like the analyticity domains are big enough. I mean, this is the things over here. And then you need that this is the things over here. And this is the important thing, which is that you need that the error in the analytic norm is smaller than these constants that depend on the Diophantine, and then this delta is the analytic. then this delta is the analyticity loss. So if you want to lose analyticity very little analyticity, then you can obtain, then you need to have just a very small error in the initial condition. So that's the story. And then you get all these things over here and so on. So let me give you an idea of the proof because I mean I think that the proof leads very fast to algorithms and then it tells you what is the geometry, right? So it tells you what is the geometry. So you start with some What is the geometry? So you start with something like this. So they give you an approximate solution, right? And think of E as small, right? Because it's an approximate solution. Okay. And the thing that I remark is that 10 is something that they give you. And since you have 10, then you can do with it anything that you like. I mean, I could square it, I could take logarithms, I could do anything. As it turns out, the clever thing to do with 10 will be to take derivatives, but I. With 10 will be to take derivatives, but I can do anything which I have because I mean, this is my starting point. So the Newton method will require to solve you this, right? So if you correct K into K plus delta, and if you correct K into mu plus this other delta, then this is what the Newton method would tell you that the changes. So this is the change in first order, this is the change in first. First order, this is the change in first order, and this is the change in first order. And then you want that this keeps to be this kills the error, right? So this is the Newton idea. And now this is the trick that I was telling you. So this is an equation which looks terrible because, I mean, this is typically, remember that we are not assuming close to integrable, so this is a matrix and this is delta and delta evaluated at some other place, etc. So this is. At some other place, etc. So, this is a non-local equation. But on the other hand, if you take derivatives, then you obtain this df d mu d k is equal to this plus this. And imagine that this is small. I know that when we teach analysis, we tell the students that just because the function is small doesn't mean that the derivative is small. But in analytic functions, if you're willing to lose domain, this is true. Now, here is my Now, here is more or less where the geometry takes place. So, the geometry is the following: suppose that you have this invariant thing, right? And then you have this decay. So, taking derivatives, what I have shown to you is that this horizontal thing, which is the tangent to the approximate solution, is more or less the tangent to the approximate solution. So, what I am going to do is I am going to draw a perpendicular. And if you draw a perpendicular, then you get this thing has a volume. Get this thing has a volume, has an area which is one, and then once you apply everything, I know that the base goes into the base, and I know that the area gets multiplied by lambda, and therefore what you obtain is that the height is lambda, right? So you know that the so this is the image of this perpendicular, and the image of this perpendicular, the volume is lambda. So, what you obtain is that this say the This shady area has volume lambda, so therefore the height has to be lambda, right? So this is the geometry. And if you translate it into formulas, this means that you have obtained a frame of reference in which the system can be described by just multiplying by identity. The heights get multiplied by lambda, and then you get a shear. So this S is the S that appears in the non-degeneracy condition, but this is. Degeneracy condition, but this is more or less what you obtain, and this is the error. So, these are very typical formulas. So, this matrix M, as I was telling you, is just taking the derivative and then you compute the orthogonal, so to speak, in this picture, which is basically multiplying by the symplectic matrix to the minus one and some normalization. So, I'm not going to do, and you normalize to have area one. And I mean, I was making pictures into. And I mean, I was making pictures in two dimensions, but of course, the interesting case is when, but you see, I mean, technology has not advanced that I can make slides in four dimensions. So this is two-dimensional slides, but you get the idea. And then once you get the idea and you can do the algebra, the algebra carries over into any dimensions, and this is symplectic. So, anyhow, so the story is that, and now you So the story is that, and now you have this good frame of references. So the idea is very simple. So that the correction, rather than expressing it into the Cartesian coordinates, you express it in this coordinate of references. And then miracle of miracles, it all satisfies a very simple equation that can be solved using just constant coefficients, difference equations with a constant shift, which can be done by. Shift which can be done by Fourier series. So that's the basically. So this is everything gets reduced to using equations of this form, right? And in which the unknown is W and A is given and lambda is given. And then you discover that this equation, of course, in Fourier coefficients can be done in this way. And when lambda is equal to one, sorry, I should have said. To one, sorry, I should have said when lambda is equal to one is an especial case because then you have an obstruction, but then you have one parameter free. So everything sort of fits together. So in the case when you have an obstruct in the equation, then you need to adjust one parameter, but then you get one parameter free. So if you play out with all this, then you can fit together this singular limit in the sense that you can, in one case, one requires adjusting parameters. Adjusting parameters, and in the other case, it requires adjusting the initial conditions. But if you look at it in the right way, everything fits together. So that's kind of very nice. So I am going to put the algorithm. And this is the algorithm. It's going to take me two slides. I don't expect that anybody understands them, but I want to put it because I think it's important that people understand what the things are. So it's an algorithm that it has all operations which are basically algebraic. Operations, which are basically algebraic and so on. And it's 13 operations, right? So it's 13 operations. So look, I mean, if you are a programmer and all the operations are very highly structured, because so you have, for example, you have k, so you have to compute the derivative, then you have to take a matrix, multiply it by the transpose, and compute the inverse, et cetera. So if you were implementing each of these operations in MATLAB, then they will be basically one. Lab, then they will be basically one line. Or in Mathematica, or in Mathematica, I don't like Mathematica, but if you were implementing them in MATLAB or in Julia or in some other good languages, then they would be basically online. Of course, you have to do bookkeeping and some of them, etc. But getting a workable program is like a few hundred lines and it can be done in a weekend. So it's an extremely efficient algorithm. And I also want to. Efficient algorithm, and I also want to point out that the algorithm involves only manipulations of functions with as many variables as the totals, and then you are operating, you are doing compositions, multiplications, additions, inverting matrices, and then you are doing shifting derivatives, etc. So all of these operations take if you discretize in N, then they require order N operations in discretizations in a grid or in the Fourier discretizations. Fourier discretizations. And so that the algorithm will have like order n log n operations because then you have to switch between Fourier and things like that. And an order n log n, I mean in Fourier transform, if there is an algorithm which is well optimized and there is even hardware, is the fast Fourier transform. And so this order n log n is actually an overestimate. It's very practical estimate. In its very practical estimate, et cetera, if you want to do it in computers. So that's the story. Okay, so the hypothesis of the theorem, et cetera. So let me try to tell a little bit about what are the mileage that we had. I mean, all these things have been implemented. And in particular, in this recent paper that just appeared this year, then we took a concrete example. And then the algorithm was implemented. And then you got also one of the things that you discovered in this game. One of the things that you discovered in this game is that the bottleneck of all these calculations and applications of KM theory, the bottleneck is the fact that you have a ridiculously small precision in the standard arithmetic, but then you use a standard extended precision, and then you can get all the things that are whatever. So, one of the things that we did in this paper is we took all the constants that appear in the theorem and we make them completely explicit and then we produce. Completely explicit, and then we produce numerical approximations with 50 or 60 digits, and then we let everything run, and so on. And it's not a horrible, and as I told, the programs are not, I mean, it's a little bit of a pain to program them. But then, I mean, it's about it's something that can run comfortably in one of the computers that we use to fill taxes or something like that. I mean, one shortcoming of the paper is that we. One shortcoming of the paper is that we didn't do, I mean, we didn't go to all the use interval arithmetic, but then we are not claiming a computer-assisted proof. But this tells you that this method can get easily like six figures correct of the of the I mean, we can compute up to six figures of the things over here, right? And for whisker tori, I mean, the same algorithm works, but so whisker tori are tori that besides having these neutral directions. Besides having these neutral directions, have several hyperbolic directions which actually have more hyperbolic constants that dominate this lambda factor. And the argument is in the center, you can use the Lagrangian tori, but in the hyperbolic directions, you use contraction arguments. So, again, there is a little bit of an interesting thing about this theorem. I mean, you know, the proof is not very difficult, but then Difficult, but then it doesn't need that the stable and unstable bundles are trivial and the stable and unstable bundles are trivial, or it doesn't need that the dynamics in the stable bundles is reducible. On the other hand, I mean, it's a very amusing thing that we could prove that the center directions are a trivial bundle. This was a little bit unexpected, but the fact that you can deal with unstable bundles. You can deal with unstable bundles, it's something which is very interesting in the sense that these things appear. So you can prove very easily that if you have a normal direct, when you have families of parameters, that the stable and unstable directions, there is a bifurcation, so near a bifurcation, and this is something that I learned in papers of Alain Chancine and Gerard Johns. Then you can get the near the bifurcations of this. The near the bifurcations of this tori, then you can get bundles which are like mobius strips, and these things appear in celestial mechanics. And then we ended up being able to compute them in the three-body problem, etc. So let me just skip a little bit about it, etc. But then there is another story which I think I wanted to talk about it because it related to the work of To the work that Professor Zhang explained. So you can do perturbation expansions in, I mean, it's a singular problem, but you know, mathematicians, applied mathematicians are very fearless crowds, etc. And then you can get approximate expansions. You can get the linsted series, and the linsted series have the feature that they solve the problem in domain. In domains, et cetera, and so on. And then you can take this as a starting jumping-off point for the posteriority theorem. So you get the small residuals, and then you figure out where the condition numbers get bad, and then you get something that looks more or less like this. So this is a conceptual picture. This is the complex plane, and you have the perturbation expansion horizontal is the real and the vertical. Is the real and the vertical is the imaginary. And then by doing this method of doing the expansions and then jumping off, then you can obtain a domain of analyticity that it's basically the whole plane or a ball in the whole plane, but then you have to start taking off balls and the balls are decreasing exponentially fast. I mean, this is a conceptual picture because if you take the real picture, then you see just the first of the second ball. See just the first of the second ball. The other bands are converging exponentially fast to zero. And then in this case, there are six lines because we were putting cubic things. And the analyticity properties are very interesting because if you look at the result, this theory of Jebreai, et cetera, right, that tells you that the width of the sectors is related to the Jebrae exponent. This does. Jebra exponent, this doesn't seem to not happen here. So, that's the story. So, what I was telling is that you take, let me just skip about all this. And there is, as I told you, the method of proof is guaranteed to sort of converge to the boundary, right? And it converts to the boundary. So, you can use this as a. You can use this as a criterion of existence or non-existence. So you let the method work, and the method doesn't require any adjustments. And when the Sovolef norms blow up, then you decide that that's the end. And this is the picture that was obtained by Kayeja and Celetti, both of whom are here. And you obtain this very tantalizing picture when you have two parameters, which is that you have something which is. That you have something which is it has smooth boundaries, so the boundary of KAM theory has some smooth things, and then it has all these spiky things. And the spiky things are sort of disappearing when lambda goes into loss of one. And there is also something which, I mean, and as I told you, the good thing about doing all these numerics is that the good numerics bring up new mathematics. And the new mathematics is the following, which is, I mean, Following, which is, I mean, this KAM tori are not only rotational tori, but they are also normally hyperbolic manifolds. And so these things have to exist for two reasons. They have to exist because of KAM and they have to exist because of normally hyperbolic. So how can something move parameters and start violating both KAM and start violating normally hyperbolic? So it's very subtle. So it's very subtle because the thing that was described periodically, I mean, if you think a little bit about it, the circle in the breakdown remains fairly smooth, I mean, a little bit more than one. But, you know, because of the geometry, the Lyapunov exponent has to be lambda, which is smaller than one. And in the other direction, it has to be one. So how can this stop being normally hyperbolic? So you have Lyapunov exponents. Of exponents, and you have Lyapunov exponents and Lyapunov multipliers. So, how can you lose hyperbolicity? Well, the only way that you can lose hyperbolicity is because you lose the fact that the two spaces are separated, right? So when the loss of hyperbolicity cannot happen because the multipliers go to one, they can only happen because the separation between the tangent The separation between the tangent and stable direction goes to zero. And if you think a little bit about it, then these directions have to go to zero in a set of measures zero, but it has to be dense. And this is calculations that was done by Jordi Luis Figueras and Renato Callaja. And this is the pictures of the stable and unstable bundles. Sorry, the stable and the center bundles, right? The stable and the center bundles. Center bundles, right? The stable and the center bundles. And you see that they are getting closer and closer, but they are getting closer and closer in really small pieces, right? Because they have to have a measure zero, but they have to be dense, etc. So this is, and then there is also something which is requiring still proofs, which is that there are quantitative ways in which these things appear, which are very similar to the things that you expect. Things that you expect in bifurcation theory, but the exponents are not the exponents that you expect in bifurcation theory. So there is a little bit of a strange thing and nobody, and so this is what I was putting over here, which is numerical observations waiting for proofs. And as I was mentioning before, there is a bifurcation theory when lambda, when the exponents get to be lost, but over here. Get to be lost, but over here, uh, this is all very different, etc. Sorry, sorry, uh, time is done, but uh, you can okay, so maybe I will just stop. You can continue, so no, no, no, you can continue. I have all the three slides, but but I was telling you that there is a little bit of an interesting remark, etc. Because one of the things that I started doing with all this is that this business of small dissipation is interest to, I mean, as I mentioned before, celestial mechanics, but also. Celestial mechanics, but also mechanical engineers. And one of the things I had fun is going to the labs of the mechanical engineers or the financial people, which are maybe, I mean, they are also interesting, but I mean, if you go to visit the labs with mechanical engineers, they have toys that you can play with. Anyhow, so there is one interesting result that let me just mention it, and I will basically stop here. I just want to make propaganda. So we all know that there exists That there exists the famous Lyapunov theorem that tells you that if you have some non-resonance conditions, then you get a family of periodic orbits. And these periodic orbits form a Lyapunov manifold. The question is, what happens if you put a dissipation into it? It's very easy to check that the periodic orbits don't persist, but on the other hand, the whole family, the Lyapunov manifold persists. So this is one of the things that was done with Floria and Kogel. With Florian Kogel Bauer. So the Lyapunov manifold persists, the only thing that happens, so you get a tube, and then the tube gets deformed, but the motion in this manifold is not periodic, but rather dissipative. And then this is interesting in the sense that this is what controls the approach to equilibrium. So everything goes to equilibrium, but they don't enter in random directions. They enter into the direction of this manifold. The direction of this manifold over here. So, maybe I will just stop here. Thank you for your attention, and then I put a few of the references, right? So, but I wanted to give an overview of the pictures, and then, of course, there are Renato and who is in the audience, then we'll be very happy to discuss all this. All right, I'll stop here. Thank you very much. Thank you. Yeah, I mean, I hope that all this is, I mean, the talks, et cetera, you know, it's a way of discussing, etc., right? So I hope that people are motivated to discuss and so on, right? Okay, there are questions? Okay, everyone. Okay, everyone is happy. Thank you very much, Rafael. Thank you again. So now we have the coffee break. 