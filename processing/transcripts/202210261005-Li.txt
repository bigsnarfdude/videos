So, our next speaker is Shane. Can you hear us? Yes. Can you hear screen? Oh, great. Oh, okay. Okay, so our third speaker is changing from Yon Dr. Matsi, and she's going to talk about emerging with incomplete data. Thank you very much. Thank you very much. So, I submitted my abstract before the schedule came out, and I realized that we have a lot of talks. So, I decided to change the topic and make this talk a little bit light-hearted, so I don't bombard you with a lot of formulas. I would like to talk about inversion with incomplete data. By incomplete data, what I mean is in our imagination, we have access to an infinite amount of data. However, in reality, Data. However, in reality, we cannot really use them. We can only choose finite many of them. And my question is: if you only have a finite amount of data, can they still be used to accurately infer the amount parameters? So this is a very broad topic. And under this very broad topic, I can only answer very slim questions. And I would like to present you today is to utilize two machine learning techniques. One is metric sketching, and the other is mature. One is metric sketching and the other one is the metric completion. Using these two machine learning techniques, we can guarantee or we can provably show that the inversion is possible and guarantee to be to be accurate. So this is a joint work. The metric sketching is the inverse problem in the linearized setting. And in this setting, our work is together with Coach and Kit. They are my graduate students. And Steve is my colleague at UW-Madison working on machine learning. And working on machine learning. And metric completion is in the non-linear setting, and metric completion is a work together with Tan and Leo. And in particular, Leo is among you. So, if you have questions, you certainly have a few days with him. So, let's get started. So, this is a very general framework of the inverse problem, and this is pretty much the framework that we share. You have the PDE, and the PDE is parametrized by some unknown parameter. This unknown parameter is denoted by theta. We feed this PDE. We feed this PDE with some source. I call this source XI. And then the PDE generates a solution, and we take the measurement of the solution. The measurement gives you the data, which is YI. So we denote the map. This map parametrized by theta, it maps the source X to the output Y. And in the inverse problem community, our question is: suppose you have the data, which is this map, I want to use. This map, I want to use this map to reconstruct the parameter. The parameter is theta. However, in most settings that we conduct our theory, we say that the map is fully known. So you have infinite amount of data over here. So we have access to the full map. And also, on the other hand, we also require to reconstruct the full function. So theta is a function, and function lives in the. function and function lives in the infinite dimensional object as well. So this is a very high goal and we are asking for a lot of information. Examples are everywhere. For optical tomography, for example, the PDE over here is a radiative transfer equation, and this lambda is the so-called orbital operator. This orbital operator is measured in the norm L1 to L1, and the reconstruction is the L infinity function. And we want to function. And we want to say that the reconstruction is in the L infinity sense very accurate. The second example is a very famous EIT problem. That's the Caderone problem. The Caderon problem, this map is a directionally two-normal map. The directionally two-norman map is an infinite dimensional object. We measure the map using H12 to H minus one half this map, map operator, this operator norm. And under this norm, we can prove that the reconstruction is The reconstruction is unique in L infinity sense. So, also over here, the map is an infinite dimensional object. The to be reconstructed function is also infinite dimensional object as well. So, now the question is, the goal is very high. We're reconstructing a function and we need the point-wise to be accurate. And the data requirement is also very high. We need the access to everybody in this map. In this map. So I want to relax both. Practically, we only require the reconstruction of the theta presented by a finite dimensional vector. I say my function is represented by d-dimensional vector. I'm already very happy. And practically, we can never really have access to the full map. We have access to the map evaluated at some grid point. So we have the finite data set utilized to reconstruct a finite. To reconstruct a finite dimensional object. Now we have all this beautiful theoretical guarantee that an infinite dimensional object is utilized to recover an infinite dimensional function. Can we still say a finite dimensional object can be utilized to recover the finite dimensional vector presentation? The answer is, of course, no, but there are many levels of questions we can ask. How fine the discretization needs to be? Fine, the discretization needs to be, and we propose to just listen to the practitioners. If the medical doctor tells you the discretization being 200 by 200, then they are happy, then we just go with that. So let's say that theta D is the D-dimensional parameterization of theta. So theta D is fixed, D is fixed. Now the question is how many data pairs are needed? In the nonlinear setting, we cannot say that to recover D-dimensional. That to recover d-dimensional vector, you need d data pairs. That's not true anymore in the nonlinear setting. Then, how many do you really need? And also, a more crucial question is probably this one. Where to look for the most informative data? So, data carries different values. Some data are more informative than the others. And I think on Monday, a lot of talks were focused on experimental design. They are probably asking the same question. So, what are those So, what are those most important data pairs that I should focus on? I don't have the answer to them, but I'd like to provide two answers to two cases. I would like to avoid using optimization of Bayesian inference. My question is truly on how many data pairs are necessary. And over here, this experimental design, I'm not in the design yet. Design. I'm not in the design yet. I'm only asking my data to be prepared totally randomly. And my question is: if you have totally random access to the data pair, so there's a full map and you randomly select some, are you able to recover them? Okay. So the first case study is a matrix sketching. And matrix sketching is a machine learning technique. And we would like to utilize it to the linearized setting for our inverse problem. For our inverse problem. So, what is matrix sketching? Imagine you have a linear system, Ax equals to B, B is your data, A is your matrix, matrix acting on the unknown variable X equals to the data, and you want to recover this X. Suppose your matrix is a very tall and thin matrix, so you have a lot of constraints, but only a finite number of variables to recover. Number of variables to recover. Then naturally, you'll say that a lot of these constraints are not really necessary. We can eliminate some of the constraints. We can remove some of these constraints. I just keep a few of them. Maybe that's sufficient to recover my unknown variable x. Mathematically, that is to say that I multiply this sketching matrix on both sides of this equation. I sketch off some of the rows and I only select the other rows. And I only select the other roles. Now, the question is: if you perform this sketching, is a problem solution preserved? Before sketching, we look at Ax equals to B. The solution can be explicitly written down. X is a pseudo-inverse A dagger acting on your data B. After this sketching, you have the same result. You sketch this matrix on the left-hand side. Matrix on the left-hand side, you take the inversion, you sketch your data, and this is your new data. So that generates me the sketched solution. So now the question is, is a sketched solution a good representation of the original solution? So that's the main goal of the so-called matrix sketching. It has its own community quite big. But I would like to cite the following results came out about eight years ago. It says that It says that if the sketching matrix is a random Gaussian IID matrix, then I can guarantee that the sketch solution approximate the original solution very well. The so-called approximate very well, what I mean is the new solution and the old solution, they're only epsilon apart from each other. And I can say that they are epsilon away from each other with a confidence being one minus. With the confidence being one minus delta. So let's say your delta is one percent, then with 99 percent of the confidence, I can guarantee xs is within epsilon neighborhood of x. But there is one caveat, the number of rows that you have to preserve. The size of this S is R by N. N is the size of matrix A, so it has to be there. R represents how many rows that you have to preserve. That you have to preserve. And how many rows is this number? This number looks ugly, but let's spell it out. This number, it linearly depends on P. P is the number of variables that you want to recover. So you have P, those many variables to reconstruct. So naturally, at least, you need to provide P those many constraints. So RBMP, it's very understandable. Very understandable. It also logally depends on delta, and this is a very good result. Let's say delta is only 1%, and this number only provides you a very small number, 2. Lastly, it has algebraical dependence, inverse algebraical dependence on epsilon. And this is probably arguably not really good. Hopefully, we can prove this, improve this, but we don't really know. But we don't really know. And epsilon appears over here. So let's say epsilon is one half, then this guy is four. So the techniques is basically two levels of technique. One is called gamma net, one is called epsilon delta embedding. Let's not go to the details. I just want to present you this result. With a linearly dependent on P, those many rows get capped. I can guarantee that the sketch solution is a good approximation to the Solution is a good approximation to the original one. So I only need a subset of the original original data set. There should be some obvious constraint on the matrix A itself, right? No, not this case. Not this case. For later on, when we have metric completion, there's some constraint on A, but not this case. But assume A is securely. A is extremely uh it's not even full rank. Oh, well, okay, okay, okay. So the rank is at least p. So you, this x is at least, this a dagger b is at least computable, sure. Right, but there's not even anything on the condition number of a. No, no. Okay, thanks. Okay. Okay, so now the question is: can we utilize this matrix sketching technique to work on the inverse problem? The inverse problem is slightly different. Inverse problem, we have two indices basically. We have I1 to index the incoming source, and we have I2 to index the outgoing measurement. So, both I1 and I2, they go around the circle. In the continuous setting, we have I1 being in. Setting, we have I1 being infinity, I2, the many configuration of I2 goes to infinity, so there are infinite amount of data. Now, the question is: can we do the sketching? So, in the linearized setting, for most of the linearized PD-based inverse problem, the linearized setting can always be translated to the flat hole integral. So, the right-hand side is your data. I1 tells you your I1 tells you you're performing the I1s experiment, and for the I1s experiment, you take the measurement at I2, this location. Therefore, this is only one number. And to obtain this one number, you map a representer onto the unknown function that you want to recover. So theta is a function that you want to recover, and theta get projected onto the representer, it will give you the data on the right-hand side. Hand side. Moreover, the representer is always the product of two functions. One solves the forward PDE and the other one solves the adjoint PDE. The forward PDE use the source located at I1 and the adjoint PDE solution use I2 as your detector location. For example, For example, in optical tomography, Fi1 solves the forward radio transfer equation with the light source located at I1, and Gi2 solves the adjoint radioity transfer equation with the light source located at I2. So basically tells you where to take the measurement. You take the product of this two function, and that becomes a representer of this theta. You project onto the theta, it equals to the data on the right-hand side. And the same story happens to the Cataron problem, the EIT problem. Cataron problem, the EIT problem. So now we discretize it. Once you have the full, the fertile integral, we discretize it. The data now becomes the right-hand side, my b. The theta is the unknown variable. And I say this unknown variable is now represented by my vector x. And my matrix encodes the information of f times g. So now there's some caveat. There's some caveat in the application of the matrix sketching. Question we had originally is: Can we sketch the system? Can we somehow just multiply both hands of the equation and sketch the system? The answer is quickly no. Matrix A, it has structures. In particular, it has a tensor structure. It can be written as the tensor product of F, the matrix that encodes the information. The matrix that encodes the information of little F and G that encodes the information of this little G. So matrix A has a tensor structure, it is the tensor of the source and the detector. So we spell it out. For every configuration of I1, you have a lot of I2. For different configurations, you have a lot of I2. So there are so many of them, the infinite many of them. So if So, if our underlying matrix has a tensor structure, then to sketch it, we have to honor this tensor structure as well. In particular, when we multiply the sketch matrix on both sides, the sketch matrix needs to act on the tensor F and the tensor B, respectively. In particular, mathematically, we are looking for the source that is a linear combination of all the indices source, and we're looking for the The source, and we're looking for the detector that is a linear combination of all the detectors, respectively. Physically, that means you combine all these sources in a random way, and you linearly combine all these detectors in the random way. So you need to sketch F and G respectively using different sets of the random parameters. So there are two ways to get around that. To get around that, one way is to say my sketching matrix is a tensor product anyway. So this P sketches F and this Q sketches G, and the tensor get applied onto individuals. And the second case, each row of this S is different. And for each row, for example, I pick a row, S i row. The I's row is going to be a tensor product of Pi is a parameter used to sketch F. Parameter used to sketch F and QI is a parameter used to sketch G. And then I reshuffle, reshuffle the original PNG to design the second experiment. So there are two cases to do this tensor that honors the tensor structure. In both cases, we have the following results. In the first case, we say that the size of P, the number of rows that you have to preserve in your matrix A, is this size. So you can, according to this, So, you can, according to this formulation, this is a linear dependence over here, log dependence over here, and algebraical dependence over here. You immediately see that it's a very direct extension from the original Udof result. And in the second case, the analysis is much more involved. And over here, the dependence on D is actually very bad. The dependence on D is D to power 6. So it's to we're basically saying to recover D as many parameters, I need D to 6 is. many parameters and if d to six is many um and this can many constraints so that's a very bad dependence but i would like to focus on that focus on is that in either theorem we completely eliminate the independence the size of the matrix so the matrix a can be infinitely long but we don't really care how many there are we don't care how many data points that you can measure we only say that this many is sufficient That this many is sufficient. So, finite many data is sufficient enough to reconstruct your finite many parameters. We don't need the full map. I should say that I believe this lower bound is very crude. So there's a lot of room to improve this rate over here. So that's the first case study. We use matrix sketching to study the linearized inverse problem and show. The inverse problem and show to reconstruct the d-dimensional variables, you only need finite-many data set as well. So, the second example that I want to show is a metric completion, and this is done in the completely non-linear setting. And let's use, let's come back to metric completion. What does metric completion mean? Matrix completion, I think its origin is a Netflix problem. So, for each person, there are many, many different movies, and each person. Movies and each person watched five movies and gave them the rating. Therefore, I fill in the numbers for at five locations. And the second person comes in and say, I watched three movies, so I put the value in three locations. So there's a gigantic table with number of movies and number of individuals listed. And in reality, of course, we don't have the access to the entire matrix that requires everybody to watch all the movies. That's not possible. But nevertheless, Possible, but nevertheless, let's suppose we know the information, we know the matrix acting or acting revealed on a mask. This mask location is called omega. Suppose we only have finite amount of information revealed by this mask, can we somehow recover the underlying matrix? So, that's the metric completion algorithm, or the metric completion question. There are many strategies, and we would like to focus. Strategies and we would like to focus on today is the following strategy. It says that constraint on the fact that this matrix acting on the mask equals to the data that you gave me. That's a number of linear constraints. Conditioned on those, I'm looking for the matrix that has the smallest nuclear norm. Originally, it was the smallest rank, but rank is really the L1 norm. Rank is really the L1 norm of the singular values. So they relax it to be the L1 norm of the singular values, the nuclear norm. And it was proved also eight years ago that if matrix has the condition being low rank, if this matrix is low rank, I can prove as long as your mask give me enough holes, I can reconstruct this matrix. And the number of data points. And the number of data points that you need to provide is the following number. So, n is the size of the matrix. So, the original matrix is n by n. And now for each column, I don't require the entire column to be known. I only need require r of them to be known. So, if your rank is 5, for example, then you only need 5n those many data points instead of the original n square those many data points. And square those many data points. So that's really the optimal number of data points you can request. But of course, there are some conditions. The most condition is a matrix A, B, and low right. Okay. So now let's see if the matrix completion can be utilized in inverse problem. So for example, the EIT problem, the EIT problem, the map is a Jira to normal map. That is basically saying dirichly data, you come in, you have a lot of measurements. You come in, you have a lot of measurement for the Norman, and for different directionally data coming in, you have a lot of Norman measurement. So, this is a gigantic infinite by infinite dimensional matrix. Now, my question is, suppose I only have a mask, the Dirichlet-Norman map revealed on a mask, can I reconstruct the original matrix? The short answer is no, because the Dirichlet Normal map is full rank. If it's full rank, then R over here is N. Then R over here is N. So you're basically requiring, using this algorithm, you require me to provide you all the data points for the reconstruction. And this is pointless. However, this is actually Leo's suggestion. So I really thank him for that. However, directionally no two normal map, it's a full rank matrix, but it's diagonal, all diagonal blocks are. All diagonal blocks are low rank. So, even though you can along the diagonal, all the blocks are of full rank. So, you basically have to provide me all the information over here. But if you go far away, you pick an off-diagonal block. This off-diagonal block has a very limited rank. In particular, the rank itself does not depend on your discretization. So, this picture shows you the discretization using 32 grade points. 32 grid point, 64 grid point, 100 grid point, and 200 grid point. You pick a location of the off-diagonal block, the rank is unchanged when you increase your discretization. So this gives us a suggestion that along the diagonal, you have to fill in the entire information, and off diagonal, you can put a very sparse mask. So, physically, what does that mean? Suppose I pick a point. Suppose I pick a point on the diagonal. I pick a point on the diagonal that means my Dirichlet data comes in over here and my Norman data comes in over here. So my Dirichlet data and the Norman data, they get located at the same location. So my laser light coming over here and my take-the-measurement at the same location. If that is the case, the information is very rich. So it's full rank. You have to preserve everybody. If you are in an off-diagonal rank, Off-diagonal block. For this block, the incoming data comes in over here at the very end of it, but the normal measurement is at the beginning of it. So in some sense, I put the source in over here and I take the measurement on the other side of the lake. If that is the case, the information is low rank, so you don't need that much information. The detector is very, very sparse on the other side on the lake, but very, very dense on this side of the lake. Very dense on this side of the lake. So, final recipe: let's move on. The final recipe is along the diagonal, aerial information gets kept, but if you're far off, you can be a little relaxed. And this is probably not a good presentation of the theorem. In the end, we find that the number of the data points that you need to recover is n log n. And this is a final recovery. So, So, let me say one more thing before we finish. So, when I got into the inverse problem community, I was really amazed by the beauty of the theory. But I find that to translate this theory on the continuous setting to the inverse setting, sorry, to the discrete setting is not that easy. In particular, I don't know how many data points are necessary for the uniqueness in the discrete setting. And this is only two case studies, and this is a quite big question to be answered in the future. And I look forward to your comments. Thank you. Yeah, Jean, so thank you for introducing talk.