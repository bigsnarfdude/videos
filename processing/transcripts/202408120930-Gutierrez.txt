Speak about the work we have been doing in the last two, three years with Anna Maria Montanari from the University of Bologna. Can you hear me? Okay. So there are several results I want to mention. And so let me give you a question. Okay, thank you. So, a brief outline of the talk is the following. I will explain the motivation for this problem we consider from optimal transport. Consider from optimal transport. And the results concern monotone maps. And monotonicity here is with respect to a cost, which will have some assumptions. And we are going to prove that the monotone maps with respect to this cost are single value almost everywhere. And then from this, one can integrate inequalities because the inequalities called all. because the inequalities fall almost everywhere and then we can get L infinity local L infinity estimate for the map minus affine functions and and that will lead to differentiability properties of the map so so let me just uh briefly say uh the background which um Which two metric spaces, two measure metric spaces, the same measure and it doesn't I have many of these so I have to it's going to be like this all the time is um so a map from X to Y is measure preserving if you take a If you take a set Borent set in Y and they take the image inverse, you measure the image inverse with a measure mu, and this is exactly the same as the new measure of the set E. It's the right the right button, right? I press it and the I press it and doesn't have to do it. It vibrates. Let me go back. I have my thing I can connect, but I know it works. So you want me because it will be. Will be oh, you use this okay and then what is the purpose of this book? This is back this is back and this is forward okay so we have a cost function which Function, which and as you all know, the Monge problem is to find team measure preserving map, minimizing this integral among all the measured preserving maps. And under general conditions on the cost, there is a C concave function such that the optimal map is a C sub differential. Differential of this T-concave function, which is given by this expression. And this P super C is the C transform given by this. Okay, so this implies very easily that the optimal map satisfies this inequality. Certainly optimal. So, if you switch the order, you increase the cost. And assuming this single value, we can write this inequality almost everywhere. The point is to prove that this, if the optimal map, if the map satisfies this inequality, then it's almost Almost almost everywhere is single value. So, so here is a focus: we take multi-valued maps from Rn to parts of Rn that are not necessarily optimal and not necessarily measure-preserving, just satisfy this inequality. This inequality here for all points C in Tx, Z and T Y. Tx, zeta, and t of y, and x and y is in the domain of t, which is a set where the t of x is non-empty. And here, we are going to assume here are the assumptions on the cost. The cost is h of x minus y. And the assumptions on the cost are the following. It is convex, it's non-negative, and is C2. C2 and this positively homogeneous of degree p for some p bigger than or equal to 2. So the cost we have in mind are powers, p powers of x. And an extra assumption. Thank you. So we are going to assume the following property that the decession of the function h on the unit sphere is away from zero. And of course, the other inertia. And of course, the other inequality is 3 because the function is C2, right? Now, this, if the cost is a quadratic cost, this C monotonicity is a standard monotonicity property because if H is a quadratic cost, then you write down the inequality in X minus Y squared and you expand with the squares and you get the inner product. So, and this standard monotonicity is. And this standard monotonicity is something that has been studied extensively and for many, many decades. It has many applications to PDE, to optimization, functional analysis, etc. So this is a notion that is very well understood. So the motivation for our research is the following. So in 2020, So in 2020, Goldman and Otto proved partial regularity of optimal maps for the equilateral cost with Herald Lennon. This is a, I think, it's a remarkable paper they did. And they prove that there are open sets containing the support of raw zero. Rho zero and row one. This is the row zero is the measure on the space X, and row one is the measure on the space Y. Full measure, so that the optimal map is C1 alpha diffeomorphism between the resulting sets. This is a very useful result. We have in mind, yeah, there are some applications. I'm not going to talk about that, but these are some applications that are useful for. Some applications that are useful for optics because you can remove a set of measures zero and then get the right inequalities and apply the compositions that are called for smooth maps. So, this is a very, very interesting and important results. But this result was originally proved. Originally proved by Figali and Kim using the regularity theory for the Montgomery equation. But the point is that the Gold-Manoto result is a result that is independent of the maximum principle and the Montgomery-Pere equation. They used the formulation of Benamou and Brenier, optimal transfer, fluid formulation of optimal transfer, which is pretty well known, has many applications. Estimates of Many applications, estimates for harmonic functions and Campanato iteration technique. So this was the got interested into this result, especially in the proof by Goldman and Otto. And what we tried to do was to prove regularity for when P is not true. Excuse me. Excuse me. And I think this is a very from the point of view of this method. Now, in this endeavor, in this work, we obtain L infinity estimates for C monitor maps and interpolating maps. And so I point out that. And uh so I point out that to to prove the L infinity estimate, the map is neither optimal nor measure prester. It just we just use the monotonic C monotonicity, not even the cyclical monotonies, it is just the C monotonist. And from the technique, we obtain differentiability properties of C monotone maps. And there are results, the point is this cost that I explained before, they are different. They are different from zero outside of the diagonal. The diagonal is zero, and there are results, partial regularity results, pretty recent results, assuming that this is different from zero everywhere. Or they can assume that the targets are far apart. So we didn't obtain the regularity results for p equals two, but Results for peak was two, but I will explain at the end what are the things that are missing in the analysis. Many, many items in the analysis of gold and auto-proof work for P different from two using this L-infinity estimate, but there is a part which is not clear what is the replenishment. They call the almost orthogonality property. So let me begin explaining what are the results. Explaining what are the results. The first result is that the H-monotone maps are single-value almost everywhere. So I'm going to assume all along the talk that H satisfies these properties. This is homogenous of degree P is C2, is convex, and the Hessian in the unit sphere is bounded away from zero. So the theorem says that Tx is a The Px is a single term for all points in the domain, except the set dimensions zero, possibly a set of measures. So the proof of this result is not trivial, but is inspired in an idea of Caffarelli, on an older paper of Caffarelli, I think from the 90s. And I will explain the proof in the case of using this. The case of using this Caparius idea, the proof in the case of when H is the quadratic cost. So, in other words, when the map is standard monotone. And the idea is to prove, to use the following. This is a setup, like call S to be the set where T is not a singleton. And then SK is the collection of. The collection of points where the diameter is bigger than one over k, and then cover all the space by balls of radius one over six k, just in many ways, and define the set BJ star are the points where the image of TX intersects the ball. And then obviously the union of this set is the domain of. Is the domain of the map, as this is the set where Tx is not non-empty. And let's call SKJ to be SK intersection Bj star. So S is the union of SKJ. Pretty straightforward. The point is this. So we prove that SKJ has no points of density. Density. Therefore, the measure of the set must be zero, and therefore, the measure of each of these SKJ is zero, and therefore the measure of S is zero. And here is the argument. So take a point SKJ, X0, in SKJ, and let Y be a point in the TX0 intersection Bj. Tx0 intersection Bj, which is possible because of the definition of Bj star, and then there is a point So I think do you think it would be better if I use my computer or because if this is going to happen every five minutes, I mean we it's okay. And then I need the connect the Zoom session into laptop and the lab. Okay. At least this is not going to happen anymore. I mean. Is not going to happen anymore. I mean, if it will happen, you think it's going to happen continuously or yeah. Okay, let's try. Okay, I will just point. Okay, I will just point out with my finger then. So there is a point y2 in tx0, so that the distance between the y1 and y2 is bigger than 1 over 2k. Because otherwise, if this point doesn't exist, tx0 is containing the ball of radius 1 over 2k around y1. And this implies that then the diameter of tx0 is less than 1 over k, and this is impossible. Okay, and it's just impossible. So, this point is the next case here.  Well, I'd be happy to give you the slides. I mean, you don't need to copy, but this slide is important. This slide is important. Yeah, so I have enough time to copy. You can take a picture. Yeah.   Okay. All right, so this is not going well. Okay, so the last thing I said was this: that this is the That this is this we can pick a point y2 in tx0 so that this is true. Okay. And we also have another important thing is that this y2 is in is not in the double of bj and this is because of the choice of this of the radius and the choice of the the case and so on. So this the point y2 is away from from this 2b is outside 2bj. To be is outside to be J. Okay. You are moving this from there, right? You are moving this from there because I'm not touching this. I think it's well when you respond. You have to give it maybe two seconds and it'll be more than two seconds. Okay. Okay. So this means that if you take a point okay. So take so that we have these two points, take the vector difference between the two points and consider the cone vertex at x0 and axis e and opening delta. It's just a cone at x0 with axis e. With axis E, small opening. And then by the monotonicity, so if you take a point x bar in this cone and z in t x bar, since y 2 is in t x 0, you have that z minus y 2 x 0 minus x 0 is bigger than or equal to 0, standard monotonicity. So this means that the angle, the point z is in the cone which has angle less than or equal to an angle. has angle less than or equal than pi over two plus delta so the angle between x zero minus x bar minus x zero and e is delta is less than delta and the angle between c minus y two and e is less than uh is less than pi over two because uh because of this the angle between these two vectors is less than pi over two so the angle of the combination is so it's it's a it's a cone that has opening a little bit more than pi over two More than pi over 2. So this means that t of gamma is contained in gamma zero, but the y2 is not in 2Bj. So for delta small, this cone does not intersect VJ. So here's the picture. So this is the configuration. So this cone, if delta is small, since delta is very big, it's going to cut, but if delta is small, this doesn't intersect. This doesn't intersect the ball. So this means that PX bar intersection BJ is empty for all points in gamma in the initial cone. And then that means gamma intersection BJ star is empty. Okay, so Okay, so since SKJ is SK intersection Bj star, we have that you write SKJ intersection Br, now Br is an evolved around X0, and you have that this is empty. So there is no intersection with the cone gamma. So this means XKJ intersection, the ball BR is just the intersection with the complement of gamma. Complement of gamma. But that implies that if you take the outer measure, there's a point here, you don't know the SKJ is a measurable set, so you have to take out the Lebesgue outer measure. So if you take the outer measure of SKJ intersection of the ball, this is simply the outer measure of this set, but this set is contained in the ball intersection of the complement of gamma, but this gamma has opening delta. So this is a Delta, so this is small than one minus a constant depending on delta, which means that this ratio is always less than, strictly less than one for any R. So this means that the point x0 is not a density point. Okay, so this is the proof in the standard monotonicity case. So the implementation of this proof for a general cost is non-trivial and it requires. And it requires a formulation of the H monotonicity used in the inner product. The definition of monotonicity used in the H is not very friendly. So we can write what? The monotonicity before is because is the standard monotonicity. So The standard monotonicity. So is the x minus I can write here. So the is within a product. Can we use that? I'm talking about the quadratic cost here. Okay, so moving forward to the general cost. So, this is the definition of monotonicity. Now, we write this in terms of integrals, in terms of gradient. We only want to use our condition with on the Hessian. So, we write the first difference as an integral of the gradient, the same, the second. And then you put together the two, and then you form a second cation. And so you switch here, just the smallest thing, switch the signs. And then you get that you have a quantity here, which is a matrix depending on the four points. Depending on the four points, x, y, c, and eta, and zeta. And this inner product is on negative. So this matrix is the identity when you are in the standard monotonicity case. So here is a matrix which is given by this formula. And then the map is H monotone is if this inner product with this matrix is bigger than or equal to zero. Is bigger than or equal to zero. And this is the matrix. The matrix is symmetric and satisfies its properties. If you switch X and Y and you switch C and theta, they change. It's the same matrix. And since the H is homogeneous of degree P, the Hessian is homogeneous of degree P minus two, so then you can have this ellipticity condition on the matrix A. Condition on the matrix A. You just write, multiply and divide by the absolute value of the argument and use the p minus 2 monotonicity, and you get this kind of elliptic condition with this function. Okay, so far it's okay. I mean, the okay, so this is the notion we use and. The notion we use, and then we have to do re, I mean, you have we have to figure out how to do the argument with this cone, cones that depend now on this matrix that varies. And this is complicated, but I'm not going to explain this here, but I'd be happy to share the paper with you. Okay, so some extra consequences of this is that if T is H monotone, If T is H monotone, the inverse is H monotone. And the set of points where the set of points that belong to two different, the images of two different points by the map in T is measure zero. And then you can define a push-forward measure in this way. But this is just for a monotone map. And there is an extra consequence. Consequence which is the set of points xc, where xc is in tx, x in the domain is rectifiable. This was pointed out to me by Robert McCann. And this is the ideas are in a paper with Warren and with Brendan Pass. You consider sets that are monotone and you prove rectifiability. But the map, this doesn't imply. But the map, this doesn't imply the map is a single value because you can have a map. You can have a map, and then if you add K, so you have a pile of maps, and then the map is multi-valued, but the sets are rectifiable. Okay. So here are the L infinity estimates. So p is greater than or equal to 2. The function is The function is homogeneous of degree p. The Hessian is strictly positive in the unit sphere and it's a non-negative and convex also. And we have a map of this, we did some integrability on the map, is Lp minus one to this monotone. And then what we estimate is how far is Tx from a fine combination. A fine combination. So we take Ux is Tx minus Axb, A is a matrix, B is a vector, and then for each point and each radius, we have this estimate. I will come back to the estimate in writing down in a more friendly way. So the supremum of a small, slightly smaller ball is controlled by this quantity. Sorry, by this quantity, if the p minus one average is small compared with r and is smaller than this quantity if the p minus one average is big compared with r and the constants that depend only on the on the ellipticity of the medication of h and the norm of the matrix. The matrix and the parameter B of beta, of course. So, this gets worse when beta is close to one. So, here is a rewritten of the inequality. So, if you write the p minus one average of the u, u is tx minus the affine function, then the bound can be written in this way. The supremum is bounded by this power. Is bounded by this power of R and the P minus one average to this power, but you see if A P minus one average is smaller than R, so this is the kind of a convexity inequality because the sum of these two exponents is one. So if the AP minus one is smaller than R, so you gain something here. You can put this quantity. And if the AP minus one is bigger than The R the AP minus one is bigger than R, it's just controlled by the average. This second inequality is kind of the reverse Kelder inequality. Okay. So I don't know if I have 25 minutes. So okay. So here is the idea of the proof. I will the proof is non-trivial, okay? The proof is non-trivial, okay? It uses the homogeneity. I mean, you have to, it's hard work to do it. But let me just point out: the starting point of the proof is this potential theory formula. So if you have a function st two, you can write always a function at y as the average of the function on the ball of radius r plus this term, and the gamma is the fundamental solution of the Laplacian. Fashion. Okay, and then you have to introduce some quantities. So the main idea is you introduce some quantities and rewrite the monotonicity condition in a way that you have all the variables x on the left and no u of y, no u of x on the left. No u of x on the on the left and you have the u of x on the right because you are going to integrate this inequality using this this formula so there are there are terms you have to estimate i think i will i will just briefly there are you apply the potential theory representation formula with this function you have to you have to adjust the radius of the ball you have to look at what You have to look at what point to use the formula. And then you substitute, and then you have to estimate this from below and these two terms from above. And well, this can be done. And so one that the first term is not difficult to prove is bigger than. Remember, we went to bound u of y, the absolute value of u of y. So the homogeneity estimates the term two. And to estimate the term one, we use the inequality, we use the monotonicity. And in the end, there are quantities you. There are quantities you have to optimize, which when you optimize, it's not surprising that you get some convexity inequality. Okay, so this is here is the application to differentiability. So I'm going to do this definition. I'm going to introduce this definition, was introduced by Caleron Singman in the 60s. The problem they were studying was how differentiability is preserved by singular integral operators. And the standard notion of differentiability is not preserved by singular integral operators. But this is, this is. So they found the notion of differentiability that is preserved. So here it is. So you have a number P. A number p and you have a real number bigger than or equal to minus n over p for figurative reasons. You have a function in L P and then you say they introduce the classes TKP. I don't know if you have seen this before. Probably you did. So K a function is in the class TKP if there is a polynomial. If there is a polynomial p of x0 of the degree strictly less than k, if k is negative, the polynomial is zero. So that when you take the average, this is modeled on the Taylor formula, right? The average of f minus this polynomial is big O R to the K as R goes to zero. So this is the space PKP, and there is a And there is the little TKP in which the polynomial has degree less than or equal than k, and the big O is replaced by a small O. So these two notions were introduced by Caleron Sigmund. And of course, if P is infinity, you replace the average by the essential sup. And they prove a And they prove a remarkable result. They prove that if you have a function in TKP for all points in a measurable set E, then the function is in the little TKP for almost all points in E. And these order magnitudes are not uniform, so it's just at the point. And this is an extension of a well-known theorem of Stepanov. Stepanov proved this when P is infinity. And then we combine these results to obtain the L infinity estimate to obtain differentiability. Okay, so here one corollary about the differentiability. Suppose you have a T-monet H-monotone map and you have map and you have t in lp minus one of a small ball and there is a matrix an assumption there is a matrix and a vector so that the p minus one average is small over r then the function is in t p one t one p minus one so this means the function is differentiable in the ordinary sense and this uses the l infinity estimate it's very simple the proof Estimate because it's very simple, the proof. So you said ux is Tx minus AXV, and the call Pr is the P minus one average. And then from the assumption, P R over R goes to zero, and then we are in the first situation with the L infinity estimate. We can estimate the supremum of the ball. The supremum of the ball by this convex combination of these two things, which you write in this way. So you divide by R, and this goes to zero, so you get differentiability at x0. What? They are given. Suppose there is a matrix and a B. Yeah, your assumption. Leader low, lead aloe. Ah, okay, cookie, this is a little this is little t, little t. Yeah, yeah, yeah. Yeah, that's little. This has a big, oh, but this has a little t. Hypothesis is corollary thing. The hypotheses are P is bigger than 2 and H satisfies all the properties at the beginning. No, no, no, I'm not using color and signal here. But I mean the you have to assume that there is a point X zero. Yeah, yeah. No, no, no. No, no, no. This is just this is just the, it doesn't use color and signal. It's just this small O condition implies differentiability from the L infinity estimate, just from the L infinity estimate. But you need integrability of the T okay? I just mentioned the Cadder and Signal result. I'm going to use the Stepanov actually in a moment. I'm not going to use the full Calderon signal. So here. So here is the corollary. Another corollary. So again, it's log in p minus one. So the other one is the pointwise result, right, at the point. This is that. So it's big O, the average of Tx minus B minus one is big O of R for some B. And this is true for almost, for all. This is true for almost all points in a measurable set E. Let me suppose this. Then you have that tx is in T1 infinity, little T1 infinity, for almost all points. This will use Stepano. Okay, so here is the proof. Proof. So for each x0, by the assumption, there is a constant and a radius and a vector so that this p minus one average is smaller than m of x0 times r. Okay, so this is just the assumption. Now we ought to use this L infinity estimates. We ought to use the cell infinity estimates. You have to test. So, given R, we have either the P average smaller than R or it's bigger than R. The U is Tx minus P. So in the first case, from the L infinity estimate, we are in the case in which you have a convex combination, but this is smaller than Mx0 R, so altogether it. zero r so altogether is less than r constant times r depending on x zero in the second case we have to use just the supremo is bounded by the p minus one average but the p minus one average is bounded by m m x zero r so for at each x zero we have this inequality so this means this the supremum is smaller Is small, is big over R. That means that Tx is in T1 infinity, but now uses Tepanov. Stepanov implies that this is differentiable almost everywhere. Okay? So the L infinity estimate makes it. Okay, so if The result for so this is related to there are some consequences of this estimate for monotone maps and bounded deformation. So a locally integral mapping is a bounded deformation if the symmetrized gradient in the central distribution is a radon measure. This is the notion that appears in elasticity. It's a very much study. Now, if you have a standard monotone map that is locally in L1, then the map is a bounded deformation. And this is because the standard monotonicity implies that the distributional derivatives are non-negative and therefore they are radon measures. Now, there is a very nice paper. Now, there is a very nice paper from 1987 from Ambrosio, Koshi, and Almaso that they prove that if T is in bounded deformation, then T is in T11, little T11, for almost all points. But if E, the the map T is in in the space T one one at X zero, this implies in in the big T one one. In the big T11, which is the condition of the second corollary, and then we obtain differentiability of T. So this combination of this corollary before and the Ambrosio Cauchy and Dalmasso result implies that the map is differentiable almost everywhere. And this generalizes the map. This is for a standard monotone, right? This generalizes the result. This generalizes the result that proved earlier by Mignon, but it was proved for maximal monotone maps. This is just any monotone map. Okay, so what happens if you consider the cost? The question is, if you consider the cost, what can you say about the differentiability? Okay, so the differentiability for general cost. Is that this matrix? Now, T is H monotone. This matrix is positive semi-definite in the sense of distributions. This can be proved just from the monotonicity. And then the matrix A can be represented with a matrix value Radon measure. And when the H is quadratic, this agreement. When the H is quadratic, this agrees with the notion of bounded deformation. This is just this extra term, which is harmless. But if T is locally bounded, which is a consequence of the L infinity estimate, the matrix there is a matrix value distribution of order zero. And therefore, the matrix the matrix here can be represented as a sine radon measure. Therefore, this is in T11 of X0. One of x0 little t one one of x0. So, the colour of the colour So then we have the following result. Just to wrap up the result, if you have a monotone map, H monotone map, which is L P minus 1, locally L P minus 1, then DH of X TX is of boundary deformation. So the T is inside this expression. And ideally, it would be good to take it out of this expression, but we have this combination and belongs to T11. Now, this, I have how much time. 10 minutes. Okay, so let me finish with this. This is the consequence of the result I just explained. There is a colour continuity result. A Helder continuity result for H-monotone maps. So if the maps are locally in Lp minus 1, they satisfy a Helder time condition. So T is in space T1 P minus 1 infinity. P is bigger than 2, okay? Bigger than or equal to 2. So this can be proved using first a couple of lemmas. The first lemma is a calculus lemma, which is the variant of the lemma that Which is the variant of the lemma that we use to prove the L infinity estimate. So, in the proof of the L infinity estimate, you need some calculus lemmas. And this is a simpler situation of those lemmas. So, we have this quantity, which is bounded, is comparable to the maximum of the vectors v1, v2. Some constant and also it follows that from the previous formula and you write down the using the integral representation, h representing with the gradient, this follows from so the difference of the gradients at two different points is bigger than a minus b to the p minus one. Okay. Okay, so then here is the theorem. So we have an H monoton map, which is in Lp minus 1 locally, and H satisfies the assumptions I said at the very beginning, then the supremum of Tx minus Tx0 is big O of R of 1 over P minus 1 for almost all points. This is the kind of Helder continuity condition for T. For T and the proof uses the fact that the gradient of h evaluated at x minus tx is t11 from the previous theorem. And then at each Lebesgue point of this function, it follows this. Because if it's T11, the polynomial is constant. Polynomial is constant. And in the definition of T11, if there is a constant, there is a polynomial, it's unique. Polynomial is unique. You can prove it's unique. And this satisfies that a similar thing with the different, with the Lebesgue, because it's a Lebesgue point. You got this is the difference of the gradients is big O over. Okay. Now, applying triangular inequality and the previous lemma, so that the gradient of h at a minus b is bigger than a minus b to the p minus one, you estimate tx minus tx0, p minus one, this triangle inequality. Put the p inside, you lose the power constant, and then Here, this is bounded by this, and this is just calculation: O R to the P minus one, but P is bigger than two, so this is big O R. So, this is O R. So, this altogether is O R. Then, for each, for almost all points, you have the P average of Tx minus Tx0. minus tx0 is bounded by a constant depending on the point times r to the one over p minus one and now apply the l infinity estimate so you have to test either this is smaller than r or this is bigger than r so in the first case if it is smaller than r you have the convexity inequality which all together gives you a different constant Together gives you a different constant times R. And in the second case, you just get by the L infinity estimate the quantity, which is R to the P minus 1. So, but this is bigger than this. So, you get the supremum is big O of R to the one over P minus 1. R to the one over p minus one. So you get the color continuous. And the final remarks. So L infinity estimates for optimal maps were proved. So there are L infinity estimates available in the literature. They were proved by Bushite, Jimenez, and Mahadevan. Had the one, but they use the P-Wasserstan's distance. And now, as I said, so an important ingredient in the proof of the partial regularity of optimal maps are L-infinity estimates. And several of the steps that Goldman and Otto proved for quadratic costs can be extended. But as I said, there is a But as I said, there is a missing part still in the analysis, which is the quasi-orthogonality condition that we don't know how to handle it for costs that are not quadratic. Also, another observation: we prove a first paper with Anamaria in which we have L-infinity estimates in terms of. a value infinity estimates in terms of average of p averages of tx minus x p and here we have estimates in terms of p minus one so but the the p average um and the p minus one average are related it seems that the the p minus one average is is is more fine so you might get finer estimates Finer estimates with p minus one than with p, but I don't know how what is the impact on that in the argument of Goldman and Otto. Maybe this is something to to be checked. And then the argument we use to prove L infinity extends to power costs in the Heisenberg group. So there are some optimal transport made by the Heisenberg group. By the Heisenberg group, there is a pioneering paper by Ambrosio and Rigaud. And some of these things we prove can be extended in that setting. And here are the references. So the first paper decays the estimate of Tx minus X, which they call the excess. Call the excess. I don't know what they call the excess, but we prove it also. All these papers are with Anna Maria Montanari here in 22. And this is in our key. This is version two. Version one was a more condensed version, contain differentiability properties also. Differentiability properties, also, but we decided to split the version one into two papers because it was too long. And so, this contains just the fact that monotone maps are single-value almost everywhere. And this paper is the concerns about the differentiability. And that's all I have to say. Thank you very much for listening. Okay. So thanks for a great talk. Um so you you're getting the lymphidity estimates just from sort of two monotonous C. Sort of two monotonic two h two monotony. Yeah, so I guess what it means to there's that result like Carfailli and Millman, where if you have like two plus epsilon monotonous C plus bounded information, then you get C alpha. So we're going to say it again. So if you have more monotonicity, oh, two plus epsilon, two plus epsilon, maybe for like very you get. You get, I don't know, I didn't think about that. Yeah, I know the paper by Kafaria Milman, but I didn't realize the connection. Good that you point it out. Yeah, yeah. Thank you for the talk. You proved that this test this thing. That this singular set has measure zero. Do you expect more structure? Can you prove? Yeah, I think that's a good question. I think the proof, or at least in the standard monotone case, I think with the argument with the balls, if you move the balls in a certain way, you probably can estimate the house or dimension of the set. Would you expect it to be equal dimension one, rectifiable, like that? Well I think these are probably n minus one, but I don't know. Thank you. Thank you for the very nice talk. Uh okay, so I believe the Golden and Others result uses the the transport. I wonder if you can use that formation too in the details or P yeah we we use you mean the the with the with the Brainy approach yeah yeah so we prove these things in the first paper yeah this is proven the first paper we prove uh estimates of this type yeah yeah yeah Thanks for the nice talk. I just have a kind of naive question about the homogeneity assumption. So, is it true that the only place that this assumption plays is like the kind of getting the elasticity constants with the A's? So, remember the P homo, like the cost, like H, having to be homogeneous. Yeah. So, I was just curious as to how much you would expect. Curious as to how much you would expect to be able to relax. Like, do you expect this to hold? Well, we use heavily the P monodon institute very much because we are modeling the case of the P power. I mean, what other, what other, I don't know what other, yeah, you probably you can extend, but I don't know what would be the interest. I mean, it's a class that is pretty clean, the monotone, but I don't know. But I don't know. Yeah, probably, but