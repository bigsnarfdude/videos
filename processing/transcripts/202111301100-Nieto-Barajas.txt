Back in the business. It's been like some time without speaking, without masks. And coming back to the talk, this is some work I did while I was on sabbatical in Oxford. So this joint work with James Watson. Watson and Chris Holmes, and I am the black bean in the rice. These two guys. So this is part of James Watson PhD thesis, and it was published in 1917. Okay, so. Okay, so I am not an O-base guy, but this is something closer to O-Base. So I will speak about this more in the objectives of the talk. Okay. So, how do I move? Okay, the arrow keys. No, it's not moving. Not moving. There you go. Okay, there you go. Now it works. Okay, so it's only three parts. It's going to be an easy talk, probably quick as well. So I will speak about the objectives and it will have two parts, quality trees and bootstraps. And I come first to the objectives. So it's just one slide. So basically, what we have is a random probability measure that we That we call F, and we center this random probability measure on some known F naught. So the idea is to characterize the dispersion of any random probability measure F around its centering measure according to the KL divergence. So, how dispersed this F is with respect to the centering measure using the KL. Using the KL. So we have two ways of defining the KL. The KL is not symmetric, but in general, it's just the expected value with respect to the first measure that we write here. And it's just expected value in log scale of the ratio. Okay, so we are going to study and characterize these two different KLs. And the way to identify them, we define the first one. them, we define the first one as KL and the second one as reverse KL. But it's the same one, just it the just the way we put what measure we put first. Okay. And the F's we are going to study are poliatris and Bayesian bootstrap. And the way this relates to objective base is how to define the prior distribution. Distribution of the random probability measures in such a way that we have a specific dispersion in order to make inference. Okay, so how to calibrate the prior such that we have like a certain amount of dispersion of the prior and variability of the prior to carry out posterior inference. Okay, so we'll go first with the polya tree, and this is just the notation. The polio tree, and this is just the notation slide. We are using a notation from Labine and one of my previous papers with Peter Mueller. So the way we characterize a polya tree, we need a binary partition tree. This is going to be called pi, capital pi. The partition elements are going to be indexed by two indexes, m and j. M defines the level, and j defines the level. The level and J defines the location within the level. We have a set of constants that define these are the parameters of the polya tree. So the polio tree is defined through the partitions and the parameters. The parameters define the shape parameters of the beta distribution for each of the branching probabilities and any probability of any set. Of any set is going to be defined by the product of the descending branching probabilities y's. Okay, this is just the notation to identify to which parent it belongs. Okay, so but it's just the product of the y's. And these y's are beta distributed. Okay, now we want to center the prior in a specific f naught, and there are different ways of doing that in a Bollywood. In a polya tree, the simplest way is to match the partitions of the tree using the centering measure, using the quantiles of the centering measure, plus defining a constant alpha parameter of the beta random variables to be only a function of the level, not a function of the location within the level. Okay, so with these two elements, this the partition department. This partition defined by the quantiles and keeping the alpha parameters to be only a function of the level, we obtain that the expected value of the polyetry is F naught. Okay, so we have here a parameter that is called alpha. This alpha plays the role of a precision parameter in the tree and a function rho that controls the speed at which. That controls the speed at which the variance of the beta random variables increases as we go down in the tree. Okay, there are different choices for the row. If we choose the row to be 1 over 2 to the m, we obtain a discrete measure. So with the polya trigger, we define a digital process. If we take the row to be constant, so it doesn't change according to the length. So, it doesn't change according to the level of the partition, then we define a singular measure, but continues. And if we take the rho to be m squared, which is the most typical choice, and this is the choice suggested by Lavin, we define an absolutely continuous measure. But we know that there are other ways of defining an absolutely continuous measure, and these are the ways we are going to define. The ways we are going to define here. So there's a paper by Kraft. I don't cite it here. But for continuity, we need that the sum of the raw measure, inverse raw measures from one to infinity to be finite. And there are different ways of doing that. So if we take m squared, m to the cubic, two to the m, four to the m, any power. And in power, we define an absolutely continuous measure. So, in order to consider a large range of possibilities, so we take one over two to the m to define a discrete measure, a constant rho to define a singular measure, and two options for continuous measures, m to the power of delta and delta to the power of m. Delta to the power of m. And in these two cases, row three and row four, we need delta to be greater than one so that we satisfy the condition for continuity. Okay, so these are the four choices we are going to study. All right, so in practice, to study the behavior of a polio tree, we stop partitioning at some level, capital M. At some level, capital M here. So instead of growing a tree up to infinity, we stop partitioning it at capital M. And then to keep the centering measure such that the expected value of F is still F naught, so we spread the probability in the last set in the set B, capital MJ, according to F naught. Okay, so before. Okay, so before level capital M, so we have the branching probabilities wise, and then we spread the probability in this last partitioning set according to F naught. Okay, so this is just a final polya tree, and we still have the expected value of F to be F naught. All right, so in the notation, we are going to define the level of the last level of partition we are considering. level of partition we are considering the precision parameter alpha the concentration parameter rho and the centering measure f naught okay with these finite polyetries we want to characterize these two kls pullback liber divergences between f naught and f and f and f naught as a function of the random variables we have this expression for the coolbat library this kuhlbat Kullback Libra. This Kullbach Libra, remember that this expected value. So taking the expected value, we have this expression. And if we take the reverse K L, we also have a nice expression here. Okay, so what we want is to compute the expected value and the variance of each of these two. Okay, and these are the results we present here. And the spectrum. The expected value and the variance can be computed analytically, and they are functions of the d gamma and the three gamma functions here, but they can be computed and they can be evaluated. They are not so complicated. Okay. Now, with the reverse KL, so when we switch the order of the F and F naught, the expression And f not the expression for the expected value is not that bad, but the variance becomes messy. But thank you to the previous talk. I am not afraid of showing messy things. I actually, I wanted to comment: if he's listening, I appreciated the proof that he was explaining. Because nowadays, we don't usually see that in talks. Well, I don't see. TOX, well, I don't see that in talks. So it's messy, but not easy to evaluate. No, no, it is not. Actually, we can compute it. It is just messy, but it's easy to evaluate. Yeah. Okay. Now, so these are the main results of with those equations. We can see these nice graphs that can be interpreted and we can obtain some results. Okay, the first thing we show here is that. Okay, the first thing we show here is that there's the empty dots are the, these are all means, this is all expected values. The empty dots in all graphs correspond to the expected value of the K L F naught F. The red dots, the solid dots, they correspond to the reverse K L. So in all cases, the expected value of the reverse is smaller than the other one. So there's a kind of domination. Other one. So there's a kind of domination. One is larger than the other. Okay. And the four panels. The first panel is the discrete polyotry. The second one is the singular with constant alpha, with constant rho, sorry. The third one is m to the power of delta. And the fourth one is delta to the power of m okay. So in this case, Okay, so in these cases, what we can see is that the expected value for the discrete measure, as we increase the number of partitions, it diverges. So it increases. So this is the Dirichlet process. So the Dirichlet process has full support. So if we see the paths of a DP according to the centering measure, then we respond. Then, with respect to the Kl, they will be very, very dispersed. Okay. And if we go to infinity as the number of partitions increases to infinity. Okay. For the singular continuous, so what we see is that we have a constant behavior. Even if we increase the number of partitions in the tree, the divergence is constant in both cases. In both cases, KL and reverse KL. And for the continuous choices of the polio tree, when we make inference, we usually choose one of these two. So because we want the process to put a prior probability mass on continuous distributions. So these are the choices we usually take. So what we see here is that if we increase the level of partitions in the Level of partitions in the polio tree, what we gain is not that much. So there is a decreasing gain when we increase the depth of the polio tree. Something similar happens with this other choice. And even with this one, from level from m equals six, there's no gain at all, practically, in the expected value of the KL. What happens to the variance? What happens to the variances? These are standard deviations. It is one. But okay, these two are increasing, but we don't see this one, but this one is still increasing. This one goes up to a thousand, yes. And that's something we're going to discuss later. And that's something we're going to discuss later on. I hope not. So we'll see. Okay, but bear in mind this number here, because we will get to the digital process again through the bootstrap. Yeah, but bear in mind this number. So we have to check it. We have to check it. If the other graph doesn't coincide, then there must be a problem. The reverse scale is the full dot, this one. It increases, but because of the range and the scale, it is increasing. It is increasing. Yeah, but it's not that fast. It has to go to infinity. Both. So we will see that with the bootstrap. They have both got to infinity. They have both go to infinity. They have to diverge. Actually, in the standard deviations, something similar happens. So these are the variances of the KLs. Again, same thing, empty dots is KL, full dots is reverse KL. What we see here again is a divergence behavior as we increase the number of levels of the polya tree. This is the Britishlet. Okay, okay. Okay, we can see the scale. The scale is again small, but this has to go to infinity. And as compared, if we compare the scale with the others, so these are finite scales, the variances for the singular measures, one diverges and the other one stays steady and is finite. For the two continuous choices here, Here, what we have is that the variance doesn't increase at all from m equal two onwards, the variance stays the same. And something similar happens here again. Again, from m equal three, the variance is steady. Okay, now this is a nice graph we wanted to see. Okay, in the other choices. Okay, in the other choices, everything was fixed. So, in these graphs, so alpha was equal one and delta was equal to what we what we did later is moving alpha and moving delta only for the continuous cases. Okay, so this is the choice of m to the power of delta. This is row three, so this corresponds to the third graph, but what we To the third graph, but what we do here is we move in the first one, we move alpha in the left one, and in the right one, we move delta. Okay, now if we move alpha, okay, this one is small, it becomes larger. As we increase delta, the expected KL increases. Now here, if we move the power, if we move delta, something reverse happens. So this is 1.01, this is 1.1, this is 1.5, and this is 2. This is the typical choice suggested by Labin, this power of 2. So in terms of the expected value, it is constrained. It is constrained, so we can make the expected value, so the prior to be more diverse, more dispersed. If we make the delta parameters to be smaller, closer to one. Okay, so this one is with delta 1.01. So the expected KL is higher. So in terms of the prior, this prior could induce like more dispersed, more paths. More paths that are further away from the centering measure. Okay, so the choice of levin is very tight in terms of the KL. So it is not that different. So the parts of the prior are not that different from the centering measure. So this is the message here. Okay. Now, okay. In terms of the alpha, in Epolle tree, the typical choices of alpha are small values. Small values. Okay, so what we do is when we have a data set and we don't take alpha lower than 0.5, if you make a data analysis, you only take alpha to be 0.5, 1, 2, but you don't take very large values of alpha, okay? But what this graph says is that if we increase the value of alpha, The value of alpha, we have more divergence in terms. If we increase the value of alpha, we have less diverge measures. So we need a very small value of alpha. That makes sense because usually small alpha is more non-parametric than with larger alpha. That's good. Well spotted. Well, spotted. You are not falling asleep. Okay, now. Okay, I will come back to this graph. I will come back to this one. Okay. How to calibrate a polyatry? How to choose a prior. Okay. So what we have is measures in terms of KLs. So K L is positive. The range of values for any KL is just the positive real line. So how to choose a specific. Line. So, how to choose a specific value of a KL such that we define the parameters alpha and delta to match that KL? So, we found a very nice paper of George Caravatsos in 2006. He published this one in a psychometrics, psychological journal, something like that, but it was a Bayesian opparametrics paper. And what he did was to consider two Bernoulli's for F naught and F. For F naught and F. F naught was a Bernoulli with parameter 0.5 and F was a Bernoulli with parameter 0.99. So what he assumed was, okay, a Bernoulli with parameter 0.5 and a Bernoulli with parameter 0.99, they are pretty much away one from each other. So if we compute the KL, the KL is 1.61, which is not that high. Which is not that high. Okay, so if we take the log of the log of this KL is approximately 0.5 Okay, now if we want to match the polyetry such that the expected K L, the expected KL are the continuous black lines and the dotted lines, the dashed lines, they are the standard deviations. Okay, so if we want Okay, so we want the expected KL to be 0.5, so we are in this line here. If we move from left to right, the variance decreases. Okay, this is minus 3, this is minus 2, minus 1, 0, 1, 2, 3. Just the red lines. So one typical choice, so if we want to match the KL to be around in log scale. To be around in log scale 0.5, what we do is to take values of alpha around 0.5 and delta closer to one. Okay, so that the variance is smaller. Okay, this is the row three. This is m to the power of delta. Yes. Okay, so we choose a row. Okay, so we choose around alpha 0.55, something like that, and delta close to 1, but we want delta to be larger than 1 to make that a continuous prior. Okay, so this graph is going to help us to decide the choices of the polio tree such that we have this value of KL. Okay, so that was the message here. us the message here. Okay, so we can choose different other different values instead of choosing this one. Okay, we choose this reference point because we don't know the values of a KL. So it's just positive. So it's not bounded between 0 and 1. It's unbounded. Okay, so we found this to be a nice way of calibrating the prior. Okay. Now we've got to the other. We've got to the other measure. So we leave the polya tree and then we go to the bootstrap. Okay, to define the bootstrap, we consider a discrete measure which has a specific set of atoms, this psi i. So we have n atoms, and then we have probabilities pi, and this is going to be our centering measure. So this is just a discrete distribution. Okay. Uh, distribution okay, this is all fixed. This is there's there's no randomness here, okay. Now, uh, if we make the weights like an earthquake, okay. So if we uh if we define a random F, now we are going to define a random probability measure to be similar to this one, but instead of taking fixed pi's, we take In fixed pi's, we take random weights. Okay. Now, if we substitute the atoms by the data, and then we take the weights, n times the weights to be multinomial with a probability of success one over n for each of these multinomial distribution, then we get the frequencies bootstrap of Fram 1979. Of Ephraim 1979. Okay, so this just so the weights have to be n times the weights have to be multinomial. And now, instead of taking the multinomial distribution, we take the Dirichlet distribution for the weights. And again, we take the probabilities to be 1 over n and the alpha to be n. Remember that the probabilities here for the digital is the product of the alpha and the ps. Of the alpha and the p's. So if we take the product of the alphas and the p's, we are just saying that the d shed has parameter one in this case, and then we get the Bayesian bootstrap. Okay. Of course, we can make this different use of a multinomial, but instead of taking the piece to be one over n, we take a generalization of that. And here, if we take the alphas and the p to be different than these two choices, then we have a generalization of the Bayesian bootstrap. Of the Bayesian bootstrap. Okay. Now, again, we do the same. We characterize the Kullback Libert of the centering measure and the random probability measure and the reverse KL. They don't depend on the locations on the atoms. The atoms disappear. And the first thing we see here is that there's always a domination, something similar to what. something similar to what we had before. So the KL is larger than the reverse KL for these two random measures. Okay. And if we take the expected value, so we have analytic expressions, but in this case, we don't have an equality here. We have only about an upper bound because the computations and the algebra was not that easy. Algebra was not that easy, but we were able to find an upper bound for the expected value. Okay, so in case of the we take p1 pi equal 1 over n, this upper bound simplifies to this log 2. Okay, so we have some nice upper bounds for this expected KL for this in the case of the frequency bootstrap. Okay. Okay, and for the Bayesian bootstrap, we also have nice expressions here. Again, we need this: the dig gamma and the three gamma functions. But now we have equalities here for the generalized Bayesian bootstrap. The word generalized means that we don't only consider the Dirichlet with parameters alpha equal n and one over and the piece equal one over n. Okay. Okay. So we have three choices here. And then we come back to the previous slide and see the, then we see a graph here and see the scale. Okay. If we take, this is the generalized Bayesian bootstrap. So if we take p equal one over n and alpha constant, and if we take n goes to infinity, then we have the true process. Okay, so this is again with the polio tree when we have the discrete choice for the measure row, for the function rho. So we again have the Diligent process here as a limiting process when we increase the number of atoms. The mean and the variance of the expected KL, they diverge. As they were the graphs, the previous graphs, they were increasing. And the reverse KL, we haven't spoken about the reverse KL here yet. Haven't spoken about the reverse KL here yet, okay. But the KL in mean and variance they diverge. We'll see a graph to see the scale. Okay, if the alpha increases with n, since the product of these two, this defines the Bayesian bootstrap. So the probabilities of the Birichlet, they are constant once when we take the product of these two. So with the Bayesian bootstrap, Two. So with the Bayesian bootstrap, this is the case. When we take the atoms to go to infinity, the expected value of the KL goes to a finite, a finite number, it converges. And the variance goes to zero as we increase the number of atoms. But if we take the alpha, I missed here that the PI have to be one over n here, but if we take the alpha to increase as n squared, then the product. Squared, then the product of p and alpha is n alpha times n. So the parameters of the digit they increase as the number of atoms. In that case, the mean and the variance, they both converge to zero. Okay, so what about the reverse KL for the generalized Bayesian bootstrap? So we also have an analytic expression for the expected value and for the variance. Value and for the variance. These covariances and variances, they don't appear here, but they can be computed. We have the specific formula in the paper. And for the specific choice of the Bayesian bootstrap, so the product of these two is constant. When n goes to infinity, the expected KL goes to a finite number and the variance converges to. And the variance converges to zero, something similar to what we had before in the KL. And these are the graphs to see more or less the same scale. Okay, so we have three choices. This is the Dirichlet process when n goes to infinity. Mean in the top row and standard deviation in the second row. Dirt process. This reprocess Bayesian bootstrap, and this is another bootstrap generalized with this is n squared. This is when alpha, when alpha is n squared, when alpha is n, and when alpha is, sorry, this is when alpha is n is constant. So in this case, what we have is the Dirichlet process. So the expected KL, it goes to infinity in both cases. And the scale what we have here is 15. The scale what we have here is 15 as compared to 1000 we had before, but it has to this has to diverge so but it is slow, slow. The divergence is slow. It takes time. Takes time. So you have 20 points here. So to make it non-parametric, you have to take n to go to infinity, but it will eventually diverge. Okay? But this is slower than the other one. So one thousand. Other one. So 1000 was not that bad. Okay. So if you want to, you want to have a prior. So like the DP. We know the DP, it has full support. The DP has, it's very, very diverged. All the paths of the DP, they can be very diverged with respect to the centering measure. So it's a good Bayesian non-parametric prior, the DP. Okay. Whereas the polya trees, the Whereas the polio trees, the polio trees in the continuous setting, they might not be that good. Okay. But the bootstrap, this is the choice for the Bayesian bootstrap. The expected KL, it increases, but a slower rate. And the variance, this one has to go to zero. And when we take alpha to be n squared, in both cases, mean and variance, they both converge to zero, which Converge to zero, which is just a graphical representation of the three cases we had before. But it's actually it puts much on this privacy with but not necessarily on the data. Now, what I mean with the digital process is that for the discrete measure, we are considering here. Discrete measure we are considering here with n number of atoms if it yes if that's on the data so for that to be the the tiles process so we need an infinite number of atoms and we need a specific choice for the weights yeah so the weights here they they are uh constant in the dirichlet so we get the weight constant So we get the weight constant here. So this is the dp case. So the alphas are constant, but the probabilities is one over n. So the parameter of the Dirichlet is a product of these two. So for the discrete measure that we have here, for that to converge to a DP, so we need the parameters of the Digit. The parameters of the diriget to decrease. So we want them to be one over n. Okay. So this was the message. These are the references and the paper is here. It was 2017. Yes. Okay, if you have further questions. So thanks for that. I really enjoyed the talk. So I was wondering, with some other random probabilities Random probabilities. One often characterizes variation by looking at the sidewise evaluation of the random probability. So, for example, P evaluated on A. Right, right. And then looks at the variation of the, sorry, of the variance. The variance. Yes. And so I was wondering what is the advantage of looking at the caldivergence or maybe even the difference with respect to Maybe even the difference with respect to square, okay. So, what we wanted to consider is a different approach because the mean is always F naught. So, we can study the properties of the variance of any random probability measure, could be the DP or this discrete measure. But what we wanted to see is it is in a different perspective using like a divergence. Like a divergence to see in two measures, how do you compare two different measures? So, what we wanted to compare is the paths of the processes, the random probability measures, how different they are from the centering measure. So, one approach is, as you mentioned, is to study the behavior of the variance. But another one is to consider a divergence, how diverse they are. And that can be studied in different ways. Different ways. I had to can I stick here or do I have to get an answer point or it's loud so it's perfectly cool or okay. That's fine. So no, it's a simple comment I was I was I was thinking I was wondering for the first part of your talk so you're looking at the at the trees but then looking at the tree part of But then looking at the two part level divergence between something that's going to be singular with respect to a continuous measure and F naught doesn't really make a sense. I mean, you can't expect it. And so saying that it's more diverse is a bit like a fake thing, no? Yes. Yeah. So for this case, for type two. Yeah, even for type one actually. It even works for type one because for type one that you process is like you have discrete distributions and it's not discontinuous. Have discrete distributions, and if not, it's continuous, and so the cube average has to be infinity, yeah, right? Right. So, what you're saying is that it makes no sense to consider the divergence between a discrete it's not that it makes no sense, but it's just saying that, oh, you have saying that the conclusion, I sort of disagree with the conclusion which says it's a more diverse trial because the divergence was the the di distances is larger, it's just that because you're looking at something which which makes no sense. Working at something useful which makes more sense. Well, in this regard, what motivated the calibration was using two discrete distributions, two Bernoulli distributions. Yeah, but they have the same support. The other ones, they don't have the same support. And so it's like if you have a continuous density and you consider a different process which has a discrete okay, yeah, yeah, yeah, I see, I see. Yeah, then the path has to be intimidated. That's right. Yeah, if not, it's continuous. Yeah. And the parts of the process in this Paths of the process in this case they are discrete, yes. Okay, yeah, yeah, I see. So, so it's just like I was questioning the conclusion for the polyetry in particular, and for this case, for this case, and so and so then it sort of leads to can you make the same conclusion even for the like is it the right conclusion to make the fact that it's still biased actually the conclusion is um is consistent with the other case in the other case we have we don't have any problem because the support is exactly the same in in when we consider the bootstrap Bootstrap and then we arrive to the dealership process and then we update similar things. But yeah, I see your point. And then so, and then I had another question. I wanted to understand your point of view. Like, for instance, there was a statement by Castillo who studied posterior contraction rates. Of course, it's a different perspective and in my question is a little classic for using these poya trees. And what he found out was that if you wanted to have a Out was that if you wanted to have a Fourier compaction rate which is like reasonable or like minimax or the thing that you would get, the best one you could get in a class of functions, you would need a delta to the power n, so like the worst case for you, type four. Case four, yes. Which in your consideration is like the least good because it's the least diverse. So somehow you need to be super smooth. That's right. And so what do you think? You think it's like we shouldn't look at posture contraction rates? It's not the way right. I think these two different things. One is for the posterior contraction rate, what you're looking at is posterior consistency. So you don't want to be that away from a specific F star, which is the true one. And then you want to be close to that one. And then you cannot have a prior that is very divergent with respect to that F star. To that F star. Here we just the properties of the Dirichlet process as a prior probability measure and how they behave with respect to the paths it induces when we use the prior. Yeah, but for data analysis and if you are looking at posterior consistency, then you might need something very strong, which is this case when the KL is not that not that large. I think it's I mean it's a very interesting point of view, like so, but you you get the So, but you you get the very different answers whether you look at the posterior categories or whether you look at your thread of the poster of the prior. And so so then the question is whether you choose which would be fake rate approach. I usually use this one actually I use delta equal 1.01, which is the one that induces more diverse. So it's this case this. Case this empty dot here, that one is induced with delta 1.01. That's my favorite one now. But yeah, if you're looking at different perspective, then you have something that might contradict or you have to compromise on what you want to do. Yes. Just as a follower to the first part of the lead question on some discrete and continuous. Discrete and continuous. The fact that they don't have the same support, which means that what's the same with the candidate for comparing them? Okay, the thing is that they do have the same support unless you go to infinity. At infinity, then the end. Yes, but before when they are finite, they have the same support. So if that you're approaching through right. You're approaching through the right constructions, and it makes sense the KL, but in the limit, then you have this thing. Yeah, but before you go there, the support is the same. It will be interesting to see, like, if you consider the different distance, it's a bit like what Marta was saying. So, that's. When you have alpha N that goes to increase or alpha N that sort of increases very fast, then you're shrinking the priors to a half for each of the points, actually your prior is less diverse anyway. So that is a function. That's something this case. But the sense you get the Sumerian strong, but the Samira from the right straight that might be worth studying. That might be worth studying, yes. Yeah, this is this expected value of a distance and the variance of a distance. You can only, you can, you can go direct for the variance of the process or use another distance. Yeah. All right, thank you, Riz.  That's it for me. Forward, backward, and this is the pointer making. The pointer.