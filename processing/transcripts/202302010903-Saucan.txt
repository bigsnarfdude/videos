differential geometry, complex networks, imaging and graphics, and quality regular mappings. And we'll be talking today about the versatile Forman-Ricci curvature and its applications. Okay, thank you very much. Thank you for the introduction. And thanks to organizer for inviting me to give this talk. Unfortunately, I couldn't make it to Ben, which of course I regret. So before I start, I should make clear a few of things. First of all, I'm First of all, I'm discovering more and more to my surprise and even to my dismay that my talks are more on the engineering side of each conference I'm participating most of them. So somehow this is still a surprise for me, but I hope it won't be too disappointing for you. My second observation would be that and there is a connection between And there is a connection between the two of them that the talk, my talk today, it's somehow different from what I saw in the days before, namely it's much more applied and less formally mathematical. On the other hand, there is a and perhaps a connection with the deep theory of theories less manifest, but there is a connection which is very But there is a connection which is very concrete. I have some problems navigating this way. Just a second. Okay, so now that should be okay. So, but connection is through the very definition of the notion I'm talking about, which is a forman-ricci curvature for networks. So I don't want to start I don't want to start from the beginning by talking about why one would like to use curvatures to study networks, except perhaps as saying that one can combine a term which goes only that far and you would like to have something more interesting to say. More interesting, more deep is geometry. And as the late and regretted Robert Brooks used to say, the study of geometry is, in fact, almost coincides with the study of Coincides with the study of curvature. Therefore, if you want to understand networks better, it's natural to look at curvature. And for different reasons, some of them will become more clear during my talk. Ricci curvature is another thing to look at. Now, the type of Ricci curvature which I'll talk about is a state is a formal Ricci curvature and it's Curvature, and it's defined from the so-called Bochner-Weitzenberg formula, which gives a Riemann-Laplacian operator or Hodge-Laplacian. That's the connection to this workshop, with a graph Laplacian and the curvature term, which is very complicated. So, this is a simple form. I mean, it's hard to understand how difficult this can be. Difficult this can be because this convention list, this kind of writing is in terms of or in form of differential forms. Therefore, it's made for easy digestion. It might be more easy to understand at first glance in its form for functions, but it's not relevant when we try to study networks. So, this simple form is easily. Simple form is easier to adapt, it's more natural, it's the only one you can really adapt. But I should say something about this term, which is hideous. It doesn't make much sense. I mean, it contains some curvature, types of curvature, but it's more and more strange in higher and higher dimensions. But in dimension one, it's just in the classical case of smooth manifolds, Riemannian manifolds, it's just Riemannian curvature. Sorry, Ricci curvich. Curvature, sorry, reach a curvature. Therefore, the idea of taking this object and no formula and adapting into complexes. So for complexes, this was done by Foreman, Robin Foreman, some time ago. And I'm sure he, well, I don't think he had in my networks at that point in time. But in any case, it holds for a general CW complexes, weighted CW. CW complexes, weighted CW complexes. So the decomposition you show on the previous slide now becomes written usually in this form. And there is some arbitrariness in his definition, but it doesn't matter. It functions. So you will get a Laplacian and a combinatorial curvature function. And this combinatorial function can be defined formally in a manner which Formally, in a manner which is by now, I would say, quite standard for those working with complexes as a scalar product. And the weights are given in this formal manner. I can't be technical, even if I want it. I really don't want, because I would like rather to show the application of this deep ideas. And as I said before. And as I said before, in the classical case, in the dimension one, you get the Richie curvature. Therefore, you define if you have a one-dimensional object in a cell complex, you call the attached curvature as the Ricci curvature or curvature term. Now, one-dimensional objects in classical sense would be In the classical sense, it would be vectors, directions here, it's just edges. So to each edge, you attach forman, its forman curvature. There is an expression for it, which is horrendous, by any means. Because it has a lot of notation and it's hard to understand any I mean, it's not that bad, but it's hard to understand what it means. Hard to understand what it means. People here understand this means that it's a phase off, and then you have the notion of the other notion that appears is that of parallel phases, which I'll discuss it in a second. So this is a formula for the general case of curvature, and there is a formula for the Laplacians in any dimension. So there is this beautiful, if you want. If you want, it's graded. You can look at any dimension. You have a curvature and two Laplace, and corrected and uncorrected one if you wish. And that's excellent if you want to study objects as many scales and many dimensions. What does parallelism mean? Parallelism means, at least for us, that two cells are parallel, if they have a common parent. Have a common parent, higher-dimensional gentleman phase, or common child. For instance, in this case, if this is our edge, these two edges here, sorry, I can't see it, so E1 and E2 are parallel to it because each shares a common face with E0, but no other edges. Parallel also, these two edges here, E3 and E4, I would say, because they have a common vertex. Because they have a common vertex, I a common child, but not a common parent, not another common face. This is the same thing. We have two parallel edges in a real life network. I think it's the dolphins social network standard example. What is the meaning of this notion? So, perhaps this is not the best illustration possible, but if you had some edges, this But if you had some edges, this pink edge would go from x to y, this would be the real distance, the intrinsic one. Whereas the distance as measured by this edge, the red edge is just something in my drawing. So this would be an intrinsic notion and this would be an extrinsic notion. And since Ricci curvature is extrinsic, this would be a good thing to take. Pushing things back, you will get this image here. Some integration for this. This image here, some integer for this one, and taking this grouping this, you might get the triangles of which we'll talk more shortly. So then, so this is a classical thing, it works in all dimensions, so you need, but you need typically or classically, you need higher dimensional neighbors and lower dimensional neighbors. So you would need something which has two faces, one faces and zero. Which has two faces, one faces, and zero faces. But for networks, and we are looking with Jürgen Joss from Max Planck for different notions of curvature for networks, except that already famous than Olivier 1. And he said, well, we need something that doesn't take faces into consideration. I said, okay, let's go to the limit. And we discarded the higher dimensional faces. And then is it still possible? You don't have higher. This is still possible. You don't have high-dimensional neighbors, and you'll get a nicer formula, which still doesn't have much geometric intuition, doesn't render much of it, but it's easy to compute, that's sure. Adding, subtracting, dividing, multiplying, and taking square roots can be done by hand, theoretically. So, simply put, this is. This is so. If you want to compute the form and curvature of this edge, you have to look at the waves of all the edges incident to its vertices, the blue ones and the green ones. There are no faces to look at. Okay? So back to the Ricci curvature. Ricci curvature quantifies two aspects in differential geometry. Two aspects in differential geometry: it's the growth of volumes. Look into a direction, take a small cone, and look how much it grows when you go from that small one to a slightly bigger cone. And the dispersion of geodesic, which is illustrated in this dispersion of these red rays. Only V curvature, by its definition, captures the volume. The volume growth, whereas formal Ricci curvature captures the dispersion of geodesics. This still can be not very clear from the general formula, but then go to the combinatorial case, which is based on any such study or comparison, at least when you talk to people in network theory, then the formula becomes this one. Then the formula becomes this one, it's just four minus the sum of the degrees, and it's clear it's connected to dispersion of geodesic, i.e., which in this case are just edges. So this is meaning beyond or behind form and curvature, at least in the case of networks of I graphs, if you prefer. The thing is that you should. The thing is that you shouldn't be surprised that I say, Olivia does this form and does something else. Nothing is perfect. Nobody's perfect. In particular, no discretization is able to capture all the characteristic or all the phenomena behind a classical Riemannian notion. Okay, so of course, you can take the mean of the curvatures of the edges in a node and you define in the form of rich curvature of a node, which of course Of a node, which of course people would in the beginning were studying only nodes in networks, but this, of course, properly should be a border scalar curvature. Okay. Now, as I said, I forgot to say something. So this is an overview talk. And as an overview talk, it suffers some drawbacks. One, since I one hand, I didn't want to throw out anything, so it's a bit too much. On the other hand, of course, since you have a lot of things to tell about, nothing is too concrete. So now I'm trying to say that here I'm going a bit back on more older stuff we did. And one thing we tried to show, and it's important, it's not because when you want to convince people that your notion is good for something, and when I'm talking. Is good for something, and I'm talking about people I'm talking in applied fields, then you want it to make some sense. And in particular, I don't want it to coincide to something that our body knows. So this is, for instance, it's think about this combinatorial one. I'm talking about now about the combinatorial case. So, about combinatorial case, it's not the degree because that's dangerous. So, you have two examples. Dangerous. So you have two examples of two very small networks: three nodes of degree two, two nodes of degree one in both cases, and the curvatures are very different. You can check for yourselves if you want. So some real-life examples. This is a protein-to-protein interaction network. So that's how it looks. So you have the bigger the node or the thicker the edge, the more. The more higher absolute value of curvature. Sign is, please pay attention in general. I mean, most networks, most edges and most nodes, therefore, in the combinatorial case, have negative formal curvature, therefore, the red coloring. This is something similar done for by importance of roads in Europe, and then you. Roads in Europe, and then you can see Rome. It's not all roads back to Rome anymore, unfortunately. For some reason, Yarosa is more important. I don't know why. And these are two, some weighty networks this time, peer-to-peer network. And again, you see the same distribution. Some histograms will talk about more about those. There was some comparisons. Those. There was some comparison with degree because you define it in the combinatorial case by means of degree. So you would like to see there is a correlation. So, no, there is some negative correlation. And then, of course, the clustering coefficient. Why the clustering coefficient? Because it's the most classical of all these network measures, I would say. Apartheid degree, to be sure. And moreover, we know that this is a discretization of classical Gaussian. Of classical Gaussian curvature. And as you can see, there is practically no correlation. So we have something new. Okay, so then our encouragement can go forward. And there are more comparisons, as you can see. For instance, with betweenness, which is very popular, but it's, of course, a global measure, not a local one. So the comparison is a bit not that relevant, but still. And I won't talk about this too much. There is something comparable. Much there is something comparison seeing how it behaves in terms of robustness if you eliminate edges by curvature, no, by something else like Olivia or something else. And I want, as I said, it was fun to me to discard everything, and I would like to still show you a bit of an over real overview. Overview. But what's more important than this comparison, which are not of that much interest to everybody, is the fact that most of the real-life networks and many of interesting ones are oriented ones. So you can talk about the orientation of an edge for sure, and then you can define at any vertex the incoming Ricci curve, which. Ricci curvature, the outgoing Ricci curvature. Just look at the incoming, outgoing edges and the incoming edges, and you take only their mean. So then you can talk about the formal curvature of a node, or if you prefer the total flow from a node, just taking this difference. Of course, this is a matter of choice. We could have taken the other one. This is minus and this is a plus, but this is a choice we took. Is a choice we took at least Jürgen believe this much better to have. It's, of course, up to sign the same. And then you can represent such networks. So this is what is called a heat diagram. Every pixels stands for a node. Why? Sorry, for an edge. Because an edge connects a vertex I with a vertex. I with a vertex j, so we have the pixel ij. So, how do you know it's oriented? Well, it's not symmetric. So, this is a web graph of Google pages. And there are no, this is its representations on the network. And then, if I don't know if I can see it, I hope. Let me see if I can play with it. Okay, so you can see a bit better here. I hope. So, there are these big notes which are very heavy. So, there are these big nodes which are very heavy in terms of reaching curvature nodes. Important hubs in pages which attract a lot of attention. There is more than one connection from a page to another and it's directed because you can have a page pointing to another one, but not the other way around. Okay, so sorry, let me okay. So, this is great. Okay, now. Okay, now we have the same thing for the so heat maps can tell you a lot about the networks, but about reaching curvature and about measures of networks as well. So you can see here the heat maps of three real-life graphs: a social network, the web graph, and the biological network, heat graph of Richie curvature, which is a concrete measure. Which is a concrete measure that you saw before. And these are typical heat maps of classical random networks, the three classical models. And you can see they're very far from real life networks, at least in terms of curvature, which is after all a very this combinatorial one is a very simple one, very natural one to look at. So it would seem. It would seem that, at least as far as curvature is concerned, these models are not very good in describing networks. And why? So let's have a look at those networks again and look at the histogram of the curvatures. So you can see that the social network and the biological networks, which are real-life networks, not constructed by human beings, as social networks, it's between human beings. It's between human beings, but it's not first tracked by them. It's more or less spontaneous. It's a conglomerate of people, so to say. So they have this kind of distributions. A web graph, which is an engineering product, has this quasi-kind of Gaussian distribution, normal distribution, and Erdősen in what struggle. Erderson and Watt-Stroga's models have the same kind of distribution. So they resemble more a man-made one, whereas Alberti-Barabas is closer to the real ones. We saw that in the heat map itself. And you can, I don't think I'll go over this. You can really see how far each of the network is from another or a model, one from another model, and there are measures of doing that. Sorry. Doing that. Sorry. I should also say something about Olivier versus Foreman. We started saying that at that point in time when we started talking about this, Olivier was all the range, and still use, I should say, in all fairness. So as you saw, there is clear that formal curvature is much easier. Curvature is much easier to compute, even in the weighted case. You don't have to take all the paths, you don't have to do some linear programming tricks to just start computing the Basserstein distance and so on and so forth. It's way faster, I mean, by a few terms of magnitude. But perhaps they capture some different things. And the nice answer is that they are not capturing, they are really. They are not capturing. They are really more or less, at least in an experimental sense, they are practically the same. They are very well correlated, negatively correlated. I mean, absolutely correlated because, as we said, formal curvature tends to be negative. And so you can see this in these real nice networks and some other ones for edges and for nodes as well. And there is some very nice new. And there is some very nice new work of Florentine Munch and Jürgen Joss from Max Planck in Leipzig that indeed they are the same notion up to some renormalization, if you want something like that. So in practice, even if you prefer the flavor of only the curvature for this very nice bold transportation of things, and you don't dig Bochner-Weizenbach because it's, well, it's a bit too far. It's a bit too formal, too, not here in general. Then you can still use forman with a clean conscience as a proxy for Olivier. But that's okay if you want to do that. I'll try to show you shortly that it's paste to use format because it can go beyond this one-dimensional thing. Thing. Okay, so let me have a sip of water and look at some more examples. So, as you see, there are two real-life networks, one of the brain networks of neurons and the other of gene expressions, and two technological ones or man-made, more precisely. One of those word occurs, not Correspond in text, and the other one I forgot where this specific example and the Google web graph, which we saw before. And you can see there is a clear difference. I mean, that's something which immediately points out to the curvature, stresses or underlines the difference between these networks, which you couldn't really see in a matrix, for instance, in their representation. Is that namely that brain. Is that namely that brain real-life network has these humps, multiple humps, of which I'll talk shortly, whereas man-made networks do not display them. They have this kind of normal distribution kind of form. There is something really important about these humps and the fact they characterize real-life networks. So if you want to see if there is men-men or not, you should. There is men, men, or not, you should look at these distributions. Okay, so talking about brain networks, so you have these two types of so these are fMRI obtained networks which exemplify what happens when you I think the subject had to clap their hands so you have something which in red let's say which is the motor part of the brain Of the brain lining up, and then you have some purple or blue one, which means that some visual part of the brain cortex is active. Why? Because it turns out people are cheating. They have tend to look at their hands when they're clapping. So there is some things happening here. So you can see this in many perspectives. It's realized. This is something which we did with Jurgen Jost and Jürgen Jost and Gabriele Loman, and Melanie Weber, who is now at Harvard, I believe, and of course Arigit Samak from Chennai. I'm sorry, I forgot to mention their names earlier. And there are too many participants in this overall project to write each time all the names. And I do apologize, I don't want to take all the glory from. To take all the glory for myself, quite the contrary, okay. And here you can see the same networks, and you see the humps. So the main process, motor, motoric, if you want, clapping part of the brain has this big hump. And there is a secondary activity in this hump, which describes the visual aspect. And I should say, since then, we progress and this. Say, since then, in progress, and this is work mainly done by my co-author and friend Arijit Samar and his other collaborators. And it turns out that you can understand the OT spectrum disorder by looking at what is different in the connections in the brain using the same method. So, this is a beautiful world. Is it a beautiful work, if I may say so, because I'm only on the mathematical part of the study there, but I can't go into it. Okay. And there is another thing that you can do with this kind of networks, just looking at market crashes and predicting crashes in times of crisis. Yes, when you have something like, yes, just crisis. Crisis. Again, I won't go into that. I'm just trying to show that there are some real-life applications. This is also done with Rigi. And of course, when you talk about Ricci curvature, you can't not say Ricci flow. I mean, people started looking at the Ricci curvature again, apart from textbooks in Riemann and Geometry because of the great work of Dan. Great work of done, of course, but of Perelman. And there is some beautiful work done by David Gu and his collaborators based on the theory developed by Chao and Lu on the discrete curvature for meshes, two-dimensional and combinatorial curvature, and it has a lot of buckling coefficients. Has a lot of buckling circuits. So, of course, we would like to do something for the formal Ricci flow, which formally looks something like this. The difference in the new curvature minus the old one equals to this term. And again, I should say that, and that was a mistake of mine, this is a real flow, because real formal Ricci flow and not a scalar Ricci flow, a Mabe one, because it One, because it really evolves the edges, I mean, curvature of the edges, which corresponds to the classical Richie flow. Whereas a much better known and much more successful, I should say, combinatorial curvature flow looks at the curvature of nodes. And that's something very important. At that point in time, when it started, people were not really. They started, people were not really looking at Ricci curvature as such. They were studying still nodes, something attached to nodes, scalar curvatures. Whereas Ricci curvature, formal Ricci curvature, by its very definition, it's an edge measure. And it's very natural, in fact, to look at edges because a network is not just a collection of vertices. It's not sand. It's defined by their connections, i.e., by edges. Okay? So, Okay, so the intuition is behind this thing is very simple. You have a surface, let's say, which evolves. So it pushes these fingers in and rounds the surface. So we'll get a round sphere in the just before it dissipates. So it goes to the limit. Okay, so high curvature evolves faster than no curvature. So you can. Faster than no curvature. So you can use that in practice because you can take snapshots of your networks different times and force their evolution faster to make things more clear. So we did that first with the formatic curvature for images, for medical images more precisely. And I won't go into that. It's what was done in a friend Eli Appleboyman, why was the EVR? Boyman, why the evidence from the tefnium, and that is very natural. So, you have a CT image, this is the image, this is a curvature map, and using it, you can find the tumor here very easily and automatically. We just painted it to point it out. The same thing goes here for another CT image of the lungs. The image, the curvature map, and here you will have. Curvature map, and here you'll have. I don't know if, oops, sorry. Yes, you can see these red points, which are the nodules which were discovered. And I remember at that point in time, even the doctor behind it was surprised that we could detect them. I mean, he didn't see them before. Okay, sorry. So you can do the same thing for networks, and this is a small. For networks, and this is a small presentation done by Melanie, very nicely, I would say, putting presenting algorithms. And in practice, why is it good? What can you do with it? Well, you can look at two consecutive days in a peer-to-peer network, for instance, and you want to see what changed. Well, you can't see it. They don't look the same. I mean, it's just noise. But if you But if you make things evolve faster and pushing important nodes/slash edges faster, then you can detect the edges. So these are the edges which appear here. You discard the edges which don't flow fast enough, let's say, not important enough. Okay, so let's go back to the brain networks. So there is some. Networks. So there is some good news. They have very nice structures. You have these big hubs and you have these groups and you have some long-range connections. But that's because I painted them. It's harder to understand who's against whom. And even if you do some thresholding and you throw the lower curvature edges, it's still a big mess. In a sense, it's bigger because now you see more detail. Because now you see more detail and it can't understand too much of much sense out of it. You can do two things. One is take sub networks and then you can see what is called the backbone effect. You have this strongly negative edges that are the backbone that connecting the structure, basic structure of the network. But it still is too much, yes. And of course, you throw. Too much, yes, and it's of course you throw out a lot. The other way, it's you can see the same thing here, and how curvature plays a role and the important ones. But you can do something more. You can take a flow and then reverse the flow, that's enhancing the important edges even more. And then you will get you'll detect the backbone faster. And this is some work. The backbone faster, and this is some work which we did with Melanie a few years back. And this is a social network, the Mr. Rabble characters network, and you can see the backbone detection here. Okay, so again, this is a small example, but we have, so going using this in brain networks, you can see it the networks superimposed on the model of the brain, a model brain. A model brain to be more precise. And you see, each iteration throws away more and more of these curious edges. And in the end, you'll get something very small which looks like this. And these are the important edges or the important connections which you should concentrate on if you want to understand what happens. Okay, so how much time do I have? How much time do I have? Sorry. You have until the end of the hour. Okay. So I can breathe a bit. Okay. So I'll have another sip and because I'm rushing. So as I said, networks are not just nodes. They are mainly defined by their edges, by the connection between these nodes or pairs of edges. But But so this was a motivation of reducing the dimension or producing this definition which we had of forman curvature for networks because we discarded higher order phases, let's say two-dimensional faces for sure. Why? Because you wanted to concentrate on edges alone. But that's not quite true. A network is not just edges, it's not just a graph. not just a graph even though even this took some time things this model took some convincing to make social networks practitioners accept it it's that there you have a higher order correlation so if you had three people let's say together it's something else that each pair of them talks knows each other so three people talking to together People talking together, collaborating on a paper would be simpler. This is a filting triangle. Four people doing the same thing would be a tetrahedron, but four people working in pairs only would be a quadruple in plane. So, this is again, you can see the same thing. You can have these quadruples appear in this is the same networks you saw before in. It's a Dolphinsiest. And therefore, you understand that it's not very efficient to discard these high-order faces, but actually it's a good thing to keep them because they illustrate or they capture the behavior, higher order behavior or deeper behavior, if you prefer, of networks. Behavior if you prefer of networks. So, of course, you go to the model, you go back to CW complexes. So, then you go back to the hobbit, or as it is called, subtitle is there and back again, without name, for sure. The thing is that even though you take into account all the faces with their weights, which is the plus of Ferman curvature, you still in computation, you look only in the two scales. You look only in the two skeletons because, again, you want to compute the rich curvature of an edge, but edges are determined only by lower-dimensional faces, i.e. nodes, and higher-dimensional adjacent faces, that is one dimension higher, that is two faces. Therefore, you look only at the two skeletons. So, again, there is some statistics which is less important now. Important now. So, again, let's stop a bit and think deeper about form and curvature. I made some case for it. In particular, I said that it's easy to compute. It's easy to compute, it's comparable with Olivia, it captures edges, it's great. Moreover, it satisfies some classical stuff, so to say, of differential. Of differential geometry. I mean, it satisfies as expected for Riemann Ricci curvature on a Meyer-style theorem. It gives information about the homology groups and even of the fundamental group, let's say, for positive curvature. It caps it more importantly, I would say in practice, but it's not explored, that it's a big hiatus in our research, anyone's research, I would say. It comes coupled with a placent, and therefore you can use. And therefore, you can use Laplacian-based methods to study networks in many dimensions and filter them. But there is something which is not nice for a curvature. It doesn't satisfy Gauss-Bonnet-type theorem. And one way of getting out of this, overcoming this problem was proposed by Ethan Bloch, who looked at a triplet of A triplet of discrete curvature functions in dimensions 0, 1, and 2 only. So one, R0 is of vertices, R1 is of edges, and R2 operates on faces, on measure of faces. And it turns out that for polyhedral complexes, R1 is just formann's curvature. And which is very nice. I won't bring the definitions here because they are very technical. Here, because they are very technical, some counting functions. This is combinatorial, and then you get a very nice result which says that this alternating sum is nothing but, as you expect, by its form, it's nothing but the Euler characteristic. Therefore, you can have on your networks using Ricci curvature, you can define Fine an Euler characteristic, which is important. Now you have a shape, so to say, a model shape, and then you can compute, take long time flow, which should flow towards the space of constant curvature, which we call prototype network. So in the classical case, it would look like this. In our case, it would be the same with the Richard. Case it will be the same with the Ricci curvature, where the Ricci bar means just the mean of the Ricci curvature of all the edges adjacent, all the edges. And sorry, so my apologies. So then you have a definition of prototype networks. Formally, you can define a network to be spherical if chi is greater than zero, Euclidean if chi equals zero, and If chi equals zero and hyperbolic is chi is smaller than zero, and of course, you can compute chi, but that's a bit tricky, and you can it's easier to redefine them in terms of curvature precisely like in case of surfaces of the mean curvature. Then you'll have it will be positive if R1 is greater than zero, it will be Euclidean if R1 is zero. Euclidean if R1 is zero, and hyperbolic is one, R1 is smaller than zero. So let's see how this works in practice. So let's start with a small network, which is spherical by computation of R1, of Ricci curvature. And after three iterations, it becomes like this. After four iterations, like this, after five iterations like this, and you see. Like this, and you see it tends to disintegrate very fast because it's very small, but it's more important, it tends to become tree-like, therefore ideally hyperbolic. Another example, we start with this network, which has negative curvature. It's hard to see it from here, yes, of course. So, when it evolves, sorry, after five iterations already, it's almost spherical like. So, indeed. Spherical light. So indeed, the sphericity, formal sphericity tends to look like real sphericity. And we have something very similar here. It's a one 3F5 iteration. This original network is the wind surface classical social network. And as you can see, it evolves a more and more spherical shape. You can see it's spherical because the edges are more and more blue edges. And remember, Edges and remember, red edges are hyperbolic, mark hyperbolic negative edges, and so forth. So, this is done using the full forman, or if you prefer the forman taking into account faces as well. Okay, so in the time that I have, I'll show you some other directions. Um, so of course, not everything is just a network, so there are So there are you have things which I showed you before, which I modeled as polyhedral complexes, which are sometimes called multiplex networks, or they also model multiplex networks. And you have something which is called hypernex or even directed hypernetworks. So there is no just one model for what a hypernetwork is. There are people who maintain this just a hypergraph. On hypergraph, we had another model more general. We adopted with Melanie in a short paper. But in any case, for all these different conventions, there is a way of attaching a Forman curvature and studying their geometry. So one way of doing it, one example is this of chemical reactions. So you have two substances which go over one reaction and then they produce. Reaction and then they produce to other substances here. So you have some acid and a base, they produce a salt, and I don't know what, some gas. And then this one goes to another reaction and so on and so forth. So it's clear this each reaction is just a directed bipartite graph, yes? And it's they're very canonical models. So we have some examples. So, we have some examples in chemistry. These are very old results, and I include them because they are nice, and because I am an author of this small study, but this was developed way beyond what I'm showing here by Wilmar Realt, who finishes not long ago his PhD again at Leipzig. And they have far deeper results. You can. You can recently, my student Heim Cohen from the Hebrew University and his friends developed a series of studies, which I had the pleasure of supervising, so to say, the application of Ricci curvature and other notions of discrete curvature to the hypernetworks in the study of semantics or semantic networks. So, this is an example which is the image is not very clear. The image is not very clear. I'm sorry, there is large animals. The colors show that they're of the similar kind, let's say rodents or felines or canines, they have the same color, and they appear in the study of psychology. These guys are from psychologists, and they study these networks because they try to understand how people make connections in the brains. So, you start somebody to produce as many names of animals in a short span of time. So, if you start from dogs, then you can go to cat and from there to fish, which is kind of fishy, but you can go from cat to dog to wolf to jacket to hyena. So, the way they produce these networks tells a lot. So, they have this deep inside. Have this deep insights with practical clinical applications and implications, therefore, applications, which I won't get into them for sure. And hypernetworks, in particular, directed hypernetworks, are very natural in biology, more specifically in genetics. So, if you talk about two proteins to collaborate, together produce an amino acid, or two genes that together collaborate, Together, collaborate to produce some biological result, then you have these two proteins, it's an edge. But you can have three proteins, you can have 20 proteins or 20 genes working together to regulate, I don't know, some endocrinic process. And for instance, so this will generate higher order phases. So three such proteins collaborating together, you model like a triangle. Triangle. So we use this with Rome Sandu, and was then at Stony Brook University, and Kevin Murgas, who is a PhD student. We applied this for genetics, more precisely in the study of the epigenetic landscape, and to understand two things, how stem cells evolve to specific To specific cells of specific functions, so from generic to specific, and the other way around, in terms of entropy, the way looking at how normal cells evolve to cancerous one. So this was done previously by Rome and Alan Tunenbaum in terms of entropy, but we are the first to do it in terms of curvature. And it was our pleasant surprise that, even in this incipient study, Study a curvature. So, this is Forman, one dimensional, is Foreman two-dimensional, and this is the differential gene expression, which is classical things in biology. So, each of these curvatures detects far more statistically significant genes which intervene, let's say, in cancer, then in terms of Then, in terms of operation, then the classical biological criteria. So, another thing would be that the two-dimensional one, the full, what you call full or true if you prefer, formal curvature is even stronger detection. You can see here, uh, the purely purple one is much stronger than the purely blue one. Stronger than the purely blue one. So I'm almost almost at the end of my talk, except that there is something which I promised in the abstract and I won't be able to talk about it. I mean, that would be, it's another ball game, and there the discussion should have, I mentioned it briefly, but there the discussion should be much more mathematical. Mathematical in the technical sense. So you can use it for other things. We did it not long ago with my student Rad Barkanas from Browde. He just finishes undergraduate studies. And with Yubrianoša, you can use it for sampling. So you can sample by curvature. Why? By curvature? Well, because this is classical in Riemannian geometry. There are many. There are many names attached to it, usually for two-dimensional objects, but the most general I would say is the work of Glovan Peterson and shows how to construct these networks approximating and reconstructive auto-omotopy remaining manifolds just by so the sampling is done by Ricci curvature. So it's natural to apply formal Ricci curvature to do the same for. Curvature to do the same for networks, and indeed, this we did. This is full forman and edges, the full forman sampling. And now I'm showing a picture of a classical test image. And why? Because you can look at an image as it's just the grid of pixels. So you look at the dual grid, so you have to. So, you like as a dual grid, so you have just a grid, square grid. I mean, i.e., a network or a graph, a very regular graph. So, you can use it as an edge detector. You can see this is a map of the Forman curvature, one-dimensional graph. This is a full formal curvature, and you can see the difference. This is mostly negative, this has positive and negative values, and the full formal curvature in this is a captures better, it doesn't produce. Captures better, it doesn't produce this noise in the background, and it's a much better edge detector. So you can use networks to go back to the images as well. We use it also with Vlad and Jürgen to produce these kernels, which is a problem of some importance, I would say, in imaging. And this is a particular case. So you can produce different kernels for embedding this. Can produce different kernels for embedding this as a CL gas network to embed and to compare them. It shows, turns out that the curvature has a better separation, for instance, you can use it for that, but at a higher cost. Okay. And I think I'll finish with the, I'm not sure, but the last, presumably the last example is its application into rewiring. Into rewiring networks, graph neural networks in learning to avoid information over squashing. And it turns out that this simple one-dimensional forman curvature and other simple curvatures are much more efficient than more classical and more famous methods, and a huge advantage as far as computation runtime is concerned. And this is work done with Antemono. Work done with Atemono and her student and her colleague from Imperial College. Jakob and Kevin, respectively. Sorry, my bad. Okay, so I'll wrap things up. What's good about formal curvature has good mathematical heritage. I would add that it's simple, it's an edge measure, and you can. It's an edge manager, and you can use it for high-dimensional order correlation as well. It captures, it's not just combinatorial, it works with any weights. It works for directed and undirected hypernetworks. You can apply flow on it. It comes coupled with a Laplacian. It allows for the classification of networks and via the homology of connections, it captures the topology of hypernet. Connections, it captures the topology of hyper networks. And I should say there is one more thing which I said I would do, and of course, I can do it, and it's better for everyone if I want. You can apply it to do persistent homology, just looking at curvature as a height function, but that would necessitate a deeper discussion. I mean, it's too much. In any case, for It's too much. In any case, for all of us, I'm sure. So, thank you for your attention. Questions? One hint: why this picture? And the answer is that in riccio in Italian means hedgehog, and in plural is riccio, ricci. More than one hierarchy. Due rici. Okay, that's all. Thank you so much. Thanks. I can breathe. I mean, sorry. Questions beyond hedgehogs? I know I did too much. It's my always tend to, I'm afraid it's not enough. I'm afraid it's not enough, and I'm doing in the end getting things wrong in the other direction. I do have a question about one of the applications that you showed. It is this application which I think you were using WichiFlows to detect tumors in like medical images. And I was just wondering, like, how does that work? Like, if you could tell me a bit more about why. A bit more about why does the Ricci flow give you information and allows you to detect those shapes? Okay, so what happens is that it's not just a Ricci flow per se. So curvature is natural to use because curvature is an edge detector. So we first use Richie curvature as such. And it's a good edge detector. Now, when you have something that is very noisy or where the regions of interest are not Regions of interest are not, you know, these contours, but rather localized. They look like noise. So you would like to force them to become more clear. And using Ricci or reverse Ricci flow produces that result. Things that are slightly more, slightly edgier or more curved become more and more pronounced. More and more pronounced. I see. Okay. If I'm not mistaken, we will meet shortly. Yes, we will. So I'll show you much more. I have I threw all that part out, yes, because the title was about networks. Yes. Networks are more general structures, I would say, but images are more easy to comprehend. See? Yeah, at least we can see that. Yeah, at least we can see that. And I say more that best results are obtained by coupling some form of rich temperature with some Laplace and so on and so forth. I mean, it's and we use that in different applications, but it's quite powerful. Okay, well, so I can show you more, okay? Okay, thank you very much. Okay. Okay, thank you very much. Okay, well, that's the R. So let's thank Emil again. And we'll be back. Thank you so much for your patience. Thank you. Thanks. 