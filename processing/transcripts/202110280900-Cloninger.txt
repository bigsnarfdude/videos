Morning section on Thursday of this workshop. It's our great pleasure to have Professor Alex Krollinger to give a first talk this morning. So Alex is an associate professor in mathematics and the data science institute at the UC SD. So Alex is an expert in the area of the geometric data analysis and applied harmonic analysis. He did a Analysis. He did many very interesting work in terms of the machine and deep learning. So today he's going to give a talk on learning with optimal transport. Alex, please. Awesome. Thank you. Thank you guys so much for the invite. And as you can see on my slide, it makes me very sad that this is not actually in Oaxaca, but hopefully we're going to get towards the non-virtual stuff soon. Yeah, so I'm going to talk about some learning problems related to optimal transport. Some learning problems related to optimal transports, and my slides are not. There we go. And I want to mention just my collaborators, and in particular, I want to mention that most of this work is driven by a postdoc that I work with named Caroline Musmuller, who is on the job market this year for assistant professor positions. So just as like a, you know, keep an eye out if you have any open positions because she's a fantastic mathematician and a really great applied mathematician. And so just kind of keep that in mind. So, just kind of keep that in mind. So, list a couple of grad students that have worked in this in various capacities, some funding stuff. Yeah. So, cool. What's the problem that we're kind of interested in? All right. So, we're going to be thinking about learning in a little bit different capacity than you're used to, right? Usually you think of having a point in a vector space, and then you can define some type of Euclidean distance or something like this. I want to try a slightly different problem. I want to think about learning on the space of distribution. Think about learning on the space of distributions. So, what we're going to say is: given a bunch of different distributions, I want to be able to say things about how they're laid out in relation to each other as a first question. So, for example, if I'm given in distributions, let's say they're all supported on RD or something like this, I want to find all pairwise distances and then do some clustering, right? Well, what is a distance between distributions, right? So, there's statistical distances and different things we could think about, but you. Things we could think about. But you can sort of tie this into some unsupervised learning problems because you could do things like, you know, clustering. And one example would be treating an image as a distribution, but another example would be something like flow cytometry, which is this like every person is represented by like a point cloud, you know, of blood samples. And so, you know, then you can do things like define pairwise distances. And in particular, one of the goals is going to be to figure out how to do this quickly because statistical distances generally are very, very slow. And then the other question. And then the other question that I want to mention, which is really going to be the brunt of the talk, is supervised learning. So, this is sort of an interesting question: which is: all right, if I have distributions and I have labels on those distributions, how can I figure out how to build a classifier? It's a little bit of a weird question because you're used to thinking about building classifiers on at most a Hilbert space. But in this case, it's not a Hilbert space, it's a family of distributions, right? So, how does one even build a classifier on this? And this can help for, again, Classifier on this. And this can help for, again, if you want to treat images as like densities on the unit square, this could help for something like images. But more importantly, you can think about incorporating different invariances into some, you know, I'm putting images here as an example just because it's the most visual, but this could just be a general distribution, point cloud, anything like this for each data point. Okay. So the main ways that people would generally think about approaching statistical distances. Distances. There's a couple of them. So, one would be something like a maximum mean discrepancy, which I've done some work on, and maybe depending on time, I'll be able to talk a little bit about at the end. But another one is optimal transport. And I'll go into optimal transport more description in just a second if you're not familiar with it. But I'm just going to point out really briefly the problems. So statistical distances are really expensive to compute. So if you have n distributions, you need to do n choose two distances. That's brutal. And choose two distances, that's brutal. That's not going to work well. Okay. And also, the complexity is sort of independent of what the family of distributions are that you're working with. And then also, there's no really feature space for doing off-the-shelf supervised learning. Okay, so that's sort of the problem. And what we're going to do is we're going to address these questions by thinking about the space of optimal transport or thinking about optimal transport and basically embedding the whole problem into an L2 space where then we can do things. Okay? Okay, so we're going to be talking about different families of group actions that you can apply to a distribution and what happens when you apply those group actions in this L2 space and kind of go from there. Okay. And we'll use this for talking about some unsupervised and super quick appointment problems. So there's the overview. That's basically what we're going to be trying to do. So let's talk about optimal transport. So, if you haven't seen it before, you know, it's also called like an earth mover's distance or something like this. Basically, the idea is that you're going to capture the amount of work it would take to move each piece of sand that's in the mountain of the distribution mu into the valley that is the distribution mu. Now, nu, it's not actually a negative distribution. It's just, you know, you're doing one minus the other, so that's why it flips. But the point is that you're actually But the point is that you're actually capturing not just whether or not the shape has changed, but also how far you had to move each point. Okay. And that winds up being a pretty, it's kind of like work, if you think about it from the perspective of physics. And there's, you know, works, this is fantastic for shapes and for distributions and things like this. We're going to be specifically looking at a Vosstein 2 distance. So the Vosteen 2 distance. 2 distance. So the Washington 2 distance is basically if you're trying to define a distance between μ and ν, your two distributions. It's going to be an optimization problem, which is the minimum over all possible transportations that take mu to nu. And when I say all possible transportations, I mean technically they're measure-preserving maps between mu and new. So they moved all the sand from mu into new. And what we're going to do is measure how much cost. How much cost it costs to move every single point. So that's the t of x minus x, and then integrate over our distribution view. Okay. So yeah, so this is sort of the problem, right? Because there's all these different costs that we could have done. And we maybe want one that's going to be, we want the optimal. That's where the, that's where the optimal and optimal transfer comes from. And I should say that there are different ways to deal with this problem. Different ways to deal with this problem. There's this idea of what's called the Kentucky relaxation, which is basically it allows us to not just move one piece of sand to one piece of sand, but might allow us to move one piece of sand to multiple pieces of sand, right? It's more of a coupling than it is a function. We're going to be specifically focusing on the function stuff for now. I'll, towards the end of the talk, talk about what happens when you only have a coupling. So, this is kind of a basic overview of optimal transport. Optimal transport, and I should just mention that it's got a lot of benefits. So, one of the benefits is that it sort of scales correctly. And what I mean by that is, even if you just like, if you took two Gaussians and you were just defining something like, you know, L2 distance between them, like a total variation distance or something, once the Gaussians are pretty far apart, the distance is basically maximized and saturates. And as they move further apart, they basically stay the same distance. And that's just They basically stay the same distance, and that's just because you know you're gonna have all the vertical stuff here and all the vertical stuff here. And when you take a two-norm, it's it's large, right? But in optimal transport, it scales. So as you move away, optimal transport scales with the shift that you've applied. Okay. Similarly, as you rescale a distribution, it sort of rescales naturally. It sort of has a natural distance that parallels the group action you've applied to the original distribution. Applied to the original distribution. Okay. Also, we're talking about distributions supported on RD, but we could easily talk about distributions supported on manifolds or other things like this. But it really incorporates the underlying geometry of the space that your data lives on. And it incorporates it by basically this distance right here. I have Euclidean distance, so we're assuming we're on Euclidean space, but this could be an arbitrary distance, okay, an arbitrary cost. An arbitrary cost. Okay, so that's the benefit. The downside is it's slow, slow, slow, right? You have to basically solve a linear programming problem and it becomes an issue to solve once, let alone if you need to do it and choose two times. Okay. So there's ways to speed this up if you want, which we'll talk about a little bit towards the end. There's this thing called regular syncorn regularization. Regular synchorn regularization that creates some speed ups, and we'll talk about what we can do there. But still, you need a lot of pairwise distances if you wanted to talk about optimal mass core, optimal transportation between a bunch of different distributions. And more importantly, there's nothing about learning here, right? Like these are distances, not features. Okay, so that's the problem. So, what we want to do. So, what we want to do is take this idea and figure out features that basically the geometry between the features matches optimal transport distances. That's the idea. So that's kind of the basic setup. Oh, by the way, if anybody has any questions, feel free to just unmute and ask. Happy to field anything. Field anything. Okay, cool. So you're going to talk about what's called linearized optimal transportation. And this is basically the idea. Okay. So what we're going to do is we're going to define a feature space. And this feature space is kind of interesting. Basically, we're going to fix what we're going to call a reference distribution, sigma. Okay. And in this paper. Okay, and in this picture, it'd be like the green distribution. Okay, uh, for the moment, sigma can be anything, and we're going to define the optimal transport from sigma to mu, where mu is one of our data, right? One of our distributions that is in our data set. So this transportation, well, first off, you can think of this. Well, first off, you can think of this as kind of being like: if you have n distributions, this is like spokes on a wheel, right? You're like mapping out to each one of the distributions, okay? But that doesn't tell us anything about pairwise distances. It doesn't, you know, why, why, what it, what feature space is it giving us, right? Well, the feature space we're going to define is the transport map itself. Okay. So, what that means is I'm going to claim that I think you guys can see. I think you guys can see my mouse. So I'm going to claim that the transport, the optimal transport map that takes me from the green distribution to this red distribution, you know, or density, if you want to think about it that way. That is a feature. It's telling me about the red distribution. You know, if it had to shape in certain ways. Had to shape in certain ways, if it had to, you know, move really far, that map, that function is informing me about mu. Okay. So the thing is that we can actually take that as features of mu. Okay. And then what we're going to do is see, well, what do those features actually give us? Okay. So this is the idea. So, this is the idea, and it allows us to define what we're going to call an LOT embedding, linearized optimal transport embedding. And that is that we're taking a distribution and mapping it to the L2 space, which is the set of functions. And it's specifically that function is the optimal transport between sigma and mu. Okay. All right. Well, we're there. That's great. For one, that means that we can define distances now. means that we could define distances now. So what we could do is take all right we have the optimal transport from sigma to mu and we have the optimal transport from sigma to nu and we can define a Euclidean distance between those transports. Well a weighted Euclidean distance weighted by sigma right um so it would be great that's what we're calling this w2 LOT right it would be great if this W2LOT told us something because now think about it from a computational Now, think about it from a computational perspective. Originally, I would have had to do n choose two distances that are really, really expensive to compute. Now, if LOT distance matched to Wasserstein distance, I only need to compute n optimal transports and then n choose two really cheap Euclidean distances. Okay, so that's going to save us a lot of time computational. And then the most important part for the learning is that what we can now do is define a function that takes in. So it could take in a distribution and output a class, but in reality, what it's going to be is a function, a different function that takes in the transport function and outputs a class. So now we might be able to build something that's just a classifier sort of off the shelf, feed in. Sort of off the shelf, feed in something from L2, spit out a value. Okay. But this leads to all these questions. So it leads to questions about what family of distributions do we do this for? When does the W2LT match the Vosteen distance? What do we actually need to learn anything about mu, about this classifier? Okay. So I want to just give like a brief description of what's going on with like literature stuff of this. So the original sort of pose. So, the original sort of posing of this LOT thing was actually done by Gustavo Rodi in several papers, sort of from an applied perspective, and talking about this just sort of as an algorithm for approximating Voston distance. But there wasn't really any theory to this until in 2018, he wrote a paper about how this works in one dimension. It winds up in one dimension. It's a great paper, but in one dimension, this is. It's a great paper, but in one dimension, this is actually a very the problem is much different. Uh, it has to do with the fact that if there exists one measure-preserving map, it's the measure-preserving map. There's like there, you know, so optimal transport is kind of easy because you say there's not, you don't have to solve the minimization scene. There's only one map. Once you find it, you find it. It's actually the CDF of the distribution. But anyways, so, or a generalized CDF. All right. Or a generalized CDF. All right. So, anyway, so that was some stuff. And then Caroline and I have worked on this. And I just wanted to mention that Akra Maldrubi and Gustavo Rede actually put out a similar paper pretty much the same day. I think it was within a couple of days of each other on archive doing some of what I'm doing here. It's a different theory. We have more general statements that can be made. But I also don't want to be the person that just like completely, you know, says, like, oh, we did this. No one else has ever thought of it before. So I want to be clear that, like, you know. So I want to be clear that this is sort of an interesting field. It's not just something that Caroline and I developed in the Alex. Yeah. You mentioned that you're focusing here on the case where you have an actual mapping. To what extent does this remain practicable? I suppose you will comment on that later, but to what extent does this remain practicable when you have couplings? Yeah, so for now, it winds up that we're going to focus on a set. For now, it winds up that we're going to focus on a set of problems in which you're guaranteed to have a mapping. But later, I'll have a couple of slides where we can talk about it. Where I'm actually going to change the problem just a little bit, and rather than thinking about it from the perspective of continuous stuff, I'm just going to move straight to Syncorn, which is discrete. And then I can have any coupling I want, and I'm totally fine. So that's sort of going to be the way that I'll deal with couplings. But for now, there are a lot of problems in which you will have this actual map. All right. So there's the idea. All right, so there's the idea. So let's talk about it. So I want to think about some group actions. Okay. And these are going to be group actions that I'm going to say sort of interact nicely with optimal transport. Okay. The two that really come to mind in arbitrary dimensions are shifts and scalings. Okay. So, and some affinity, you know, some combination of shifts and scalings. Okay. So the reason. Okay, so the reason that these are nice is that basically, well, as I said, optimal transport, there's a very nice relationship where as you shift, you know, the distance increases according to the size of the shift. That's great. But also, really what happens is the reason that they're so, the reason these operations are so nice in some sense is because of this quantity here, which we're going to eventually call a compatibility condition. Compatibility condition. And basically, what it is: is if I wanted the optimal transport from a distribution to the push of the distribution, that optimal transport is the shift operator. Okay. So I'll define that a little bit more clearly in a second. But basically, these are going to be families of group actions that interact nicely with optimal transport, with LOT, with LOT. Okay. Okay. So now basically the problem becomes: okay, well, what do we have? Well, we have, you can think of this as sort of like a, you know, commutivity diagram type thing. You have a distribution sigma, you have the optimal transport to mu, and then you have the group action to S push mu. Okay, so that's like shifting the distribution, scaling the distribution, perturbing the distribution in some way, right? And then in And then, independently, you could have taken the triangle, the hypotenuse of the triangle, which is to go straight from sigma to the optimal transport to s push mu. And the question is, when are these two things going to be about the same or exactly the same? Okay, when are they going to be the same function? That's what we need to answer. Because if we can answer that, life becomes a lot easier. And I want to mention real quick. Mention real quick that maybe this feels unnecessary because, you know, okay, like I can go from sigma to mu and to s push mu, and I could go from sigma to s push mu directly. Well, both of them are functions that take me from sigma to s push mu, right? But the problem is that if I don't have this necessity of optimal, there's a whole bunch of functions that could have done that. That could have done that. And they're all over the LOT space. So basically, if I don't pick the optimal one or some other highly regularized one, then I don't even have a unique, it's not even unique, right? So I need a unique embedding, right? So I need to pick the optimal transport, optimal in some sense. Okay. So this is really the game. It's basically when does S in some sense, when is it compatible with the transformation? With the transformation with the transport from sigma to mu. Okay. Right. So this is just kind of putting it here as a more sort of mathematical statement. This is our compatibility condition. And you can really just do it basically from some distribution to the push of the distribution. But in general, this will wind up applying when you have some distribution, you know, sigma or tau at the bottom and some. You know, sigma or tau at the bottom, and some other distribution mu at the top, it'll still hold. Okay, so that's the idea. And then there's also a notion of what I'm going to call an almost compatible condition, almost compatibility. So almost compatibility means, oh, well, you know, whatever group action I had on my distribution, it's not exactly the optimal one to just have done it directly to the, you know, the optimal transport and just doing the group action are not. Transport and just doing the group action are not necessarily exactly the same, but they're not too far off. So, the best example of this is a perturbation. And what I mean by that is maybe my group action is that I take a distribution and I shift it somewhere else and I scale it somehow and then I deform it a little. You know, a little bit of warping. A little bit of warping, a little bit of, you know, perturbing a boundary or something. That's going to be an example of something that we're going to call almost compatible, right? It's not, if you had just done it directly to the distribution, it's not exactly the same as the optimal transport, but those two things are close to each other. Okay. So I know I'm setting up a lot of sort of machinery here, so let's actually get to the point, right? So, what can be said? So, what can be said about this? Well, the first thing that can be said, which is pretty fun, is: all right, let's start with some basic assumptions. And these assumptions are more proof necessity than they are fundamental. I'll show you why in a second. But so say you had some distributions that are absolutely continuous and satisfied these regularity conditions, like let's say they have convex supports, right? So, sigma and tau or something. And we're gonna say, all right, we're gonna have push forwards. All right, we're going to have push-forwards that are epsilon perturbations of these elementary compatible transformations, shifts, and scalings. Okay, uh, if you have this, then this LOT distance compared to the optimal transport distance between the pushes of tau satisfies a Halder one-half regularity in epsilon. Okay, so. Okay, so this is a it's easy to state, but I want to think, let's think about this for a second. Like what this says. All right, first off, what happens if epsilon is zero? So what if we're only considering shifts and scalings? Well, in that case, LOT distance and Vosserstein distance are isometric. Okay, which is pretty cool because that means that if you're dealing with things that are mostly about shifts and scalings, then you actually don't even need. You actually don't even need to do the n choose two distances. You could have just done the n distances and then n choose two Euclidean distances, and you would have gotten the exact same answers because they're isometric. Okay, so now this says that if we perturb, it's almost isometric. So, I mean, that's pretty great, right? I should say this borrows from some results by Geely from a while ago about optimal transport that has to be with some push forward. Transport that has to be with some push forwards. But what we're observing here is a little bit different because this didn't incorporate this notion of the fact that basically shifts and scalings are almost free. Okay. You know, you can do something isometrically with those. So this is kind of the first idea, which is LOT distance actually is a pretty good job of approximating Vausterstein distance in simple seconds. Okay. And I should say that we can. And I should say that we can make that a lot more general. Strike everything, just let them be distributions. And it's still going to be holder 215ths regular. So that's a small regularity. And again, I'm not going to claim that this is optimal, but this is again borrowing from some other results that exist of people looking at some of these holder regularity conditions. Okay. So this is. This is the idea. All right. This, this, this, you know, isometry or almost isometry exists for simple transformations. And so LOT distance isn't nothing. You know, it gives you something. The much more interesting one here is when we come to supervisor. Alex, yeah. Can I ask you some more questions? What's the constant C over there, sigma t? Yeah, we can write it down explicitly. It has to do, it's got some properties involving the. It's got some properties involving the two distributions, what we're going to call the reference distribution, and I can call it like the base distribution, basically, the thing you're shifting around. It's not bad. It has to do a little bit with the regularity of the problem. But they're not awful. I see. But do you have a general statement, like not G sharp tau, H sharp tau, like any arbitrary mu and nu and compare the L T W2 with the original W2? The orange and with W2? No. There are ways that we could, and it'll actually, the next one will have to do with having LOT between some mu and some new. I see. This is for supervised work. So it's actually, well, all right. First off, the one thing I can say for sure is that that difference is greater than zero. So it's definitely always going to be bounded below by the loss of speed distance. The upper bound becomes trickier when the base distributions are fundamentally different than each. Distributions are fundamentally different than each other. But in that case, we can still say things in the supervised setting. And that's what we'll say in just a second. I see. Interesting. Thanks. Yeah. So in the supervised setting, so what is the supervised problem? The supervised problem is, okay, now I'm going to say I have a base distribution mu and a bunch of group actions I can do to it. And that's sum of my data. Okay, so it's mu, it's a shift of mu, it's a scale of mu. It's a shift of mu, it's a scale of mu, it's a perturbation of mu. They're all over the place, all over the place. And then I have some new that also shifts, scalings, perturbations, all these transformations. Okay. So if the problem is that I want to be able to label which things came from mu and which things came from new, that is the supervised learning problem. Okay. And just by the way, I haven't really given any great examples about when this would show up, but I mean, as an example, you could think of having, you know, collecting a blood draw or something like this. And there are a lot of things I want to be invariant to, but I want to just generally understand the shape of that distribution, right? So I want to be invariant to how it scaled and how it shifted around my space and stuff like this. I'm trying to say something about the shape itself. And then maybe, you know. And then, maybe, you know, that's for somebody that's healthy. And then for someone that's unhealthy, it's a different shape. Okay. And I want to be able to classify between those shapes independent of all these group actions that I can apply. All right. So what can we say? Well, we're going to say, all right, let's say we have something that's absolutely continuous sigma and we have some LOT transformation that is delta compatible, so basically almost compatible, right? If that's Right, if that's the case, and all right, so uh, the having these two things be compact is really easy. It basically just means that the sample, all the group actions you're sampling are coming from a compact space, um, a convex space. There are things that can be said about, you know, it's a, it's an assumption on the types of shifts and scalings you're applying. So as long as the actual Vosserstein distance between any transformation of mu and any of the transformation of the transformation Between any transformation of mu and any transformation of mu is greater than delta. Oh, I just realized I had one, I have one mistake. Sorry, call these or no, no, no, actually, this is right. This is right. Yeah, yeah, yeah. As long as it's always greater than delta, then what we have is that when we do our LOT embedding of all of our muse, when we do our LOT embedding of all of our news, they are going to be. They are going to be linearly separable. So, think about that for a second. I have this family of distributions, this other family of distributions, as long as they don't literally overlap, I can embed them into a space in which there is a linear classifier. Okay. That's pretty powerful, right? And by the way, in the case of talking about perturbations and shifts and scalings and all of that, we can. And shifts and scalings and all of that, we can exactly classify what these deltas are that we need: how much perturbation you're allowed, how far apart they have to be. Okay. So what this is getting us is that now we can actually do some really cool learning, basically. I'll show some examples in just a minute. I just want to give a really brief sort of idea of how this works. Of how this works. So, as I said, in one dimension, things are pretty nice and they can be shown for a lot of different transformations and then shown and ready. But the way that this works in general is basically this sub-result, which says that if I'm sampling my shifts and scalings from a convex set, which is pretty easy to do. I mean, that just says that, you know, you're sampling from there. Like, so if that's the case, and it was almost, and those things were almost compatible with our transport. And those things were almost compatible with our transport. Well, then, what winds up happening is because of our compatibility condition, the set in LOT space is going to be almost convex. So convex sets go to almost convex sets, which once you have two convex sets, von Bonak says that you can separate them. Okay. So that is the basic idea. And then there's just a lot of machinery, right? But that's the overhead. Right, but that's that's the overarching idea: is that when you have compatibility, you're able to go from um distributions to, or you're able to map a convex set of group actions to a convex set of points in LLT space. Okay. Um, in general, in general, do you have any assumption about H in your statement, the theorem? So, this compatibility condition is the strong one. That's the, but I. Is the strong one. That's the, but edge can be any complex. Yeah, yeah, yeah, yeah, yeah. Again, think of it as being grabbing, like in this case, because we're talking about things that are almost compatible. If we say they are compatible, think of it as being that you have the family of all possible shifts and scalings, and then you're grabbing eight, and then you're grabbing ones from a convex set. Okay, so really, that's not that bad. You don't even have to randomly sample from that. You can, you just, the whole idea is that this condition. That this condition of the minimal separation has to apply on the convex set, not just on the points that you sample. So basically, it has to apply on the convex whole of the shifts and scalings that you had. Okay. That's the idea. All right. So I want to mention that there are some other things that we can do. So, another thing that can be done, another family of compatible transformations are actually going to be shearings. And in shearings, you can get some really cool stuff. So, by the way, this is some ongoing work. This will be out probably within a week that basically says, all right, under some assumptions about the map that took us from sigma to the base distribution mu. Base distribution μ, namely looking at the eigen decomposition of the Jacobian. That then induces a whole family of compatible transformations. So we still have shifts. And now what we actually are going to have is that if you rotate into the eigenbasis of this Jacobian, you can then scale in the various principal directions as much as you want and rotate back. You want and rotate back, and that's still going to be compatible. Okay, uh, if we're thinking about this for Gaussians, effectively, what this says is: if I have something anisotropic Gaussian and I want to perturb it in some sense, I can stretch it along one of its axes and shrink it along another one of its axes and it's not big. Okay, so I can try and kind of mess with the width of the problem of the eccentricity in some sense. Okay, but this is again something. Again, some compatible condition. All right. So, this is work with Caroline and also with a couple of the grad students. The previous results I talked about was work just with Caroline. This is work with Caroline and also a couple of the grad students that I mentioned. And motivated, but there's some other things that we can say that I'm not going to go into too much detail about. It basically have to do with the fact that I've been talking about having one fixed reference distribution this whole time, but you could actually have a bunch. Whole time, but you could actually have a bunch of reference distributions and define LOT, which embeds into basically the product space of the LOT to each one of those reference distributions. And then you can prove some cool stuff still about separability and all these other things. And you know what? I just realized I don't like the, I'm going to show the examples real quick, and I'm actually going to come back to this single thing. I dislike how I. I dislike how I laid it out. All right, so let's actually talk about some examples. So, the first example, and again, these are just ones where I just wanted we wanted to compare it on a sort of base problem that everyone loves to think about. So, one thing you can do is you can basically treat images like MNIST or something like that as being a density on R2, right? And you normalize it to the sum to one, right? So, if that's the case, you can think of each one of these, each one of these. Each one of these images is kind of being a density on R2, right? Well, that means LOT applies. Okay, so what we're going to do is we're going to actually make the problem harder. All right. So right now I'm doing this for binary classifiers, and the reason is because I only said stuff about linear separability between two classes. So I'm just picking two classes to do this. But what we can do is we're going to make the problem harder by applying shifts and scalings. Shifts and scalings of MNIST, okay, and perturbations. And the perturbations in this case are slight rotations. Um, so what we're getting is that it's not, no, the problem is no longer this nice centered problem. You now have like a one up here and a two over here and like a small two and a massive two and like all these different things. And we want to identify what are ones and what are twos, right? So, what we do is we sample some points, embed them in. Embed them into LOT, learn a linear classifier between them, and then check the testing error. Okay. And this is what we have here. And I want to emphasize something about this. Linear classifiers are really easy to learn. They're very robust, which means that the place you're really going to see the benefit of this. Going to see the benefit of this is in the small data regime. I don't need that many labels to accurately determine my hyperplane. Okay, so because of that, what we're doing is plotting the training error as a function of the number of training point, or sorry, the testing error as a function of the number of training points we've done, we've collected. And this is sampled over a bunch of Monte Carlo simulations. So look at LOT, right? By the time we have 60. Look at LOT, right? By the time we have 60 examples, we're down to an incredibly low classification error. And in fact, it's lower than a lot of other things, such as if I had done PCA down to a low dimension and then try to build a linear classifier. That doesn't work very well. Well, all right, as with all things, one has to compare to neural networks because that's obviously a question. So let's compare to a neural network. We can build small convolutional neural networks. We can build highly Convolutional neural networks, we can build highly over-parameterized convolutional neural networks and try to see what the testing error looks like here. And even then, we drastically beat the convolutional neural networks. And the reason is we don't need many labels to accurately determine our parameters. Okay, because it's a linear classifier. So the other thing we can do is like an LDA embedding. So an LDA embedding is basically projected. An LDA embedding is basically project out onto a subspace that keeps the two classes separated. Okay, so by the time we've collected 40 points per image, you see we have a pretty clear separation of our data. And by the time we have 100 images per digit, you see that you have an almost perfect separator. So that means that in this high-dimensional space, there was that really, really nice separator. And this is exactly what they look like. Okay, they really were sort of embedding in the convex sets, and they really are linearly separable. Sets and they really are linearly separable. So, I want to show that this can work for other examples too, which is kind of fun. So, there's another way. So, I'm not going to go into the details of this, but basically there's a million different ways that we could have tried to figure out the solver, right? The optimal transport, like how do you actually solve for it? So, one is to do what's called a fully discrete problem, which is you take discrete samples of sigma and you take discrete samples of mu and you do the optimal transport. There's another version. Do the optimal transport. There's another version called a semi-discrete operator. So basically, you treat the reference distribution as being continuous, but the thing you're mapping to is discrete. So that's this blue curve, the semi-discrete operator. It winds up giving a lot better performance. But what we can do is we can actually think about, you know, again, trying to build some type of classifier, you know, between sevens and nines. But now, rather than just doing shifts and scalings, we're also going to do shearings. So these are examples of the sevens and nines. We're starting to apply shearing. And nines. We're starting to apply shearing in the directions of the digits, right? And you actually can still have a fantastic classification on a small label regime, even to the point that you have horrible shearing, horrible shearing. This still works. It still works pretty well. So, this, I mean, let me tell you, in a different research life, let's say I Different research life, let's say I work a lot on neural network stuff. I can tell you a neural network is going to fail at this miserably. I don't have the example here, but like these, you're drastically changing the feature set. Okay. And still, you have things that are basically linearly separable. One other thing I wanted to mention, which is kind of fun, and this is some sort of newer and ongoing work, but I find it kind of interesting. All right, let's pick a different example. This one's a little bit more realistic. Example: This one's a little bit more realistic. My images are going to be two different, you know, brains, and uh, in one, basically, in one of them, there's going to be some type of tumor. Okay. And so we're treating these as densities and these as densities, and we're trying to classify between them. So this is a little bit more of a realistic problem than MNIST, but I want to emphasize a point, which is again, we're building a linear classifier, which means that we can actually go into all of the really cool literature that exists about the best way to build linear classifiers. Classifiers. So, if you're not familiar, there's this idea called active learning. So, active learning says, All right, you know, what I can do is sample a few points and create the family of possible linear classifiers. Okay. And now that actually tells me the next places I need to sample my labels. Because if across my entire family of separators, there was a point that was always going to be classified the same way, then I don't need to sample it. I already know it's labeled. I already know it's labeled. I'm sure it's labeled. You only sample along the margin of this family of classifiers. So this is called margin-based active learning. And so you can do margin-based active learning in the distribution space, in this LOT space. And very, very quickly choose, you say like, all right, I need the label of this point. I need the label of this point. I need the label of this point. And very, very quickly, your testing error or your testing accuracy is going to skyrocket. Skyrocket. Okay. Because again, linear. So this is just sort of another example of something that one can do. You know, and these are some experiments that we've got ongoing. We've got some more. And I also want to mention if anybody watching or here has a problem in which they're thinking about, you know, having point clouds and they want to think about some type of supervised learning on this. About some type of supervised learning on this, I would love to talk. We've got a couple more things ongoing, some related to biology, some related to the splow cytometry thing I mentioned earlier. I won't go into too much detail of that now, but I think this is really an underexplored area within machine learning, which is basically how do you build labels on distributions, right? With provable guarantees. Because, like, in deep learning, With provable guarantees, because like in deep learning, there's this idea called deep sets, and you're able to do all this stuff, but there's no guarantees. So, this is a really nice, simple model. We can do some cool stuff. So, I want to mention one other ongoing thing, which kind of gets to Guido's question earlier. So, another extension that we can do is, all right, we could actually move to a To a completely forgetting about the fact that we were on continuous space, completely forgetting about the fact that we needed functions and just have general couplings. Okay. And also, we want to maybe be able to solve an optimal transport problem a lot faster than doing linear transportation or doing a linear program. So, about 10 years ago, Marco Katori wrote this really great paper called Wrote this really great paper called Synchorn Distances. And it's basically discrete optimal transport, and it can be solved lightning fast. All right. And so I'm just going to set up the problem really briefly, and then I'll just kind of mention a result that we can say here. And there's ongoing stuff that we're trying to build up even further. But with Syncorn, now think of having, instead of your being over an entire space and having a cost on the entire space, we're just going to have a cost on end points to transport to N. End points to transport to end points. So, this is basically how much does it cost me to move from here to here? How much does it cost me to move from here to here? And that cost can be created from maybe those points lie on a grid and you take Euclidean distance squared. This would parallel what we were just doing for optimal for LOT, okay? But in a completely discrete space. So, you have this cost, you now have discrete densities rather than continuous densities, and now the family of transformations. The family of transformations, sorry, the family of transports is the set of all matrices such that they are row, stochastic isn't the right term, but the row sum gives you A and the column sum gives you B. I might have said that backwards. Yeah, the row sum gives you A and the column sum gives you B. This is the family of all possible transformations. And then it winds up. And then it winds up that you can actually pose the regularized optimal transport problem. And that is that you take the inner product of the cost and the distance. This is basically like what we had in LOT, right? Except in seven integral analysis. And this you regularize with entropy. Basically, there shouldn't be, I forgot to delete that one there. There shouldn't be a one there. Sorry. But you regularize with entropy. And what that does is. Does it tries to encourage your transport to be really sparse? Now, if it were, you know, if it became function level sparse, that would just mean every row had exactly one entry. But, you know, this could generally result in a coupling. But the point is there's still a defined optimal transport between A and B. And you actually, the reason it's so great and so fast is because you don't have to solve this minimization scheme with something complicated. It winds up, you can just do a bunch, a series. Up, you can just do a bunch of a series of um row and column multiplications, and you converge to a fixed point, and then heat marker proved that that fixed point is, in fact, the solution to this problem. And it winds up that all of the solutions will be of this form. Okay. So this is optimal transport in a completely discrete case, intrapy regularized, right? It winds up. We can still say stuff. And I'll just kind of put down. I'll just kind of put down like an most general version of what we can say here, but there are other things that we're working with right now. It winds up a lot of this comes down to eigen decompositions and joint diagonalizability and some kind of cool stuff about spectral theory for matrices, which I always love because I come from like a manifold learning background. But basically, an example of something you could say is: all right, say we had grid points that were on the Taurus. We're on the torus in D dimensions. Okay. And so that's what induces our cost matrix. So this could be complicated, but the point is you're on some toroidal structure in D dimensions. And we take our family of push forwards to be the Kronecker product of a bunch of these SIs, where each of these SIs is basically a circulant shift matrix. So that means that when you apply SI in a certain dimension, you shift A to the right by one. You shift A to the right by one, or you shift A up by one, or you know, these types of shift operators. Okay, so if that's the case, if that's the family of transformations that you're doing to your density, that family is compatible with the transport plans. And this is the LOT embedding, and then it winds up that if you applied some shift to A. If you applied some shift to A, you're able to pull the shift out. And so it's now S times the transport between the reference distribution and A. Okay. And that's great because once you have that, then you can pretty easily prove the linear separability result that I was just referring to earlier. You can prove the stuff about convexity. We can start talking about what almost compatibility means. It's things that are close to that, right? So, you know, this really opens up a whole new thing, which is entirely matrix. A whole new thing, which is entirely matrix, entirely linear algebra, but has some cool results. And this is an ongoing thing that will be coming out very shortly as well. But if you're interested and you have any questions, I'd love to please send me an email. I'd love to chat. So that's kind of my thank you and summary and all that. Thanks. Thanks. This is very, very Cool, thanks. This is a very, very interesting talk. Any questions? Actually, maybe I have a question. So, thank you very much for the talk indeed. So, if I understood you correctly, so you are, I mean, you're creating these features for the probability distributions by way of looking at the optimal transportation from a base distribution to them. And then with this, And then, with this, you were able to look at some transformations, and you noted that you would get a linear separability in that feature space. So would you still get the linear separability if you were to be using a different mapping between the distributions? Let's say not optimal transport, but some other criterion? This question. Sorry, I'm sorry, I missed it. The end. Did you mention a specific mapping? There's some other one. Okay. Urge some other one, okay. Um, so the question, I guess, is to what extent, uh, you know, optimal transport is necessary for that property to hold? Yes, right. Great question. So the proof mechanism we have here is specific to Vosterstein II. And the reason is that in Vosterstein II, you have what's called Brenier's theorem, which basically says that the transport can be expressed as the gradient of a convex function. And it winds up. Function. And it winds up, that's kind of the like root of everything that we can branch out of. But I will say that experimentally, this works for a lot more general stuff. And you might not get perfect compatibility, but you still get this approximate compatibility. And in fact, by the way, there was a poster on Monday at your guys' poster. On Monday at your guys' poster session, in which someone was considering this, where they were thinking about having a human pose and then basically taking a map from a template pose to basically the sort of tensor structure of whatever the actual, whatever the metric tensor structure was of the actual shape, right? And then they did things where, and then, and then they were basically able to like linearly interpolate in this space. Interpolate in this space, right? This is basically the same idea here. Basically, what we're saying is if you start with a template and map out, you're now getting a series of maps where you can now do things like linear interpretation, you know, linear convex sums, right? And those convex sums are going to respect a more complicated geometry in the underlying space, right? So I think that this is actually a much more general phenomenon. But But I think it also becomes harder. And I'd love to look into this further. I'll kind of mention right now, I'm actually looking into something along these lines for maximum mean discrepancy, which is a different form of distance. It doesn't exactly satisfy this because MMD has a distance saturation, but there's certain things that can still be said. But that's kind of a pretty new work. So I have a question, Alex. So, I have a question, Alex. I have a question. So, actually, just on the maximum mean discrepancy you were talking about, because people have been using this in statistics and machine learning for a very long time on their distributions, actually. And there's something called the support measure machine, SSM, support measure machine. They were introduced like in 2012, actually, so like a long actually check 2012, so that's like nine years ago before like the CNN, actually. So, I think it might be a good idea actually to compare your kind of your formulations with the Your formulations with the um with these kind of ideas for machine learning, and yeah, no, I'm very so I'm very familiar with MMD stuff and support measure machine. I think what it is is building an SVM on the mean embeddings, right? Yeah, yeah, yeah. But funny enough, uh, I wasn't sure how long this talk was gonna go. Okay, okay. Uh, if it had gone short and I wanted to do more stuff, my next thing was actually going to be talking about MMD. Oh, okay, okay. So, so, like, did you compare? Did you compare your method with this? compare your method with this uh with this like has i will say i don't have those comparisons here okay i will i will say it more as sort of a a um these comparisons have been done in this in the in the basically they i i asked my i asked caroline to do them initially just to convince me that this lot was worth it basically oh okay okay so they're no they're not here and that's actually a really good point we should add we should add the uh the svm on the the mean embeddings um the biggest problem is that mean embeddings it's really hard to prove what Is that mean embeddings it's really hard to prove what happens when you do a group action to the distribution? There's really no guarantees there. So, great, thanks. Yeah. I think due to time, maybe we can have one more question. Parvo, do you have a question? I see you raise your hand. No. No? Well, sorry, was it Peter or me? Then, sorry, I hope it. Maybe, maybe. Ask you if you had a question, but I didn't. I have a question. No, I don't see any hand here. No, I mean, yeah, I didn't. I didn't raise a hand. I did have a, I mean, it's just, you mentioned Gigli's work from 2011. I just wanted to know, like, what the, what specifically was it that the result of Gigli that you were using? You said it was something to do with push forwards. Yeah, yeah. So what he's, let me think the best way to describe it. Let me think of the best way to describe it. What he's doing is considering a path from mu zero to mu1, where that path basically involves I'm blanking on the context now. But the results wound up being that he was thinking about what happened when you linearly interpolate the transport. When you linearly interpolate the transport planes, yeah, I think the context was between the Wassertan two distance and the linear one that you had, right? Yeah, yeah, well, yeah, basically. So the difference here is that we're bringing in this notion of compatibility, and it winds up giving basically a much stronger result because basically what he had was a very sort of rough upper bound in which, um, I mean, it's a great result, don't get me wrong, but like it's a rough upper bound. But, like, it's a rough upper bound in which, even if the two distributions were like shifts of each other, his statement would still say that the error was quite large. And what we're saying is, you know what? Actually, it's not. Like, you actually, if you consider a little bit more carefully what the types of distributions were, anything that involves a shift does not induce any error. Okay. And so, in some sense, you can kind of think about it as like if your optimal transfer. Like, if your optimal transport plan involves moving forward some amount and then perturbing in some way, right? That basically the LOT distance induces no error or almost no error. Okay, that's that's that's kind of the context. Yeah, thanks for the great talk. Thanks. Okay, all right. Thanks, Alec.