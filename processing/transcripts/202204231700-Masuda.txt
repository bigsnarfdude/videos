He's going to talk about recurrence, so quantification analysis and landscape analysis of dynamic brain networks. Please go. Thank you very much. First of all, I mean, thanks to organizers, Simon, Javier, and Johan, for inviting me here, and also to say Calgary. I mean, it's a university workshop on Thursday, so this is my first time in Calgary. He really, really do that. Thank you and say, you know, talking. So this is my original title, but say, given this workshop on Thursday, it's like. I found that I should swap the order because I mean, yeah, I want to say talk about the first one, which would be more, let's say, related to many of, say, you, even though I am also interested in the second talk. So I'm going to say start the first one. And if I have time, then I'm going to do the second one. So we have multivariate chemistries, and of course, here we are primarily into signals, multivariate signals from the brain. So in my case, I'm primarily analyzing, say, femme data. Analyzing, say, familiarized it, like which some people talked about today, and also those today. And one say major way to analyze is to look at pairwise correlation of the time series to create correlation matrix. And in the simplest way, if we threshold on the correlation value, then we get the narrow data. But of course, some people do crit size. I mean, say this is not good, so there are many fancy methods. But here, size ask the same different question using a correlation matrix data. So we are not. Matrix data. So we are not precisely creating network. But my question here is to say discuss whether we can discuss the dynamics or regime shifts. And I say rely on the Ising model to model the multi-bay fMRI data. And the use so-called energy landscape analysis, which I want to introduce you to. So here is a one-slide summary of what I want to do. Of what I want to do. So let's say: so here's a healthy brain, and we have data, right? So fMRI data, and we can actually infer the so-called engine state, which you're going to explain, and we use the eyesing model for doing that. And once we have done that, then we can abstract the brain state into just the location of one blue ball. And this blue ball is actually constrained on the energy landscape, so it can go uphill. Landscape, so it can go uphill, but with a smaller probability or rate, and it tends to go down, but because the ball's movement is noisy, so it can sometimes go over this barrier to transit from this one local minimum of the energy to go to another one. However, let's say in the case of autism or the oxygen composite disorder, the assumption, well, the hypothesis is as follows. Okay, if we say take this ephemeral data from those people, From those people, and they infer the energy landscape using the same method, Ising model, then we expect that energy's barrier is higher. So, this implies it's more difficult. So, I should have a pointer, right? So, this one's used that pointer. Okay, you can also use the reality. Careful. So we uh predict this uh say would happen. The what the difference between the two is that now the ener uh the high barrier between the two uh say local minima uh is higher. So this implies that the uh this whole brain state finds it more difficult to say transit from one to the other and back. And the important thing is that we are not assuming anything, we just infer the shape of the energy landscape purely from data, purely data, without any neuroscientific. Without any neuroscientific assumption or any mathematical models other than the Ising model. And we expect that because those people, let's say autistic people, say have some behavior symptoms where they stick to the same behavior, right? And similar for this amino acid names, such as OCD. In contrast, if we say take the data from let's say schizophrenic individuals, then we say hypothesize that this low separate virus is low. That this slow advancer is over because I'm a schizophrenic individual symptom, behavior symptom that you see. They tend to jump to conclusion. They show something, okay, this is a conclusion. So this is, you know, they may say implies I'm say more frequent switches. And similar for the say ADHD is the hyperactive discord. So this is what we want to do. The methods. So we do the following. So we have said a time series. Actually, it's So we have say the time series data. Actually, this doesn't have to be any say brain data, but we are using say familiar data primarily. And let's say we have the discrete time, which is most natural for most observations. So let's accept that. And crucially, we binarize each node's time series data into positive one, negative one. So in this way, we lose a lot of information. But this process is necessary to fit the model. But this does not justify the use of whatever. That does not justify the use of anyways. We are doing that. And then say, okay, actually, so Dahan said, Choice presented his talk on Thursday to explain this procedure, so I can sort of skip that. But the point is that, okay, we have done this binarization. Then we can present this ball. Say I showed a blue ball, right? And the blue ball corresponds to this binary vector. So each time point, say, the corresponds to this say the one binary vector corresponds to each time point, right? It represents the brain state. Represents the brain state, which is dynamically removed. So, in this way, we got the distribution of each two to the five states, and using this and correlation matrix, we fit the model for the inverse eigen model, which we encountered on firstly this workshop. And then we do some more analysis. Let me just explain one by one. So, then, okay, so we come up with this. Okay, we basically say infer this JIJ, which Basically, infer this JIJ, which represents connectivity parameter for those who know, I don't know. But anyway, this is an acceptant functional connectivity. But, well, as I did say intervening to say this is a dance discussion on Thursday, so this actually matches this structural connectivity better than more naive methods. This is something we showed in the first paper. However, so the inference of the AIDS model is not new. So many people have done that. So the Han summarizes this, right? So from 2000. This right, so from 2008, so 2006. I mean, say there are people have been doing that. But this part is new to the analysis. So once we have done that, then we infer the so-called disconnectivity graph, which comes from the physics-chemistry field. And this is essentially inferring this. So I'm going to explain that, but in the end, we find the set of so-called local minimum energy. Local minimum as an energy local minima, and how they are connected, and how much barrier they have to overcome to go from one local minimum to another. So, we can infer all this from the GIG parameters. And then, once we've done that, then we try to say quantify. This is just a figure, right? This is great, but we need to quantify, for example, using the number of silo minima or the height of the barrier or the size of attractive based on each one. And we try to find, say, some association between this. Some association between this, which comes from brain signals purely, versus something, say, which comes from the individual, like a behavior score, or this symptom, severity of a symptom that is hit. So this comes from behavior or the medicine, and this comes from the brain score. And if you find some correlation, association between the two, then we believe that this whole analysis may have some selected correlation. Okay, so this is our Ising model. So I don't get into technical details, so there are many several methods to infer the parameters. We use just one of them, so we have some choices, but we don't talk about any technical part. And the input is the multi-variant time series, so it can be anything. So I'm also interested now in this using this for say there are other types of data. So we're using negative data, but we could do many other things with this idea. Image analysis, the cause. The calcium order images or the electrophysical physical data. And we binary signal. This is a huge limitation, but we do impose that. And also, we don't assume causality. So this JI, JI, this parameter is symmetric. So we cannot say for i to j or j to i. Of course, there are extensions of, say, the Ising model to try to incorporate say directionality, but this is very difficult and we are not doing this for now. And this part, what does this mean? And this part, what does this mean? So we have this data, and the first thing we do is to look at, say, one of those buttons and calculate so-called energy. But this is actually the priority. So I skip this part, but the low energy means high priority on a multi-system statistics. And then this is actually, so there has a precise correspondence between here's the priority of signal pattern and here's the energy. Single pattern, and here's the energy of a single button. This is the partition function that appears, right? So, the say by stop, for example. So, this is the sum. So, the okay, so calculating an energy means we just count the frequency with each pattern. And then, if we look at this particular pattern, say we first identify the five neighbors. The neighbors are the the patterns, binary patterns, where only one bit, one low, basically one node has the opposite pattern. The one node has the opposite button. Here, it's in negative one turns into one, right? So, in this way, there are five neighboring buttons because there are five nodes. And then we say, look at this energy frequency, the appearance frequency of each button. And in this particular example, so we have say they say most things negative 0.2 here, right? And it happens that they say all the other five buttons have a higher energy, which means lower frequency of appearance. In this case, we declare. In this case, we declare that this is the local minimum of energy. So, this is a locally highest probability pattern. And in this way, we identify such a pattern from all the 32 patterns. And then we consider the network of activity patterns. This is not the function network. So, each node now corresponds to the activity. Node now corresponds to the activity button. We are not using any functional network buttons. We are thinking about different networks. We call each one activity buttons. We are showing six activity buttons here, right? So we are trying to think about the network of that. And this is actually hyper tube, right? So this is a case of just one node, two, three, four, five. And we just allow it, say to say the state to change, just one node can afflict the state from positive one to negative. The state from positive one to negative one or vice versa to be connected by a edge. So this is our network. And the next thing we do is suppose, okay, suppose this is our network of activity patterns and these four red nodes are the local minimum of the energy, the locally highest frequency patterns. We can say do this computation quite quickly. So there is no worry about computation time here. And then we say look at the And then we say, look at these two patterns, and we look at all the paths connecting these two local minimums. And we are just showing three of them in different colours. And here are those three. And then we look at, say, what kind of energy values they have, each mode has. And in this first path, it has a say energy value 0.1 as the highest. So this is the amount of this mountain you have to go over to go from this say alpha one pattern This alpha 1 pattern to alpha 2 pattern. So they have a relatively small energy square, 0.1, negative 0.1 negative 0.1 pi. But we have to go up before reaching there. And this is the highest, say, mountain's height. But if you take a different path, this might be smaller, right? And if you take a longer path, it might be smaller or say not. In this particular case, actually, so this is worse than this because I mean the highest say point is 0.3, right? But if you take a long Point three, right? But if you take a longer path, this particular schematic example, the highest point is 0.2, so it's slightly better. So in this case, we nevertheless declare that we take the best path, right? And say the energy barrier it has to overcome to go from alpha 1 to alpha 2, or vice versa, is equal to 0.1. So in this way, we can identify how much energy barrier they have to overcome. So this result is this so-called disconnectivity graph. So this is a method. So this is a Mac. Can you see uh some applications? So the play, the video, does it work? Oh my god. Oh right, okay. So just I mean uh see it for say 10-50 minutes or so seconds. Right, so have you experienced some you know uh uh proceptor switches? Proceptor switches, rotation, yeah, direction switched from left to right, or vice versa. And actually, this is the point. And the weird that there's a stimulus is not changing at all. So this is your brain which changes the perception. And this is a very famous experiment. And the natural hypothesis behind this, I mean mecha a brain mechanism behind this is this. Okay, say one say for example the counterclock coin rotation uh say corresponds to the uh say uh the one uh say token. One token element and say the other commitment. And once your brain state switches from one tile, then you see the autopsy switch. It is a very intuitive hypothesis. However, so there is no, this is a very famous hypothesis since 1980s, but there has been no evidence. So we try to look at that. Okay, so let me just, this is a practical application of this one. So let me just present something. So here we see selected seven roads. So these rows, selections, are in So these voice selections are informed by the uh biological evidence in a sense that say they are say suggested in the lit previous literature said that these particular voice are among those related to this type of visual information processing. And only seven? Yes, actually this data as method is very data demanding. So this is a huge limitation and we're going to come back to that. But yes, only seven roles. We cannot do easy dose. We cannot easily do something to 100, even though I'm gonna say present a very large number of analysis data, though, as our attempt. And then we construct, okay, so we say collect the data from many people, right? And we construct the disconnectivity graph. So this is this summary disconnectivity graph. I mean summarizing the data from many people. And we have 10 local minima, but say we want to say remove some of this local minima where the energy barrier is too small, barrier is too small. Small, barrier is too small. This means that we can too easy flip from 4 to 5, or let's say, for example, 8 to 7. So, even though this is technically local minima, but we don't regard they are important. So, we want to say see something very deep because I mean significant, we haven't established a significant test. This is something that we are actually doing, but the next particular analysis we just visually look at that to determine that we have say three major states which are separated by say very highest say low. By say very highest energy virus. So one, two, three. So this is we call the say three major locomotives space. And then we can, this is a specimen figure on C2D, but actually the network space is a hype. But first, we can determine which any of the CD's binary pattern belongs to the attractive basis of which one, one, two, or three. We can say the calculate that easily. So this is the quantity we use. So this is just We use. So, this is just a figure, right? So, we need to quantify that. So, we did that. So, in this particular case, we looked at the size of attractive base in the frontal area opening, which means that, okay, if you look at this, what say the rows are active or inactive? In this case, so the frontal say this is, I think this is a active, and this is plus one, and this is minus one. So, if you look at individual the uh say the rows, so they are say brief uh frontal say uh say areas. Frontal say areas, rather than the D7D5, rope, L C and C visual related areas. So the prefrontal ones are active and say visual ones are inactive. And this is the opposite pattern. This is basically the all active pattern. And if you look at how many other binary patterns, you know, they attract it based on that. So it's like releasing a ball here and say look at which. Here and say, look at which of the three local minimum say this global release at here. So in this way, we can determine each, say, this binary patterns, say attractive basin at each site. So if when we did that, so this is the size of the attractive basin of the so called frontal area, so local meaning which is this. And this just looks, please just look at say red say so, and then you say this. And then you say this comes from the energy landscape. So this comes from the say affirmative, right? And this y-axis comes from the say experiment. So this is a mean duration in each alternative perception. So we have counterclockwise versus scope-wise, right? So for each individual separately, we measure how long the participant sustains this perception before he or she experiences the rotation. Switch. So this is an individual quantity. So this is the individual quantity, so they will measure that behaviorally. So we have some found some say the uh say nice summing uh negative correlation. So in this way, so this summinessive quantity maybe creates a related switch of visual conception. So this is one of the new first neural mechanisms, I say evidence, which we find to sustain this hypothesis. But this is not directly supportive. What we found is just length of the visual perception rather than such a single vision. There is two long Single here. Whereas I have two, two local minima. No, we didn't find that. So we found three and say these each one that did not say correspond to the say the counterclock process crop by say more quotation. But nevertheless, we found some energy landscape evidence of very long-standing experimental path. So this is one application. Let me just show another application very shortly, briefly. So this is a similar application, but to the Similar application, but to the to seven, sorry, seven, seven, okay, okay, it's quite quick, so perhaps I should go with that. So we say there is similar analysis for to compare between old versus young individuals. One other thing we can do with this is to say, look at this so-called Markov chain picture. So this is related to the Jordan cups that we talked in the seven sector today. So we say, establish this, and we can ask, and this is Blockoff. This and we can ask, and this is local attractive basin, and this is the local minimum, and this is the local minimum attract of the basis. We can count numbers of transitions, etc., to say establish how many transitions happen. And this is a quantity we used, and in this case, we found the efficiency behavior, the cognitives for the individuals, and how it is related to the switches between these two. But let me just for a minute. The one thing I want to say uh uh highlight, okay, this is the data demanding. Data demanding. Okay, so let me just spend one minute here. So this method is very data demanding. So we have to take a long data, we have to pool many participants into one group, which is sometimes problematic to be able to use this one. So we are trying to work on that. And so one thing, two things we are working on is as follows. So one is to use a patient. So these guys from Korea, where I mean, I don't know whether it's a Korean folks and European teams. So they has been developing these energy analysis as well. The energy analysis, as well, so and so they're using the Bayesian method. So they say we are trying to implement that and say contact data, et cetera. So another thing is related to his talk, I mean on Thursday, which is to use Markov model. So what we want to do is to say state transition pictures, okay? And if whether station transition is very frequent or not. So we don't have to necessarily use the Isaac model. So if we abandon that, we might be able to do something more data efficient. So this is another direction we are looking at. So I'm just saying the rest remaining, say something like four minutes, or they say six minutes. So the intelligence. So we discussed the critical brain hypothesis. So I mean, I'll show the application of our method to this one. So our particular hypothesis is as opposed. Okay, more intelligent people are closer to quick quick. Not just our safe brain works are quickly. So how can we do tests? So, how can we do tests such a hypothesis? So, we did the following. This is a phase diagram, the Eisen model, which some presenters showed today, those days. And so this comes from so-called SK model, or the very prototypical model diesing system. And then this has the two key parameters, which I don't explain, but as key we change the parameters, we can say uh switch uh between say three main phases spin mass phase, ferromagnetic, ferromagnetic. Ferromagnetic fermion. And this is the statistics of physics, the text, very popular one. And we fitted this data, say, fMRI data, to the ISI model, right? So we can infer B parameters from data. That's what I have to create. And the next thing we developed is to fixitively scan the parameter space by artificially changing the JIJ or the other JIJ and another parameter. There is another parameter. And there is only one single data visual here. But what if we change the parameters, I mean, artificially? And then we find something different. So in this way, we can scan this parameter space, and then we can also measure the other parameters of the Ising model to recreate sort of similar phases. Okay, we call this a phase directory for our MRI data. And then we also need to map out each single individual. out the each single individual's uh the uh say fMRI data onto this uh say map. So these are results. Okay, these I mean each uh circles uh are the results. And we have IQ score from each particular data set. So we look at the association between so the association between the two. So the association was actually not very strong but we still found something so which is that okay so if we magnify this part then we say look at this low IQ participant right versus high IQ participant and then it IQ participant, and then what is very weak, the result is automated weak. But still, the higher IQ participants are closer to this. I mean, this is a phase transition like we like. And particularly, this is a border between the paramagnetic versus sting glass phase, not this or that. So this, and also there are several different types of IQ spores, but such an association was only found for this performance IQ, but not the For the performance I do, but not the probable linguistic related pathways. Okay, this is something okay. So, the how much time do I have? I don't think much. Two two minutes. Okay, let me just uh do something very quick. Yeah, the last season, sorry. Right, that's fine, so it's enough actually. So, this is this is a summary of the energy landscape one. Let me just spend the last two or three minutes to talk about this differently. So, the this is not the the energy landscape analysis is not about this functional narrow, so it's time split analysis. Functional narrow, so it's time series analysis. It's time series analysis, actually. But we are also interested in how the connector itself changes over time. And some people say that this is nicely connected. And there have been zero analysis. And I want to say use the time series analysis method to say some to be combined with the temporal neural analysis method to say the say just some, you know, the analysis of that. So we are say we are the developing the following. Developing the following. So we have a so-called recurrent method analysis, which is non-linear time series analysis, not a network analysis. The idea which came along in the 1980s is to look at the recurrence of time points. If the two time points, two signals are similar to each other, then we call this the recurrence with some tolerance threshold. So we say, take this T1 and this is T2 and we look at all the pairs of times and when you look at this and we can get And when you look at this, and we can get and say a recurrence for support. And if this is just a picture, as I think. So, if we say perform any quantitative analysis of that, we do this means a recurrence quantitation analysis. And there are some major methodology length of testing, et cetera, this or that, or this density of work cost forever. So, we want to do this network version of this to see what it tells us. So, we developed that. So, this is now the time series of the functional connectivity. Series of the functional connectivity. This is a classical functional connectivity analysis. And then we use a sliding time windows to create and say temporal network or say functional networks. It's a very good method, but we did that. And then we need to say measure the distance between two networks. So learning from the 14th of the IUC really is a thing that we should use something as geometric things or this manifold thing. But here we do something, very simple. But here we did very something very simple. We just looked at the say the distance between the two algorithm matrices element-wise. I know this is a not a good method now, but let me know this is one. We did actually we tried some other distance measures as well. But once we say define a distance measure, then we can say which node pairs are close to each other, which time points are close to each other, so that we can draw a recurrence template. This is just a distance plot. And if we binarize this with a certain threshold of X on, then we get a recurrence plot. Getting a recurrence problem. And we run this recurrence quantity analysis on some steroid or EG data from the epileptic patient. So I don't have time to explain this very properly, but we have the data of seizure epochs. And then this is seizure, and this is pre-seizure, and this is post-seizure. So in this particular patient, so there are, say, three seizures reported. And then there actually say a long period of interval between the first seizure and the second second seizure, but we've only kept. Seizure and the specific concedure, but we only keep that. And then we say this seizure period has a similar narrow. And also the seizure period narrow functional narrow similar to pre-cedure, but not post-seizure. And even the seizure is different. So we see some similarities. So the first seizure narrow is similar to second seizure function here. So in this way, so we quantify this difference and we try to, for example, classify these individuals also. So this is something. This is something. And we are analyzing this simulator. You can potentially networks of airports. So, like an airport network changes over, say, months, but this is something I don't have time to explain, but okay. So, here's the conclusion. So, I say basically, so this exposes to the so-called energy landscape analysis. And this is a rising model-based and purely data-driven. And limiters, and we see some evidence of say the being able to say explain the market data. Playing the market. And here are some of the future work limitations we are working on. So let me just acknowledge my people and say these are me the things I say they say developed by these two guys basically, but this is my current team and say two of guys that are working on related things and these guys for example doing test analysis of the energy landscape analysis. This is a problem. Okay, thanks for your kind attention. Thanks so much. Nice presentation. We have time for a few questions. So go back to the time when you're presenting the different paths to join two different states, alpha one, alpha two. Oh, sorry, that's fine. A lot of slides, I mean. I know, I know. I have to. So I just want to listen to you understood whether you have. I I just want to easily understand whether you have there's something if I understood this correctly. So you get a score of energy at each state, which is different than if you would have gotten a score of the amount of energy to go from one state to another. In other words, your paths of relative energy are independent of where you come from. It's just because I'm preaching this state. From is just because I'm reaching this state, the score is 0.1 or 0.3. It doesn't matter if I come from a state that could have been possibly similar or more different. That's right. So the energy is first assigned to each of those states. And then you raise this. No matter where this comes from, so we look at this and score. So this cumulative energy, which is very meaningful, is it correctly doesn't account for, let me put it this way, how much functional the continuity. Let me put it this way: how much functional configuration are needed to go from one state to another, regardless how much energy is associated to each state. Yes. However, so this, yeah, you are correct, and at the same time, this is sufficient because, okay, regardless of the locations, right, this or that. So, if you discover, okay, this is IRL same as you have to overground, then you have to go this and back. But if you're starting from here, then if this is the lowest, yes, you go backwards. But so the so yes, and you're right. So the so yes, I mean you're right, and also the it tells us how much energy we have to hold up. Because we specify two points and then calculate this, I mean the minimum, say, uh say height of the paradigm. So it automatically determines how much, in this case, 0.3 rate difference. This is the energy we have to overcome. This means that this is a high frequency state and this is a relatively low frequency state, and we have to go that much. And because this is some energy difference, or the probability difference I mean can be translated into the probability uh say transition. probability uh say transition using Arrhenius say theory. So it tells I mean say what rate at which this transition happens. Yeah and basically the idea was it could be that there are two states that both of them are very low energy maybe but maybe my functional computation to go from one to another it's it's it it takes me a lot of energy even though both states are very low energy because I'm assuming very low energy does not necessarily be mean very low energy does not necessarily mean that they are very similar. No, no, so they are generally very different, maybe. And if you have to go up to that energy, then you have to overcome this. So this means that this cannot be easily transformed. It takes a very long time to go from heated to cold. And the last question very quickly, one or two slides later, when you have the retroground, when you find it three clusters, yes. Just wondering, I understand that the clusters. I understand that the threshold that we use belongs to a very small regime of thresholding, where you can actually get the tree. That I see that there is a wide regime of energy thresholding that would have maybe before. Yes, I agree. So this is arbitrary, I would say. So yeah, the okay. But if you do that in before, I think we cut it here. But then say nine is very shiny. But Venusin nine is very shallow. I know. But I say I still admit that this is very arbitrary. So now we are trying to develop to do this automatically by using sort of shuffling. If we do shuffling, then we will have some much shallower, say, the energy landscape. But if this height or depth is more significant than the energy landscape, which are rather than constructed, then we can regard that, okay, this height is probably significant. So we are trying to develop such a thing. But at this point, To develop such a thing, but at this point, I admit that this is just heuristic. This is important. Any other questions? At some point, you mentioned that this was very computational demand name to compute. Where does that come from? So it's not computational demand. That's a good point. Thanks, I mean, because I didn't really skip that part. And the computational demand, that's sort of true, but we have to do that. The money, that's sort of true, but we have we can use some say approximate method for pseudo-likelihood or some other functions method. So we can say infer the data up to say 100, 200, 300. That's fine. What is annoying is that this is really data hungry. So if we don't have sufficient data, then the accuracy is very low. And actually, if you add one node, then you need twice more data. Because it's a binary vector, so the dimension of the binary vector increase. Dimension of the binary vector increases by one. And okay, for example, if you have ten nodes, then there are two to the tenth, which is about a thousand patterns, right? And if there are some, say this pattern has been observed five times in the whole time sequence. This is 20 times, this is 35, this is 4, then we can estimate that. But if the time data is too short, then we don't observe many of the samples patterns. For example, typically, so if we measure For example, typically, so if we say measure, say, fMRI for five minutes for myself, then typical length is, let's say, three hundred, let's say, if the spacing, right, one second or two. Then the majority of these states are not observed. In that case, this is very bad. I mean, say accuracy deteriorates. So this is what we want to say fight for. So that's why we are considering these things now. Great. Maybe let me close with one philosophical question. Question. Everything you do is on the binary things. Why is the word binary? Yeah, this is the question we don't have an answer to. So we just, I mean, started by saying this is the Ising model because some people use that. Actually, so this is, I mean, preventing authority, but some people use that close to the electrophysiocoid data. But this means that it's sparking data. So there is sometimes one. But in many, say, time points it's equal to zero. So it's quite imbalanced. And I thought that it's much better put And I thought that it's much better this sort of method, I think model method much better fit with the balance between zero and one and sort of balance more balance. So why don't we go to the continuous signal? So this is why we started this. However, so by binarizing, we lose a lot of information. But it somehow works, you say, it works in the case that it explains various behavioral data, but it does not justify the finalization. So one part we have been trying to do is to, you know, using some Markov models in some sense to Markov models in some sense to try to avoid binarization, but to effectively achieve the same goal. But philosophically, I don't have an answer. We just follow the okay, so let me just add one, you know, the justification. So using Ising model, so we can resort to the status quo physical ideas because there are lots of physics theory results in this Ising model, right? So they are, let's say, this image landscape thing or the phase transition. The phase transition. So, this is another reason, for example, free energy or this, how can I say, it is susceptibility measurement. So, with this model, we can more causally use those accumulations, that's Coffee's theory, to try to understand the brain, this analysis. But let me emphasize that this does not justify. But this is my process. All right, all right. Well we'll chat about that later because I remember you did your communications paper when you but Remember, in the data communications paper, you were equal, but the fMRI data went to three states representing the dynamics with the pairwise interjections when balancing differently. But we can check both. Sure, sure, let's do that. All right. So thanks again. So now we come to the last but not least talk of the day. Stop the recording here and then restart it. Yep. Got it. Got it. Yeah, got it, got it.