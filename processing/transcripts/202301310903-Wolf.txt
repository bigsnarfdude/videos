Just posted on the Slack that we're getting started, so hopefully, people will come in. But yeah, we're really happy to have Arne Wolf, who's going to give a first talk this morning or this afternoon for us in London here. So Arne is a first-year PhD student with me at Imperial College, and he's also a PhD student at the London School of Geometry and Number Theory. And he very recently got started on reading some stuff about. Reading some stuff about sheaves and networks. And so he's very kindly agreed to give sort of an overview talk today about what sheaves are and how you compute these things and how they can be used in applications on networks. So yeah, Arne, very happy to have you. And thanks for agreeing to give this talk. Thank you very much that you shared the slides. So not screen again, right? Yeah, online for us, we can see your slides and it looks good. I don't know in the meeting room if everything is okay. Yes, okay, good. Just start. Yeah, thank you. Thank you all for. Thank you. Thank you all for coming to this talk. And I also want to thank our reading group at Imperial College and the other PhD students of Antia for helping me to prepare and to improve it. And I thank especially Anthea for introducing me to sheaves on networks, to which I now would like to introduce you. So I will start. I will start giving the definition what sheaves on graphs are and what their sections and sheaf cohomology is. And then there will be a relative cohomology theory and also a kind of Laplacian operator that is associated to the sheaf. And for the second part, I will try to convince you that sheaves are useful as models because they are suitable to express local compatibility conditions. Compatibility conditions, and in particular, we will see how a sheaf can be used for modeling opinion dynamics on social networks. If you have questions, yeah, I don't see the chat now, so maybe just ask or ask at the end. First of all, what are sheaves? So roughly speaking, sheaves assign data to subsets of a topological space in a somewhat consistent manner. Somewhat consistent manner. The first definitions of sheaves came up in the 1940s, and since then they have become powerful tools in differential and algebraic geometry, and in particular in algebraic topology, because there's a cohomology theory related to sheaves, and a version of this we will also see later. And the modern definitions of sheaves used in these areas are very general and therefore also very abstract. And we will And we will consider a special case of the sheaves here that is adapted to our topological space being a graph and our data living on real finite dimensional vector spaces. And our version is usually called cellular sheaves in the literature. And those of you who want to know more about going from the general definition of sheaves to the more concrete version can have. The more concrete version can have a look into chapter 4 of Justin Curry's PhD thesis. So let's get started by the definitions by fixing that we will denote a graph as a pair of vertices and edges, and we will see the edges as ordered tuples. So every edge has a fixed starting and a fixed end point. But nevertheless, we see the graph as an undirected graph. And we also assume that it does not have loops or Have loops or parallel edges. And then we can define a partial ordering on the graph by saying that a vertex is smaller or equal than an edge if the vertex is one of the endpoints of that edge. And now let us fix such a graph. We can define a sheaf on that graph, or more precisely, a cellular sheaf on that graph, because it could be defined for cellular complexes as well. As well. But because there is none of these more general sheaves around, we will just call that a sheaf. So this sheaf assigns a finite-dimensional real vector space to every vertex and to every edge of the graph. And these vector spaces are called the stalks over the vertex and the edge. And to every incidence, so whenever a vertex is part of an edge, it assigns a linear map going from It assigns a linear map going from the vertex to the edge stalk, like this map here. This map is usually called restriction or projection map, but it does not have to be surjective. And if we reformulate that in the language of category theory, we could say that the sheaf is a functor from the graph as a partially ordered set to the category of finite-dimensional real vector spaces. And putting it this way hints towards the general definition. A general definition of sheaves used in algebraic topology. Down here, you can see how a sheaf on a rather small graph looks like. And there is a very concrete example of sheaves that works over every graph, namely the constant sheaf, in which all the stalks are just some fixed Rn and all the restriction maps are the identity map. Are the identity map. You can think of the stalks of a sheaf as containing data, and the restriction maps they represent the consistency conditions for this data. And to explain what this means, let us introduce sections. So probably you will have seen sections in the context of vector bundles on manifolds, for example. And here the idea is very similar. So let's fix a graph and a sheaf on that graph and look at That graph and look at a subset of the vertex set of the graph. And we say a section of the sheaf over that subset picks one vector from every stalk over the vertices of that subset, such that whenever there is an edge between two vertices of the ZW, then the projection from these vertices to the edge via the restriction maps agrees. Why are the restriction maps agrees on the edge? So we have this condition over here. And clearly, the sections over W form a vector space because we can pointwise add two sections or multiply it with a real number. And if our subsets happens to be all the vertices, then we call the section a global section. Let's have a look at how the global section looks or how the global section looks like. Section looks or how the global sections look for a very concrete sheaf. So this sheaf down here, so the graph is a triangle and the sheaf is almost the complete constant sheaf. But instead of a plus one, we have here one minus one. So the condition of being a global section tells us that the value here and here have to agree, right? Because the projection onto this, which is in. Onto this, which is in both cases multiplication by one, they have to agree. And similarly, we get the same value over here again. But if we now look on the edge to the left, we see that these two values will actually have to differ by a sign minus one. So the only global section of this sheaf drawn here is the section that is zero everywhere. And the zero section is a global section of every sheaf, but that's also the most boring global section. Global section. And by an argument that is similar to the one we saw over this edge, one can show that constant sheaves have locally constant global sections. So the global sections are constant over each connected component. Sections can encode linear local consistency conditions. And one of these conditions is the conservation law, like you know it from. Law, like you know it from physics. And let's have a look at the conservation of the flow of water in a system of water pipes. So these system of water pipes can be represented by a graph. And we assign a flow sheaf to that graph by setting the stalk over each edge to be the reels and the stalk over each vertex to be a copy of the reels for every pipe that goes. For every pipe that goes away from this vertex. And then the restriction maps are just the projection onto the corresponding edge stalk, which is here encoded by the color. But we will add a sign or we will multiply by a sign minus one if the vertex is the starting point of an edge. So what does it mean to be a global section of this flow sheaf? This flow sheaf. In this case, down here, it means that the blue value in this stalk will have to be minus the green value over here. And so if we identify in every stalk the value in a component with the flow that flows from this point into the corresponding direction, then this is equivalent to saying that whatever water flows. Whatever water flows into this pipe comes out at the end. So, global sections of this flow sheaf represent flow values on the sheaf, on the system of water pipes that do not leak on the edges. But they could still leak on the vertices. And to ensure that this does not happen, we can use a little trick. So we extend the graph to a graph. The graph to a graph G prime and then we also extend the sheaf. Well, so for every vertex that we had before, we define a copy of the vertex, which is only connected to the original vertex by an edge. And we set the stock over the new vertex to be the zero space, and the stock over the new edge to be the reals again. So it's already clear that this map here must be the zero map. And we let this restriction map here be the map that takes the sum over all these components. So now, being a global section of this extended flow sheaf tells us that the net flow out of a vertex is zero, or in other words, that the system does not leak at this vertex. And one could easily modify this to say To saying that there is a certain point where water enters the system, or maybe a certain thing where water can leave the system. Are there questions so far? Otherwise, I just continue. So global sections will have a nice interpretation or will provide a nice interpretation for the zeros sheaf cohomology group. Sheaf cohomology group. So let us define what that is. So such as the beginning can be defined for very general sheaves. And we will use a more easily computable version of this, which is adapted to the sheaves that we look at. And Justin Curry shows in chapter seven of his thesis that these two versions compute the same cohomology groups. And this is good because it lets the same thing. And this is good because it lets us use all the machinery that has been developed for the general sheaf cohomology, also in our setting. For example, we get a Meyer-Vieter Torres sequence and we get many more properties that you can find in your favorite book about sheaf theory. So our version is defined similar to cellular cohomology. We define the space of zero code chains to be the direct sum over all the vertex stalks of that sheaf. Vertex stalks of that sheaf and the one co-chains to be the direct sum over all the edge stalks. And the higher co-chain groups are just zero because from topology, we would not expect a graph to have the homology in degree two or higher. And then we define a co-boundary map going from the zero to the one chains. And because this map is supposed to be linear, we can define it on the on this. On this, or we can define it stalk-wise. So, look at a fixed vertex V, and then we map an element of that stalk to the projection onto all the edges that the vertex V is part of. But if V is the starting point of the edge, then we will include the sign minus one here, and otherwise we don't include the sign. And so, with the help of And so, with the help of this co-boundary map, we can now define the sheaf cohomology by saying that the zero cohomology group is the kernel of the co-boundary map, and the first sheaf cohomology group is, well, the one co-chains modulo the image. And this is sometimes also called the co-kernel of the co-boundary map. So, from now on, I will sometimes omit the coefficient chief F here if it is clear where the coefficients have to live. Where the coefficients have to live. Let us compute this for a concrete example, namely the sheet that we've seen before. So now we give names to the points of this triangle, and we also give names to the edges, small A, small C. And we also have to fix an orientation of the edges. So we go, for example, counterclockwise here. Counterclockwise here. And then the columns of this co-boundary map correspond to the points and the edges they correspond to the vertices. So let us look at point C, for example. Point C has an incoming edge A. So we get this one here because the restriction map here is multiple. Restriction map here is multiplication by one and it has an outgoing edge B. Outgoing means we have to include the sign, and then we have this one here, so we get the minus one over here. If we look at B, B has an outgoing edge A, so we get the minus one here, and has an incoming edge C, which gives us this one here. And if we look at A, A has an outgoing edge C, which gives the minus. C which gives the minus one and it has an incoming edge A, no, B, sorry, incoming edge is B, but here the sign or here the restriction map is multiplication by minus one, so we get the minus one here, and this minus is precisely this minus over here. So we end up with a co-boundary map of full rank, and this tells us that the zeros and the first sheaf cohomology. And the first sheaf cohomology vanishes. And yeah, this is not what we have expected from cellular cohomology, because the cellular cohomology of this thing, which is basically a circle, would have R in both components. And so that shows that they don't necessarily have to agree. In general, the co-boundary map is a block matrix where the columns are indexed by Columns are indexed by the vertices and the rows by the edges, as we've seen it here, but here all the blocks were one by one. And if we would swap the orientation of an edge, that would change the orientation of certain rows of this matrix. But doing this has no influence on the kernel of the matrix. So actually, the cohomology is independent of the orientation that we've chosen at the beginning. We've chosen at the beginning. And so, what does it mean to be in the kernel of the co-boundary map? Well, if a code chain is in the kernel of the co-boundary map, then this code chain, after applying the co-boundary map, has zero contribution in all the edge stalks. And if we look on this equation edge-wise, this is precisely the same condition as being a global sector. As being a global section. So actually, the zeros cohomology group is precisely the space of global sections. And if we look at the constant sheaf, which is R everywhere, then we will get our cellular cohomology back. And in particular, in this case, the co-boundary map is the transpose of the Is the transpose of the so-called incidence matrix. And the incidence matrix is a map for a graph from the edges to the vertices. So that the entry is plus one if the vertex is the end of an edge. It's minus one if the vertex is the start point of the edge, and otherwise it's zero. Another concept from topology that we will Will import here is the relative cohomology. So, relative to a certain subgraph of our graph. We define the relative cohomology complexes or the chain complexes by taking the direct sum over all the stalks over vertices that do not belong to the subgraph. And we do the same for the edges. So, here we sum over all the edges that do not belong. So here we sum over all the edges that do not belong to the subgraph. And we define the relative co-boundary map by restricting our original co-boundary map to the corresponding spaces here. What that means concretely, if you have this block matrix here, you delete all the columns that belong to vertices of the subgraph. And then in the rows that belong to edges of the subgraph. That belong to edges of the subgraph, there are only zeros. So you would also delete these rows then. And yeah, then you get this one over here. And this construction gives us a so-called short exact sequence of chain complexes. So the star can be zero or one. And the sequence here is exact, meaning that the image of a map is always the kernel of the following map. Of the following map. And in this case, the maps here are the inclusion map, and this is here the restriction. And also, these horizontal maps over here, they commute with the vertical co-boundary maps, just by the way we have defined the relative co-boundary map. And from homological algebra, we know that such a short exact sequence of chains A short exact sequence of chain complexes gives us a long exact sequence of cohomology groups. So this long exact sequence starts like this. And it gives a nice interpretation of this relative cohomology groups. Namely, they are the obstruction for global sections of the whole graph being in bijection with global sections of the subgraph. For a long exact sequence, being a Long exact sequence being a bijection here is equivalent to the vanishing of these two groups. And this map here is just, we take the restriction of a global section of the graph to global section of the subgraph. And to see what that means concretely, in a concrete example, let's go back to the water pipes. And a very natural question to ask. And a very natural question to ask in this context is: well, what are optimal nodes to measure the system? So we want to know the flows everywhere in the system, but we want to make as few measurements as necessary. So let's assume we only measure at nodes because that gives us strictly more information than measuring at the pipe. And that means that we are looking for a subgraph. Or a subset of the set of nodes, such that the global sections on this subgraph are ideally in one-to-one correspondence to the global section of the whole sheaf, because then we could, from a measurement, we could get the values everywhere. And by what we've just seen, this is equivalent to looking for a subgraph for which. For a subgraph for which these two relative cohomology groups vanish. Okay. And another interesting object to consider is a version of the Graph Laplacian that is related to the sheaf. And so recall that the Graph Laplacian, or the usual unweighted Graf Laplacian, is just the product of the incidence matrix with its transpose. Transposed. And we also saw that for the constant sheaf, the incidence matrix was the transpose of the co-boundary map. So it is a very natural idea to define the sheaf Laplacian by taking the transpose of the co-boundary map times the co-boundary map itself. So in the case of the constant sheaf, it agrees with the graph Laplacian. And in general, it is a map from the zero code chains to the zero code chains. Code chains to the zero code chains. And just as the co-boundary map itself, the sheaf Laplacian has block structure. And this time, the blocks in both directions are indexed by the vertices of the graph. And the blocks have a similar shape as the entries in the explicit formula for the Graph Laplace. Namely, if the two vertices agree, then we get the sum over all edges that have this vertex as an end point over going from the vertex to the edge stock with the restriction map and then going back with its transposed. If the vertices are different, but there is an edge in between them, then we go from the other vertex. From the other vertex, via the restriction map to the edge, and then with the transposed back to our vertex v, and we add the minus. And if the two vertices have no edge in common, then it's just zero. And this block structure tells us that the Schieff-Laplacian is also independent of the chosen orientation, because switching the orientation of an edge would, whenever this edge appears here, would give us a sign. Here would give us a sign minus one, but this edge always appears in pairs. So actually, all these signs minus one cancel. And like we do it in analysis, we call elements in the kernel of the sheaf Laplacian harmonic zero code chains. And it's a fact from linear algebra that the kernel of a matrix transposed times that matrix is the same as the kernel of the matrix itself. So, actually, the harmonic zero code chains are the one in the kernel of the co-boundary map. And we just saw before that these are the global sections of our sheaf. Okay, and for the second part of the talk, I would like to apply the theory of sheaves on grass to describe how opinions in social Opinions in social networks can change over time. And those of you who don't like social networks like Facebook, you can just think of the social structure of society. The ideas presented here were developed by Jacob Henson and Robert Greist. And of course, we will model our social network as a graph with the vertex set being the persons and the edges being connections. Edges being connections between these persons. And opinions and discourse will be modeled as a sheaf, the so-called discourse sheaf, which is schematically drawn down here. So the stalk over each vertex corresponds to the so-called opinion space of that person. And this opinion space is some Rn, where we say that an orthonormal basis of the space. Basis of the space corresponds to basic topics that this person cares about. And in every of these dimensions, having a positive value means agreement with the topic. For example, the person likes a certain politician. And having a negative value, and the more negative it is, the more the person dislikes the politician or disagrees with the topic. And edges stand for. Edges stand for discussions between people, and the edge stock is the so-called discourse space, which is also an RN. And here the basis are basic topics that the two people could discuss about. But these basic topics of discussion do not necessarily have to do anything with the basic opinions. For example, I like chess. Example, I like chess and I like Italy, and I could talk to a friend about Venice, then I will probably like Venice too, or I don't, because in fact, everyone can have a very unique way to project their opinion onto the topics of discussion. And this unique way is given by the restriction maps, which are the maps over here and over here. And on every edge, there is consensus. There is consensus if the projections of the two opinions to this edge agree. So then, on the discussion space, one could not tell the two original opinions apart from each other. And so, in this setting, global sections have consensus in all discussions. So, they correspond to a harmonic situation in two respects. So, now we've seen how to describe. We've seen how to describe opinions and their discussion, but probably the most important bit is to describe how the opinions change. And there are many models for this. Maybe the simplest approach is to say that a certain agent changes the opinion towards the average opinion of their friends. So let's first have a look at only one edge. And we look how a person And we look how a person V changes her opinion due to the friend U with whom she's connected via the edge. So V can measure their disagreement on the edge, which is given by the difference in the projection on the edge. And she can compare this disagreement with, well, for a chosen basis of her opinion space. Her opinion space, she can compare this difference with her projection of all the basis opinions onto that edge. And yeah, this comparison works via the inner product. And depending on how much they agree, she changes her opinion into the direction of this basis opinion here. And then we have this alpha, which is just a global constant that describes how quick opinions change. Quick opinions change in general. And we can modify this a little bit. We can move this to the other side of the inner product and transpose it. And then we are left with this sum here over the orthonormal basis, but this is just the identity, so we can omit it. So we obtain this evolution equation. And now we can. And now we can add this over all the neighbors that the person or all the friends that the person V has. And then we make a vector equation out of it by looking at all persons V in the network simultaneously. And then we get this dynamics down here. And by the formula for the Schieff-Laplacian that we've seen before, this is just minus the Schieff-Laplacian times the opinion vector x. X. And because that reminds of the heat equation in physics, which says that x dot is proportional to the Laplacian of X, we can call it a heat equation too. And Hansen and Greist solved this heat equation for a starting opinion distribution X of zero. And they've shown that after After a long time, this starting distribution gets projected onto the space of harmonic opinions. And the proof is not too difficult. So first, we can observe that the Schieff-Laplacian is a symmetric and positive semi-definite matrix. So we can diagonalize it in its eigenbasis, like here. And then the general solution of And then the general solution of an evolution which looks like x dot is a matrix times x is the one over here which involves the exponential of the matrix. But in this case, the exponential only acts on the diagonal part. And this diagonal part has some zero eigenvalues and some positive ones. And if this t goes to infinity and it contributes Infinity and the contribution of all the positive eigenvalues gets zero. So what we are left is with is the orthogonal projection of this x0 onto the zero eigenspace of the Schieff Laplacian. And zero eigenspace is just another word for the kernel. So what the head shows is that after waiting for a long time, everything would end in half. Time, everything would end in harmony. But we all know that this is not what happens in the real world. So, for example, there could be some stub-born people who don't want to change their opinion at all. And we can model them by saying that, well, a subset of all people is stop-born. So we modify the heat equation by leaving it as it was for the non-stub-borne people, but setting the change of. People, but setting the change of opinion to zero for the stubborn ones. And Hansen and Christ have shown, first of all, that for every opinion distribution on the set of Stubborn people, we can find a harmonic extension, meaning an extension to the opinion of all the people, such that the opinion of all the non-stubborn The opinion of all the non-stopborn people is the average of the opinion of their neighbors. So, all these non-stopborn people won't change their opinion anymore. Basically, this is as harmonic as we can get when considering that some people are stopborn. And they also showed that when this relative homology group here vanishes, then this harmonic extension is unique. And also, for every starting configuration, Starting configuration, the heat equation over here converges to a harmonic extension of the starting configuration restricted to the space of stubborn people. So that means that stubbornness is no obstruction to getting harmony in the end. Or in other words, the harmony kind of adapts to the stubborn people, which gives them some power. them gives them some power right so in particular we could we could use this um to to control the network in a certain sense first we can measure it by assuming we can measure the the opinion of a set of people for which this relative cohomology group here vanishes um then by this uniqueness condition here we know that Condition here, we know that assuming we waited long enough so that the system is harmonic in the end, then we actually know all the opinions of all the people because there's only a unique extension of the opinions on that space that we've measured. And we can also control the system in the sense of influencing it. So, again, assume we could control such a set view of people, which People, which gives a vanishing relative hormonal group here, then we could steer the system to the harmonic extension of these opinions on who that we like best. So it's not a direct control of every single opinion, but still a certain control which is possible. But in particular, we've seen that the stub-born people could not explain. Born people could not explain polarization of opinions. But here's an approach that can. So, the model is called bounded confidence, and it is a prevalent model in opinion dynamics. The idea is if the opinion of two people is too different, then these people just don't talk to each other anymore. So, we can model this by introducing a threshold for every edge. Using a threshold for every edge, and then a bump function, which is a function from the non-negative reals to the reals that strictly falls from zero to that threshold and vanishes for values bigger than the threshold. So, like the function you can see over here. And then we include this term here into our dynamics, and this term makes. Makes the following. So, if the difference in opinion on that edge E is the threshold or bigger than the threshold, then this edge is completely ignored. The effect of this edge is set to zero. And the closer the difference in opinion gets to the threshold, the smaller is the influence of the edge. And Hansen and Christ showed that the stable points. Showed that the stable points of this modified dynamics are those opinion distributions for which on every edge we have either complete agreement, so that the differences in projection is zero, or we have complete disagreement, and so that the people wouldn't talk to each other anymore. And they also showed that starting sufficiently close. Starting sufficiently close to such a stable configuration, the system will converge to that configuration. Maybe a quick note on the proof here. So the non-linearity that comes from this bump function makes it impossible to solve it explicitly, the dynamics, as we've seen for the basic model, but we can still get this qualitative result. Result. And this result tells that very polarized opinions can survive in the system. And if they do, then they will split the system into clusters of mutual agreement within the cluster and not talking anymore to people of a different cluster. A completely different approach to extend the basic model. Extend the basic model is to assume that the agents can not only change their opinion, but also their way to express the opinion in the form of these projection maps here. So again, we can assume that they change them in order to minimize this difference. So they would change it. Such that their projection onto the edge of their own opinion goes in the direction of the difference in opinion on the edge. So, basically, having this over here is the same as having the opinion of V on this side and assuming that the opinion of V just does not change. Okay, and if we combine and this beta again. And this beta again is just a global constant that tells how quick these restriction maps change. And if we combine this dynamics with the basic dynamics from before, then we have quite many parameters and we will already in a very small system observe interesting behavior. So assume we only have one edge and two people and all the stalks are And all the stalks are one-dimensional. So, person U at the beginning has a positive opinion on a certain topic, and person V has a weaker negative opinion. And at the beginning, both the projection maps are only multiplication by one. And we can set the time constants to be one per hour. Then, Mathematica calculated. Then Mathematica calculated the following dynamics on the right here. So, in blue and red, you can see that the opinions get weaker over time. And also the way of person V, no, person U, the green one of person U to express the opinion gets weaker over time. But for person V, it gets so much weaker that it actually turns into a lie. Turns into a lie at this point, right? Then it's multiplication by a negative number. So person V starts to lie to get in agreement with you. And after the switch to a lie, the opinions slowly adjust to equilibrium. But I am not going to lie about this talk, it's coming to an end. So let me conclude what we've seen. So we saw how. So we saw how to define sheaves on graphs and their sections and relative cohomology. And all these concepts can be generalized to cellular complexes. We just have to be a bit more careful with the science when defining the co-boundary map. And we also saw Sheaves describing conservation, a conservation law for a system of water pipes, and also how to phrase sampling problems in the language of sheaves. In the language of sheaves. And more work in this direction has been done by Michael Robinson, for example. And we've seen the sheaf Laplacian. And also, we experienced the flexibility of the discourse sheaf in modeling several approaches to opinion dynamics. And with that, I thank you all for listening. And now I'm open for your opinions and the discussion with you. Yeah, the the sources that I looked at. Okay, questions? So I have a question about the opinion dynamics model. I was curious if, yeah, so it was the model before people were able to lie, but where they did sort of stop communicating if their opinions were too different. I was curious in that model, if you imagine. In that model, if you imagine driving it with some sort of like news source, right? So people's opinions are informed by some outsized news source for somebody's. Is there anything you can say about the stability of the different clusters that form or on average, maybe the number of clusters you get or their sizes? Is there any way of talking about the sort of pattern formation in this heads of some way for I get um I think I have I think I haven't seen anything in this direction, but I mean, I would imagine that I could imagine that if the difference on an edge is already big enough, then somehow it will usually never get small enough. So, if the edge was vanishing once, then it probably will be vanishing for forever, more or less. But yeah, no, I don't know. Yeah, no, I don't know. Maybe we can discuss that later. Other questions? Wouldn't my global node source be sort of represented by a node that's connected to all the other nodes? It really couldn't change it to. Yeah. Was just a question, or I can't really hear the people in the room. I was just wondering if the global news source could be represented as a node that's connected, a stubborn node that's connected to all the other nodes. Okay, what can be what can be regarded? What can be a news outlet, like a television? Ah, yeah, that sounds interesting. Okay, well, I suppose if there aren't any other questions, any questions on Slack, we can check there. But let's thank Barton again for next one. Okay, so then we'll break now for coffee and we'll be back at 10:30. 