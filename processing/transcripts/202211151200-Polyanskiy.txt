Should I wear the mic or not? I forgot. I don't need to. Okay. Right. So, first of all, I wanted to say that this is my first trip since COVID, and I'm very glad to break my fast for this great workshop. So, thanks a lot for inviting me. Sorry, I ran off. Okay, so this is a joint work with this guy, Qian Yu, who's a postdoc at Princeton. Postdoc at Princeton. And right, so let me get started right away. So I'll start it as a sort of math talk and I'll tell you later about some motivations. Okay, so mathematically speaking, what we will study is the following operator acting on measures. So we'll fix a probability measure mu on a real line, and then what we will do is we'll generate DIAD copies from this measure mu. From this measure, and apply some nonlinearity to them, and then sum them with random signs. So, that's an operation. And okay, so everything is independent, and this nonlinearity is a strange one. It's wisely arctangent of multiple of a hyperbolic tangent, right? So, it doesn't express you. I think many of you guys have seen this. This, uh, this nonlinearity looks like this, uh, you know, so some functions. So again, so through purely mathematical point of view, we can just you know define this operator, right, and not worry about its prop how it occurs. So what's uh oh yeah, and so throughout the talk, I will mention that you know there are some extensions to this to this basic version. So this basic version has fixed d, for example, d equals two. For example, d equals 2 is already very interesting. And so sometimes we'll take the plots on. We can also sometimes consider the case of what we call BP with survey. And so survey corresponds to summing this alternative of non-linear transformations, and then adding some constant to it, which is generated again independently from some fixed measure mu zero. So all of this will be denoted by this operator q, right? By this operator q, right? So you have to just know what I'm talking about from the context. But this is the most general form, right? These randomly add some constant term. Okay, right. So the main question in this business of belief propagation is to characterize fixed points, namely solutions to Qu equals U. Okay, so let's first consider the basic setting, right, where L0 is zero. Okay, so then you Okay, so then you can see that if you, because that non-linearity was passing through zero, right, if you take your measure to be delta zero, and all these terms are zero, right, so delta zero is a fixed point of this operator, right? So we call it a trivial fixed point, right? So the big question is, are there other fixed points, right? And how many? Okay, so the main result, and I mean, I really like giving this talk because this is really, that's it. So like, that's the end. There is no. That's it, so like that's the end. There is no nothing, I'm not hiding anything. This is the complete story, right? So, this is the full theorem. So, it said the following: so, in addition to delta zero, there can be at most one extra fixed point, right? And not only that, but if you start from any other measure, mu, you are guaranteed to converge to mu star, right? So, from any non-trivial initialization, the iterations converge to this fixed point. Okay, so that's so that's the Okay, so that's so that's the end of sort of of the of the statement. So let me so what I what I will try to do is I will try to tell you how it resolves some conjectures. Yes. What does it mean if there's no fixed point? There could be no non-trivial fixed points, right? The trivial fixed point would be the only one, right? And so what if there's no non-trivial fixed point? Right. So then every inch. Right, so then every initialization just converges to trivial. Yeah, okay, so right, okay, so this resolves multiple conjectures, right? So I'll explain them at the end when I describe the proof of this result. So I guess the earliest one, I think, is by Jamie, actually, in his work on Elkanal, where they conjectured certain Where they conjecture certain property called optimality of local algorithms, and this implies that it also implies lots of other things, including robust reconstruction, conjecture from also New York and slice, if I didn't include it here, and so on. So now I put these two words with asterisks. Why? Because they actually proved a partial version of this result. So this result is cool, but you know, it's not like it appeared in vacuum, right? So there was partial. Appeared in vacuum, right? So there was partial sort of progress, which already established this, this theorem under the condition that one minus two delta squared times the d, where delta is parameter of these operators, right, is much bigger than one. Okay, but what's interesting is that, so I participated in this work, right, which improves this. So okay, this work, I think, proves it under this product being bigger than 75, and then we dropped it to 3.5. To 3.5 there by what looks like a completely different method. But what I wanted to say today is that, so, I mean, I spoke with Helkinal, with Alan Sly, and I mean, also, of course, I mean, I spoke with authors of this, right? So all of us believe that, you know, the techniques in those works generally are incapable of resolving the full conjecture. So what I want to start with, I want to just tell you a brief hint about how those things were proven. Brief hint about how those things were proven because my proof was will literally be two slides. So, and I'll show you those two slides, right? I just wanted to convince you. So, maybe can I have some light on the board just for a second? So is there anybody in light? Can I put some light here? Light here? Or maybe there? I mean, maybe there is better. Yeah, I think it's better. Yeah, yeah, this is great. Yes, thank you. Yeah, so I don't know. I don't know. Yeah. Yeah, I know we are before lunch, and I think I'll, you know, my second part of the talk is very adaptive, so we'll finish on time. Okay, so how does this work? Right, so let me first show you one trivial proof. So this is my f delta function, right? So its maximal slope is f0, and this slope is one minus two delta. two delta right so here's a here's a very easy proof right so if one minus delta d is less than one right so let's look at something interesting here so suppose you start with some measure whose support is this minus a a right so you start with the measure which is supported by minus a a right so then after you apply f delta its support drops down to one minus two delta a right and then when you apply And then, when you apply this operator d times, when you sum d of these guys, right, the support at most becomes one minus two delta, right, d times a, right? So if this is less than one, then this is less than a, right? And you see that supports of the measure, right, just iteratively go to zero, right? So this means not in this regime, right, the convergence, like iterating applications of Q, right, will converge to. Q right will converge to delta at zero, and of course, every fixed point will be delta at zero, right? So, this is a contraction proof, right? But it works in a very useless regime, right? So, this is like a weak, uh, weak regime. So, we know that there are no non-trivial fixed points in this case, right, for one minus two delta squared times d less than one, right? So, also, like, when, right, so then delta zero is a unique fixed point. So, this method does. So, this method doesn't work for that case, right? So, there are other tricks, and the methods in those two asterisk papers roughly, I mean, I'm exaggerating significantly. So, if you know that work, I mean, Jamie knows that work so very well, right? So, so roughly speaking, I mean, when one minus two delta square is significantly bigger, this is the SNR parameter in SBM, right? When this is significantly bigger than one, then you can show that any fixed point should have a giant support. Should have a giant support on high R's, right? So giant measure on large R's, right? Large value of. And then you can see that in a large R's, this function is already very contractive, right? Because the slope is much lower, right? The maximal slope is achieved here. So roughly speaking, that's how those methods operate. They first prove that for high SNR, you have any fixed point measure will leave, you know, will have a significant mass on high values. Significant mass on high values, right? And then on high values, this F delta function is very contractive. So if you couple two different measures, right, after application of F delta, they will, you know, come closer in coupling, right? And that's why sort of, so our method doesn't do anything like this, right? And that's why, I mean, we and those guys all believe that, you know, there's probably no way of a nice proof along these lines for, you know, going all the way down to one, right? Yeah, and all of a work by constructing some, you know, Work by constructing some, you know, elegant potential and showing that it contracts, blah blah blah. So, anyway, but the rough idea was this: so, you will see nothing like this in this work. When you say uniqueness, do you mean you can check to delta zero or it's it's the other one is unique? Yeah, the other one is unique. Yes, so yeah, not uniqueness, but uh, yes, not more than two, right? But it's because I mean, this regime is useless, yeah, right. So, when this is bigger than one, then we know there is at least two, yeah, right, and then what this proves that. Yeah, right, and then what this proves is that there's actually two, yeah, right? So that's yeah, that's the thing, okay. Uh, okay, is there a known criteria for existence of non-trivial? Yes, it's one minus two delta squared d equals one. Is it less than one? There's only trivial, and if it's bigger than one, there's definitely exists a non-trivial one. Yeah. So it's an if and only if. Okay, so right. And the fun part is that this actually, the proof is going to be very information. This actually, the proof is going to be very information-the ready, very soft proof, okay? So, and I mean, this the natural question, right? So, when I gave this talk, I first gave this talk to physicists, right? And so, this was my sequence, how I convinced them that they should listen to this proof, right? I said, okay, there's statistical physics, stochastic model, and also there's communication. But today I'll start and I'll do this application in reverse order because I don't plan to finish on time. And so, I want to write it. So, the most important thing is the third one, because that's how. Is the third one because that's how the proof goes, okay? So let's uh let's start. Okay, so I'm going to describe broadcasting on trees. I think majority of you know it, but let me still refresh it in your mind, right, a little bit. So we start with the tree, right? And then we inject some Bernoulli one-half random variable at the top, right? And then every time you see a junction like this in my talk, this just means hopping the p, right? The beat, right? And in this case, you copy the beat, right? And then as it traverses this edge, it can get flipped with probability delta, right? So you start here, and then the beat arrives here, maybe potentially having flip, right? And this process continues like this, right? So after depth K, after K iterations, you get two to the K variables, right? And sort of as a communication theorist or information theorist, right, you're interested in like, okay, in this propagation. In this, in this propagation downwards, right, there are two opposing forces. One force copies information, so kind of works for preservation of some information, right? And then there is noise which kills information, right? And the fun question is always to balance this, right? Okay, so let's go ahead. Right, so forget, don't worry about what the branching number is, right? So this is the goal here, right? And the communication goal or broadcasting on trees goals, knowing the bottom, you can try. Freeze loss knowing the bottom reconstruct the top, right? So that's uh, that's that's the question. Okay, so, um, right, okay, so how do you solve? I mean, you guys all know the rule, right, that if you're on a tree, belief propagation is exactly optimal, right? So that's computationally very easy. So just run belief propagation, right? So let's do this, right? So what does belief propagation propagate? It propagates this global likelihood ratios, right? So xk, I think. Right, so xk, I think I denote the knowledge of layer k, right? All the variables at the level k, right? So you can, if you know all the bottom guys, right, you can compute posterior ratio of posteriors, right? And this is what I call random variable r of k, right? So, so given the bottom, you can upward define this r of k, right? Random variables, okay, and mu k, in my thought, will always be the law of this low likelihood ratio condition. Of this log likelihood ratio conditioned on the root being plus one, right? So it's slightly skewed, right? So like this ratio that slightly skewed towards the plus, right? Because you're conditioning on the plus, right? Yeah. Oh, and this is how you get this belief, the strange belief propagation recursion, because if you analyze how these equations work, right, you will see that Rk plus one is given exactly the, I mean, distribution of Rk plus one is given by this by this. One is given by this by this formula, right? Um, okay, and that's so right. So that's my definition, right? So I call mu k plus one is the law of rk plus one, and the operator of going from mu k to mu k plus one is my q, right, my pp operator, right? So that's uh, so that's uh that's the story here. Yeah. Right. Oh, actually, yeah, so there's some kind of martingale argument here because you're progressively conditioning on the bottom. We're progressively conditioning on the bottom, right? So we can show that this actual iteration always converges to something which is a fixed point of dp. But I don't think we need this for the talk. Okay, so okay, let me also define a variation of this problem, which is known as a robust reconstruction on trees. I think Miki mentioned some robustness. I was thinking that maybe you're mentioning this kind of robustness. I don't know. Maybe it's related. It's right. So, anyway, so. Right, so anyway, so so so what is robust reconstruction? So now, so before we were trying to reconstruct the root, given all the bottom, right now, suppose I only give you the bottom with some noise, right? With some noise tau, right? So you only know the bottom with some noise, and you still want to reconstruct the root. Of course, your reconstruction will be worse, right? But the question is how much worse, right? So this is the question of robust reconstruction, right? So you can see. Right, so you can see that log likelihood ratio. Again, I mean, you can again do Bt, right? And here, the measure, the distribution of log likelihood, is going to be this very same operator, but initialized at mu zero tilde as opposed to delta at infinity, as we as before. So, right, so where mu zero is like this measure, you know, which I mean, written here, right? It's a bunch of delta functions related to this noisiness. Related to this noisiness, right? So you can see that the only difference in broadcasting on trees and robust broadcast and robust reconstruction on trees, right, is that in the vanilla version, we start with mu, which is delta at infinity, right? Notice that there is no problem with infinity because this function is bounded, right? So you can apply f delta of infinity easily, right? So, right, and for robust reconstruction, you start with something that is large if tau is small, but is not infinite. If tau is small, but it's not infinity. So that's the only difference. Now you can immediately see, right, if you believe in my main theorem, right, that something funny happens, right? Because, I mean, this iteration will converge to the unique non-trivial fixed point, right? Which is the same, right? For regardless of the starting initial condition, right? So this is the first conjecture, which was numerically and partially provably verified by Mosul and even Sly. This was somewhat. Right, this was somewhat of a interesting, shocking kind of situation, right? That no matter how noisy your bottom observations are, the reconstructions of the root is not going to be affected, right? So it's a very kind of strange thing, yes? Except if tau is exactly one half, right? Exactly, yes. Because then right, yes. So that's exactly right, yes. So, but other than that, right, so I mean, I don't know, the first time I read it, I thought that I'm misunderstanding something. That I thought that I'm misunderstanding something, right? So, how can this be, right? So, for any finite depth, right, tau does make a difference, right? But for infinite depth, somehow, you know, even if you know these correlations, you know, a little, you know, with high noise, you still reconstruct as if you knew them perfectly. Okay, so, right, so that's what I already described. Okay, so let's talk about proof ideas. As I said, I mean, the proof is going to be really a couple of slides, plus one check, which I'm not going to do, but I will tell you how. Check which I'm not going to do, but I will tell you how to do it. Um, it's not the kind of check that we was advertising, it's not a 30-page, uh, you know, uh, it's something you know, easy, I would say. Okay, so, but for that, I need to tell you about something that maybe much, a lot fewer people in the room are know about, which is the concept of BMS channel. Let me actually ask you: who knows what the BMS channel is here? Okay, one, two, three. Okay, so that's a good time. Okay, yeah, so yeah, because this is how we rephrase the problem, and then for me, it becomes much more manageable when I understand this, right? Okay, so to understand this problem, okay, so this is a channel which takes binary input and produces some arbitrary crazy output, right? And we call it BMS. Forget about the acronym, it is meaningless by now, but so. But so we call this a random channel BMS. There exists an involution of the y-space, right? Which, if you apply it to the minus one, right, and apply involution, it's the same as flipping the input, right? So like the canonical example would be x is plus minus one and y is x plus z, where z is gauge, right? Then you just flip the sign of the observation, it's equivalent to flipping the sign of the input, right? So that's what we call BR. Input right, so that's what we call BMS channel, and oh, yeah, the most canonical examples, of course, binary symmetric channel, right, where you where y is the minus one times uh the ruler delta times x, right? So we flick x with probability delta, right? So that's okay, and I will denote this as so. Bruce was telling us a story yesterday about how he traveled in the Soviet Union. And I know one story that the Brush always made fun of the American information theorists that they cannot give a talk without. Information theorist that they cannot give a talk without drawing a box and connecting it by an arrow. So I'm definitely verifying this. So, yeah, so somehow for me, it will be very useful to draw these boxes and connect them by arrows. Okay, so okay, so what is the three channel of, yeah, so the second example of BMS channel is the tree channel, right? It's a channel which takes this input, does this very complicated iterative process, and outputs two to the k spins, right? Now, why space? Right now, Y space is a little bit more complicated, but you can still see right flipping the bottom is equivalent to flipping the top, right? So that's again a BMS channel, and I will denote this as WK. Okay, so right. Okay, so another thing I need to tell you, and this is a little bit right, so yeah, I mean, information theorists like to think about channels, right? Draw these boxes and also kind of try to understand which channels are best. Kind of try to understand which channels are better and which channels are worse. So, for that, we have various notions. And this is the notion which, I mean, predates, I think, information theory, right? So, it's defined by Blackhawk for statistics purposes, right? So, we will say that one channel is better. So, Pz given X is better than Py given X, if you can factor, if you can represent X going to Y by first X going to Z and then adding some more stuff, adding more noise, right? So it's more noisy in a very general. Are more noisy in a very generic sense, right? So it's like general PZ UNX is really much better because you can just simulate y right from x, right? So this is the very, and it's called black hole pre-order. And I will just denote this, you know, partial order. Okay, so for example, we can clearly see that wk plus one is a worse channel than wk, right? Because if you know layer k, you can always simulate layer k plus one, right? Okay. Right. So Right, so how can we compactly represent a BMS channel? That's the next question here. Okay, so you can, you can, you know, right, that if you have y and your job is to, you know, decode x, you don't actually need to know y, right? The only thing you need to know is its log likelihood ratio, right? It's a sufficient statistics, right? So this means immediately, right, that every channel with arbitrary crazy y, right, this could be space of functions or something, right? Or like this 2 to the case pins, right? 2 to the k spins, right? And or okay, actually, for stochastic block model, the channel outputs the bottom of the spins, right, plus the random graph that connects to them, right? So it's a much more nasty random sub-tree, right? So it's even nastier channel, right? So it's like its output space is graphs, right? 2 to the k spins, right? Or not 2 to the k even, right? Variable number of spins at the bottom. Variable number of spins at the bottom, right? So, but you can always think of all these channels as stating real values at the output because you can always compute sufficient statistics, right? Okay, so that's that's enough. So, there are lots of these kind of simple steps which somehow together will combine into a two-slide proof, right? That's what I'm trying to advocate here, right? So, oh yeah, and because of symmetry, you don't really need to know the distribution of a sorry, is that no worries. No worries. Okay. Yeah, so right, so you only need to compute the distribution of R under one of the plus one or minus one, and then you can reconstruct the rest from the symmetry, right? Okay, so right, so what I will denote, right? So for every channel, I can define this mu, right, which is the law of R under the input plus one, right? Okay, so that's very good, right? So from now on, whenever I talk about DMS channel, right, I will just replace it by this memory. Right, I will just replace it by this measure, right? Which is a posterior distribution of like likelihoods when the input is plus one, right? Okay, so yeah, so there is some symmetry condition like this measure is not arbitrary. The existence of involution implies that when you push forward along with the minus one, minus one, right along with axis flip over r, this measure transforms like this. Anyway, so um, that's not very important, but uh. Not very important, but uh right, and I would now denote my box by box with mu written inside, right? So, box with mu written inside means it's a BMS channel whose mu law, you know, is right, it's new. Okay, so that's uh that's the thing, okay. So, now let's let's look at those three channels, right? And let me explain how connecting boxes and arrows helps us here, right? So, what I'm trying to do is I will try to tell you how WK plus one channel can be built from WK channels. Channel can be built from WK channels. So notice again that's right. So if I already understood WK, yes. Oh, sorry, sorry, I just need a way of thinking. So in this kind of encoding, the from the previous slide? Yeah, from the previous slide, one way, one place where, like, the size, the kind of complexity of this state space Y, script Y, you can see it in, I guess, kind of the support. Yes. Yes, it becomes super nasty and super. Yes, it becomes super nasty and super crazy. Some fractal thing, yes, yeah. And for fixed point, it's going to be some undescribable fractal thing, right? But it's uh, it's still just a measure on real numbers, right? No matter what the channel measure is. And a probability measure, right? Yes. Right. So, I mean, yeah, so you're right. It's a secret way of like, you know, sweeping complexity under the rug a little bit, but yeah. Okay, so right. So, so, in particular, right, so now this. So, in particular, right, so now this drawing box becomes very convenient because, right, so if you already understand WK channel, right, for depth K3, then building WK plus one channel is very easy, right? So, you just put this WK, you say, okay, so now when my input is this, I pass it through BSC delta, and then I pass it through this WK channels, right? And that's what WK plus one is, right? So that's a cool way. Okay, so now, right, so now we can also understand the V operator. We can also understand BB operator q mu right in a in a in a very natural graphical way. Okay, let's see if I want right. So, so the bb operator, okay, let's again let's ignore the survey. I will tell you how to add survey here, but now so for every new which is symmetric, there is a BMS channel, right? Symmetric means that condition new minus dr equals e to the minus r nu dr is verified, right? So, for any mu which is symmetric, there is no mmm. So, for any mu which is symmetric, there's a BMS channel, right? And Qmu actually preserves symmetry, right? So, this means for every mu, you can get the qmu BMS channel, right? And how do you visually think of it? Well, what qmu does is it takes this potentially insane, insanely complicated BMS channel, right, and produces a new one by this operation, right? Copy bit, pass it through two independent BSCs, and then pass it through the crazy channels, right? Here you get a crazy plus one channel, right? Crazy plus one channel, right? So, right, and survey corresponds. So, what so what does this plus L0 mean? This means you also observe these guys on the side through some other VMS channel, right? So, in a stochastic block model, what it will correspond to is that we're doing reconstruction where you also observe each label through some noisy channel, right? So, that's that's the right, so that's the visual picture. Picture okay, so ah, right. So, I forgot to mention this in the beginning, right? So, remember that, okay, so this is where I get excited, right? I mean, for me, the, you know, for maybe for number theorists, the number is the basic object. For me, channel is a basic object, right? So, this, the fixed point of this, means there exists some secret channels, right, which have this amazingly awesome property, right? What is Q mu equals mu? This means this is a channel, right, which you insert here, you copy it twice, you pass for BS. You open it twice, you pass through BSC, right? And you get, to your amazement, exactly the same channel back, right? So it's kind of like infinite divisibility, right? And then you can actually see that, you know, solving fixed point equations of this type is, you know, if you take this function to be linear, right, and you pick some weights, this is exactly infinite divisibility in the canonical sense, right? So this is like infinite divisibility of this kind of operation, right? So if you're an information. I, you know, this, if you're an information theorist, this is one way to understand these fixed points, right? This is such an awesome channel, right? It channels this self-stability property. Okay. Right. Okay. So now let me tell you about the stringy tree lemma. Okay, so the stringy tree lemma was proved in this paper by Evan Skinny and Paris Schulman. What does it say? So in a graphical language, it says the following, right? So it says that if you take two BMF. Following, right? So it says that if you take two BMS channels, right, and you first flip the beat and then pass two independent copies from new and new, right? This is definitely worse than first copying the beat and applying noise independently, right? Which sounds very natural, right? Because here you kind of lost information in both arms, right? If the flip occurred, first here, you kind of have a chance twice, right? So it sounds very natural. So if you ask. Sounds very natural. So, if you ask me at the end, if I have time at the end, I'll tell you a funny story involving Elkanon about this steamy tree lemma. Oh, I will tell it only after recording stops. It's also actually related to future workload. Okay, so anyway, so there's this theorem, right? So, how did those guys use it? Okay, um, maybe I'm not going to use the board. So, how did they use it? So, they said, okay, let's draw this depth k tree, right? And then Right, and then we can progressively apply this operation, right? So, when you do BSC delta, right, and then the rest here is like the you know the stuff hanging off that edge, right? So, then you can progressively improve the channel, right, by splitting that edge in two edges, right, and adding the SC delta, pushing, making the edge noiseless, but pushing the DSC delta to all the siblings, to all the sorry, children edges, right? So, now think about what happens if you're. So now think about what happens if you apply this operation, you know, 2 to the k times. And what you get is you will get a tree which has like, okay, it has degree 2 to the k at the root, right? And it has depth one, depth one, right? And the channels are BSC delta, BSC, delta, BSC, delta, BSC, delta, k times, right? So now you get the broadcasting through the channel, right? Which is very simple, right? Because it takes two to the K copies, but then applies crazy, noisy, you know. Applies crazy noisy, you know, crazy noisy channel to each copy, right? But that's much easier. You don't need any recursions to analyze that, right? You can just do some union bound, right? So that's how EQPS actually proved that one minus two k squared, one minus two delta squared times d is the fundamental threshold for reconstruction for broadcasting on tree. Okay, so right, so you will see that, so the okay, so this is the main technical discovery that. So, this is the main technical discovery that we did. Okay, so what we found out is that this stringy tree lemma can actually be improved, right? So, in what sense? So we discovered that no matter what non-trivial BMS channels you take here, right, if you pull BSC delta down, you know, down the edges, you always get some improvement. So, you can not only that. Uh, so you can not only that channel is you know worse than this channel, right, without with epsilon equals zero, right? But you can actually even insert BSC epsilon on top, right, for some small epsilon, potentially depending on nu, right? Actually, definitely depending on nuk, right? Okay, so that's the improvement, right? So it clearly you know clearly implies the STL, but it's also slightly stronger because you're showing an upper bound. Stronger because you're showing an upper bound by a noisier channel, right? I guess you might put epsilon on the right. There exists uh oh, never mind. Yeah, what is the BSC epsilon out there? Right, so so so stringy tree lemma, right, says that okay, that's where you put the noise in, right? So, right, so stringy tree lemma says that if you flip once and do decopies, right, it's worse than copying D times and flipping. Than copying d times and flipping independently, right? But we say it's even worse than flipping some tiny probability epsilon first and then applying the previous operation, right? So it's kind of like stronger, stronger version, right? Okay, and so now if you right, so okay, so just to sort of whet your appetite a little bit, this might sound natural, but it's actually wrong. For d equals two, for d equals two, it doesn't work. For d equals three, it works. And for d equals four, it works. And for random d Equals four, it works, and for random d as long as you know probability of d equals bigger than two is positive, it works. So for d equals two, it doesn't work, but it works for two iterations. So if you draw a three of degree two, but it's at depth two, then it applies. So that's uh, so it's not as trivial, you know, as just checking something, right? So it's not very fundamental, uh, something that should be true for, you know, just from general, I don't know, continuity or something, right? So it's a special thing, but I'll tell. So it's a special thing, but I'll tell you about the proof of this as well. Right, so let's now understand how it implies a uniqueness of BP fixed point. Okay, so this is the slide which contains the proof, module that result, and then I'll tell you how that result. Oh, actually, no, this is not, yeah, this is not entirely of the proof. Okay, so let me define this B phi of mu, right? What is it? It's like one minus phi times mu plus phi times u minus dr, right? So it's a kind of convex. Right, so it's a kind of convex combination of mu and its symmetric flip across zero. Okay, and in the sort of boxes and arrows notation, this is just that it corresponds to making a PMS channel, right, which first passes the input from feed, from sweeps the input from probability, and then classes from the PMS new channel, right? So that's the operation, okay? Okay, so now we define this degradation. Okay, so now we define this degradation index. I star. Okay, so you give me two measures, mu and nu, right? There's two channels, BMS channels, right? You can ask the question, okay, mu and it's a partial order, right? So mu and nu can hang and be not comparable, right, in partial order. But then you can start saying, okay, let me now degrade mu progressively by adding more and more noise at its input, right? And eventually, you know, eventually it will land below mu, right? So how do we? Right, so how do I know this? Well, because, for example, if phi is equal to the probability of decoding error under new, right, then already that will be below new, right? So that's why, I mean, certainly if you take phi to be one half, you will be at the bottom of this partial order, right? But what I'm saying is that you will also be there. If new is non-trivial, you will be there before one half, right? Okay, so that's uh Right. Okay, so that's so that's that's the definition. Okay, so okay, so now what I will show next in the next that's why I said the two-slide proof, right? So this is the first slide, right? So let's check how so in the next slide I will show you that the improved STL guarantees the following estimate that actually if you apply the P operator to new and you degrade it by new and you degrade it by phi star you always get something slightly worse than the q nu right so that's uh i mean you can rephrase this as q b phi is strictly smaller than b ph q something like this so sorry the other way around b ph is strictly smaller than q b phi right so there's some kind of when you commute the operation of adding noise with the bp it gets trick decreasing right so that's uh that's Right. So that's, and STL just tells you that there is a non-strict decrease, right? And we prove it's a strict decrease, right? Okay, so if you trust this, right? So then you don't need to do anything else, right? Because if this is a fixed point and this is a fixed point, right? Then you prove that BP star mu is less than epsilon nu now, right? But these two guys are community. You can apply E epsilon inverse on both sides, and suddenly you decrease P star, right, which was defined as the smallest possible. Right, which was defined as the smallest possible degradation, right? So, this means that you know, there, and I mean, we also use the fact that new is not trivial because if new is trivial, then like there's no, right, I mean, everything is, you know, this BT star is going to be one half, right? And then we cannot cancel the edge standard. So that's uh, it cannot be smaller than one half. So that's it. So, you know, so now we only need to check. So now we only need to check this idea, right? So, as I said, I mean, there's some kind of STL, you know, strictly improving is at play here. So, so, so let me prove this key estimate now with a slide. Okay. Oh, yeah, of course, right from symmetry. Right, so oh, yeah, this proves that p star should be zero, right? Uh, yeah, so if Right, yeah, so if p star is right because if p star is non-zero, then we can that's right, right. Okay, so how do we prove this estimate? Okay, so so let's try to understand what it means that boxes and arrows, right? So, what I'm trying, so BCQ mu means this channel, okay, right? So, I flip the bit with probability P and then I apply the usual. Probability p, and then I apply the usual, you know, three recursion with new observations at the bottom, right? Okay, so what do I do? So now I'm I apply our improved STL, right? So I push this box down the edges, but then I gain epsilon on top, right? So this is the improved STL thing. Okay, so this is what we proved so far, right? We proved that Vcqmu is upper bounded by this thing. Bounded by this thing. Okay, so here is where we use something seemingly benign, but very crucial, right? This binary symmetric channels are commutative, right? So it doesn't matter if you flip by P first and then by delta or by delta first and then by, right? So it's the same operation. Yeah, so we do it like this, right? So then let's observe what we got. So this block, the S. So, this block, BSCP times nu, right, we know it's nothing else than BV mu, right? And we know if we replace this by nu, we get a better channel, right? So, you know, let's do that, right? So, we did that and we obtained this, which is nothing else than the state, right? So, you know, I think the Brushen would be very appalled by this fact that somehow, you know, as I said, in all the other proofs, right, there are a lot of Right, there are a lot of Dablusion-friendly, you know, derivations and you know, potentials and estimates, but this is somehow very soft, right? So, there is no, okay, as I said, I'm lying, right? Because I didn't tell you how we proved improved STL. So, let me maybe, yeah, that's the end of the proof, right? So, that's the only thing I didn't prove before. And as I said, I mean, the better way to state it is something like, you know, BCU is strictly smaller than UB. And you right, so that's right. No, there's yeah, that's that's it, yeah. Uh, where strictly smaller means you can insert b epsilon on this side, right? Okay, so that's uh right, so that's it, right? So, okay, so yeah, so there is another way, right, to be a little bit more, right? So you can. More right, so you can you can show that with the gradation index, right, it's satisfied triangle inequality in a certain I mean, it's not symmetric to mu and nu, so you can define log one minus two phi star mu nu and log one over two star nu mu, right? And then you add this two, and then this satisfies triangle inequality for symmetric BMS channels, right? Then our proof, what it says is that under this insane distance between channels, right, this PP operator is. PP operator is strictly contracting, right? And there is some old theorem that on compact spaces, strictly contracting operators have to converge to a fixed point, right? Not like if you insert alpha here, then of course, right? But this is like without alpha also, I think. But anyways, it's not needed because I think we prove it without this, without that theorem. Right, so but it's an interesting like open question, right? And in this distance, I think we checked that mitris has weak convergence, but. Check that metric has weak convergence, but uh, other than that, you know, I don't know how to compute it. I don't think we even know like distance within some trivial channels for this, like Gaussian, you know, additive Gaussian noise and BSC, for example. I don't know what the distance is. So it's a strange distance, but you know, it's apparently kind of useful in this. Okay, so let me tell you now a few maybe words about how this is checked. Right, so recall that right, so we define this black hole pre-order, right, where we say, you know, one channel is better, you can simulate that somehow. Okay, so now I need to tell you a few other ingredients, which again would be known to these three guys who know about BMS channels, right? But might come a little bit surprising. So the first fundamental theory of BMS channels that every BMS channel is nothing else in the mixture of BMS channel is nothing else than a mixture of binary symmetric channels. So that's somehow, or in our words, this means that any mu which satisfies that symmetry condition can be written as a mixture of these guys. So when I say mixture of binary symmetric channels, it means we play the following game. You give me a bit and I output to you a random delta and the output of BSC delta. Right, so so I input a bit again, right? Then I actually first generate random delta without looking at the bit, then I take the bit, right? I it passes through BSC delta. I tell you both the delta and the result of BSC delta, right? So this simple construction represents all possible BMS channels, right? So that's so why is it cool? Because now all those fancy inequalities you can only check for Uh, for VSCs, right, and then just apply mixtures, right? Just apply like averaging, right? So, somehow, the key inequalities you can all check for VSC, right? Okay, okay, now the second interesting point is that, so how did STL guys, the original stringy tree lemma, oh, that's I forgot to say, that's why they call it stringy-tree lemma, right? Because after you apply it, it becomes becomes a tree with like long, I think in modern world, we would call them chandeliers. Call them chandeliers. Very fine word. Yes, it's like very depth K, right? And there are two to the K, whatever, candles sitting at the top, right? And they call it stringy tree, right? Right. So how did they prove it? I mean, it's a paper written by Propobulus. They said, okay, this is the definition. We have to construct the coupling, right? And they constructed the coupling by brute force, right? So, as I said, I'll tell you a story about this, right? So, what we did. Tell you a story about this, right? So, what we did is we actually used some analytic criterion without any coupling, right? So, we just used what we call beta curves. So, maybe that's another useful thing. Okay, so what is a beta curve? So, the beta curve is defined as the following. So, you give me a mu, and then I compute this strange function, right? So, it's an expectation of the hyperbolic tangent of the absolute value of r, a minimum of that and t. And t so that's a value which is always bigger than t, right? And in particular, right? So, sorry, what? Right, or maximum. Yeah, I always forget. Yes. Oh, that's, of course, how could I? That's the rate thing in the statistics, right? It's a max. Okay. Right. So, okay. So, but again, for engineers, right, what this means is that this is basically a reparameter. So, what does this beta come? A reparameterization. So, what does this beta curve mean? It means we're playing the following game, right? I give you a bit, and suppose it's biased, right? It has bias actually one minus t okay, bias t. That's right, right? So, probably it's Bernoulli one minus t over two, right? So, right, and then you can ask the question, what is the maximum aposteriori estimate of x through this channel, right? For observing this channel, that's a number. So, this number is essentially a bet of t, right? So, it's just a way to speak about, right, and To speak about right, and to understand the channel is to understand this probability for all t simultaneously, right? There is nothing else there, right? Because essentially, you understand the distribution of log likelihood if you know this number, right? For every t okay, and so here's the cool theorem attributed to many people. So that, you know, one channel is strictly is sorry, worse than the other channel, even only if these paticors are related monotonically. curves are related monotonically. So, I mean, the if part here is trivial, but the if non by the reverse is interesting. Right. So what we proved is that that mu is, you can insert some b epsilon here if and only if this this curve is strictly smaller, right? So that's that's the only, right? So somehow this is a non-strict and this is a strict version, right? Now I will show you a couple of slides. Now I will show you a couple of slides so that you will see that I mean you cannot district bigger for every t right but up until certain t max. So that's a that's again that's an if and only if criterion. Okay so how do this beta t curves look like? So let's draw a diagonal, right? This is beta of t equals t, right? And for binary symmetric channel with crossover probability phi, what you get is a horizontal line up until here, right? So it's basically what this tells you is that It's basically what this tells you is that up until your bias is significant. So, when your bias is very significant, you basically don't get any extra from the channel, right? Something like this. So, oh yeah, and the trivial channel corresponds to this diagonal, right? You don't get any improvement or declaring all the ways. Okay, so right, so the general BMS channel, remember, is a complex combination, right? Remember, is a complex combination, right? And of course, this beta remember is an expectation. So, expectation commutes, right? So, if I give you decomposition of a BMS channel into a mixture of BSCs, then beta curves are just convex combination of the beta curves, right? And you can see that every convex monotonically increasing function can be represented as a convex combination of these guys, right? So, basically, you draw me any curve like this, and I can construct a BMS channel, right? So, BMS channels have a equivalent representation. Equivalent representation in terms of a beta curve, right? Oh, and T max is the point where you hit one. Sorry, hit the diagonal. So, in the worst case, you will hit the diagonal at one. This is basically the largest log likelihood ratio that your channel can output. So for Gaussian channel, this T max is going to be at one, right? Because you can have arbitrarily large log likelihood ratio. But for some other channels, you know, it can be less than one. Right. Okay. Right, okay. So, I mean, this beta curves have cool properties, right? For example, when you apply Vphi mu, then what you get is a homo petty, right? You just scale proportionally beta downwards, right? So if you have some curve like this, then and you apply Bphi at the input, what it does, it just shrinks the beta towards the diagonal, right? So, yeah, and right, so and remember, right, what we want to show, right, some. Remember, right, what we want to show, right, some relation like this. And you can see that, okay, when can you scale one better curve at least a little bit down? It's an if and only if they're strictly related, right? So if they're strictly less, right, everything is compact and small, right? So you can always find some scaling, right, which will move down and not touch the other, right? So that's the upshot here. Right, so that's why we like this characterization. Like this characterization, right? Okay, so now we only need to show some strict bounds between beta curves. Okay, so let me let me, I think, so I have like a few minutes left, right? So let me tell you about the proof of the stringy tree lemma, right? As I said, I mean, those guys, they're cool, they prove the direct coupling, you know, direct coupling that black hole definition requires, right? So, what we can do with beta curves is do an alternative proof, right? So, we want to prove this. Alternative proof, right? So we want to prove this, right? So again, because now you guys know that every BMS channel is a mixture of BSCs, right? If I check this, I can take a mixture of delta one, delta two, right? And get the general BMS channel version, right? But now you can say, hold on a second, this is a very simple calculation, right? Because, I mean, you can just explicitly compute the beta curve for this, right? That is like, okay, there's plus, plus, plus, minus, minus, plus, and minus, minus, right? There are four different. And minus minus, right? There are four different log likelihood ratios you can get, right? So that's it, right? And it turns out if you do this calculation, this is what you will observe, right? You will see that the beta curve for the worst channel looks like this, right? And the beta curve for the better channel will be, well, they start from the same point here, right? And then this one will be here, right? And this is, this was our, this was the start of our. This sort of the start of our project, in some sense, right? Because when we looked at this, we said, okay, so a right, so I mean, stringatrilema simply says that one beta curve is above the other, right? But we can see there's like a peak range where you're strictly bigger, right? So this means like there is some leftover residue stuff right left after you apply STL, right? Because you could also benefit from this separation. Could also benefit from this separation somehow, right? And so we try to mine this separation a little bit, right? And maybe, right? So this proves the STL, right, by averaging over delta one, delta two. But now we need to understand how can we benefit from this extra space. Okay, so this is, so I showed you how beta behaves very nice when you apply P, right? So when you do serial concatenation with the BSC, it's just homotated transformation. So for parallel concatenation, right, where you have new. For parallel concatenation, right, where you have mu and some other BFE, right, parallel observation of the beat, okay, it's a little uglier, but the important thing, yeah, and I didn't even give you T0 or T1 because they are very friendly, but the point is that when you concatenate, when you do parallel observation between you and PSC channel, you just, the resulting betty curves like average of betty curves measured at two different points, right? So if those two points land in the region where you're going to. Points land in the region where you have strict improvement, right? Then you can get strict improvement for the convolved measure, right? So that's so basically, okay, so I mean, the analysis is still a couple of pages, but it turns out that when you do this for degree three graph, right, you always land in the regime of strict separation with embedded curves, right? So that's somehow, you know, yeah, I thought about how. Yeah, I thought about how to give you guys an intuition of why this happens, but I can't. I can tell you why degree equals two does not work, if that's interesting, but why it works for degree three, I cannot explain. Yes, it's just, you know, it's a check. Okay, so maybe, right, so that's what I wanted to tell you, but I think this is all the ingredients you need to reconstruct the proof, right? So yeah, basically, it's the idea of doing comparison by the beta curves and doing the strict improvement of. And doing the strict improvement of STL. So that's it, right? The rest is just some kind of logic. Okay, so right. So I think at this point I wanted to show you how this uniqueness of BP implies something that you guys care about here, but I prefer to spend more time on that because I think many of you would know this. But let me maybe quickly, so Bruce, can I or Alex, can I spend like five minutes more? Alex, can I spend like five minutes more? Yeah, we started. We started with later. Okay, okay, cool. Um, right, so community detection, uh, right, so we're trying to maximize community overlap. This is only for fixed degree, right? So this is not exactly construction. So the seminal work of Mossoniman Slai, right, coupled this to broadcasting on trees. As we know, right, with parameters where, right, there is a number of possible. Where the number of offsprings at each stage is what's on A plus B over 2, right? And the crossover probability is B over A plus B, right? So this is how my deltas and degrees from my strange broadcasting entries, right, relate to 2SBM. Okay, so what does our result imply? So it implies. Yeah, so yeah, so this is K equals 2. Yeah. Okay, so right. So I think the first thing that Right, so I think the first thing that implies is that the optimal probability of recovery in the USBM is completely characterized. Module computing fixed point of broadcasting on trees. Okay, and right, so right, so how do we prove this? Well, I mean, one way to prove it is to say, so if you take a local neighborhood, If you take a local neighborhood in the SBM, right, as a tree, if I give you the bottom exactly, right, your reconstruction performance corresponds to fixed point when you start from delta infinity, right? And if you first run, let's say, spectral algorithm, initialize the bottom, right, then it's like running it from, you know, from some measure, which is not delta infinity, but a little bit worse. But because there is a unique BP fixed point, right, you will converge. So upper bound and lower. You will converge. So, upper bound and lower bound are exactly the same. So, not only probability of error, but actually, posterior distribution is the same. So, oh, and the same works with survey. If you observe each label, let's say through the eraser channel or flipped, you also get this characterization. Right, so that's what I explained, right? So, Pulsa implies a very efficient algorithm. Of course, that's Mosul Newman's Lie ingenious contribution, right? So, we just prove the We just prove the missing ingredient, right? The uniqueness of fixed point. Um, right, so this is the stuff that I find fascinating. I mean, and that's what Jamin, I think, conjectured, right? So, okay, let's put this here, right? So, so this is completely crazy. I don't know why I have a minimum here. Let's suppose y epsilon is my survey, right? Survey means you observe epsilon fraction of labels, right? So, if you apply this uniqueness of VP fixed point, Of VP fixed point tells you that the knowledge about the root, if you observe the survey and all and all the nodes at the bottom, is exactly the same if you only observe the survey. And so, I mean, this blew my mind when the first time Emmanuel explained to me when I was in the PFL, when we started that 2021 paper, right? So, he explained to me this conjecture. I said, this cannot be true. So, I tried hard to just prove it, right? But, of course, that's all. Of course, that's all. Yeah, so, anyways, right? So, again, and the epsilon, this is independent of epsilon, right? So, if I tell, if I give you 10 to the minus 10 fraction of nodes, you don't need the bottom of the tree at all, right? It contains no additional information, right? So, that's one thing, right? And from this, by some kind of simple ratio-theoretic one-on-one argument, you can show that also Jamin's conjecture holds. Also, Jamin's conjecture holds true: namely, that the vanishing reconstruction performance under vanishing survey is the same as reconstruction performance with the bottom given exactly. So you can also get closed, well, closed form formula for this entropy characterization, right? The amount of residual. Oh, sorry, this is a fight. Oh, sorry, this is by this is all of x, not x1, all of x given g, right, and normalized by n. So you can just get a closed form formula, which is in terms, so which is like integration from 0 to 1, right? I mean, I'm saying it's quote, closed form formula, because you have to compute fixed point of VP recursion, like those crazy fractal measures for every survey for every epsilon. If you could do this, I mean, there are probably Like, if you could do this, I mean, they're approximation, of course, right? Arbitrary precision, but in principle, you have this like a closed form approximation. Okay, so I think I'll stop here. And yeah, thanks for your attention. This paper has a little bit of a strange situation. Even though I gave a talk on this six or seven months ago, it's still not on archive and only on my webpage. It's because we're still trying to finish up. I mean, anyways, it's going to be there hopefully this week. Thanks, guys. Thanks, guys. Yes, you can. I guess the obvious question is that it's many varying on the unsolved case, like base dimension binary or alphabet. Okay, so the larger alphabet I will okay, so I guess corresponds to a backer like the ratio. Yeah, yeah, so okay, so this is where the joke or about the story about Elkanon will start. Can we stop the recording? I say stop because Elkan disagrees with the details, so I it was told to me is sometimes screwed up the EMS other. Yes, not only that, there's okay, there are multiple barriers. Do we still have, I mean, I don't care, so it's okay. So, okay, so here's the story. So, they proved this in 2000, right? So, back then, it was actually a little bit before, right? So, and Yuval Veres said, okay, I had this bright student, you know, have this bright student, Elkanan. You know, have this bright student Elkham, right? So, so I gave him, so you know, we proved it for binary. I said, why don't you prove it for pots? Right? And he said, you know, this guy was very, very, very, you know, very efficient before, you know, returned very quickly. And then he disappeared. You know, he couldn't do it. And then after half a year, he basically, you know, returned and said, I can't do it. And he also said, well, it's very surprising because usually I can engage the guy, right? This is not very good. So we moved on, you know, we proved a lot of other things. And then, but Elkhan was, you know, very agitated about this. Very agitated about this, and then later he proved actually it's not true, right? So it's wrong for Post's model, right? For anything except binary, it's wrong. And it's very surprising, right? Because the first time you show to people, people usually nod and say, okay, move on, let's do the interesting stuff, right? It's obvious. You first degrade the bit and you copy it, it has to be, right? That's not true, reports, right? So, somehow, this is one objection. The second objection is we strongly use this, you know, I would say, visual representation of the channel by the beta curves, right? You can see homotetic. The better curves, right? You can see homotety, right? In the k equals three, right, this would be a convex function monotonically increasing, but now only on the unit square, right? So there's like a lot more other, I mean, I didn't tell you how the analysis goes about the corners, right? When I said that you always land in a degree where they separate, right? So, huh? Yeah, but it's a calculus now in like of wheels caliber. So it's yeah, so it's So it's tough. Yeah, so it's, it's, it's, yeah, but most importantly, I mean, what are we trying to prove there, right? We know this is wrong, right? And we're trying to, like, our whole program hinges on improving this. We know the basis thing is wrong, right? So it's very hard to come up with a yeah, with a, with a good, yeah, plus like the final results about the fixed points of BB. No, final point, yeah, okay, no, that's, that I think is still conjectured to be true. Okay. To be true. Okay, at least there's like assortative, disasterative, k equals three, four. Yeah, I forget exactly. But you don't have a statistical computation. Yes, I think. Yeah, so in some of those regimes, yes. I mean, I think. Yes. Actually, we so we try. We can do it, right? But we want to do that. Yeah, yeah. No, that's that's exactly true. Yes. I mean, you can, so the, yeah, so the and there's no obvious metric to. And there's no obvious metric to just to see if they could have it or not. I mean, we couldn't do it. Yes. So, I mean, another crucial piece here is that every BMS channel is a convex combination of BSCs, right? So it's a one-parameter family. For ternary, let's say, it's going to be a strange channel which we call QRI fully symmetric channels. It's just like, anyways, it's a channel acting from SK to SK. So there's like a family. So, there is like a family, it's not one parameter, they're not related. Anyway, so it's you can do some characterizations, but it's, I don't think it's possible to say something smart. Plus, as I said, I mean, sometimes we fixed point is not unique, right? Or let's see. Yeah, I won't remember that. I mean, every time I say this, there's some kind of like, I remember there's some many people conjecture, but I think that like this jump is not equivalent to not having unique VP. Maybe it's still unique. Can you can you just give me a I would have thought if you had the block model with a statistical computational gap, you have like one fixed point that's sort of the easy one and one harder or like right you know maybe it's just a trivial one and you have another one but it's like hard to get yeah that's right that's right that's the thing right yeah exactly that's that's right that's the point yeah exactly so you know that the non-trivial fixed point what we know is not not traveling to the one uh right so it just stays there right somewhere but it's So it just stays there, right? Somewhere, but it still could be unique, right? I don't think people believe in this, but I see. There's still some other problems where you possibly have like multiple wells and all sorts of things. Right. But you see what I redo here, it could be the domain of attraction of the here, there is no domain of attraction of the trivial point, right? Any non-trivial gets you to the non-trivial. But there, what it could be, right, at the fixed point, the non-trivial fixed point is still unique, right? But just that now you have a domain of attraction of trivial. Just that now you have a domain of attraction of trivial, and the rest is attracting to non-trivial. Okay, thanks. Yeah, now I remember. But I don't think people believe that this is the case, actually. So, but yeah, I'm not very good at this belief. So, statistical physicists, yeah, it's uh yeah. This fact that the strong version of the Fangy tree lemma doesn't only hold a depth two for binary trees, yeah, does that have any uh like Any kind of reflection in properties of some specific problems in having oh yes, okay, thanks for this question. Yes, so this is a fantastic. So Bruce, I think is the only one who will immediately say the answer, but let me play this game. Okay, so so Bruce, please don't respond. Okay. And okay, so let's play this game, right? So I give you a bit, right? Unbiased, right? And you observe it through binary symmetric channel, right? Through binary symmetric channel, right? What's the probability of error? It's a binary symmetric channel with crossover probability though, right? So, what's the best estimate? Come on, guys. This is not a trick question. The trick question follows. It's just y, right? And you get probability of error, which is delta, right? Okay, so now suppose you observe it, you observe it for parallel, right? So I give, so I take x, I give you two independent observations, both from BSC delta. You see y1, y2, right? Clearly, you gain knowledge. Clearly, you gained knowledge. You can check mutual information did grow, but what's the probability of error? What do you think? That's the same. It's the same, right? Because there is no decoder, right? You see Y1, Y2. If they disagree, what do you do? Right? So that's right. So this is a funny, right? It actually is related to something known as herding in statistics, in economics. Economics. But yeah, so that's right. So that's the effect. So in the language of beta curves, what it means is that for depth two, sorry, for degree two, the beta curves separate everywhere except zero. So that's why you need just one more additional iteration to also separate them at zero. Because the beta curve at zero is exactly one minus twice probability of better. So that's, yeah, so that's. So I think that's a, yeah. Right. Yeah. Yes, could you comment about the custom signal limit in this picture? And also, isn't there if you have a poor area? Yeah, I'm not an expert. I mean, I know Castenstigo, yeah, but I always forget like the exact, you know, zoo of when it's tight and non-tight, assorted if these are sort of, I mean, yeah, I would probably say something wrong. For this case, it's very easy because Kefn's theorem is exactly one minus two delta square. Kathmandu's digging is exactly one minus two delta square d, bigger than one, right? So that's uh, but uh, for carry, yeah, yeah, something to put into this mix if you're gonna generate, go to higher alphabets, right? I mean, it's known, it's not tight, right? Some for some for some values of k, but exactly what the conjectures are, I don't know. Yeah, yeah, um, like one comment, I'm not sure it's related to this about like to this forecasting. To this forecasting tree part, we don't have for the robust infrastructure that you mentioned. This clear structure is actually a white structure. Yes, yeah, yeah. It's a Johnson Mussel. That's right. My favorite paper, yes. It's proved by contraction of some strange information measurement. Okay, other questions? Okay, thank you again.