Yeah, so I'll be talking about tightness of KBZ La ensemble. We have seen the following KBZ La ensemble in Einste lecture, so I'll just quickly remind you some key properties for the KBZ La ensemble. So here we have the solution of KBZ equation embedded as the top curve of this KBZ La ensemble. Also we know that the KBZ Laun ensemble enjoys a softened version Ensemble enjoys a softened version of browning Gibbs property. By soften, it omits the curve for going out of order. For example, for these two parts, the curves are out of order. More specifically, the Browning-Gibbs property provides or like helps us specify the marginal law of the KPZ Launch ensemble inside some compact region. For example, if we zoom into this. Example: If we zoom into this first three curve here, and if we look at the region, the domain from A to B, we can specify the conditional law inside this interval for the second curve using a Radon-Lithuanian derivative with respect to free Brownian bridge. And this factor here is the so-called Boss bandwidth. And I'll call this W. What is the W? And even though it looks complicated, there are only three facts we need to know for the talk today. So this W, as you can see that is bounded above by one. And also, it is bounded below by some positive value if the curves are ordered. If the curves are non-touching. Non-touching, and also in order to have a probability measure because the factor here is less than one, then we need this normalizing constant. It is just the expectation of this Bosman with respect to the free bounding bridge measure. And one of the core directions in the KPZ University class is to study the Class is to study the large time osmatological behavior of the models in this university class. For this KVZ law ensemble, if we consider this characteristic scaling, then we will get this so-called rescale KVZ law ensemble, which note as this is more HT. And let me just note that directly by change of variables, you can show that the Hamiltonian now play into this new exponential function with the actual factor. Actual factor. One of the big open problems in this film is the conjecture by Corvin and Hanman in their KBZ Laun ensemble paper. So they conjecture that what they expect as t goes to infinity, the KBZ law ensemble should converge to the every line ensemble shifted by a parabola. This could be a strong evidence, could be simply just by looking at the Hamiltonian function. At the Hamiltonian function. So if we let t go to infinity, we can see that the Hamiltonian function originally is a smooth one, but it will degenerate to this step function. When x is greater than zero, which means the curves go out of order, then the parallelization is the infinity. So this gives exactly the non-conselecting Bronach-Gibbs property as enjoyed by the area launch ensemble. And as you can imagine, in order to show convergence, the first step Convergence: the first step we can think of is to prove a tightness, and this is exactly what we did. So, we can show that this scale keeps the line somewhere tight along the scaling. Also, we can see something about the sub-sequential limit. For any sub-sequential limit, it will be a nine-to-second line ensemble, and also enjoys the same Browning-Gibbs puppy as enjoyed by the parabolic. As enjoyed by the parabolic shifted paraval ensemble. So I will focus on the tightness today. So before I move into the argument, I would like to remind you the tightness criteria. Instead of working with the for La ensemble with out of the generality, we can just look at the restricted law ensemble to some arbitrary compact domain. So for example, So for example, if you look at this illustration, we'll only look at the first three curves restricted from interval L to R. What do we mean by tightness? So suppose you have a sequence of probability measures. The tightness says that you can always find a compact set. And then the mass outside of the compact set could be bounded from by bounded from above. Only from about as small as you want, which means we don't want to lose mass when we take the limit. And then how do we compute this probability for this large number to lie outside this contract set? Remember, we have this Brownian-Gibbs property, which specifies the law of this restricted ensemble. So this is with respect to your free Brownian Gibbs measure. Reflect your free volume base measure. The Radonic derivative is this one. Remember, this Bolsonari is always less than one. So we can compute the probability to be outside of this complex s. We can bound it by this term. Now, this is the integral against with respect to three Brownian bridge measure. And it's easy to find such a complex set for Brownian bridges because the Brownian bridges are very regular. Because the brownie bridges are very regular. Then the control up boils down to problem to binding the normalized constant from below. And in order to have this to bounded from above, then what we hope is that the event such that the normalizing constant being bounded from zero is very typical in the sense that the probability will be very close to one. So this is what we need. What do I need to prove the tightness? And instead of proving this stronger version, I'm going to prove a weaker version today. So instead of having the normalizing Thai bounding function below, this probability very close to one, I'm only going to show you that normalizing Thai from below has one positive probability. Let me remark here, this is actually not. Here, this is actually not far away from the stronger version, which means we can upgrade or further strengthen the weaker version using Dips property. And now let's work with this. Not zero, that it's a little bit. Okay. And then you go from one to the other. And then you go from one to the other using some sort of resample, like important sample, not important, some tilting argument or something. No, just a sampling. I will realize to prove and to go from the weaker version, I will realize this normalizing constant as the Radon Lincoln derivative instead of as the normalizing constant. Normalizing constant. So, what I meant the word I meant, and Alan reminded me, is size biasing argument. Is that what it is? Yeah, okay, yeah, this is exactly the observation in the paper by you and Alan. I think this is a very smart observation. Now, let's compute this probability. Let's try to find our lower bound. So, let's recall that Let's recall that the normalizing constant is the expectation of Bozeman weight. And as an expectation, suppose we can find some probability event such that with positive probability such that the Bosman width is always bounded below on this event, then we are done. So now we are looking for some probability events with positive probability. With positive probability. And the event E way set is the following one. So we restrict the curves H1 to H4 to stay in the yellow region. So we want them to be well separated. And if they are well separated, then the Bosman width is definitely bounded from below. Then the question reduces. The question we do is to show that this probability event E is bounded from 0, has a positive probability. So now I'm going to use the Gibbs property to obtain a lower bound for the probability of this event E. Before I show you the argument, I want to list the main ingredients I'll be using. So we have already seen the first two in Ivan's lecture. The first two in Ivan's lecture. The first one is the custom model nesting, and then the second one is a resample invalids provided by Gibbs polity. So let me show you what do I mean by minor or tailbone. So let me recall you the event we want to control is this following one. So the first curve stays in and this curve stays in the region respectively. So in Major, we want to have this. Imagine we want to have this such configuration. We hope that we can give some bashful region for each curve and let them evolve and configure themselves. Then they will rise up to this region and stay there. So instead to have them to evolve, then we need some initial condition. We need to have some starting point. This is my, it's got a starting point. It's exactly a starting point as that. And the three curves in the blue color are the restricted first three curves, and then the curve in the black color is the fourth curve, which serves as the boundary curve. This starting point is all of the values of these curves between minus m and n. So imagine if we set m to be large enough and together with the s Together with the estimates in the paper by Cormin and Hammond, they prove that all of the curves in the K-C-Law ensemble are uniformly bounded. Then by picking M not enough, then this curve should stay in this region. Okay, now I'm going to first erase the curves to be above this yellow region. Be above this yellow region, and I'm going to push them down to have them stay in the yellow region. So, what I'm going to do is first with the first curve. Let's resample the first curve from these two ending points. Then, what I want is to have the first curve jump up to this level, 11m. So, I gave union travel. I gave you a travel time for the first curve, and I wanted to jump above 11m and then stay above 11m. So, which means I want to give a lower bound of this probability of this following event from L minus 2, R plus 2, the first curve to say about 11M. And why this probability has a positive probability has a positive event has positive probability. Imagine we don't have the other curves. So suppose we don't have, we don't see the second curve and the third curve, so on and so forth. Then this probability measure is exactly the free Brownian bridge measure. And for the free Brownian bridge measure, then there is a possible probability for it to jump to some certain level and stay above that. Then let's put back these curves. Back these curves. So due to the stochastic monotonicity, the presence of the lower curves will only push up this first curve. Then naturally, basically event will have an even larger probability. So it will have a positive probability. And now I'm going to do the same thing for the second curve. Also, give one unit travel time for the second curve and then For the second curve, and then let the second curve to stay about this region, and then stay about that. This could be proved using the same argument. The actual difficulty is that we have the presence of the first curve, which will kind of push down the second curve. But this is not too bad because the first curve stays above 11. First curve stays above 11m, so there is enough room for the second curve to jump to jump. Then we can do the same thing for the third curve. So now let's look at the vision we want. We hope that the curve instead of just sitting above this level, we also want them to stay in the level. want to want them to stay in this yellow region. So which means we have to rule out the cases that the curve jump too high. See the second the case here, the second curve jumps too high here. So we want to resample and then to force them to stay in the region. So I will do this in a reverse order. So I'll first resample the third one. Suppose the third curve rises above five Rises above 5m, then we can search from left to right. There is a stopping time such that the circle will pass over this full M and there's also another stopping time from right to left. Then we can use the strong Markov property to resemble the trajectory in between this stopping time. And then argue that there's a positive probability for There's a possible probability for this green factor to stay in this yellow region. And then we can do the same thing for the second curve and the same thing for the first curve. Every time these events have positive probability, so in the end, we achieve the event we want to control. Then this implies this event has a positive product. This event has a positive probability, and which further implies the normalizing constant boundary from below has positive probability. And that's the argument. So it's also the end of my talk. Okay, thank you very much, Sean.