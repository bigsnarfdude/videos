Great, thank you. So, thank you for the organizers for organizing this nice workshop. It's due to the time difference, I can only attend the morning sessions, so that's a bit of a shame, but everything I've seen has been very nice so far. Right, so I'll be quite quick in this talk, given that we have quite little time. And since the organizers told us that we have quite a broad audience, I wanted to speak about something that could be. Audience, I wanted to speak about something that could be perhaps of general interest. So, this is somehow a topic which is slightly in between stochastic analysis and using some ideas in stochastic analysis and machine learning. Okay, so this is based on joint work that we did last year with Andreas Gerasnevich, who was at Bath, now he's at Credit Suisse, I believe, and Hendrik Weber, who at the time was also in Bath and now has just moved to Münster. Okay, so. Okay, so for the structure of the talk, I'll speak a little bit about signatures, which we've already seen. So I'll try to be brief here. I'll speak about our kind of the contribution of what we did. So this is moving to higher dimensions. And finally, I'll conclude with some numerical experiments that we did. Okay, so let me just begin with a very simplistic picture of machine learning that I simplistic picture of machine learning that I sometimes like to keep in mind. So one often begins a machine learning task with some data. So this is just raw data that you receive. Then what you have to do is you have to extract some useful features for it. So often this raw data is just not usable. So you have to transform it in some sort of a way. Then you run your favorite machine learning algorithm. This could be as simple as linear regression, or it could be a neural network or something even more sophisticated. And then finally, you have your output, which you then interpret in some way. Output that we should then interpret in some way. Okay, so the focus of the talk will really be in this first step. So, how we move from data to features. And so, and what I'm going to focus on is on data that's defined over spatial domains. So, I have some sort of a function d, which is defined on a multi-dimensional domain, and it can potentially take values in a In a multi-dimensional vector space, and eventually this little n will be one, but for the moment it can be anything. And so we have these sort of defined over multiple, a multi-dimensional space, takes values in multidimensional, takes values in multidimensional space. And I'll be particularly interested in the case when this little d is bigger than one. That's going to be the case of interest for us. So considerations in extracting useful features is that we want these features to be descriptive, so we don't want to lose too much. Features to be descriptive, so we don't want to lose too much information about the data. We want them to somehow be kind of in the form of a vector because typically scalable learning algorithms need vectors to work. And we furthermore want it to not be take values in such a high-dimensional space. So whatever features we extract from this function xi, we don't want it to be huge, take values in huge dimensional space because that's hard to use. And so, if you want a motivating problem to keep in mind, and the one that I'll use in my numerical experiments is that My numerical experiments is that given observations of this input function ψ and observations of an output u which are kind of related through a certain type of PDE, then we want to kind of learn the solution map to this PDE. So given observations of, so mathematically we have a map that maps every ψ to a U, and we want to learn this map. And we want to learn this map. And so, here this map might be in the form of a solution to a PDE, like the ETO Alliance map that we've heard of. This would be the case of an ODE. But the catch is that we don't know what this mu and the sigma is. And we don't want to estimate the mu and the sigma. We just want to abstractly learn this map. Okay, so that's the motivational problem. Okay, so one of a naive approach that one might try. A naive approach that one might try is one might want to just discretize your spatial domain. So pick your maybe equally partitioned points and then use sample your function ψ at those points and then use this thing as a big n-dimensional vector. Okay, so this naive approach sometimes works, but it sometimes runs into problems. So some of the problems that it runs into is that sometimes to be descriptive, you need this capital N to be very, very large. And this will break this that we want a low dimensional. This, that we want a low-dimensional description of the signal. And it can also have a huge computational cost as well. It can also be quite unstable to noise. So, if we add noise to our observations, then these feature vectors tend to oscillate a lot, and this can ruin our learning algorithms. Moreover, we don't really have access to this whole collection size. So, D is maybe some, think of it as like a unit ball, but this is a continuum object. And we typically But this is a continuum object. And we typically, and algorithms, are only allowed to put in kind of, of course, only finite observations. And sometimes these observed points, we don't even have a choice for them. So they might be somehow detectors sitting in water or something like these that are maybe floating around and they're just wherever, like kind of, they are wherever they are. So you don't have a choice of picking these observed points. And so sometimes even this number of observed points can vary from sample to sample. And what this means is that you kind of, when you move from sample to sample, these numbers of observed points might change. And so your vectors that you extract in this naive way would have different dimensions. And so this often also causes problems. So they might not be directly comparable. Okay, so let me come to one of the ways that people have found useful to resolve all of these issues in the one-dimensional case. Dimensional case. So if you start with a original signal, is so I've now changed notation slightly, but it'll become clear how this x is related to the previous xi. So we have this some sort of a function. So our spatial domain is an interval and taking values in Rn. Then a useful way of extracting a feature from the signal is the so-called signature. And okay, we've seen it in many previous talks already, and I think members of the audience. And I think members of the audience are familiar with it. But just in case you're not, it's basically given by a countable collection of real numbers, which are defined using these iterated integrals. So you define them explicitly like this. And so this has a lot of, this definition of a signature has a lot of very nice properties. It has a long history. So starting from Chen in the 50s and Rhea and Magnus and later was picked up by the And later was picked up by the controller theory community, in particular Brockett, Susman Fleas. And later it plays a role in line's theory of Rafmaths. This is how kind of we enter it here as well. It has a lot of beautiful properties. So you can expand solutions to ODEs as linear functions of the signature with very explicit error rates. It captures a lot of geometric descriptions of X. And perhaps the most important property for us here is that it has very nice algebraic properties. It basically means that this is. Properties that basically means that this is, in a suitable sense, a universal feature map. So you can characterize precisely its kernel. And in suitable situations, if your signal takes appropriate forms, it has trivial kernels. So it's completely descriptive. So it has this nice universality property. Okay, so the signature can and has been applied to a number of learning problems. And the key here is. Problems. And the key here is that we, because of this definition of iterated integrals, we need the data to be time-ordered. So, examples of time-ordered data are financial time series, texts. You might not always think of this as time-ordered data, but this is in a way one of the ways of looking at it. So, the order of the letters just gives you the correct order. And yeah, so it's maybe not canonically parameterized by time, but it just has some sort of. By time, but it just has some sort of an ordering associated to it. Or it could be something quite non-linear, like a time-evolved network. So the signature can say something interesting in all of these cases. Where it's much harder to say something to use the signature is when you have purely spatial data. So you don't necessarily have some sort of a canonical way of ordering time. And so examples of this is something like image recognition. So you might want to be able to detect in these images when you see a building or when you see an You see a building, or when you see an open field, or something like this. Or something like when you have time and space, spatial and temporal data. So something like meteorological data. You might see snapshots of weather, and you might want to somehow predict what are the chances that a hurricane will form. Okay, so it's these types of data where the signature is not directly appropriate. Okay, so let me now try to describe a possible way to. Try to describe a possible way to handle this issue. So, okay, so the natural question is then that I've hopefully motivated is: how do you generalize signatures to higher dimensions? This has received, I think, quite a bit of interest in the last few years. So, there's a work by Shang Lin and Tyndall, which builds on works of Tyndall and Chuk, that kind of generalizes signatures by certain spatial. Images device certain spatial differential operators. And they looked at applications of this to image and texture classification. There's another kind of more algebraic topological paper by Justi Lee Nanda Oberhauser. And I believe that Lee will be speaking about this later on in the workshop. And they generalize this via so-called cubicle mapping spaces. So this is quite, in some senses, a little bit more faithful to the way Chen looked at. To the way Chen looked at the signature. So, the approach that I would like to present today is based on regularity structures. So, regularity structures are a generalization of rough paths that, again, maybe perhaps people in the audience have, some people in the audience have seen. And it generalizes rough paths to multiple dimensions. So, a very natural question is to ask, so how does the signature transform under this generalization? Transform under this generalization. So, how do you move from so if regularity structures generalize rough paths? What is the natural generalization of signatures and regularity structures? And the answer is models, which I'll describe now. Okay, so let me try to describe it through a motivation. So the motivation will be looking at Picard's theorem. So let's come back to our signal psi. Let's come back to our signal ψ, which is we recall as this function, and now I'm restricting just to one-dimensional target space for simplicity. And what we want to approximate is this output. So we think of u always as some sort of a function that depends on psi. And in this case, it's exactly what we had before. So it's the speed. So we think of L as some elliptic, some arbitrary differential operator. It can be elliptic, might be the heat operator, might be even the. The heat operator might be even a wave operator, so it doesn't have to be parabolic. And these mu ang psi are polynomials. We have some arbitrary boundary data. So to solve the PD, we also need to be given some boundary data. Okay, so think of u as a function of psi and the boundary data. So given xi and the boundary data, we can solve for u uniquely, and we're trying to learn this map. Okay, so one of the first things that you try to do is invert this operator. So you build an integral operator i, typically given by convolution with the Green's function of L. So explicitly, I satisfies these properties. So you solve the linear part of the PDE with zero boundary data. And then Picard's theorem tells us that we can express U under favorable analytic conditions as a limit. Favorable analytic conditions as a limit of a sequence of functions u n where u n are defined recursively. So we start, u1 satisfies the homogeneous linear equation. So L of U1 is equal to zero, and U1 has the correct boundary data. Okay, so this is a linear problem that's typically very easy to solve. Then u n plus 1 for n bigger than or equal to 1 is given by, well, we add the linear part of the add the linear part of the of the problem plus this um uh non-linear part um convolved with uh well we apply this integral operator to the to the non-linear part evaluated at the previous uh iteration so this is the standard Picard iteration that we that we use every day to solve uh to solve PDEs um okay so so this is this is the content of Picard's theorem and of course I'm dropping a bunch of conditions Theorem. And of course, I'm dropping a bunch of conditions that one needs for this to hold, but this is the general picture. And then, so I'm trying to move this. The bar that tells me that I'm sharing the screen means that it's blocking half of the text. Okay. Okay. So then what we observe here is because, okay, we assume that mu and sigma here were polynomials. This is largely for simplicity. Polynomials. This is largely for simplicity, but it makes the statement precisely true rather than just an approximation. The point here is that each of these is a multilinear function of u1, so the first Picard iterate, and the psi. And it's these multilinear functions that are going to comprise precisely this definition of a model feature vector that I want to give. So it's exactly these solutions to these approaches. It's exactly these solutions to these approximate, these approximating solutions that appear in the definition of a model. Okay, so let me come to this definition of a model feature vector that generalizes the concept of a signature. So as input, we have certain static objects. So the static objects are set D and some linear operator I. And this linear operator I is acting on functions from D and to say the real number. From D and to say they're real numbers. Typically, you also need some regularity here, but let's forget about this. This is just some arbitrary functions from D to R. And think of I as being the convolution with some Green's function. And then moreover, we have some input. So we have some UIs, so finite set of functions, UIs, and a function psi. So these UIs kind of play the role of the U1 that we saw in Picard's. Rule of the U1 that we saw in Picard's theorem. And for certain applications, we want to take more than one. I'll give an example where we want to do this. Okay, so given this static object that doesn't change, and given this input that typically does change from sample to sample, we can construct a model feature vector, which is the family of a countable family of functions. And these functions are given recursively, just like in Picard's theorem. Recursively, just like in Picard's theorem, with M0 being just this initial collection of functions, UI. So this is sometimes called the initializing set. And in the inductive step, what we do is we, so first of all, we always include everything that we had on the previous level. So kind of this is an increasing family of functions. And then the extra things that we throw in when going from n minus one to n. From n minus one to n is this linear operator i applied to arbitrary products of this function psi together with derivatives of everything that we saw in the previous level. These fi's are things that we saw in the previous level, and a i's are arbitrary multi-indices, and j and k are arbitrary natural numbers. Okay, so we kind of apply i to arbitrary products of everything that we saw in previous levels. So here psi is called the forcing by Psi is called the forcing by analogy with what we saw in Picard's theorem. So, it kind of played the role of the forcing noise, the forcing term. So, what one should think of here is that each of these Fs is actually indexed by a corresponding symbol. So, if anyone has seen regularity structures or branch-graph paths, one can actually represent each one of these functions as a tree. So, and then if you have and then if if uh if you have um yeah so um so you should you should think of them as as being uh you should think of this as a big vector indexed by tree so so it's not really just a set of functions actually more like a hash map or or a dictionary so for every tree you get a you get a function quickly convince you of this now um so Office now. So, in the case of a signature, okay, my internet connection is not stable. That's not good. Okay, so to see how this generalizes signature, let's pick psi as being a function from 0t to r. And here again, we can take rn if we wanted to. The operator is the integral of xi. So, this it of psi, this plays the role of. t of xi displays the role of the x that I had in the definition of the signature a few slides ago. So this would be x t. So that's the relationship between the psi and the x. This so-called initializing set that contains certain generalization of the generalizations of the initial condition, this is just going to be empty here. So we're not going to, so this initializing set could very well be empty. And this is an interesting thing to consider. And then basically the function And then basically, the functions that appear in this model feature vector, so evaluated just at the end point of the interval t, evaluated at a single point, they encode precisely the signature of this function x that I mentioned. So this is, they contain some extra terms, but it will certainly contain the signature as a subset. Okay, so So, okay, so this was all just definitions. And I actually, in this talk, I have no theorems. I just, this is more of kind of experimental mathematics, if you like. So it's just trying to see how well a certain idea generalizes to a new context. So let me present some of the numerical experiments that we tried to see how well this works. So, we effectively looked just at this motivating problem of looking at PDEs. We thought this was a natural place to start. So, the first PDE that we looked at, trying to learn the solution for, was a parabolic PDE with forcing. So, we tried both multiplicative, additive, forcing, different types of non-linearities. But let me just present just one example at the moment. So, we tried solving one-dimensional time, one-dimensional space and Space and with periodic boundary conditions in space and the time interval zero to one. Don't worry so much about the exact form of these nonlinearities, they're not so important. Just think of it as a cubic nonlinearity. The leading order term is the heat equation. And don't worry so much about the initial condition either. It's just some arbitrary but fixed initial condition. Okay, so the aim of this experiment was if let's fix a point t and x. This is going to be fixed throughout the whole experiment. This is going to be fixed throughout the whole experiment. What we want to do is we want to learn this UTX, the solution evaluated at T and X from this realization of xi. So xi is the object that we can observe. And what we're going to do is just we're going to apply linear regression on the model. So we're going to construct this model feature vector and apply linear regression to it. And the point is that we only evaluate the model. only apply we only evaluate the model at a single point which is this point uh tx so very similar to to the philosophy of signatures so in signatures you typically evaluate it just at well not always but but typically you evaluate it just at the one point um so some details uh for the training and testing we so for both for both sets we you compute um the models for approximately we we looked at about 60 functions we we did i think 60 functions. We did, I think, about 700 training sets and maybe 300 testing sets. Here, this operator I is the inverse of the heat equation, of the heat operator. And again, we forget about the initial condition, just the classic conditions. Oh, I have to speed up. Then we, okay, so in the learning phase, we fit a linear regression of UTX against these models evaluated at this one. These models evaluated this one point t index, and then a testing split said we apply a linear regression. So the outcome was interesting. So we got a reasonably good fit that, okay, in these slides, I don't give a comparison with any other algorithms, but we basically compared it with sort of naive algorithms where you just view size a big vector. And it performed significantly. And it performed significantly better than this. I just give the numbers here for our own experiments. And so, so, in these lines, these are kind of on the left, you see the predicted value, on the bottom, you see the true values. And so, the closer the numbers are to sitting on the diagonal, the better. So, what's interesting here is that here we tried it first for a time very close to the initial time. So, here you expect to get a very good approximation. So, here you expect to get a very good approximation. And then, at time quite far away from the boundary data, you expect this to deteriorate. And at least in the scales that we looked at, you didn't actually see a significant deterioration. Probably if you looked out much further, then you would see it. And this would require further investigation. And what's interesting here as well, that I would like to point out, is that the higher we went in the model, so the more levels, kind of the more functions we included in the model, the better our prediction was. Included the model, the better our prediction was. So, this is somewhat analogous with looking at, so it wasn't just say maybe the first level of the model that was useful, kind of looking at higher orders actually improved prediction. Very much like in the signature, you often see better predictions when you look at higher levels. But again, not always, but typically. Okay, so yeah, okay, brilliant. Let me just quickly go through maybe a couple of Through maybe a couple of minutes. So, the next experiment that we tried, so this is perhaps a little bit more interesting, is a wave equation. So, a very similar setup. Again, forget about the nonlinearities. The point is that it's somehow non-linear, multiplicative in the noise. And the key point here is that instead of a parabolic equation, we're looking at a wave equation. So, this is now dispersive. This is so parabolic and dispersive PDs have entirely different analytic sort of solution theories in some sense. So they have very different behaviors. And this is actually, we're going to see something similar. I saw something similar here. So the aim is exactly as before: we fix a space-time point and then eval, and then we're going to, on the learning phase, we're going And we're going to, on the learning phase, we're going to regress the solution evaluated at this point against the model built from this noise evaluated at T index. But what we're going to do now is that rather than forgetting about the initial condition like we've done in all the previous discussions, we're going to actually retain the initial condition. We're actually going to, in a way, look at, we're going to enhance it. So we're going to actually have two initial conditions. The reason for that. Initial conditions. The reason for that is: well, if you're familiar with the wave equation, you can probably see exactly why this is the case, because to solve the wave equation, you need two pieces of initial data. You need initial position and you also need the initial velocity because it's the second-order differential operator in time. So this is an example where the initializing set actually contains two pieces of data, so two functions. So here we denote ICU and ISV0, and they just solve the corresponding homogeneous. Have solve the corresponding homogeneous solutions with the respective initial data. And so, the reason that we did this is because we tried the experiment with various permutations, and it turned out that if you don't include these initial pieces of data, your prediction is essentially zero. So you're not going to be able to predict the solution at all. So you end up with a picture that looks like this on the left-hand side. So you have zero prediction. This is on the left-hand side. So we have zero prediction power. But if you include both the initial velocity and the initial position, then you end up with a very, very good fit. So this is a drastic improvement. And if you include only one of them, so like maybe you don't include the initial speed, then you end up losing something, but still having a relatively good, somehow a reasonable prediction. So kind of both of these pieces of data play an important role. play an important role. So this is something that was quite, we thought, quite interesting. Okay, so we tried another experiment, but let me not go into this right now. Blah, blah, blah. So we kind of, there's another interesting equation that you, another interesting experiment that you can do. So you can learn this, kind of learn a dynamical system. So you have a PDE with now with no forcing, but with a random initial condition, and you're trying to learn the outcome after a certain amount of time. So you can apply this. So, you can apply this idea of a model in a slightly different way than the first two experiments to do this as well. But yeah, let me not go into that. Okay, so let me just conclude. So, kind of in some sense, the point of this project was perhaps to ask some questions rather than to necessarily answer them. So, some of these questions were basically, so can these model feature vectors be applied to PDEs? Be applied to PDEs. And we maybe set up an initial answer that at least there is some promise to doing this. So, natural question is: can you move beyond PDEs? So, can you actually generalize to other situations, such as this meteorological data that I mentioned at the beginning, or image classification? So, this would be an interesting thing to try. All of these nice properties about the signature that I discussed, we don't have any answers to this. So, does it have any nice So, does it have any nice algebraic properties? Can you get some sort of a density argument and apply something like a Stone-Weiestrass theorem to prove density? So this we don't know. Another very interesting question is how do you choose this hyperparameter I? So this I was fixed throughout, but in reality, you would have to, especially if you applied it to meteorological data, you don't know if there's an underlying PD. So you need to be able to learn this I somehow. And then another natural question is. Another natural question is: so, if you can it could be combined with other learning algorithms. So, we just tried linear regression, but maybe there's a more sophisticated way of embedding it in there. So, there's a couple of works that more recent than ours that try to address this problem. So, primarily by using neural nets. Okay, so I think that's all I want to say. Thank you very much. Very quick question, maybe? I have a question if that's okay. Okay, sure. Yeah, hi, Eliot. This is William. Hi. Hi. I was wondering, is there