Actually, it's probably closer to 1995 actually when I started. So maybe almost 30 years experience of Kaiser. So just some thoughts I had about pairing this talk. Because my perspective, I trained in epidemiology as well as well as statistics and genetics. So this is my perspective about these biobanks and their use. I think of it from the perspective of epidemiology. And a fundamental to epidemiology, of course, is. Epidemiology of courses, as opposed to randomized clinical trials, is causal inference. And study designs are at decreasing strengths in terms of perceptions about their evidence of causal relationships. So I've ordered that studies could be cross-sectional, they could be control studies, they could be cohort studies, and ultimately, more recently, people have developed what are called emulated clinical trials. We actually try to recapitulate some of the things. We can pitch about some of the features of clinical trials and observational data. So, here's just some considerations, immunologic considerations of biopacts. How representative is the cohort you've got, or the population that you've got, or the general population biases that come from volunteers? How diverse is the cohort? Ethnic diversity, otherwise, SES, gender, other features, urban, rural, for example. For example, what how and I'm going to get more about the data, but is the data cross-sectional between each single time point? So it's basically fundamentally used for cross-sectional abuse control analyses only, which are really mostly typical of GWAS studies, are really just those kind of cross-sectional abuse control analyses. The data longitudinal, knee-blame retrospective or prospective over-related clinical trial analyses, when we develop project risk scores, can they Develop project risk scores. Can they be used in these biobikes? Can they use a prospective and a reduced analysis? Covariates are often very important for a variety of reasons in ecological studies. For coordinate analyses, are they available as time-dependent covariates that change over time? But how available are potential confounding variables that need to be included in terms of issues of causal influence? And what about the environment? And what about the environmental data? What kind of environmental data is available for doing lots of different kinds of studies, but in particular if you're interested in genetics, you need environment interaction studies? And of course, one thing that's critical in these biofuels also, especially if you look at them from a longitudinal perspective, is what kind of attention do you have and how much loss to follow-up is there, and is the bias in terms of whose loss do you look at? Phenotypes, I know this is going to be discussed here also. I saw some of the topics. Here also, I saw some topics. Extracting phenotypes from biobanks is also extremely important. What is the quality of the phenotypic data that's available? Is the data from self-report? Is the data from electronic health records? Or is it obtained from direct observations by researchers? What's availability of other types of data? For example, behavioral risk factor data for us, in many cases, that's collected by personal surveys. How continuous and multiple are the Continuous and multiple are the phenotype data that you've collected. So, do you have, like for blood pressure, and I'll talk about this, do you have multiple measures over time, or do you just have measures that are taken to see it? And again, I mentioned the importance of a lot of environmental variables. And of course, a lot of the conversation here, I think, is going to be about electronic health records. Not all these biobanks focus exclusively on electronic health records, but many of them do. And electronic records have their advantages and disadvantages. First of all, Disadvantages. First of all, depending on the system you're dealing with, like Medicare or whatever it is, there's potential biases and false claims. So claims data, you know, doctors can make diagnoses based on whether they think it's going to be reimbursement. So there is that potential that often comes up when dealing with health data. Also, how much missing data, the accuracy of the data, and so on. One of the advantages potentially is that. One of the advantages potentially is that you often have a choice of the setting in which the data are obtained. So for example, the data is blood pressure, was it obtained in the ICU? Was it obtained in the hospital setting? Was it obtained by the general practitioner, by a specialist, and so on. So you may be able to have a choice about where it was actually obtained to avoid bias. For example, if it was the ICU, you might think I'm not getting, as an ICU blood pressure, and it's not necessarily reflective of that person in normal circumstances. Person in normal circumstances. Repeated measures over time can be extremely important. We have a lot of experience with this with psychiatric diagnoses. So, if somebody gets a diagnosis of schizophrenia over 30 years and that's the only diagnosis they've got, we have a little more reliability that that's probably an accurate diagnosis, and especially if it's by psychiatrism. The other thing that's important, I think, to recall, to take into account is that these biobanks, especially the one that I've been involved with, they're clinical, and the data are collected for clinical. And the data are collected for clinical care. The needs are collected for research. And so we may need to do some work to collect that clinical data for research purposes. That's an entire field of the reference self. Just want to acknowledge, this man is Canadian. He was Canadian, Harry Newcomb. He was also a president of the American Center for Genetics. And I think he's probably one of the earliest proponents of using electronic health records for genetics research. And he only talked about this in 1959. Talking about a linkage of libel records for bullet pedigrees, and again in 1967, and you can see here the title of his talk: Efficient Systems for Linking Records into Indigenous Family History. So he already talked about using computers not just for calculations, but actually for creating a data source, data source for doing the basis of research. So at Kaiser, I mentioned, I started there around 1996, maybe, as an adjunct investigator. Maybe as an adjunct investigator with my colleague Kathy Schaefer, who I knew from my Yale days. I know what she means from Yale. I actually was there myself for 10 years. I met Kathy, who was a postdoc at the time. And I knew she had moved to Kaiser. And when I moved to Stanford, I could be in touch with her. And I thought, you know, there may be some really great opportunities at Kaiser because they have this fantastic population. They were building electronic health records. They still had a lot of historic data and a stable, pretty stable population if you're doing research. And they have a division of research that already would. And they have a vision of research that already a lot of that collection research. So we started this program on genes, environment, and health around 2004. Actually, it started earlier than that with the SNP consortium where they wanted some data and we thought, you know what, not just provide them some samples, that we can actually build a resource here for two years. So the advantage is, of course, the KP electronic health records that are there, that's the basis of the rationale for it. But through surveys, we can collect the annual demographic. Surveys, we can collect behavioral demographic data from the members. Environmental exposures, I'll talk about that in a minute, and potentially genetic data from biospace. So about 2006, we actually got some funding finally. It was not easy. 2005 was because for years we struggled to get funding for this. We finally got our first philanthropic funding from the Wayne Gladys Valley Foundation, which ultimately the Ellison Foundation and Kaiser started to match. So we set up. Match. So we sent out a mail survey to about two Roman adult members of Kaiser. We got about 42,000 back. We asked them if they would provide a saliva sample for doing genetics research, which many of them provided. Ultimately, we ended up with about 200,000 individuals who provided a bio specimen and consent for doing genetics research. The survey had many questions about demographics, personal, family health history, smoking, alcohol, physical activity, and so on. And because it was And because it was a multi-ethnic population, it was written in different forms that represented the population. Again, the environmental part of this, mostly this was obtained from geocoding, where we actually knew the addresses of all the members over toilet. So we could overlay that information with known databases from U.S. Census and other sources. Created by colleagues, I did this, colleagues there, developed a built environment. They're developed a built environment variable for the members and actually use that. The state of California also has exposure data for pollutants and toxins and so on like that, which we can link to or geocoded for adversities. Now, the turning point for us was the ERA funding, the ERA, the Reconstruction Act in 2008, where there were these RTTO Grand Opportunity Awards where you had to apply for over half a million dollars a year for two years. A year for two years. I'm like, okay, at that point in time, yeah, that's really the case where you're told you have to apply for more than half a million, not you can't apply for more than half a million. Right? It's like, okay, yeah, well, there's no limits, so it's okay. So at that point, we had about 85,000 biospecimens. And Kathy Metrics was one of our vendors that came to us and used stuff. And we talked to them about the fact we had these samples. And they told us they were developing a new technology, which are called these gene titans, which were much more efficient, much less expensive. Much more efficient, much less expensive. And I sat there talking to them and asked, How much is this going to cost? You know, they said, like $125 a person or whatever. And I said, well, but if we do $100,000, how much is that going to cost? So ultimately, Kathy and I worked on this and put together a grant. And it was actually a $30 million grant from which they cut $5 million. So we actually ended up getting $25 million from these. And here's the And here's the specific aims. At that point in time, they gave us the opportunity, because it was multi-point, develop ethnic-specific arrays, four of them, which we nowadays people don't do that anymore, but that was a state of the art at the time. So we constructed our own arrays to do genotyping on, there was a minimum of 675,000 markers. As time progressed, we actually put more markers on the arrays, on 100,000 participants. And due chilling, this blackbird found out about our. This blackbird found out about our project and wanted in. And we said, okay, so actually we're able to add till we're length analysis collab, but we've done this before. And ultimately, to link all this genomic information to the electronic health record and the survey information that we have, and to make this a resource and make it available through PBCAT, but also through collaborations with Kaiser. So I think I'll get into the genotyping, which was that's a 10. Which was that's a 10-hour talk in and of itself. But you know, when you do projects like this, I think we were probably going to be the first ones. One of the first ones to do a large-scale genomic project like this was 14 months in which it had to be done. And there was a great deal of things, decisions that had to be made, and technology, such as a new technology, and so on. So it was really quite an effort. I'll tell you who was really assembling all of this. But this is just the characteristic: it was 58% female, 42% male. The real virtue of this is that Virtue of this is that the age at specimen donation, you can see it's an older cohort. It was actually the aging institute who took the lead on funding this because a lot of this is age-related diseases because of the age distribution, but you can see over half the cohort was over age 60, and there were substantial numbers of people already under HD. So, this was a great cohort for doing each related research. Here's the ethnic breakdown. We included every minority we possibly could. Every minority we possibly could at the time. That was a decision. It turned out to be a little over 20% of the full worth. 78% non-Hispanic white, about 11% Latin trans were mixed, 8% Asian, about 3.5% African American. Now this is really critical also. In fact, the length of KP membership. So you can see here, over 75% of these participants have been members of the KP for at least 10 years, and over half for 20 years. And this is back in 2009. So you can imagine by now. So, you can imagine by now, it's even a greater percentage of people who have been members for over 20 years, which means you have vast amounts of longitudinal data on these individuals through the electronic members. This is the geographic distribution. Kaiser Northern California has a lot of outreach. It's not just around the Bay Area, as you can see. It's all around Sacramento, it's around Fresno, and so on. So, it's a very diverse population geographically, but also in terms of urban versus remote. Urban versus both. Now, the phenotypes, of course, are critically important, as I mentioned. So, Kaiser's had a comprehensive electronic health record starting in 1995. So, by now, it's almost 30 years worth. These are epic-based EPIC-based system they call Health Connect. As I said, lots of longitudinal data, which means for some things I'll show you in samples where you have lots of measures. Kaiser historically has validated registries that we have validated against. Registries that we validate against in terms of all of our, where they use all the information they have available. For example, there's a diabetes registry where they actually characterize people as type 1 or type 2. They use medication history and a lot of other things to actually make these diagnoses valid. But we've developed some relatively simple, simpler algorithms, ICD-9, ICD-10-based, involving multiple diagnoses over time, and validated them against the registry. And basically shown that repeated observations, I think, is well known, increases. I think it's well known, increases the reliability and validity of diagnosis. We also have very extensive pharmacy and albentinist and lab data on these individuals that can be linked into all this diagnostic information as well. So just some types of data and the multi-dimensionality of it. ECGs, and we've done analyses of these is ECGs, over 70,000 subjects, MRIs, demographic density, I'll mention this, also 45,000 women. Everybody basically has had ophthalmology. Everybody basically's had authologic exams, audiograms on large numbers, extensive limit panels of glucose, blood pressures, MMIs. Can I ask a question? Yeah. So I know that JARA's genotype data is not amusing, actually. But all this beautiful, amazing data set that you're describing, how available is it? Yeah, yeah. But okay, so what you do is actually, because it's not, so you can't just download it, you have to have an internal client. You have to have an internal Kaiser collaborator. So, what you do is when you apply through the portal, they try to identify an internal Kaiser person that you can collaborate with. And many people have done this, and they help with programmers extract exactly the data that you want that you need to make the key types of analysis. I'll show you a few examples of that. I have a follow-up question. So I have done this quite a few years ago and it took quite a long time. It has for me extremely helpful, but the process was extremely long. Extremely remodeling. Yes. But I have a student, I warned him that he's going to do a blood pressure analysis. I warned him it may take a few months. Right, there's a whole bureaucracy that you have to deal with. Correct. Yes, because there is a bureaucracy. But I just want to let you know, me and my folks are not advantaged, and it either we can do the same thing. Yeah. So, you're not the only one. And believe me, we completely do it, but whatever. It is what it is. They have limited resources, probably what it's called, also. Resources is probably what this is about, also. And they have to pay for it internally. You don't have other funding, you know. So, so this is a property of Kaiser? Yes, these are all patients. This is all Kaiser patients. And the funding, you know, this grant was funded and required to deposit data in PBGAP. And you can get a lot of data out of DBGAP itself. If you want more detailed phenotype data, then you have to go to Kaiser to report. Now it's called, I can talk about this later, it's called the Kaiser Research Bank. Now it's up to 400,000 individuals, but you're going to wait. 2,000 individuals, but you're going to wait for that too because they'll be doing what they've done the genotyping and mutation. There's a lot of stuff they're doing for that data. It's going to be available, but again, there's going to be a wait. So just warning there. You can decide if you think it's worth the wait. I think it is, but so you mentioned the genotyping. These were several people that look well, and really it was Puy Kwan, my colleague Puy Kwa, who did this. He's a dermatologist, geneticist, biologist, the task of The task of 100,000 individuals in 14 budgets with new technology. And we got a lot of help from alphabetrix helped us. It was a lot of hands-on experience. Stephanie was the postdoc. He assigned the array. Tom Hoffman designed the arrays. And Mark Folly is our genius informatics guy who did all the QC on all the arrays that were coming through. Each one of these arrays was like $100,000 worth. And if there were problems coming out of it, it would have been a disaster. So this was really a blank conference. Of course, you never. Of course, you never know about this, you never read about these things because it's all history of how these things happen, but it was really one year. And everybody learned a lot, including Affymetrics, as now Thompson, and now they've gone on. The VAMVP, UK Biobank, have all used the same technology, and it's really helpful to them with all this experience we had in terms of designing and executing these ways and analysis. This is showing the ethnic distribution. The ethnic distribution. I already mentioned that by race, ethnicity, we have more detailed questions, but you can see quite a few different kinds of Asians, Asian populations, Latino populations from different parts of Latin America, and African Americans and so on. So here just, again, I wanted to just give you a few examples. I'm just going to go through this very quickly. So we have colleagues there. We were interested in studying glaucoma. The other thing we can do, this is just an example of the genetic gradients. The genetic gradients of risk. So, what you're looking at, now it's known that glaucoma, primary POAC, primary open glaucoma is more common than African Americans, and that's reflected, if you look at the bottom line, in terms of a genetic access. This is from PC analysis, that you can see that, or it's from an admixture analysis, that the more European on you are, the lower you risk. Over the right, the more red, African. But maybe, maybe more interesting to many people this audience. To many people in this audience, was this. So in the East Asians, on this y-axis, the x-axis is again, which you see, people who have 50% of the middle of European U.S. history. But what you're seeing on the y-axis, the red at the top are people from Northeast Asians. So that means Japanese and Koreans. The blue are Chinese. Further down are Southeast Asians. The down and the black are 14 years. A very dramatic climb. You can see the scale there, a very dramatic climb. You can see the scale there, a very genetic line of risk of COAC from the north to the south in East Asia, which I think probably some people know, but maybe not as well recognized as the high-risk African Americans. Okay, and this is just the results of our GWAS analysis, 14, because of substantial, at that point in time, a fairly large number of cases. Now, the other thing we have is IOPMA, intra-Iclaim pressure measures on all these cases. So, this is where we have a real advantage. So, this is where we have a real advantage because now we have 357,000 ILP measures on 7,000 people. So, this is again, I talked about the importance and value of repeated measures. So, in this case, we discovered 47 lozide, but in this case, 40 were novel because we had such vast amounts of dating around IOP. Emmographic density, the same kind of story. Now, again, 31 hits, of which most were novels. 31 hits, of which most were novel. Again, as my co-author stated here, tripled the number of hits for MIMA density phenotypes. But I'm not going to read this, I'm just going to tell you. This, the one line here, all measurements were performed by a single radiological technologist trained by the software developers. So this was not just, you said, clinical stuff, you can take it, but it can't be used for research. This required somebody, this took eight months for this person to sit down and read all. This person sit down and read all these mammograms and score them in terms of density measurements. You can see 23 batches. They used two systems, logic and GE. They had a meta-analyze across these two different systems. So this, I'm just saying this, because this was a very clear example of how clinical data has to be manipulated and the intensity of what effort that it may take to turn it into useful research data. And the validation of that approach was the fact that we got some. Approach was the fact that we got so many hits from having draft hits that were novel. And you can see that prostate cancer, another good example here. So we added some new hits for just prostate cancer. Among the males who have prostate cancer, a fairly substantial number in the homework. But at the time, this is now we're talking about characterization and what you can do in longitudinal data. So there had been this rare mutation, this G84E mutation Hotspot 13, which was associated with the 1980s. Hotspot 13, which was associated with prostate cancer. Turned out they argued whether it was ancestral or not, whether it was a haplotype, we actually found there was and were able to impute it. It was not genotype, but the imputation R-score was about 0.57, which is still pretty good. It was enough to do an analysis. But what we did is took prospective analysis of all the male members of this cohort in terms of their risk of developing prostate cancer as a function of how they were. And prostate cancer as a function of having this mutation. And the top figure shows the lifetime risk up to age, I think, age 80 here, for just who was determined to be a carrier of the mutation or not. But that was not taking into account the incomplete LD. When we do that, you can see we clearly show a risk profile of individuals who have evidence. So not only do we have like a three or fourfold odds of prostate cancer, we actually have the actual lifetime risk associated with carriers and not carriers disputation. Carriers and market carriers instrumentation because we have decades of longitudinal data on these members. So we know actually the incidence of when they develop, how diagnosed and prostate cancer. The other thing is we have data on all these other cancers that basically show at the same time, but these are in case control analyses, that this mutation is associated with lots of other cancers. And the fact of multiple primaries is an even greater increase in risk. PSA. Now here's just another example where Example where 35,000 men who did not have prostate cancer had 253,000 PSA measurements. This is like huge, okay? There's a huge number of PSA measurements. Here's the GWAS. What's in blue are all the novel hits. So again, large numbers of novel hits because we have so much mass amount of data because Kaiser does this kind of screening on MIT, you know, these PSA screenings. So 19 novel, 15 that were previously identified for PSA, but six. Identified for PSA, but six that were not, but they were for prostate cancer. So we also looked at the relation to prostate cancer. We also have relatives, we have brothers, we have sibling pairs, we have parent-child pairs in the core we identified for genetics. And we saw actually the correlation for PSA was high below the age of 54, but it was zero above age. So apparently the genetics of PSA is really, very ancient kind of. Lipids, we've done lipids as well. Again, so I'll get to the Again, so I'll get to the most important point here, I think. But million measures, this is years ago already. 500,000 were untreated. Again, the virtue of having treatment is we know what people have untreated, so we can take those data out of the analysis for a few losses, for example. So you can do it, and lots of novel hints here, and you can see them all in work. You know, for the different cholesterol measurements, okay, we looked at the heritability, how much was explained by these missions, a lot we discovered. There's a lot we discovered. We found a strong interaction between the secretor, which was sort of interesting. We found big differences in the polygenic risk scores for these Asians and for African Americans for some of the freedot types and so on. So a lot we can do. But here we can actually determine the lifetime risk, again, following people over time, when they got their first prescription for an anti-leupemic as a function of their polygenic risk scores for both LDL cholesterol and triglycerides. Sorry, I can. points. Sorry, I can show all this more later if you're interested in this, but the blue, green, and red are the different LDL, the threshold, the high or low scores for the LDL PRS, and the two shades within our triglycerides, both were predictive. Both apologisers were predictive. This is perhaps the most interesting, which is my former postdoc and I was my colleague at the W. Morrison did an analysis of dose response of these lipid statins in reducing. Of status in reducing LDL cholesterol. Can you do this in a cohort like this? So we have many people who had 33,000 had cholesterol measurements before and after starting on statins. So we could actually look at the percent change in their LDL cholesterol before and after the statins. Okay, and this is just the distribution of statins. Now, the intensity of the statins in terms of the LDL lowering is different from the different types of statins. This coordinate, historically, This cohort historically has mostly been on Simbostat and Pravastat, but we have some with Lovastatin and Simbostat. We have a few on Prava, but a bigger number on Torvostat. And we can stratify analysis based on those. But basically what he did was, I'll listen about this. Sorry for the extra lines on here. Just look at the bar class. So really what he discovered was a perfect linear, a log linear dose response. So the higher the dose, the more So, the higher the dose, the more the LDL reduction. So, basically, reproduce in observational data here, what you see in randomized clinical trials. So, of course, this is a much larger population than any kind of randomized clinical trial. And also, we can look at primaries and found that there was an ethnic difference in the dose response, you know, a sex difference in the dose response. So, there are things you actually can do in a cohort like this that are not really so amenable in a randomized kind of thing, including genetics. And one of the things we could do. Including genetics. And one of the things we could do is what's the heritability of response? So we have 230 parent-child pairs, 300 SIP pairs, and the correlations were pretty similar, about 0.06 for both. So the heritability is about 12%. So it's not just the heritability of LDL, but of response to status. So there's some heritability, but it's not huge. And did a GWAS. I'm not showing those results, but basically we got a few hits in terms of response. Just as the last example, we did this also. Last example, we did this also looking at fasting glucose. Do statins increase the risk of diabetes? So we have a real opportunity here because, and show the numbers, huge numbers of individuals who have fastened glucose measurements. And the thing is, some studies will look at stat users and non-users and show that the stat users have higher fasting glucose. But that's not because of the statins. It's because they start off before they are on statins of having higher fastening glucose. And that's what's shown in this figure. And that's what's shown in this figure. The red are the individuals who never took a statin, and the green and the blue are those who took statins before, the green is before they started a status. So the green, you can see, is higher than the red. And that means people, even before they start on statins, had higher fasting glucose than those who never wanted a statin. So fasting glucose is correlated with whether you go on a statin in the first place, not a consequence of being on a status. So that's an important confirmation. Being us now. So that's an important confounder. But what we did, again, here's evidence that we have from this longitudinal data where we looked at people over 20-year time. In terms of their fasting glucose, it basically show there is a shallow increase. And this is stratified by everybody. Also, you have to take the new council of drugs. People are taking bunch of drugs for diabetes or other things that influence a cassine glucose. And we had to do filtering, you know, based on whether they were taking these drugs or not. And you can see. Are you taking these drugs or not? And you can see, you know, there is, in every one of these groups, there is a slight increase, but not, it is highly statistically significant, but not clinically significant. So our conclusion from this is probably it's not a major factor in terms of increase in this. So I just want to finish and then by summary. So basically, you know, I guess the point I'm trying to make, this is my experience, people have lots of experience with these other biofanks and so on. From my perspective, again, From my perspective, again, as an epidemiologist, especially about causal inference, the relationship of genetics to all these outcomes, you know, is that the study designs are really important, the cathodes are really important. And the different cohorts, I think, offer different opportunities for doing different kinds of things. Okay. So here's a cast of many people, as I said, this is many, many people, at Kaiser at UCSF, Stanford, exactly. At Stanford Ex Animal Appropriations for Health. Here's our funders. I mentioned this. Okay, people. Yeah, so beautiful work. My question, in particular, regarding the last part, I think it's a beautiful example of using data. And I'm interested in how genetics could play. I mean, I'm interested in whether genetic profile can also predict their dose responsiveness if the same way. So, yeah, okay, I have. That was in that paper. No, that was in a separate paper. That paper. No, that was in a separate paper that he did. So, turned out to be a methodologic paper more than actually the results. There are, I think, four hits for LDL response from statins. Okay? But the problem is some people include, you know, they look at the change, the percent change, but they include the baseline level before statins as a covariate. That is a problem. We showed methodologically you're not supposed to do that. So they recapitulated actually things that are associated with the basic. Things that are associated with the baseline measure that are not really associated with the chain. So, but he did find valid results, like three or four. But I showed you the heritability is not so high to start with, but yes, you can develop polygenic risk scores. He's done this, polygenic risk scores for LDL response. It doesn't predict how well you respond to status in terms of preventing an MI. What does predict is your MIPRS? Your MIPRS is a predictor of how well you're going to respond to status. Predictor of how well you're going to respond to status. That is predictor, but the LDL PRS, no. Yeah, yeah. Yeah. Lovely work. Thank you so much for presenting such a wonderful overview. One thing that I really appreciated about your talk is you mentioned sort of the converting the odds to absolute risk, and that requires a lot of longitudinal data. Well appreciated point. Genetics, the scale you want to say is feasible in some cases. Feasible in some cases, but I'm curious about your thoughts on not just absolute lifetime crisis, but also onset progression, diffractal diagnosis, and how new biomarkers that are being scaled in bioxystays like Hytipa plagomax and phiomics and other assays might be able to help facilitate that. Awesome. Yes, you know, all those things, yes, all those things you can add in. But again, I think there's such value. I don't know what you guys think. I just think there's such value in the longitudinal data. There's just so much power, and you can address so many more questions. Like, and the real comparing to RCTs, RCTs last only a couple of years, maybe a couple of years, right? And how could you look at this question about the diabetes? You know, because people take statins for decades. Some people don't. They stop. But that's another big problem, is the adherence, you know, taking account of that. That's not so easy. But, you know, those are the important questions, right? But people want to know. And putting biomarkers in there, we don't have them in our homework. Other homeworks do. That, yes, certainly that can help answer all these questions. Completely smash up. Yeah, yeah. Thank you. Couldn't be smashed. Yeah, yeah.