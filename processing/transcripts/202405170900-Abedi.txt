Okay, hello. Thank you for this opportunity. It's a great pleasure to participate in this workshop. So I'm going to present the work on para-STG method that is mainly developed by my former advisor from the University of Illinois. And I work on that while I was, I mean, at Illinois and after that. But I didn't know after that. I don't know why this is not okay. So, this is the outline of my presentation. I should give a heads up. Unfortunately, we didn't get enough time to implement any traffic spaces function in our method. However, I tried to include a couple of things that are to some extent related in terms of reducing the matrix or, I mean. Reducing the matrix, or I mean calculating any integration or reducing the integration. So these are the two parts that have some connection. So I'm going to start with the overview of space-time, this causal space-time discontinuous gathering method. So this is a schematic that we generally use to explain the method. So basically, if this is the space axis, which can be one to three-dimensional problem. Dimensional problem and the vertical axis, we use that for all the subsequent results for time. So we can lay out the space mesh. I mean, this is a very structured mesh in space-time, but if we lay out the mesh such that on the boundaries of these elements in space-time, the waves are either only outgoing through these phases, which we call them outflow, or they enter the element from these red faces. Spaces. This element in space-time can be solved locally. It doesn't depend on anything that goes in the future. And basically, these facets are causal facets that we have one direction of flow of information. Now, if we start from the initial condition, we have these elements labeled at one. We can solve any of those based on initial condition or here from initial condition and boundary condition. Initial condition and boundary condition. And in fact, these solutions can be done in parallel, which I will talk shortly about the parallel implementation of the method for the elements. So like as soon as for example, I have these elements number two, number one, I can go ahead and solve this element number two and so forth. And this doesn't require the solution of the other element number one. So it has a very nice low-cost solution feature and Feature and even though we have a lot of these same good properties with explicit time marching schemes that we can solve one element at a time, the fact that in those cases the element facets in time are vertical, we have the flow of information in two ways. And one of the disadvantages is that the time step of explicit methods would be affected by polynomial order, for example, can be decreased by more than a factor. Decrease by more than a fact order of magnitude, say if you go to element order of five or higher. So that's the thing that these methods, the only time step constraint is really the causality constraint. So no matter what order of the element we use, we can actually go to the maximum stability limit allowed by wave propagation. And one other thing, obviously, these methods are designed for hyperbolic PDs. I will comment on how. I will comment on how one may think about some extensions to parabolic PDs later. So, this is how the method works. We start with a like in 2D cross time. We start with an unstructured triangulation. We peak a vertex and then we pitch it in time. So, we form these tents and we solve that tent, I mean, that collection of elements, which we call a patch. Collection of elements which we call a patch, then we erect another. So, basically, this is if we accept that patch of element, this is already added to the space-time front. We pitch another vertex, and then this process continues. Every time we need to pick a vertex that is the time coordinate of that vertex is a local minimum, and starting with the triangulation, we end up with a space-time mesh. So, this is a method that I didn't. That I didn't include all the background where the method initially was developed, but we were not the first people who came up with this idea. It was Falk and Richter. I think that was a very influential paper for our group for neutrons. I mean, so that was for some kind of advection equation that laid out. Equation that laid out the groundwork for our implementation and formulation. So, next, I'm going to talk about adaptivity operations that we have used in our methods. So, for a long time, we were limited to 1D problems and 2D problems, which we call 1D and 2D cross-time. And in two dimension, we used an algorithm called New S Bisection Algorithm. So, basically, Bisection algorithm. So basically, this is we are looking at the space mesh from the top. And then as we erect a patch, for example, we get this vertex, as I mentioned in the previous slide. We create a patch of elements and we solve that locally. Now, when we want to do adaptivity, for example, I have solved this element that we can see the boundaries in space-time by these blue lines. We need to refine one or all of these elements. All of these elements, and the way that it works, elements have these markings that this is the vertex that would bisect the opposite edge, and then, like, for example, in this case, let's say this yellow element is marked for a bisection, so we bisect that, and this creates its own marking of newest vertex. And then, unfortunately, one of the things is that this algorithm continues refining the neighbor elements, and at some point, this algorithm stops refining. This algorithm stops refining neighboring elements. And in this case, for example, we see that in this, the refinement chain is completed at this location. In schematic of how the refinement works in general, if looking at the 1D cross time, if you have large errors in this location in space-time, we discard these two elements. We refine the front. I mean, as we can see, the front here is refined. We can see that the front here is refined, and then the next one, the temperature algorithm starts, we end up with smaller elements. And this is exactly how the front is refined in a 2D application. I will mention what are the challenges in the 3D cross-line with whether we can use this algorithm or not. However, let's still continue with adaptive operations that we use. So, opposite to refinement, we do coarsening. So, the coarsening 1D cross time is very easy and is good to explain the general idea of the coarsening. The coarsening is basically the opposite of refinement. So, in this case, we see that the front initially for the 1D cross time, there are these two line segments, and we can create one element that fills this gap in space-time. It doesn't involve any projection error from an old mesh to new mesh, and that basically And that basically we end up with one long segment as opposed to here. In 2D cross-style, we have similar operators. For example, this is one patch of elements. It actually has three elements. If you look at the inflow of this patch, it has five triangles, but the outflow has three triangles. Our coarsening operation in 2D cross-line generally reduces the number of triangles in a region by In a region by a number of two. So, like here, we go from five to three. We have implemented in 3D cross-time as well. And so, with refinement and coarsening, if we add some other mesh adaptive operations, we have a lot of flexibility. And again, we see that this coarsening, we use that as also a mesh smoothing operator because it changes the triangulation up front. We do edge flips in space-time. Edge flips in space-time. The benefit of that, we don't have any projection error as opposed to going from a space mesh to another space mesh. This space-time region actually transfers the solution from the old front to the new one. Another very useful operation is basically when we erect the tents. For example, here I go from E to E prime. Often we erect the tents vertically. One option is we erect it non-vertically. Is be erected non-vertically. So basically, by doing so, as we can see, the location of this vertex E is changed to a new location E prime and we can improve the quality of the mesh. Or in fact, we can track some moving boundaries using this trick. In fact, using those operations here, I'm going to show some examples how we can use the tracking algorithm to track some solution features. Track some solution features. These solutions on top are sod shock tube problems. So it's a 1D problem. So this is again horizontal axis is space. The vertical axis is time. And the color of the elements is basically the polynomial order. Zero's order is polynomial order one. In this region, we have a rare fraction wave, so that's where we are going to high-order polynomial order. Going to high-order polynomial order, and here we have a shock contact discontinuity. And these are the regions if you just simply use adaptivity to refine the mesh. We end up with a lot of elements, about 473,000 elements. However, if you use our mesh tracking algorithm, something like this operation, that we move the vertices in space-time, we can cut the number of elements by about a factor of a thousand. About a factor of thousand. And you can see that we exactly align the element boundaries in space-time by shock and refraction wave fronts. So this is the same idea applied in the 2D. So this is a shock capturing versus this is a shock, I'm sorry, front tracking, this is versus front, I mean shock capturing, this is tracking. So in terms of application, we have used this method to a lot of We have used this method to a lot of hyperbolic partial differential equations like Euler's equation, hyperbolic advection diffusion equations, including a hyperbolic version of heat conduction, elastodynamics, electromagnetics, and most set of focus has been in solid mechanics, again, elastodynamics with fracture. So, I'm going to show some samples. To show some sample results from solid mechanics, so this is a wave propagation, it's a quarter of a plate that we send 10 solid wave from the top, and I'm just showing this portion of that. It hits a crack tip at this location, so right here, we have basically this is the shear wave, this is the pressure wave, this is the Rally wave. So we get all this wave reflection. All these wave reflections and wavefronts. And if we look at basically how the space mesh evolves, sorry. So here is the space mesh corresponding to that adaptive operation. We track the wavefront, as we can see as the sharp wavefront enters the domain, we get a lot of element refinement. Law of element refinement, and behind that, you have coarsening, and then I'm sorry, and this vertical direction is time if I didn't mention that. So, this is where the wavefront hits the crack tip, and then these are the wavefront of the shear wave and the pressure wave. For example, here, the pressure wave has a milder wavefront, so it's not refined as heavily as that. So, one of the advantages, I mean, I think the best advantage of this asynchronous space-time method is we can get very high refinement ratios with no problem. So, what we can see is that, for example, if we draw a constant time value here on this blue line, we have elements that are in the shop in the wavefront. But in the wavefront, the time step, the time advance of this region in space-time compared to the largest time advance we have, actually, we can easily on a like very modest computer, we can reach refinement ratios of 10 to the 4. And obviously, this is an issue if you wanted to use a synchronous time margining scheme, we would have a very difficult time. We had to use one of these approaches, like implicit, explicit, sub-sub. Issues like implicit, explicit, sub-cycling, or any of these things, they have algorithmic difficulties and they may not be performing perfectly. That means that not necessarily every element reaches its own optimal time advance. Whereas if you use CSTG method, we see that the smallest element and the largest element, every element would take the maximum time advance, which, as I mentioned before, this time advance is not a This time advance is not affected by the order of solution, which is again another very good advantage. So, these are some other examples from adaptivity that we do in space-time. With those mesh, so the smoothing operations I mentioned, so like that's the one that we said we can erect the vertices non-vertically in time, so we can basically get these element interfaces between the elements and move them to a new oriented. them to a new orientation or by doing the refinement we can actually track moving interfaces. So this is the case that we have adjusted the wave the front of the mesh. So basically this is the front when we look at from the top and we can align the element boundaries such that it can capture these very complicated fracture patterns in time. So like these are going from low These are going from low energy to a high energy input for fracture, and you know, in fracture, the higher energy input to the system, we get more and more cracks to dissipate the input energy. And this is an animation of this process. So we see that the mesh is basically evolving and changing in response to crack propagation. And it should be noted that as the That as the crack propagates, these elements are extremely small to capture the crack propagation, and we can even do some sort of smoothing on the fracture after the crack has passed the certain region. These are some other applications for earthquake modeling, and that's actually the project that we were working on parallelizing the code and extending to 3D main. Many faults have very complicated geometry. So, there is this main fault, and there are some sub faults that we need to simulate. Now, there are very interesting physics that occur from this basically fishbone type networks of faults. And it's a very multi-scale problem. We have to go from kilometers for geological length scales, and the fracture process, where, for example, this crack starts to propagate. Starts to propagate, actually, they can initiate from length scales that are in millimeter to centimeter length scale. So it's very challenging. I mean, a lot of existing results in the literature, even with high level of adaptivity, they get to one meter length scale, but we have been able to, with this basically, very nice feature of the method that I mentioned, we can have very different time steps in different regions. Different time steps in different regions in space, we have been able to get to one millimeter length scale where the fracture starts with very high order polynomial orders of 12. So, there are very nice physics actually be sort of discovered by getting to this very fine time scale. So, this is a snapshot of basically a dipole-like fracture propagating from the initiation of the. Propagating from the initiation of this fault. So, these are some other examples of adaptivity that we can achieve in space-time from fraction mechanics. So, second part, I'm going to talk about the parallel implementation of the method. This is the part that I personally know the least amount. This is basically all my Bob Hager's work at Illinois. At Illinois. So I'm just going to provide a very short overview of the work. So there are two key challenges. One is if you have a time marching scheme, so like this is time step n, time step n plus one, time step n plus two, a great deal of parallel methods are bound by this barrier or this constraint, like basically bulk synchronous parallel methods. We have these time steps, we have to. We have these time steps. We have to stop at this, and all the processes, parallel processes, need to get there. And obviously, this we end up with some wait time. And going to exascale computation, actually, there are some issues reported with basically noise and things like that that basically decrease the efficiency of the method because of these time barriers. So, we are going to talk how the CA. How the CSTG method alleviates that. In fact, there are a lot of very interesting works in this field from many different groups that use temperature method, and all of those directly benefit from that. So the algorithm is asynchronous. So the method already starts from a very good position that we don't have these time barriers to synchronize. The second one is. The second one is, so we don't have the time synchronization, but how about domain decomposition? And should I, for example, let's say this left side of the domain with a very highly defined mesh, should that be one domain and this one the other domain to break the load between different processors. And again, we have a solution to not do the domain decomposition method. In fact, for a long time, But in fact, for a long time, we were looking at avenues to do domain decomposition, but we never got very good scaling, especially when adaptivity happens and I mean, things like that, that basically completely changes the balances of load. And none of the methods that we were looking at was performing well. However, what I will present today, I think that this sort of the local approach that we have is. The local approach that we have is doing much better than that. So, this is the architecture of the parallel, or everything we try to make it very local. So, there are three different components of this parallel design. One of them we call it the server. I just very briefly say what the server does. Server is actually works on distributed memory, what it does. So, since we are not so. So, since we are not, so this is a space-time domain I'm showing here, and this is this top part of it is basically the front of the mesh. In the domain decomposition method, we basically break this into chunks. We are not going to do that anymore. So, we are going to just any of these servers have a collection of vertices that we will create space-time meshes. So, like, for example, this server would have maybe a vertex here, a vertex there, a vertex there. A vertex there, a vertex there. So they don't have to be a continuous set of, I mean, neighbor set of vertices. So the goal, I mean, the task of these servers is basically keep track of these, each one have a list of vertices that would send to solution and data. The second component is what we call a Wrangler. Wrangler basically, at this point, this is a shared memory. So this is a distributed memory. Wrangler works. Distributed memory, Wrangler works with these servers, gets the vertex, a smaller collection of vertices that it should every time gets a vertex on that vertex, get all the neighbors of that vertex from the server, erects a patch around that vertex, and sends it to the solver. And the solver is completely, I mean, is embarrassingly parallel. So those are the components. Again, it's the server that manages. That manages the distributed collection of vertices, communicates that with a shared memory Wrangler that gets the vertex, erects a patch around it, and then sends that to the solver. And in a break, I mean, roughly, I mean, this is an example of a 4D socket node, sorry, 4D core node. About 5% of our About 5% of our basic computation went, or I mean, core distribution went to having basically one server for 40 cores. One server actually does all these Wrangular work for all the Wrangular and Solver combinations. And then the rest of them, 90%, actually, 18 out of 20, they are actually the ones that carry the heavy load and solve this space-time. Solve these space-time elements. So now I explain the two processes that make our algorithm local. One of them is what we call basically so let me see if I can play this movie. So this is how we distribute the front, and the idea is the following. Front and the idea is the following. Again, as I mentioned, these servers keep track of the vertices that are on the front. And there are so many vertices. And I mean, obviously, we can have thousands and thousands of vertices and a large number of servers. So it's very expensive in parallel that we communicate with all of these vertices, sorry, all of these servers and say, how many vertices do you own? Vertices, do you own at the moment? Like, and then see which one is underloaded and then distribute vertices to that. So, the way that we do this is an algorithm calls balls into bins. So, for example, at every moment, instead of dealing with basically a large number of servers, we only sample a very few. For example, we work with only five. And then, among those five servers, we ask, I mean, we basically query. Ask, I mean, we basically query how many vertices each one owns, and then the ones that are underloaded, we send more vertices to them as I showed in this movie. Sorry, this, let me, if I can show that first movie one more time. So you see that initially we put all the vertices in one of the processors, on one of the servers, but very quickly they get very nicely balanced. Very nicely balanced. And the second part is going to the shared memory part: that these wranglers, what happens is these wrangulars, each one get a collection of vertices from this top level. And we want the fronts, basically the key idea is we want the front to be more or less flat, as much as flat as possible. Otherwise, we would have issues with causing. Have issues with causality, whether away from this location reaches another location, a lot of global coupling. So that's the thing. We don't want any of these solver wrangler collections to get too far ahead in time. So before I show the movie, the idea is this. Again, the query is not global that we query every Wrangler. It's something very similar to that. We only sample a small collection. And by the way, this convergence of this is logarithmic, means that. Of this is logarithmic, means that with five samplers, it's as if 10 to the five numbers are efficiently sampled. What we do, we check these Wranglers, each one that has the farthest ahead vertex in time, like a horse race, the horse that is farthest in the race, we put more load on them, so they are slowed down. This movie shows how that algorithm works in terms of not. That algorithm works in terms of not letting any Wrangler solver to be too far ahead in time. You see that these are the time values of all these Wrangler solvers, and basically they're more or less all at the same time value. So as you can see right now, everything is local. So this part of the algorithm replaces a domain decomposition method because we don't break the front into chunks, and this one. Into chunks, and this one breaks the barrier of global time step. And as I mentioned, any 10 picture algorithm has this advantage that the time step is actually asynchronous, time advanced. So these are the parallel efficiency plots, number of cores, nodes versus efficiency. This is the strong scaling, this is the weak scaling. Obviously, the scaling drops a little bit, but as you can see, the drop in efficiency. But as you can see, the drop in efficiencies slows down. I mean, this is sort of a model that estimates how the drop would go as we increase the number of nodes. The asymptotic behavior is very nice. We still are working on some issues. I mean, the biggest hit in terms of efficiencies we do a lot of dynamic memory allocations in our code. So that's the thing that we are trying to address and basically improve the efficiency. And basically, improve the efficiency here. So, now going to the 3D implementation and the challenges with adaptivity and parallel computing in 3D cross-time. So, unfortunately, a lot of things that we used to employ in our method in 1D and 2D cross-time, they break down in 3D. For example, I showed some results in the newest vertex, sorry, refinement algorithm. That doesn't. Algorithm. That doesn't work in 3D. We don't have the kind of mesh quality measures that would work with the space-time meshing in 3D, in a sense, to guarantee that the mesh doesn't lock as it makes progress. So those are things that our collaborators from computer science were working on. We're working on the short story, I can just give an update. So, the thing that we do, one of the things that we started using, we just went to a very simple algorithm. If this is the patch of elements, like these five triangles, if you want to do refinement, we refine the longest edge. And there are some things that we can show that this refinement terminates within this footprint. And actually, the algorithm doesn't lock with that kind of refinement. So, that's the one that. Kind of refinement, so that's the one that we have adopted in 3D. I don't have any 3D with refinement that solves a real physics problem, but this is a made-up problem that basically there is this ball that moves this in this 3D cuboid region. And then wherever it falls, we need to do refinement. So I do a cut on this cube, and we look at the trajectory of this ball. Trajectory of this ball in basically space. So that's how basically this moves in that region and we do parsing behind that and where the ball is we are doing prefine. Okay, so this is the part that I'm getting to things that unfortunately they're not trephlets. I would have loved to have a I would have loved to have a chance in the last few months to add that to our capabilities, but didn't get a chance to do that. So, there are things that somehow we were looking in an algorithm to eliminate some integration in our method, better there on facets or therefore the whole set of integration cells. I mean, basically, interior of. I mean, basically, the interior of the element and the facets. So, the first one we were looking at, it's what we call the Riemann solution-free method. So, we formulate everything in space-time. So, this is, for example, a space-time patch that is, again, this is space, and this axis is the vertical axis is time. So, we have this non-causal facets. If I look at sort of from the side, it is. Sort of from the side, uh, if this is the time axis, this is normal to this blue facet or cyan facet. The facet itself has a certain orientation that falls in the non-causal region. Basically, it's the time-like facet, and the wave actually goes both ways. So, this these facets couple the elements on the left and right, and we have to solve the Riemann solutions for those or some other numerical. Other numerical fluxes for these element coupling facets. By the way, one of the things that is rather unique to temperature methods is we can have these non-vertical facets. Sometimes they fall into different regions of for hyperbolic problems. We may have different regions with different solutions, Riemann solutions, whereas in a lot of Whereas in a lot of DG with conventional time marching, these facets are always vertical in time. But anyway, these solutions can be expensive, especially if you go to non-linear problems like Euler's equation, magneto-hydrodynamic problems, or things like that. So one idea that we had, instead of having two individual elements, we can combine the elements into one super element. The elements into one super element like this. So, this becomes one element entirely, and basically, we don't have this interior interface. And because of that, we don't have to solve any remote solution. So, this is one trick that we can get away from solving any integral that that's a very weak link to the benefits that a TREFETS method would provide, reducing the integration. Reducing the integration. So that's a basically comparison how many basically degree. It also, another benefit is decreases the degrees of freedom. So for example, going from, I mean, 2D, in 1D cross time, every two elements are combined to one element. In 2D cross time, every six elements in average are combined. And in 3D, every 24 elements are combined to one element. The only downside is the element diameter based. Downside is the element diameter basically doubles with this sort of forming these multi-cell elements. So, is it more efficient? Unfortunately, not this Riemann, I mean, basically these supercell elements with no Riemann solutions are less efficient with our original formulation. So, to explain this plot, so the x-axis shows the log of the error of the method and the vertical. The error of the method and the vertical axis shows the wall clock time. So, and these ones with larger markers are the ones with the multi-cell elements. So, these Riemann-free things. So, if I cut a line, we see that the multi-cell elements have basically higher wall time, sorry, larger error for the same wall right time, so they're less efficient. However, if we go to problems that However, if we go to problems that the individual patch solution, so this unit solution has more degrees of freedom, the script changes. So if we have three field formulations that displacement, velocity, and strain are the inputs to the problem. One benefit is actually everything converges with, like for in this case, displacement, velocity, and strain, all of them have optimal convergence rates. And the other advantage is the individual patch solutions are larger, and that's where we get the benefit of these multi-cell elements. Not only they eliminate the Riemann solution, but they become more efficient. So we see that these bigger markers for multi-cell elements now are to the left. So they have a lower wall clock time for the same error. So that's one idea. So that's one idea. The other idea that we were looking at, and I'm pretty sure a lot of groups have done this. My apologies if I'm missing anyone. That I mean, this is something there are way more advanced ways of doing that. So we basically, if we have a structured mesh in space-time, we can categorize what kind of elements we encounter. For example, this is the x-axis, this is the y-axis. We see that. This is the y-axis. We see that there are only basically three kinds of elements on the left side, three kinds on the right side, and three kinds in the middle. And we can basically solve this transfer from the inflow to outflow of these elements, and that becomes a completely matrix-free approach. And so, this is basically shows how this idea works. If we look at this patch of elements, we have this. We have this inflow, the solution at the inflow facet Gauss points. So these are the inputs to the problem. And the goal is to transfer whatever the solution are at the inflow Gauss points to the solution at the outflow Gauss points. And then since the stiffness matrices, I mean, basically, the inputs would be whatever the input is, the Dirichlet boundary condition inputs are the Neumann boundary condition inputs. Neumann boundary condition inputs are, and we can basically translate that to element solution coefficients. And from that, we can basically find the outflow values. So it's a very simple transfer matrix approach. So go from inflow to outflow. One can look at this similar, it's similar to a finite difference stencil. The benefit we can increase the accuracy of this to very high orders by forming space-time elements that have maybe. Space-time elements that have maybe order 12 or something like that. So these are what we only have, unfortunately, implemented this for 1D. So we can get very quick, like very fast solutions for hyperbolic. And as I will show you also for parabolic cases, that there is no matrix, no integration done for each for any of the elements, no integration. And we just map from each. And we just map from inflow to outflow and then form the entire space-time solution. I just have to, one of the things that I remember is very related to this is obviously the work from Indria, Dr. Baruch, Dr. Diaz, and all the people who worked on that. I mean, they're similar, I mean, these structured meshes, they have this very, I mean, the collection of different kinds of space-time elements. Space-time elements that basically the same idea is applied or can be applied that one maps from the inflow to outflow solution and one do this mapping once and it can be used over and over after that without solving any element solution. Okay, and the final section I'm going to talk about a few of Talk about a few of the extensions of the method to other differential equations. Unfortunately, a lot of advantages of method are by design only applied to hyperbolic PDs. Like, for example, this local solution, the fact that the time advance is independent. Like, I mean, if I go back here, how much this is advanced in time. How much this is advanced in time, the vertex is completely independent of the order of the elements. Like that itself can increase the efficiency of the method by a factor of 10 or more for polynomial orders of five, I would say, compared to time-marching schemes. However, a lot of these advantages right away disappear once one to solve, say, parabolic transient problems. However, we never actually. Problems. However, we never actually could solve any parabolic PDEs. We just started to look into that. Again, it's the same idea to, first of all, to examine what's the stability limit of the method if one wanted to do parabolic PDEs. So again, the same idea. We can have the solution at the inflow Gauss points, and then we have the solution at the outflow Gauss points. We can form the transfer matrix as before. Formed the transfer matrix as before and do a spectral stability. Make sure the basic spectral radius of this map is less than one, and that would provide us how far we can pitch the vertex if we are dealing with a parabolic PD. And the time advance, obviously, for parabolic PD is scaled proportional to element size squared. We are just interested in this correction factor based on the order of the element, and we see. Order of the element. And we see that in this case, the spectral radius as we increase this beta for different polynomial orders, it just the spectral radius suddenly tends to infinity, and that's our stability limit. And that's what we basically see. Like, and it's a very sharp, unfortunately, this case, as we expected, going from order one in space-time to order five, the time, that correction factor drops by a factor of seven in this case, which is In this case, which is not very nice. But I mean, the good news is this method can be applied to solving parallel PDs with the same 10-pitching idea. And these are sort of the error convergence rates. Let me skip that. We have done this stability from many different perspectives, global stability, local stability with periodic boundary condition. We have done the stability from From Neumann's point of view, basically having block boundary condition. And again, answering this question: how big this correctional factor should be that the von Neumann method, basically the dissipation is non-negative, and that gives the stability. All of these methods give very similar stability limits, but again, as like previous. As like previous slide, higher polynomial order, this correction factor sharply decreases. And these are basically the dispersion and dissipation errors of all the methods that we are considering. Basically, this is the original method that we were working with, the hyperbolic PD that we always had been working with. This is the damp version of that, basically, like a telegrapher's equation. And this is the parabolic. Equation, and this is the parabolic equation that we now can solve. I'm not going to go over what appears from like the convergence rates of dissipation and dispersion, but in short, dispersion has a better convergence rate in general. And like many other studies in the literature, other than even there is a pairing of like every success. Like every two successive orders would have the same order in dispersion and dissipation errors. Now, this is what we right now have that we have time advanced limits for any kind of transient problem, hyperbolic or parabolic. So, this is, for example, if I have this damped hyperbolic equation, so this is a hyperbolic heat conduction equation. Heat conduction equation. If we don't have this term, that tau c term, it's a parabolic equation. And if tau is non-zero, it's a hyperbolic. If the element size is much smaller than the time scale, sorry, the length scale obtained from the elements, or basically this is the peculiar number. The stability limit is very much the stability limit that we have, the hyperbolic. Stability limit that we have, the hyperbolic PD is basically proportional to wave speed, sorry, inverse to wave speed proportional to element size. And when the element size gets larger, we go to larger space and time scales, then the problem is very much well approximated with the parabolic part of this equation. So we can ignore that. And that's basically the obviously the motivation how these hyperbolic heat conduction models are approximated well with the Fourier heat loss. Approximated well with the Fourier heat law. And in this case, we see that for different polynomial orders, we have different stability limits. So, what we are right now doing, our method, the temperature, works as usual, is just this stability limit. We feed this one to see how much we should advance for, I mean, basically, depending on the element size and the PD that we have, and solve it like that. So, these are some sample problems in 2D. So, this is a thermal. 2D. So, this is a thermal channel problem. This is the hyperbolic solution of that. We have all the wavefront features of a hyperbolic solution. This is a very smooth version that we get from a parabolic solution. And again, this is a heat propagation in a composite, fiber composite. This is again a parabolic solution. This is a hyperbolic solution that now we can get probably. Solution that now we can get method. And as the final topic, we have tried to apply the method for the solution of elliptic PDEs. The one problem that jumps out as a very good candidate to use hyperbolic solver is Helmholtz equation. It has basically very high wave, I mean, a high spatial frequency or Frequency or short wavelengths features in the solution, and this is the one that and the problem is indefinite. That's a problem that a lot of elliptic problems, sorry, solvers have difficulty solving. So, these are some very nice results in the literature that, for example, in this paper by Apollo, they have used a hyperbolic solver, what they call it, wave holds, to turn the elliptic problem, the Helmholtz. The elliptic problem, the Helmholtz equation, to and basically use the corresponding wave equation and solve it. And obviously, this is not the idea that has been around for a very long time. The idea is we solve a wave equation and we want to basically for the period of interest for the Helmholtz equation from time zero to time n times of the period from again Helmholtz equation. We want to get to a condition. Helmholtz equation, we want to get to a condition that the solution here is identical to the solution at the final front. And once we get to that condition, we can turn the hyperbolic solution to the corresponding elliptic solution. So there are a couple of different approaches in the literature. One of them is just run the problem long in time, a hyperbolic problem long in time, and eventually it reaches this condition for a long enough time, the inflow. Long enough time, the inflow, I mean, the time solution at a given time and plus that time they would be equal, and that's where we can go ahead and calculate the elliptic solution. This works when we have some loss in the problem. The other approach is we use some linear algebra operator idea, like it's basically the solution from the inflow to outflow is we look at as a linear operator. As a linear operator, and we use something like gmres or conjugate gradient to solve the solution, and this is where we apply this technique for our, I mean, in our setting, we try to solve Helmholtz equation. One advantage of time domain solution is that with one space-time solution, we can get the solution at multiple different frequencies. And these are basically 2D solutions of Helmut. Are basically 2D solutions of Helmut SQL, some sample one. Unfortunately, this was a couple of years ago. I did this with a very coarse mesh resolution. We haven't gotten back to this to make it a higher resolution solution. But with that, I think I just that there is one more thing so that we can also solve problems that other problems that have difficult direct elliptic solution. Like, for example, when we have shocks, the steady state. When we have shocks, the steady state Berger's equation or steady state Navier-Stock solution, one can use either the parabolic solver or hyperbolic solver, run it in time until it reaches sort of the steady state solution. That's the other one we have used. So, with this, I conclude, and I leave the because I think I'm a couple of minutes over time. I'm a couple of minutes over time. I'm open to questions, and the conclusions are here. Can you hear me? I can hear, but it's not very clear. Okay. Now it's much better actually. Action, yeah, okay. So, thank you very much for this impressive talk. So, now we have time for questions.  I think that's the first one. Yes, absolutely. So let me go back to Yeah, I think that's a good place. So if we have imagine this is a space mesh like a time marching method, that here we have five elements in the front, and obviously there are some more elements around it, and then we want to. Around it. And then we want to change the layout of the mesh. For example, a coarse setting operator, I want to end up with three elements in the next time step. So obviously, these elements don't match. So going from the old front to new front involves transferring the solution from these five triangles to these three triangles, in a sense, their quadrature points don't match. And this itself, This itself introduces error in the solution. Now, the way that we don't run into that problem because this vertex for us, it's lower in time, and this the top of that is basically higher in time. All these vertices, these surrounding vertices are later in time. So we end up with a geometry like that: that this is the bottom one. This is the front. If we look at this from the top, the old front. From the top, the old front was finer than the new front, which would be these blue ones. And by solving the space-time region, we don't basically the solution in space-time, the region that fills that acts as transferring the solution from the old region to the new one. So, we basically kill two birds with one stone. We not only fill the solution in space-time in the region, we also change the layout of the mesh. And the same applies. And the same applies for edge flip. So, if you want to change the connectivity in the front from old front to new one, in time-marching schemes, we have non-conforming elements, but in this case, we feed it with basically one element patch, and that does the transfer for us. Thank you. Can you hear me? Yes. All right. Thanks for the explanation. I mean, this is really good. I mean, I'm going to say something like when people try to do adaptive measures, they try to prove things like dissipation, these things they embrace. These things break down when you actually have to do these projections. Right. Then maybe we have some result here. The other thing is that when you have your simulation with the adaptive mesh in 2D and you're trying to follow the interface, I do see a large region in the front with like medium to smaller size triangles that they follow. And I think this is because you're And I think this is because you're doing bisection. So, if you do quadrisection or the equivalent of quadrision, and then you allow what I am saying people, they will love hanging nodes, then I think your mesh propagation will be much smaller. That's just yeah, absolutely. I mean, that's a very astute observation. In fact, it's very interesting that you caught that there is this transition. Caught that there is this transition is exactly because the refinement algorithm. And yeah, I think it would be good if we can use those price section or other algorithms that you mentioned. We need to extend the way that we create the space-time elements. If there is a way we don't have non-conforming space fronts, that would really facilitate to look at many more kinds of refinement. Many more kinds of refinement. I mean, that I hope there is a way to look into that. Thank you very much. Maybe we have time for one last quick question. Hi, Risa. Thank you for the nice talk and the beautiful details we have. I was wondering, so if you have these slanted tenths in time, you were mentioning you're also doing ALE. Is this so are you getting the standard from by not deforming the mesh in time, but doing ALE? Or is this a two-diped like two separate? I didn't grab that. Yeah, I think that's the. That work is that particular work is from. Thank you first. It's from. Thank you first. It's from my former colleague, friend. I don't know why this is not moving. Sorry. Just keeping one second, as I'm saying that. So yes, in that case, we are in fact moving the vertex in space-time so they're aligned with solution in the flow. Solution in the fluid. I think my screen is maybe locked here. Yeah, I think you were talking about this example, right? We cannot see the screen anymore. Oh, I'm sorry. Yeah, I think I should stop. Can you see my screen now? Yeah, thank you. So, right. Thank you. So, right. If we detect a shock in a given direction, we can send a request that moves the vertex in that particular direction in space-time to align it with that discontinuity. So like this is the 2D version of that. Obviously, there are challenges to capture shock and all that. But if it works, I mean, if you can capture, I'm sorry, detect the shock and know the speed, it works much better. But I should confess. But I should confess, I mean, this is a toy problem. I mean, if it wants to go to very complicated, I mean, contact and shock intersection, I mean, that I don't know how well these things would work. Thank you. Thank you. Thank you very much, Riza. It was a pleasure. Thank you. And now we have to move on to the next talk. Thank you so much. Bye. Yes, see you soon. Thank you. Bye. Yeah, see you soon. Thank you. Bye.