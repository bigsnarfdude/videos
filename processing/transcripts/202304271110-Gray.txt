So directly experimented here at the plane and their plan was to try to constrain these uncertainties, the schematic uncertainties related to the acceptance by a factor of 10. So at the LHC when we see something like a 10-20% reduction of an uncertainty, we perform many projects of the correlation model. Here a reduction by a factor of twenty is something huge that goes much farther than what we typically do nowadays with the likelihood of providing With the likelihood of finding piece. So implicitly, they would strongly rely on the correlation model in order to achieve this kind of strong reduction of the uncertainty. This goes back to the discussion from yesterday on the uncertainty and on the uncertainty. So I was wondering, in your case, on what do you actually rely, on what kind of assumptions do you rely in order to gain this abinitial reduction of the uncertainty, in order to shrink the uncertainties to start with? Piece to start with. Do you assume something about their correlations across the phase space or something like that, or not at all? No, I mean, much of the reduction, the increased precision on the Q squared measurement comes in this case from being able to constrain better these tracks, particularly in the IQ squared region, which is the most important for the the purpose of IONE. For the purpose of Miwone. And this is done by you have it's a bit technical. You have two sided layers of silicon strips and you can stagger one side with respect to the other in a way that is optimal with respect to the kind of interactions that you get. And then increases the positioning accuracy of the tracks at each measurement point. And this can be done if you cannot. And this can be done if you cannot, it's very technical, you cannot produce this two-sided silicon sensor with that kind of feature, but you can tilt the detector sensors and achieve the same kind of improvement. And this was published in the paper you've seen, and this is what actually Muona is now doing. And the other thing is to take these beryllium targets, which need to be thick enough that you get enough collisions, and parcel them into. and uh and parcel them into thin layers such that you know the position of of the interaction and uh uh if if the spacing between these layers is larger than the accuracy uh with which you know the z position of the vertex from the feet to the three particles, then you gain a lot more because you then can say, okay, the interaction is actually taking place here where there is beryllium and not here where there is air. So it's a combination of various technical things. I don't know if that answers your questions. Answers your questions. Yes, thank you very much. Have a look at your paper once the design is out there. Thank you very much. Okay, so Tomaso, this offers a whole new paradigm for design detectors. But from experience we know that the general property detectors like those that were in FOLAP, FP factories, Fermilab, LHC, they were highly successful, not because Highly successful, not because they were designed to solve a certain particular problem, but because people during exploiting those detectors came up with completely new schemes for analysis, for calibration, and so on and so forth. So you're trying to essentially predict in advance what kind of purpose that detector will serve. Isn't there some big point there? Would you maybe try to optimize your detector on half of your analysis? You detect the half of your analysis and see if it works well for the another half somewhere else. Yeah, so the example I showed you here is very simple because it's a one-purpose kind of thing. And there you can really focus on what counts and matters, right? In the case of a multi-purpose experiment, of course, it's complicated by the fact that not only you have several interacting, crucially interacting systems, and when you want to measure the exposure, you have, for instance, to measure where the photons are. For instance, to measure where the photons, but you also have to measure where the electrons and neurons, and you have to take it, so it becomes a complex constraint optimization kind of thing. I think that if you can still cook up good loss functions that sort of become proxies to what you actually want these detectors to produce in terms of scientific output by focusing on some benchmarks, for instance, if we were in 1910, For instance, if we were in 1990, we would be saying, okay, we want to measure the exposure well, we want a high electromagnetic resolution, and all of these ideas that have actually informed the choices that were made can be encoded in a loss function. And then the system will probably be happy with the choices that were actually made, but will be able to explore also. But we'll be able to explore also a whole wealth of other configurations. It's, of course, true that you build something and then maybe you discover, as we did, that you want to look into the adronic showers. We didn't know that in 1990 and then we discovered that boosted jet tagging was a thing. And so we found out that we had a crappy adronic sorry, pardon me, hadronic calorimeter on which we had only spent three percent of the budget. Only spent 3% of the budget, and we should have not done that. But we recover that because we have a high-field integral in CMS and we can do particle flow. So that is an example of exposed correctional exploitation of the various points of strength of your detector. So yes, it's complicated. I don't pretend to tell you that we can do it, but we need to start thinking about this. And hopefully, little by little, we will gain the... Hopefully, little by little, we will gain the capacity of including more in our models and be able to cover more ground, so to speak, that we have such a loss function that in a sense contains all of these possibilities that you are aware of cross-curling creation and other things. I don't know, this is my unwaving answer for now. It's a work in progress. We have just started. I think it takes 20 years to get where we want. It takes 20 years to get where we want to go. So, a question about reducing the effect of systematics. You did sort of quickly mention decorrelation and pivoting. I suppose it's inevitable that these result in a sort of loss of power in some way. Could you say a little bit about how you avoid losing too much power when you do this? Yeah, so this slide. Okay, so. Slide, okay. So when you include, so this is, I wouldn't, in this particular benchmark, I would say that this is not a loss of power that you're incurring in, it's actually an increase of power because the way you would do this business classifying a signal, classifying data that might contain a W. That might contain a W decay inside a FAB jet with respect to QCT background. If you do it by normal means, you will sculpt the background distribution and then you lose power. While if you decorrelate the effect, you don't lose power of the variable that you are using as a test statistic in the end, because the background will, after the cut that has increased the data of a signal proportion, you will still be able to see a You will still be able to see a peak. So, in general, it is true that you will get incurring a loss of power when you include the effect of nuisance parameters, but this is not exactly, I mean, this is not exactly a nuisance parameter in this particular case. So, probably we should look at other examples where there is an example by the original paper by Baldi Weitzen. A paper by Baldi Widson is good, and I have it in the backup of the other talk I wanted to give, which is this one. So, here you see what these guys did was to include the mass as an additional feature, the mass of the unknown resonance among the features of a neural network. And so, if you train the network to be doing its job of classification, well, regardless. Well, regardless of what the mass of the resonance you're looking for is, you are, of course, reducing the power overall, but you're not reducing it as much as if you were looking for each mass separately. In this graph that they have, the parameterized neural network where the mass is a feature as an area under the curve, thanks to the way they do their business, that is actually comparable to what you would get if you trained exactly on that mass. On that mass, but stays higher than in the whole parameter space when you change the mass and you have trained only at one point. So if you can pull it off, you can incur no loss. But this is, I mean, this is a dream scenario. It's the best you can get. But they proved it in this particular case. In other cases, of course, you will incur in a loss because the effect Because the effect of the nuisance is cannot be completely handled. So, even if you look at the Inferno application in the synthetic example that we have, you will see, you could see here that if we start adding systematics on some of the variables of this synthetic model, you see that you can recover the analogy. That you can recover the analytical likelihood uncertainty on the parameter of interest mostly but not completely. So by derotating the effect of the nuisances, you can sort of make up for it, but of course you will have a small loss of power. It depends on the case, I would say. No, it does. Okay, Lucas. Maybe just one reaction to Luis. I guess the way to see it is: you lose power in the right places. You don't want to have. In the right places. You don't want to have power distinguishing between the right places. So it's great that you lose power in the right direction. Yeah. Yeah, it's a way to distribute work. I was wondering, so, I mean, so when we did this with Nathan and Samantha, one of the things that is kind of interesting with this internet optimization is that, of course, to get an analysis published in a collaboration, it's also a highly multi-dimensional optimization problem. Optimization problem. So we had what David was saying: that we want to have a small pole, so basically, the analysis shouldn't affect our inference on our Newton's parameters and so on. So you don't want to only have a low p-value, but you want to have a low p-value, a nice impact plot, a nice bold plot. And so suddenly it becomes like a very high-dimensional thing. And so we've played a little bit of having different terms that measure each thing and finding the ways to set stuff. And finding valuable ways. Is that something that you also try to do in a CFS thing, or it's kind of no? Basically, we had our hands full with already reproducing the result and taking exactly the same path that the authors have taken. So, one thing, for example, we found is that there are configurations in template space where you get very low CLS, but also a very high, like a very Also, a very high, like a very constrained business parameter, which is not usually great. So, it's not necessarily that you find an invariant solution like with inhibiting, but it's find some kind of solution that is kind of curved away from signal which produces like no CLS, but like a strong pull. It's clear that if you go to experiment, they will get you. That's right. And by the way, I'm sorry I couldn't cover that because it's one of its Uh because it's one of it's in my backup, but there's it's one of the developments uh of these uh of these techniques. So the NEOS technique has been published by Lukas Einrich and Nathan Simpson, and it looks at the problem from the point of view of CLS and the varying confidence intervals. But it's uh similar in uh in in idea of these uh end-to-end approaches. Uh there wasn't the right thing. Maybe this question was already asked somewhere. Um Was already asked somebody, but I mean, what about this error on error? So you train the network to find the minimum of the Fisher discriminant, right? Fisher information, sorry, Fisher information. And I like this analysis with Lucas, this stopped by production. So aren't you then s more sus acceptable to shape errors in some way? I mean that's somehow I mean the network exploits the shape information, right? Exploits the shape information, right? And so I would imagine that simply then is just really some special orthogonal shape, as an uncertainty that might be a danger if we can say something else. I'm not sure. This technology allows you to include in the in in in the in the loss and therefore let the In the loss, and therefore, let the network try to optimize for shape variations. And the way this is incorporated is a little bit clutchy, so to speak, because we take plus and minus one sigma variations and we incorporate this in the model, and then the network will have to figure out a little bit what to do with it. So it is not perfect, and yes, if there are significant. And yes, if there are significant non-Gaussian effects on the features that your nuisance has, this will probably cause sub-optimality and not complete derotation, if you want. But at present, I I don't know if I if I if I can answer better than saying that at this moment this is what we were able to do. We were able to do. If you have more complex shape dependencies and you want to address them and put them in, you can probably do it by having more than three points sampled. And this becomes rapidly more complex because it's already kind of complex to put this together. But the code is public, the CMS data is public, and the processed CMS data, which is difficult part, is public now. Part of this problem now. So, anybody can try their hand now. So, there's a philosophical question behind: should we design our detectors to be extremely good with one thing, just one thing, and nothing so helpless? But many times, when we build these detectors, we also try to hedge against some part of the unknown and whatever things we have. So, this is following also. I mean, this is following also on what Igo was saying. So, your answer there was: we could just include more processes. And I think it's not the first time that it comes to my mind whether we should have some sort of hierarchy of having, let's say, intermediate observables. So not just the significance for this particular process, but some mass resolution of a certain type of object. Is this something that you see coming in or not at all? And if so, how? Well, from a pragmatic perspective, From a pragmatic standpoint, it's much easier to set yourself up to design a full modeling of your detector plus inference where you put in the loss function the final resolution on a particle momentum or photon deposit in the calorimeter rather than going all the way up and trying to encode in the loss function the percentage. To encode in the loss function on the precision on the X self-coupling of the tetra collider, right? It's much easier. And therefore, of course, what we should do is to probably go in that direction progressively. And already I would see it as a big improvement if we cooked up an experiment-wide loss function that assessed as the loss a combination of this. The loss, a combination of these resolutions on various sub-detectors in some well-informed way, rather than trying to really incorporate everything, also a model of the full analysis that you would do to extract the coupling constant or the X or things like that. But I think the more you put in the loss function, because the loss function is really what matters in these systems. The more you put in these systems and the more thought goes And the more thought goes into designing something that is a good proxy of what you really want, the more you can win. Of course, there is an intermediate step, as you say, that will cover more ground because you have done good calorimeter that will reconstruct photons and also electrons, and you will be able to exploit this in many different ways. Putting in the coupling constant of the X might get you to miss new physics in some other way. Get you to miss new physics in some other direction. So, yeah, I mean, it's an incredibly complex problem. But I think if we start easy, we can gain sort of confidence on what we are doing. And hopefully, we will not miss out on some potential signals in the future. I don't have an answer, but I don't know. I have a comment on this, actually. So, if I'm followed by the design of the Atlas MDA, there are also significant engineering concerns. There are also significant engineering considerations that need to come into the sector design, and also things like how to be aligned and things like that. How do you plan to improve this? I don't. I will retire in 10 years or so. I mean, it'll still be a business. But no, I mean, to not be facetious and answer your question, all of these constraints that are external imply that this scheme. Imply that these schemes can't work alone. You need to go towards the situation where you have a human in the middle. So these kind of systems that I was hinting at will work well if this becomes a system for informing the expert of possible alternative ways to obtain what he or she really wants, and then it will be. And then it will be possible for this person to say, okay, this would be great, but this would not go through the would not be able to lower this piece into the CMS collision hole, or this thing will not go through the road of Iran, and I can't bring this piece. So there are things that you cannot think of, of course. These are engineering constraints, typically, and other things. Another thing that you cannot really take care of. Take care of is the fact that imagine you are optimizing CMS and the system tells you, oh, you can do so much more if you spend 50 millions more on the add-on calorimeter. And you can take this money away from the tracking guys. They don't need it really. How are you going to propagate this? So these are kind of things that really are outside of this. That really are outside of this. You can use this kind of machinery to inform the experts. And you can put in the experts and the capability of testing this parameter space in a continuous fashion. So that not only you get to know what is the absolute minimum, but you also know what is the landscape of the loss function in the surrounding area. So that you can say, I cannot do that, but probably I can do this. And so by being able to test in a continuous fashion the design space, Continuous passion of the design space, you will gain access to automatic ways that satisfy the engineering constraints that you cannot put in the model. Yeah, it's almost the same question, but it comes down to money. And that is the detector you want to build, you're often told that you can't have because it's too expensive. And then you can build a smaller detector, but maybe upgrade it later. And then time becomes part of the calculation because maybe a detector that Calculation because maybe a detector that has big uncertainties, huge systematics, is quick and dirty, it's the one you want to go first because you're going to upgrade it later. And maybe you want that set of measures here because you are patient and you may be able to squeeze those systematics down only if you took more data. So how does time get into it and money and upgrade? I had a slide that I took off that was on paper you can draw utility functions in terms of budget allocation. You have a certain budget. Allocation: You have a certain budget, and you can say, Okay, they potentially I can get this budget, so what I want to spend is this, or maybe a little bit more. So, I will have a loss function that is like this. Okay? Because I want to be here, if possible. I can live with being here, and I can really exceed it. In terms of time, you can build something, you have a deadline, and you can A deadline, and you can have a loss function that looks like this. So you can encode this in your loss function. The other point, I think, is what was your other point? Yeah, I wanted to mention LHCB, the electromagnetic calorimeter for the upgrade, has been optimized by looking at some reference signals by people in mode. And they have looked at various possible configurations as a function of the number of channels, which is a proxy for. Of channels, which is a proxy for cost, so they optimized separately for the different options in terms of money spent. And then you can give this to your funding agents and let them decide. You can always have more money. You can always do more physics, but more money. All right, I think let's do one last question and then we should wrap up. Oh, I was just going to comment. I think about it, it's a bit what Tomas was saying, but I think about this a little bit in terms of maybe not as grand scale. As grand scale of optimizing everything, but much more in terms of like AI-aided design. So you take an expert that can break down the system and, most importantly, sort of the parameters of that system, and then the tool can just optimize that configuration for you. So rather than an expert saying, asking a student to simulate three random, semi-random configurations, you set down the rules and then you let a tool optimize it. And so I think it's just more sort of optimized, or it's sort of taking over the optimization step rather than sort of. The optimization step rather than sort of the two or three-point checks on possible configuration. If I can add something to what Michael says. Exactly. But also, I want to stress the importance of having as good as possible a model of your inference in these pipelines, and as good as possible a model of what will be the stumbling blocks, so the systematic uncertainty. Blocks, so the systematic uncertainties that you might incur if you put too much material in the tracker and then you figure out actually good stuff. So this is the hard part because we don't know what we will be able to do pattern recognition-wise in 20 years, but we can sort of design a map of our capability of our reconstruction techniques as a function of time, and here is the ground truth, and here is where I am today. And here is where I am today. And I can imagine that I will be doing a little bit better in 10 years from now because I have better tools. And so, if you encode this in your systems, you will be able to tweak that handle and get an answer that is stable in terms of optimized configuration with respect to your capabilities. This is very important because if you don't do that, you will have a misaligned system with respect to what your capabilities of extracting information will be in the future. Not specific problems with being an expert, you can also think carefully about putting in engineering constraints as constraints, like as a constraint optimization puzzle rather than a fleet optimization puzzle. All right, let's wrap up, so thanks again.