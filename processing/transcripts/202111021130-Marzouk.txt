Today, I want to talk about a line of work with some sort of older parts and some new parts on dimension reduction in non-linear inverse problems, if that would be of interest to this workshop. The focus is perhaps more computational than statistical, but I hope still there are some things of interest to everyone. This is a joint work with many people. I'd say primarily, currently my current PhD student, Michael Brennan, and my former postdoc. Brennan and my former postdoc, Olivia Zahm, who's now at INRIA, and also some other postdocs and collaborators, Cheng Gang Sui and Monash, Cody Law in Manchester, Alessio Spantini, and Daniele Begoni, who are now in industry, that contributed a lot to this work over time. So the broad question that we're looking at is inverse problems in the Bayesian setting. And the computational task, I mean, it's one thing to write down the posterior density up to a normalizing constant, but the computational task is really one of characterizing the. The additional task is really one of characterizing the posterior distribution in some way. And this is challenging for a whole host of reasons that I think everyone here is no doubt quite familiar with. Here, I'm going to think of the parameters x as some high-dimensional vector, but we know in principle, they often represent the discretization of a function. We can think about these things in an infinite dimensional setting. The posterior density of x, which here I'm writing is proportional to a likelihood and a prior, is typically non-Gaussian in these in the more challenging problems of interest. And also evaluations of the likelihood. And also, evaluations of the likelihood function, which is where the Ford model, which is often a PDE, sits, may be computationally intensive. And all of these contribute different aspects to the computational challenges of Bayesian inverse problems. But we're in a setting where we can write things down. Perhaps we can evaluate this posterior density that is going to grieve it as pi pos of x up to a normalizing constant. Perhaps we can evaluate gradients. In fact, we're going to talk about later. We'll want gradients of, or at least score functions. So gradients of the log density. So, gradients of the log density. So, at least given these computing ingredients, now we face kind of a computational task of how to characterize the posterior, for instance, how to draw samples from it. And this is where algorithms like MCMC or maybe some kind of sequential Monte Carlo or some variational approximations that I'll talk about later that involve transport all come into play as ways of providing us some characterization of the posterior via samples or via some other kind of approximation. Now, a conjecture that underlies a lot of A conjecture that underlies a lot of inverse problems, sort of is true in practice in a lot of inverse problems, are that the data are informative in kind of an anisotropic way. And in some sense, the data are informative, and then I put this in quotes because I'll make it more precise later, might be informative only on a low-dimensional subspace. And in the Bayesian setting, this cartoon maybe gives the intuition that I want to convey, which is that observations, you know, via the Ford model and the likelihood that results might change your prior belief about the parameters only. Your prior belief about the parameters only in some directions. So, here the direction outlined by these red arrows and not others. So, if I think of my parameters as living in Rd, what this effectively means that there's a kind of a decomposition, there's a subspace where the posterior and the prior are quite different from each other, and there's a complementary subspace where the posterior and the prior are very similar to each other. And this is emblematic of a lot of inverse problems where the forward operator is smoothing in some way, or the data are limited. Or the data are limited in number, or the noise, there's a lot of noise, or noise are correlated, or if the prior is very constraining in many directions, that the data might also be informative that, but then in a relative sense, that again we'll make precise later, that data might not be that informative relative to the prior. So this is a kind of a conjecture, and we'll try to make this structure both identify ways of identifying this structure and exploiting it in the rest of the talk. I should mention this structure is actually pretty well understood in the linear Gaussian case. In the linear Gaussian case, you can. Linear Gaussian case. In the linear Gaussian case, you can write things down. So suppose I have a prior covariance gamma prior, and suppose the conditional distribution of observations has this forward model Gx and some additive Gaussian noise with covariance gamma obs, then you can actually characterize what are optimal low-rank approximations of the posterior covariance. In other words, you can say, among all approximations of the posterior covariance as, say, a rank R update of the prior covariance, what is the optimal such rank R update? And this is what characterized. rank our update and this is well characterized similarly you can actually write um uh optimal in in various senses including for instance like in the fisher-rau sense of sort of on the natural um manifold of um of symmetric positive definite matrices um you can also characterize optimal posterior mean approximations or optimal in the sense of minimizing a certain base risk as a low rank matrix times the observations and in all of this and i won't go into too much detail the the one thing i want to point out is that there's a central The one thing I want to point out is that there's a central role of certain generalized eigenproblems, that all of these optimal approximations, whether the KR is here or the AR over here, follow from solving certain generalized eigenproblems. And these generalized eigenproblems really capture a lot of the intrinsic and important structure of the problem. And there's various equivalent generalized eigenproblems to look at. The one that I'm writing down here, you can think of as the matrix on the left as the Hessian of the log likelihood, the Hessian of the negative likelihood, and the matrix on the right as the prior percentage. And the matrix on the right is the prior precision. And in some sense, this captures a weighing between what does the likelihood inform you in what directions, and how does the prior inform you in what directions. But that's in the linear Gaussian setting. Now, I want to talk about the non-Gaussian setting. I want to talk about non-linear inverse problems today. So the more way of generalizing this idea is to say, can we consider approximations of the posterior or the following form? So pi poses are posterior, pi tilde poses is the approximation, and the form is going to be as follows. And the form is going to be as follows. I'm going to say, okay, I have my prior untouched. I leave my prior intact. And I'm going to multiply rather than by the whole likelihood, I'm going to multiply by some other positive function. I'm going to call L tilde. And the positive function is only essentially a function of r dimensions because embedded here is an argument, I have a rank R linear projector. So this approximation format also kind of captures the notion that the prior to posterior update only happens in R dimensions. happens in our dimensions. So this is an approximation format. And this induces a decomposition of the space as I talked about earlier. I can think about a parameter and I can write, I can decompose this parameter vector into a part that is in the range of the projector and a part in the kernel of the projector. And the likelihood function that I deal with here is really only a function of r dimensions. It's only a function of xr because it only depends on this l tilde only depends on the range of the projector. Okay. Okay, so why would I want to do this? If I can find an R here that is much, much smaller than the ambient dimension, the sort of naive dimension of the parameters d, then I can do a variety of things. One thing you can do, so there's a line of MCMC algorithms that exploit the structure in order to come up with proposal distributions that are particularly efficient. So I can actually use them or to actually split the space and treat the informed space and the uninformed space differently. And these are one example is these so-called dilly samplers. Is these so-called DILI samplers, dimension-independent, likelihood-informed? These kind of build on these PCN samplers that Andrew Stewart and others have proposed. Another thing I might do, if my likelihood and function is extensive, as it often is, I might want to use this dimension reduction as an opportunity to more easily build surrogates or fast approximations. In other words, function approximation of this is much easier than approximating the original likelihood function, which depends on all of x. And the final thing, and this is something that I'll highlight in the second. The final thing, and this is something that I'll highlight in the second part of the talk, is you can use this lower dimensional structure to develop more tractable variational characterizations of the cluster. And I'll describe what that means later on. So the idea is, okay, given this kind of structure, there are things that one might do that are computationally desirable or computationally advantageous if you could identify a good approximation in this format. So now this idea is not new. Many previous proposals have sort of Many previous proposals have sort of identified analogs of this general form. And the format or the idea behind many of these previous proposals is actually as follows. You identify PR as a projector onto the dominant eigenspace of some matrix, which contains relevant information. Now, what is this matrix H? A couple things that Drew proposed. So the likelihood-informed subspace idea essentially generalizes what gives you optimal approximations in the linear Gaussian case to the non-Gaussian case. It says, okay, Gaussian case it says okay um if now i have a nonlinear forward model say g of x and let me still have additive gaussian noise if i now look at the hessian of the log likelihood but i don't take it at a point because now the hessian varies with x because i have a nonlinear problem let me integrate that hessian over the posterior distribution and come up with this integrated hessian matrix h and then derive a projector from that integrated hessian matrix another idea proposed by paul constantine and others builds on their active soft space methodology says let me take the out Says, let me take the outer product of gradients of the log likelihood, and let me integrate that over the prior. And I get yet another matrix H, some D by D matrix, which somehow then I'm going to take an eigen decomposition of. But these differ in terms of what the integrand is and what measure I'm integrating over. Similarly, various definitions of this low-dimensional function L tilde have been proposed. One idea that's in the LIS paper is simply to take, to fix the complementary parameters. So basically, to say if I have. Complementary parameters. So, basically, to say if I have some parameter x, I'm only going to take this r-dimensional projection and I'll fix the rest of it to be the prior mean in the complement of the range of that projector. Another idea, this is an active session-space approach, is to say, let's take the conditional expectation of the log likelihood and then exponentiate that and let that be my substitute likelihood, if you will, my low-dimensional likelihood. So, these are many previous proposals. Now, what I want to do in this talk is kind of come up with This talk is kind of come up with, you know, well, I should say, these are essentially all heuristics. They people have been tried and they work reasonably well in practice. I'll compare them to what I'm going to propose here. But what I'm going to propose here, in some sense, tries to come up with something more controlled and something that lets you actually let's certify the approximation error in a way that's computable. So this is kind of the broad objective. And we want to approximate, build an approximation of the posterior that's in this form, some pi tilde, where again, this L tilde is L tilde is essentially a function that's effectively of r dimensions. We leave the prior intact, and I want to do that in this case such that the KL divergence between my original posterior and my approximation is bounded by some epsilon. And the rank R should depend on epsilon, but I hope that it's much smaller than D. So the full details are in this paper, and I'll give references at the end, but I'll walk you through kind of the idea. So here's the kind of the first piece. So we have this KL divergence between the approximation and between the triple posterior. Approximation and between the true posterior and its approximation. And what we can do first is decompose this into two pieces, kind of the Pythagorean theorem, but there's just equality here. One is between the true posterior and this best approximation, best approximation in a certain way, best approximation given a particular projector PR, that I'm going to call pi star post. And then the KL divergence between this pi star post and my actual approximation, pi tilde. And this pi star post, you can write it down. And this past our post, you can write it down, it actually comes from taking the prior and multiplying it by the conditional expectation of the likelihood given a projector. So you can think about this as basically like taking the likelihood function, which depends on all d dimensions, and integrating out the d minus r dimensions that are not in the range of the projector. So this is essentially saying I'm going to integrate that the extra variation of that likelihood that's not captured by PR. I'm going to integrate it out with respect to the prior measure. out with respect to the prior measure. That's the intuition behind this conditional expectation here. And one thing you can say is that this actually, well, this is actually something, this now lets you decompose the error because you can now decompose the construction of both the projector PR and this likelihood approximation L tilde. So for instance, let's just work from the term on the right. If I have a projector PR and I want to set this term to zero, then this actually tells me exactly what this profile Tells me exactly what this profile function or this low-dimensional approximation likelihood ought to be, it ought to be precisely this conditional expectation. In other words, if I put L tilde equal to this, that then can drive the second term to zero. And then the goal becomes to find the projector because all of this depends on a particular projector in the second term. So now I want to find a projector and I can say, okay, I don't need to worry about this likelihood function anymore. I know what the optimal approximation is. And now I want to find this projector such that this KL divergence is. Such that the scale divergence is controlled below epsilon. So, that now kind of simplifies the problem, and we can go to work and focus on this term here. Now, to do this, we're going to appeal to some logarithmic sub-level inequalities. And for those to be valid, we need to make some assumptions on the prior. And here's the assumption on the prior that we make, more or less. I'll give you some detail. Essentially, it's saying that the prior density, essentially saying that the prior density is essentially law concave. Essentially, law concave modulo sum bounded variation. So I have some function v that's strongly convex, so it's controlled by this covariance or this curvature type matrix gamma. And then I can perturb v, but that variation has to be controlled in magnitude by this constant kappa. And this then says, okay, essentially you can think of the log minus log prior density as this. Minus log prior density as this purple line here, where v is this strongly convex quadratic function here. So, what class does this apply to? Another way to think about this, maybe, that's more straightforward is that I have my prior, and I have some other density that's Gaussian, and I can bound it by a scaled version of that Gaussian, both from below and above. Now, what kinds of pyres satisfy this? Well, Gaussian pyrs certainly, Gaussian mixtures also satisfy this assumption. Modulus, some other Assumption, modulo some other details, a uniform prior and a convex domain is also allowed. So we generalize a bit beyond Gaussian priors, but of course, there are priors that we cannot handle. For instance, in particular heavy tails, we cannot handle. But given this assumption, you can say that the prior distribution satisfies a logarithmic symbol of inequality for some function h here that's sufficiently regular. And we can bring this back to the context of the Bayesian inverse problem by making a particular selection of h. So if I think of h as essentially the likelihood divided by the evidence. The likelihood divided by the evidence, if I think of h squared as that, and I substitute that in, then what I actually come up with is a bound for the KL divergence from prior dipper. And so I've bounded the KL divergence from prior to posterior by the gradient of this log likelihood function integrated over the posterior, weighed by this SPD matrix gamma. Now, that's well and good. Now, what does that have to do with the projector? Well, you can take this idea and kind of You can take this idea and kind of generalize a little bit to come up with what we call the subspace logarithmic solo of inequality, where now I've modified things a little bit. You see what used to be expected integration only over the prior. Now I've replaced it with this conditional expectation down here. And it involves this projector PR. And this is again true for any sufficiently regular function h, any projector PR. And you can think about this as a generalization because if r goes to zero, then this projection matrix becomes zero matrix. And what I refer to as the original log. I revert to as the original log sublope inequality on the previous slide. So, what I can do now, though, is for any particular projector here, I can put it in, I can make the very same substitution for h squared. And now I have a bound on not the difference between the KL divergence from posterior to prior, but the KL divergence between the posterior and this approximation pi post star. And what is this bound on the right? What I can interpret it, well, it's essentially this right here, where grad H is grad log of all. Grad H is grad log of all y, my likelihood. So, looking at this term on the right, if we kind of bring it to the next slide so we can squint at it a little bit, what this ought to remind you of is a little bit is of principal component analysis. Essentially, I'm saying I have this gradient of the log likelihood, I have the score function here, and I'm trying to find a projector that captures most of its variability integrated over the posterior. And so, finding a projector that actually minimizes this upper bound. Actually, minimize this with this upper bound corresponds to doing principal component analysis in this weighted way of the gradient of the log likelihood. So, this is an upper bound on the KL divergence, but minimizing is quite tractable. I can minimize it in close form. And here's kind of the link back to generalized eigenproblems. The minimizer of this reconstruction error, this reconstruction error is what I call this upper bound over here, is the gamma orthogonal projector onto the dominant eigenspace of this matrix H. So if I look at this matrix H. So, if I look at this matrix H and I compare that to what we had before in these other proposals of, say, the likelihood conform subspace or the active subspace, this is the gradient of the log likelihood, outer product with itself. So that's as if what we were doing, the active subspace work had proposed, but we're integrating over the posterior pi post. And we're also solving this generalized eigen problem with H and this matrix gamma here that reflects the curvature of the prior. So for any So, for any, you know, so if I decide to truncate this, it's some rank R, then the reconstruction error associated with that optimal rank R projector is just the sum of the remaining eigenvalues. So this now kind of gives you an idealized algorithm. It's idealized because there are certain steps that one has to approximate. But the idealized algorithm is to say, okay, let me just compute this matrix H. It's the gradient of the log likelihood, outer product with itself, integrated over the posterior. itself integrated over the posterior. Find the projector PR as the coming from the generalized eigenproblem involving H and gamma. Compute this conditional expectation of the likelihood. So essentially averaging out the likelihood over the directions that are not in the range of the projector. Put those together and I have my approximation of the posterior distribution and the format as we described and this approximation satisfies an era that comes from the sum of the left over eigenvalues. comes from the sum of the leftover eigenvalues and then if these the natural thing is these eigenvalues decode quickly enough then you can pick the rank and you can satisfy any given error that you want okay so this is an idealized algorithm in a couple senses first of all well this involves um let's say an integral over the posterior and if your whole goal is to approximate the posterior in the first place well how are you going to go do that and then also you need to compute this conditional expectation and that requires some effort and some sampling and particular some prior sampling and that requires some Prior sampling, and that requires some effort as well. At least at a high level, this gives you a way to construct a controlled approximation in the format that I described. So a couple bits on some aspects of approximating this algorithm. First thing one has to do in practice is make a Monte Carlo approximation of this matrix H. So here's a case sample Monte Carlo estimate. Here I'm still for the now assuming that these Xi's are drawn from the posterior. Xi's are drawn from the posterior, and some analysis we have basically says something about the sample size that you need for the projector to be quasi-optimal under some further assumptions, for instance, the gradient of the log likelihood of sub-Gaussian for extron from the posterior. And the key thing here is that the k scales essentially with the rank of h, which is desirable. And this bound here, this is the probability with which I want to hold, have cause optimality. And similarly, you need to make a sample approximation of the conditional expectation. Need to make a sample approximation of the conditional expectation. And what I'll say here at a high level is that the error is controlled by the same factors, essentially, that that same reconstruction error, the sum of the leftover eigenvalues, also enters the error that comes from the sample approximation of the conditional expectation. Because in some sense, that reflects how important are the removed directions? How much variability is in that likelihood function in the directions that you're not computing? So, how much essentially what is the variance that then connects to the variance of your expectation of your expectation. Expectation of your estimate of this conditional expectation. So these factors kind of can all sort of be drawn to the same order based on the decay of the seigenvalues. So, okay, so let me just give you an example. Then I'll talk a little bit about the one circularity we have to resolve, which is that I'm still demanding samples from the posterior. Okay, so we'll come back to that in a second. So here's an example. This comes from kind of an atmospheric remote sensing problem. It's a non-linear inverse problem. Problem. It's a non-linear inverse problem. The details aren't terribly important. Actually, the picture is kind of neat. It's based on this global ozone monitoring satellite system. It's based on what are called star occultation measurements. So pictorially, you have starlight that passes through the Earth's atmosphere. So these are layers of the Earth's atmosphere like an onion. And the satellite comes along and it absorbs the absorption of starlight over a whole range of different wavelengths. So you can basically look at the Wavelength. So you can basically look at the spectrum of the starlight outside of the when the path is outside of the atmosphere, and look at the spectrum when the path is in the atmosphere. And you can see to what extent it's attenuated in different frequencies. And from that, the goal is to reconstruct vertical profiles of the concentrations of particular gases from this kind of measurement. And so the Ford model basically encapsulates Beer's law, which describes the attenuation of light through these atmospheres. We put a Gaussian prior on the different gas concentration on the logs of the gas concentration. Con and on the logs of the gas concentrations, and you can discretize this, and so on, and so forth. So, now here is what follows from different approaches to dimension reduction. And I'll begin with the one I just proposed, which is suppose we get exact posterior samples, and I take an integral of this outer product of the log likelihoods, of gradients to the log likelihood, and I compute a projector, and I look at the error associated with that projector as a function of the rank. And here we see, okay, the error goes down. And here we see, okay, the error goes down as you increase the rank, and the error bound is in this reconstruction error, is in the dotted lines, and the actual KL divergence that we've estimated using lots and lots of samples is in the solid line. And you can see we satisfy the error bound and the error goes down. Let's compare that to, for instance, if I change this measure here, if I change that from the posterior to something more tractable, if it became the prior, then I get the purple, then I derive the projector from that eight. Then I derive the projector from that h, and I compute the error associated with that projector, then I get the purple line here. And if instead I chose, for instance, say like a Laplace approximation to the posterior, then I get the green circles here. So clearly, these are not as good empirically, and we don't expect them to satisfy this error bound, and they do not. But the upshot here is that the choice of measure, the thing that you integrate over, actually matters quite a lot in terms of giving you a better quality projector. And the other thing I could do is also. And the other thing I could do is also, what if I take the analogy from the linear Gaussian setting and think about integrating the Hessian of the log likelihood over one of these measures, rho? Then these new lines show up on the left. And you can see, for instance, integrating the Hessian of the log likelihood to come up with this diagnostic metric H over the posterior is not so bad empirically. I get these very light lines here, these light blue lines here. And if I do that using integrating over the Hessian over the Laplace approximation, Over the Laplace approximation, I get these orange squares here. And if I interconnection over the prior, I get these yellow circles here. So, intermediate performance, but again, we can't really say anything in general about how well those do. All you could really say is that you can come up on the error bound, but it really depends on the supremum of the ratio of the row that you've chosen relative to the prior. And in general, it's hard to say what that's going to be. And similar results you can show with Hellinger distance in addition to them. With Hellinger distance in addition to Kl. Of course, there's no error bound here in Hellinger. Okay, so that all sort of tells us if you could actually draw posterior samples, you could come up with a great approximation. But how do you do that in practice? Well, one thing that we propose is an iterative algorithm that avoids drawing samples from the full-dimensional posterior. And the virtue of this algorithm, there's certainly other algorithms one could consider, but this is sort of an initial thought, is that this algorithm avoids ever That this algorithm avoids ever seeking samples from the full-dimensional distribution. I'm always seeking samples from one of these low-dimensional approximations. And emphasizing these low-dimensional approximations essentially involve sampling with something like MC-MC in an R-dimensional subspace, and then sampling from the prior in the complement to that subspace, and thus generating samples in all of Rd. So, conceptually, what does this iteration do? I begin with some distribution rho. Say I'm going to essentially iterate over is I'm going to essentially iterate over. I'm going to basically compute this matrix here, but iterating over distributions row from which I draw samples in each stage of the algorithm. And here I might have a row L. This is an iteration L, and this comes from an approximation of rank R. I draw samples from it. I make a new matrix H. I solve this eigenproblem. I get a new projector. From that new projector, I form a new posterior approximation and I draw samples from that. And I continue in this way. So conceptually, that's what this iteration does. So conceptually, that's what this iteration does. But I emphasize the only samples that you draw along the way are samples from these posterior approximations, these pi tildes. So various ways you could run this iterative algorithm. For instance, you could fix an error threshold and then choose whatever rank you might need to achieve that. Although here we maximize the rank at 40 for computational reasons. You can see quickly after some iterations, you meet the error threshold of 10 to the minus two, which is the black line. And what happens from then on in the subsequent iterations? Happens from then on in the subsequent iterations, you essentially stabilize around a rank. Alternatively, you could just fix the maximum rank, which is like fixing the computational budget, and then proceed with these iterations, updating the diagnostic matrix as you go. And the error decreases and then stabilizes. And here for rank 30, we don't achieve this error of 10 to the minus 2 that you get with rank 35, but you can see the error at least stabilizes. So, this is an iterative algorithm that makes use of those low-dimensional approximations as you walk through. As you walk through, essentially proving that diagnostic matrix as you go. So, okay, so some sort of I'll pause. This is certainly not at the end of the talk. This is maybe two-thirds of the way through the talk. I want to talk about something else in the last bit that builds on this, but let me pause here and just highlight some questions that might be worth discussing. So, one thing is I said we want to use these kinds of this decomposition of the posterior into informed directions and prior dominated directions to kind of Dominated directions to come up with better MCMC algorithms, and there's a line of MCMCMC algorithms that do this. So, these algorithms essentially use the range of the projector both to come up with proposals and splitting schemes. So, metropolis within Gibbs schemes where you update one piece of the state vector using one kind of proposal and the complementary piece, and typically using like a PCN type proposal or something from the prior. One thing that's open is what's the impact of the subspace quality on the computational performance of these elements. On the computational performance of these algorithms. Some really initial results in a paper that TC and Shintong put up this year that essentially say something about the acceptance rate of proposals on the complementary subspace and how that relates to the quality of the projector. But in general, sort of understanding the actual efficiency of MCMC at a higher level and how that depends on the subspace quality. To my mind, that's still open. Understanding convergence through the iterative algorithm. Understanding convergence of the iterative algorithm that it has described and computational trade-offs, there's a lot of choices one has to make along the way. A very natural thing to do is extension to the infinite dimensional setting. And in some sense, it's very natural because we already already sort of separated the problem into a finite number of directions where interesting things happen, where the likelihood is important. And what you could sort of a complementary set of directions, which are infinite in number, where very little happens, and essentially are prior dominated. And of course, when one wants to think about the possibility of handling heavy-tailed priors. About the possibility of handling heavy-tailed priors. So I'll put these out and maybe we can come back to these later. What I want to talk about in the last bit is an application of these ideas to a different kind of approximation format, one that doesn't involve MCMC at all, but rather uses transport to approximate the posterior. So you can think of this as what I'm going to talk about as a variational Bayesian method that's based on mappings from prior to posterior, but those mappings are going to inherit structure from the kind of low dimensionality that we just described. Just described. So here is the idea behind these methods and pictures. I want to characterize the posterior, which I'm just going to call pi henceforth, as the transformation of some simple distribution. Think of rho as a standard Gaussian. So I want to find some function t such that if the random variable x is distributed as rho, then t of x is distributed according to my target distribution of interest, my posterior t of x. So I could think about this as taking samples from rho, transforming them by t, and having these samples all magically be distributed. Magically be distributed according to pi. And just a bit of notation: this says this is the push forward of rho, it's pi, and sort of eliding measures and densities. And the superscript sharp is a pullback, so t. So rho is the pullback of pi under t. Now, how to construct such a map? Essentially, this comes down to a variational Bayesian inference. And you could think about this as follows. So I want to make pi close to, I want to choose a t such that the push forward of rho under t is close to pi. So then we'll depict some parameterized. So then we'll depict some parameterized class of maps from Rd to Rd. And here there are many choices. A very convenient set of choices involves monotone lower triangular maps, and these approximate what are called the Noth-Rosenblatt rearrangement, which is the unique triangular map up to ordering that maps for the pi. We're going to approximate it in some parametric class. This KL divergence involves an expectation over the reference measure because just by change of variables, I can pull this map over here and equivalently pose the problem as taking row and trying to find a T. rho and trying to find a t that makes the pullback of pi under t close to rho. And this involves an expectation over rho, which is something that I know how to do. And to solve this thing then variationally, I will probably use evaluations of pi and its gradients, but this is different in character than MCMC or important sampling. This is a variational method. It uses optimization over some class of distributions that I can form by choosing a class of maps and pushing forward some reference distribution row. So this is a problem one can. So, this is a problem one can solve. In general, it's non-convex and thus pi is log concave, but inference is difficult. So, the key steps here are to parameterize to some class curly th and then to optimize within this class. And so, immediately you can see a challenge. I'm talking about finding functions from Rd to Rd. So there's a trade-off here between the expressiveness and computational effort. If I just want to parametrize generic functions from Rd to Rd, this is hopelessly complicated. So I need to find some kind of low-dimensional structure here. So, I need to find some kind of low-dimensional structure here to make this tractable. Many varieties that I can think of, but now I want to bring it back to the low-dimensional structure that I was just describing. And let's, and this is described in this other paper that I'll give in references. So let's take, oh, this should be RD to RD. This matrix should be D by D. So what I'm defining here is a lazy map. What I call the lazy map is essentially a map that is the identity on D minus. D minus r coordinates and departs on the identity in r directions. So essentially, the more formally I'm writing this map t, it's uh departs from the identity in the range of ur and otherwise is the identity and tau some other diffromorphism, but this is a function no longer from rd to rd. This is a function from r to rr. So this is a class of maps that's given by this unitary matrix u, which essentially gives me the basis, and this scalar value r, which tells me basically where, how far into the spaces do I depart from. How far into the spaces do I depart from the identity? And this gives in a class of maps that we call a lazy map because it only works in our dimensions. Now, what you can show is that for any map in this class, there exists essentially a likelihood-like function, but it's just a strictly positive function that multiplies my base density and that only acts in R dimensions and gives me the target density. So you could think of this as the likelihood divided by the normalizing constant if I think of this. Normalizing constant. If I think of this as like a target and I think of rho as a prior. But in general, those are just two distributions whose densities I'm going to compare. Conversely, if I have any density that I can write as an R-dimensional update of some base density, then there is a lazy map representation with exactly you are sitting in front of this non-identity component tau. So, okay, how could I find this basis? Well, this appeals back to what we talked about earlier. Now, rather than comparing Now, rather than comparing prior and posterior, let me just say I'm comparing target and reference. And I look at now the log of the ratio of these densities, the outer product of the gradient of this log ratio with itself. I form this diagnostic matrix H, and I find the dominant eigenpower of H. And now that I've picked this to be rho, the matrix gamma that was in the earlier analysis, just the identity, so I can solve a simple eigen problem. And what you can chose then there exists a map such that this KL divergence is bounded by the sum. Is bounded by the sum of the leftover eigenvalues of this matrix HÏ€. And what is this T star pi? Well, this T star pi uses precisely that conditional expectation that I talked about earlier. This is the optimal profile function. This is the thing that renders that second term in this two-term decomposition into KL divergence to zero. And so this is a ridge approximation of the likelihood. And the optimal ridge approximation of the likelihood is essentially this conditional expectation. So this tells me that I have So, this tells me that I have that I can control this. Now, it's up to me to go find a map that can achieve this, but it says there exists a map that can drive this KL divergence below this threshold here. So I can do that, and I can essentially use that approximation format. So I can essentially look for maps that are in this lazy format, and these are simpler to parametrize. And then, if I want to see how I've done after the fact, I could always look at the pullback of the target under the map that I've found. The target under the map that I found. So, this is just applying the same theorem again, but now to a different set of distributions. Now, I'm considering pi and its pullback, the pullback of pi under some map that I found, so t pullback pi. And I look at the ratio of that to my base density rho, and I look at the outer product of gradients of this density ratio. Again, expectation is over the distribution that's at the top, so expectation over that pullback distribution. And that again, I can bound this. And that again, I can bound this KL divergence, this forward KL divergence, by all the sum of all the eigenvalues of this matrix. So this essentially comes from the Log's Hobov inequality. And you can imagine what's the limiting case that if t pullback pi is rho, this whole thing is zero and the KL divergence is zero. So this is a bound on the forward KL divergence for a given map. So I can use this as a diagnostic after I find an approximation. Now, why am I sort of putting these pieces together? Well, you can now imagine a different kind of iterative algorithm that Different kinds of iterative algorithm that acts in a low-dimensional subspace incrementally and identifies that low-dimensional subspace in a greedy way. So, what if, for instance, the lambdas do not decay particularly quickly? What if my budget is such that I only want to use maps that depart from the identity in r directions and r is very small? Then I can achieve an approximation of pi by composition via this greedy composition. So, here's this algorithm that we call sort of deeply lazy, deeply because you're confusing. Of deeply lazy, deeply because you're composing things because you're composing things together. But here's the iteration: if I have my original target pi, my original reference rho in some desired rank R1, well, I'm going to compute this matrix H pi. I'm going to find a basis for the non-identity component of that map. I'm going to construct that lazy map. And I'm going to pull back my target by T1. Now, T1 on its own may not finish the job because of approximation errors in the map, because I've chosen R too small, because of this leftover eigenvalues. Small because of this leftover eigenvalues beyond R, for a whole host of reasons. But I can always compute the pullback density, and pullback density is actually quite straightforward to compute given pi, at least the unnormalized pullback, and T1. And now I make pi2, the pullback density, the new target of approximation. And I compute a matrix that looks at the ratio of pi2 to rho, looks at the gradient of the log of that ratio, finds an appropriate basis, constructs a next map, and pulls back. So generically, what I'm doing. Back. So, generically, what I'm doing at any given stage of this algorithm, at the L-stage of this algorithm, I'm building a lazy map to this pullback, the pullback of my target under all the maps that came before it. And so I also, and one useful thing about working on the pullback is that the reference is always Gaussian. And information about the density pi is always contained within this pullback. I'm never throwing away the original target density. And I can continue these iterations and stop when my upper bound of the forward KL divergence. Um, upper bound of the forward KL divergence is below some threshold. So that's kind of the idea there. Um, here's maybe a pictorial representation of what's happening. So here is just a kind of a cartoon target. This is a banana distribution. And I've required sort of by construction, of course, I could I could Gaussianize this distribution entirely. I could find map T that takes this distribution and pulls it back exactly to standard Gaussian. In fact, I could write it down analytically if I let that map act in two dimensions. If I let that map act in two dimensions, but for the sake of perversity and demonstration, I'll let the map act in only one direction. And what you were doing is essentially finding the directions of maximum mismatch between the target, between the current pullback and the reference, finding a one-dimensional map that straightens out that direction and then proceeds. And you can see over time, we begin to gradually Gaussianize this target, although acting in one direction at a time the iterations. The iterations may converge somewhat slowly. Here's kind of a more fancy example. So, this is not really an inverse problem. This is kind of a spatial statistical model, the log-Gauss, and Cox process. The target distribution is discretized in a 64 by 64 grid. So the unknown is 4096 dimensional. Here's some realizations of it. I'm observing it at the points marked by these circles. And here I'm looking at this trace diagnostic versus the lazy iterations for different lazy. Iterations for different lazy ranks, we all start to converge and they converge more quickly when I do more work per step. So, if I do, if I choose five-dimensional maps at each step, this thing comes down more quickly in green. If I choose three-dimensional maps, a little bit less quickly. If I choose one-dimensional maps, it makes this way. And at each step, what we're looking at is essentially the spectrum of this diagnostic matrix. And here I'm looking at the spectrum. And you can see both the early part of the spectrum, the The decay attenuates, but also the overall power, so the trace of the matrix, which is the sum of all these eigenvalues, starts to gradually go down. And that's what we're seeing in the figure on the left. Now, this problem, I should say, is exactly lazy at rank 30. So if I picked R to be 30, I could solve it in one shot, but I'm solving it step by step instead. Here's a problem that has no exact lazy structure. So this is the Darcy problem that we saw in many previous talks already. Many previous talks already. Again, we have a high-dimensional unknown, interior observations on this nine-by-nine grid, and I'm constraining the maps in a couple ways. I'm letting the rank just be four, just out of sort of computational budget, and I'm parametrizing the maps with polynomials up to degree two. Now, these maps, polynomials is just one choice. There's many other talks we could give about how to parameterize these maps. But you can see here the KL divergence or our bounds for the KL divergence go down. Or our bounds for the KL divergence go down. This is the trace diagnostic on the four-kill divergence. This is a different diagnostic that's asymptotically equal to KL divergence in the other direction as it becomes small. And again, inference proceeds. Okay, so I think I'm maybe right on time. So let me just close with this summary. The core idea behind both pieces of the talk is really to look at low-dimensional structure in updates between distributions. And in inverse problems, this happens from prior to posterior, but more generically, Posterior, but more generically, you can think about this as happening from reference to target. And under appropriate conditions, we can have an upper bound on the forward KL divergence. You can minimize this upper bound directly. And while you end up doing a solving eigenproblem, and coming up with a projector that gives you a basis that best captures the update from reference to target. And this is more principled than some other gradient-based methods that have been proposed for dimension reduction, in the least that we have a computable upper bound. And you can apply this not just to inverse. Apply this not just to inverse problems and MCMC, but to transform methods where the key idea here is really doing it iteratively because you always have a pullback distribution at your disposal. You do some work, you find a map, the map is approximate, you can pull back by that map and begin the computations again. And that leads to this composition idea. So with that, I think I'll stop. I'll pull up some references in case people want to know more and be happy to take questions.