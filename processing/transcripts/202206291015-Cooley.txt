Um, so I trust everyone can hear me and can see the slides. Um, let me first begin by thanking the organizers. Uh, Johanna, Philippe, and Linda worked very hard to propose and submit this workshop and then to organize it. And then, of course, the staff at Beers there, who I'm sure are taking care of the on-site people very well and are also providing us in Zoom land a seamless virtual. A seamless virtual experience. So, I thank everybody for all their work in putting this together. I'm going to talk about prediction for extremes. This is work that's been primarily done by my graduate student, Zhang Jin Li, who just defended his dissertation about a month ago. Although she hasn't been instrumental in the work I'm going to present today, I've also had another fantastic graduate student, Holly Matre, who's done some parallel work. Done some parallel work in time series, and I may allude to that a little bit throughout the talk. So here's our motivating example. What we have are air pollution measurements at four different locations around Washington, D.C. These three are in the city of Washington, D.C. proper. This is Arlington, Virginia, just across the Potomac. And then down here we have Alexandria, Virginia. And Alexandria no longer has a station. Longer has a station. There used to be a station at Alexandria. It turned off at about 2015. But you can imagine wanting to predict air pollution at Alexandria given these nearby measurements. And this is a well-studied problem, of course, in spatial statistics. Most people would look at this and say, well, this is a creating problem, and we know how to do that. But what makes this interesting is that the measurements on this particular day are all very This particular day are all very high, they are all above their 0.98 quantile, so they're kind of at the extreme levels that we see air pollution. And so, some of the standard methods for prediction, like Krieging, might not work so well when observations are at extreme levels, because Krieging, for instance, is based on the covariance matrix or the covariance information between these stations. These stations. And so, and covariance doesn't describe dependence in the tail very well. So, what can you do? You can try to get the conditional distribution, but there's no real candidate for this five-dimensional joint distribution. The data are clearly non-Gaussian. And even if you fit a condition, some sort of five-dimensional model, you might coming from the experience. You might, coming from the extremes perspective, you might question its ability to predict at extreme levels, particularly if it's been fit to the entire data set, because essentially that model that you fit is going to get a whole lot of information from the bulk of the distribution, and the information in the tail could get overwhelmed. Krieging, as I said, is based on covariance information. It's nice, however, because it doesn't require you to fully specify the distribution. The distribution, it's linear in its nature. So, Krieging is just going to apply weights to these observed values and come up with a point estimate that way. Not only is it based on covariance information, but if you want to do prediction intervals, we usually rely on Gaussian assumptions there. We take the mean squared prediction error and we multiply by 1.96 and we come up with a prediction interval that way. So, our goal in this work is to do something extremes-based. is to do something extremes-based, but we also want to do something linear. We're not going to try and fully specify the distribution. Instead, we're going to essentially come up with a method to apply weights to these observations, but something that's appropriate when the observations are at extreme levels. So to do this prediction work, I'm going to rely on a framework that I've been using for the last couple of years, which is kind of transformed linear operations. Operations. And so I'm going to give quite a bit of background on what lies underneath this prediction method, and then we'll get to the prediction about halfway through the talk. So underlying my method is this notion of multivariate regular variation. And we've seen a couple of talks which have either directly talked about multivariate regular variation or even Anthony's talk where he was talking about the extreme value distributions had references to this idea. To this idea, where essentially dependence boils down to understanding this angular measure. So, the idea of multivariate regular variation is that you have a multivariate heavy-tailed distribution. And the definition is going to say that the radial components, essentially the magnitude of the large points, and the angular components, the directions of them, become independent as the magnitude gets big. The radial components. gets big. The radial component decays like a power function where that power is denoted by alpha and the angular components are denoted by a measure called I'm going to call it the angular measure. Jenny in her talk yesterday called it the spectral measure that live on some unit ball. Sometimes people use the L1 norm. Sometimes I'm going to use the L2 norm for reasons that I'll explain. You can talk about multivariate irregular variation on the About multivariate irregular variation on the reels, or you can talk about it on the positive orthant. And I'm going to use the positive orthant because I believe that allows me to specifically focus on the upper tail. And I'll also try and explain that here in a couple of slides. Okay, so this is the definition of multivariate regular variation. And mostly this is here for notational purposes. What we have is a random variable x is regularly varying if it converges in this way. Essentially, you have to read. In this way, essentially, you have to renormalize it by some increasing sequence bn, you standardize it by multiplying by n, and you get this limiting measure. So, nu is going to be our limiting measure, and then this new decomposes into this radial component and the angular component, and so our angular measure is going to be denoted by H. And why is regular variation useful for modeling? There is theoretical There is theoretical justification behind it. The regularly varying random variables are the domain of attraction of the multivariate extreme value distributions that are heavy-tailed or frachet. And that angular measure, which describes the dependence, this measure that talks about what angles are likely, is also fundamentally part of the one characterization of the multivariate extreme value distributions, those with the heavy. Extreme value distributions, those with the heavy tail. From a modeling perspective, the multivariate regular variation is nice because it's only defined in terms of the tail. If we look at that definition that was on the previous slide, it doesn't say anything about the bulk of the distribution. It only talks about the tail. So throughout, what I'm going to be doing is taking the large observations, trying to say something interesting about the angular measure, and not using the information of the bulk of the distribution at all. Information of the bulk of the distribution at all. Now, often this framework requires a marginal transformation, and I'm definitely going to be doing marginal transformations in this work. Anna in her talk earlier was working to not do a marginal transformation, and it certainly does complicate things by complicates inference when you have to do this marginal transformation. Okay, so describing multivariate regular variation in two dimensions is relatively easy. I just need to describe this distribution of angles, if you will, which is described by H. But when dimensions get large, modeling H is hard. And my motivating problem is only five-dimensional. Even that is challenging. But if we're going to look at another example very briefly where I've got 30 dimensions, and I think estimating Estimating H in high dimensions is really hopeless. Essentially, what you have is this distribution that exists in P minus one dimensions because their norm is one in some sense. But trying to estimate that distribution is hard enough and it's impossible if you're going to throw away 95% of the data and only use a small subset of your data that you consider to be extreme. So I'm not going to. Extreme. So I'm not going to fully specify H. Instead, I'm going to summarize it with this tail pairwise dependence matrix that Anna was talking about earlier. But furthermore, there's another question. I said at the outset that I want to do linear prediction and I want to work in the positive orthant. And in the positive orthant, you can only have positive numbers. And so doing linear things, what do you mean by subtraction in the positive orthant? I also need to come up with a method for doing that. So, nicely, Anna talked about the tail pairwise dependence matrix earlier today. Throughout, I'm going to assume alpha equals 2, and I'm going to use the L2 norm. And essentially, you want this norm to agree with your alpha. And if you notice Anna's talk, she had these alpha over twos throughout hers where she was not specifying the alpha. If we assume alpha to be two, then things. Alpha to be two, then things are much more simple, but there's a little bit more to it than that as well. So we're going to assume that we have a random vector x, which is regular, a p-dimensional, regularly varying with index 2. It's positive. So we're working with positive orthons. And I define the tail pairwise dependence matrix this way. It's simply the integral over our unit ball. integral over our unit ball with respect to our angular measure of the ith component and the jth component these these w's are such that they lie on the on on the unit ball um so what this does is instead of trying to fully specify that high dimensional h it boils h down into some summary measures what we get is a pairwise summary measure which describes the strength of dependence between the ith and Dependence between the ith and the j-th component. This matrix is positive definite. The diagonals give kind of scale information. Anna showed this really nicely. We can kind of think of scale for regular variation where it's normalized by some kind of standardized random variable. This z has the same normalizing or sequence as x in some sense. And the off-diagonals, if they're zero, that implies asymptotic. They're zero, that implies asymptotic independence. Furthermore, this matrix is completely positive. And if you recall, what completely positive means is that there is a P by Q matrix A, think Q much larger than P, so you've got this short fat matrix. But there is a finite Q such that a non-negative matrix A exists that you can obtain your T B D M by taking A times A transpose. A times A transpose. And so this decomposition is not unique. And Anna spent a bit of time talking about her method for decomposing, going from a TBDM back to a matrix A. Now, why am I going to talk about the positive orthogonal? Why do I want to focus on that? Let me jump back a couple of slides. If this is the air pollution data, maybe this is Alexandria and this is Arlington, air pollution is. Arlington, air pollution is non-negative. And so it kind of makes sense in that setting to work in the positive orthodox. So, but let's even think about an example where the data aren't necessarily positive. If you think about financial data for just a moment, so maybe our X axis, our horizontal axis is Apple Computer, and our vertical axis is Ford Motor Company. And we've oriented our data so that we're talking about. Our data so that we're talking about negative log returns. So, out here, because we've negatively oriented the data, these would be big losses. And of course, if I'm talking about financial data, we would see big gains down in this orthant down here, which if we extended to the reals. Now, if I knew the whole angular measure in this two-dimensional case, that angular measure would exist on the whole circle, and I could characterize the extreme. And I could characterize the extremes through that angular measure in all directions. But in high dimensions, what I want to do is I want to boil the information in that angular measure down to one number. And so that one number, if I do this integral over the entire ball, that integral would capture not only the strength of dependence in the loss direction, which is the direction I'm interested in, but it would also characterize, it would combine information about the gains, which would be down here. And in my experience, And in my experience, extremes in one direction and extreme dependence in the other direction are not symmetric. In financial data, I think extremes, extreme losses have more tail dependence than extreme gains. And so if I try to boil this down to one number and I don't work on the positive orthant, somehow I'm polluting the information in the direction of interest that I'm interested in with information in the other direction. And so I think there's reason to work in the positive orthant, even when the data. Even when the data don't naturally occur in the positive orthonant, like this air pollution data. Okay, so if I'm going to work in the positive orthonant and I'm going to do something linear, I have to define linear operations in the positive orthodox. And it turns out that that's not too difficult to do. Essentially, you can do it with this transform function. So start with a vector that is in the real, so y can take both positive and negative values. Both positive and negative values. And think of any function which will transform the reals to the positive reals. So for now, you can think of t as just being the exponential function. And then what you can do is you can define your linear operations this way. Essentially, if you apply that transform component-wise, so now x is going to be in the positive ordinant. You can define vector addition in the positive ordinant this way. Just take your first vector, transform it back to the reals. Transform it back to the reals. Do that with the second vector as well. Add them up in the reals and then transform them forward to the positive orthogonal. And do the same thing for scalar multiplication. Take your original vector, transform it to the reals, multiply by C, transform it forward to the positive reals. And you can show that this creates a non-stochastic vector space. So what this allows us to do is to allow, it gives us linear operations. It gives us linear operations on the positive orthogonal. Now, what I want to do, this was just talking about locations in space. There's nothing random there. But what I want to do is I want to apply transform linear operations to my regularly varying random variables. And so I'm going to choose a particular transformation, which is this guy here, which takes the reals to the positives. And what's special? And what's special about this, I get asked this always, but there's nothing really special about this particular T other than the fact that it negligibly affects large values. So as y gets large, the ratio of t of y over y is one. And that's true of the inverse as well. And so this is the one property that T has to have so that I can apply this to regularly varied. Apply this to regularly varying random vectors and maintain that regular variation. So if I add, if I perform my transform linear addition on two regularly varying random vectors with known limiting measures, the sum, if they're independent, is just the sum of those linear measures. If I multiply by a positive scalar, it just multiplies that limiting measure by a to the alpha. A to the alpha, and here our alpha is going to be two. If a is negative, essentially it kills it off, it makes the limiting measure be zero, and that killing it off by a negative value is going to also be important here. So I think the last bit of background is that we can also do a construction method. So I can start with independent, regularly varying Z's, and I can hit them by a matrix A. Hit them by a matrix A. And this is my transformed linear matrix multiplication. And what that does is by hitting it by this matrix A, it induces dependence in X. And we know that the angular measure of this X is composed of point masses that essentially arise from the columns of A. And so this construction method essentially This construction method essentially is very similar to the max linear construction that we've heard a couple of talks about, but instead of the max operation, we're doing this transformed linear. It turns out that if I start with the same matrix A and do the max linear construction, I'm going to end up with the same angular measure. If this A is allowed to have positive and negative coefficients, the T P D M arising from this construction, so Arising from this construction. So when I boil that angular measure down to the TBDM, what happens is you have to zero out the elements of A and then do get A times A transpose. Okay, so this kind of wraps up the background, but the basic idea of where we're at is kind of right here. We have this rich history of linear methods in statistics. What I want to do is I want to combine that with regular variation on the That with regular variation on the positive orthant. And so the way I do that is with these transformed linear operations for regular variation. And I summarize dependence with the TPDM. And what this has allowed me to do over the last couple of years is essentially exploit all these linear methods and come up with analogous methods for extremes. So we've talked about extremal PCA. We built a spatial autoregressive model. Nahali's work has been on time series analysis and On time series analysis and time series construction of ARMA-like models. And then today I'm going to talk about linear prediction. But so let me pause here for just a second and see if there are any questions before we jump into linear prediction. Okay, well, there will be time for questions at the end. So, okay, what I want to do is I want to do linear prediction. And essentially, what we want to have. Essentially, what we want to have is we want to rely on the projection theorem. I've got to have a Hilbert space to do the projection theorem. And so I need to extend that vector space idea a little bit because the vector space that I talked about earlier was non-stochastic. It just talked about creating a vector space. So here I'm going to create a stochastic vector space dq. So assume that Z is a vector of independent of Q. Of independent of q, independent regularly varying random variables with tail index 2. They all have the kind of the same normalizing function here. And what we're going to do is we're going to consider the space of linear combinations of z. So take q element z and linear transform linear combine them. And so And so this is an element in VQ is a regularly varying random variable with tail index 2. And we can use our transform linear operations to talk about summing elements of VQ and doing scalar multiplication of VQ. And so you can show that this is a vector space. And essentially, it's really easy to think about this vector space because. Think about this vector space because it's isomorphic to Rq. If you give me the vector of the coefficients of your random variable, that completely specifies the random variable. So there's an isomorphism between VQ and RQ. But as I said earlier, this differs from our non-stochastic vector space that I talked about earlier. We can define an inner product for BQ. And so if I take two elements in my vector space, essentially all I'm going to do is Vector space, essentially, all I'm going to do is do the dot product of their matrix of coefficients. And so, you'll notice that, of course, I'm essentially stealing the inner product from RQ and applying it to my vector space. So, with this inner product, we get the notion of orthogonality. We get, of course, the notion of norm. The norm of a vector is just going to be the square root of the sum of its coefficients squared. The sum of its coefficient squared. This arises because of the alpha equals two observation. We can talk about a metric between two elements of our vector space. It's just going to be the difference of the coefficient squared summed, well, the square root of the sum. And so, if we have a collection of p random variables, we can take each pairwise inner product and we can construct the And we can construct the inner product matrix for those. And that will correspond to AA transpose. So if we think of generating this p-dimensional vector x with some, with a matrix A, which just is built out of the coefficients of each of these guys, we get the inner product matrix is this. Now note, this inner product matrix does not have that zero operation that I talked about. not have that zero operation that I talked about earlier. And so it's different than the TPDM. So put that in the back of your mind for a moment. But with this vector space, with this inner product space, linear prediction essentially comes for free because we have the projection theorem at our disposal. So essentially, imagine we're trying to predict xp plus one given xp. We kind of combine that vector. This is just the matrix. This is just the matrix of the coefficients arising from those vectors. We get the inner product matrix is just a product of those matrices. I'm going to decompose the inner product matrix into its pieces. And then you can either use the projection theorem or you can talk about minimizing our metric, which amounts to minimizing this. And our vector of weights is of this very familiar form. It's sigma 1, 1 inverse sigma 1, 2. This looks like symbol. One inverse sigma one two. This looks like simple Krieging in the non-extreme case. So with our framework, the best transformed linear prediction essentially comes, just follows from classical results. Let's take a minute to pause and understand this metric. We've kind of been living in this in math land of this vector space. Vector space. But this metric doesn't have much intuition. We said the metric is the squared difference of these coefficients, which doesn't have a very probabilistic interpretation. But what we can show is that the sum of the difference of coefficient squared is equal to this point. Squared is equal to this quantity here, which is a quantity that, if you've studied regular variation, looks kind of familiar. What we have is this ratio of probabilities. This is kind of a scale-like quantity that on the bottom is our reference random variable. This is our noise generator. These are the Z's that create our vector space. And what we're doing is we're taking the probability on the numerator, we've got the probability. The numerator, we've got the probability of the maximum difference of x1 minus x2 and x2 minus x1. This is not a squared difference like we're used to, we've got the max of these two differences. And the reason it shows up that way is because with this transform linear prediction, the x1 minus x2 is not the negative of x2 minus x1. And so that's why you end up with this max. So we can understand our weight vector b that we're going to apply to our observations to perform our prediction as the b. As the b which minimizes the scale between x hat and xp plus one, understood kind of through this way. Okay. Dan, you have five minutes including questions. Okay, well, buckle your seatbelts. Well, so does this framework work for modeling? At this point, you should be kind of scratching your head and saying, well, Dan, you're living in. Your head and saying, Well, Dan, you're living in Math Magic Land, you've got this nice inner product space, but how does this work for modeling? Particularly, you should be worried about these negative coefficients. I need the negative coefficients to define my inner product. Without that, I don't have a vector space, but they really aren't important because if I take a matrix that has only positive values and create a vector x, and I think of a similar matrix with the And I think of a similar matrix with the negative values zeroed out and create another vector x plus, the angular measures of those are the same. And so we can really restrict our attention to the subset of VQ, which is composed of just random variables with positive coefficients. And in that case, then my inner product matrix agrees with my TPDM. And so now I'm back to something that I can estimate. I can take data. Something that I can estimate. I can take data, I can estimate the TPDM, and I can go forward. And because of the denseness arguments that we talked about earlier, assuming that X arises from some linear combination Q is not too restrictive because we don't really have to specify Q to model. So our modeling strategy is to assume that X is regularly varying with alpha. Is regularly varying with alpha equals two. We might have to do a marginal transformation. We then summarize with the TBDM and we can obtain our prediction weights this way. And so when we apply this to our Washington pollution data, here is X hat. On this axis is our predicted NO2. Here's the observed NO2 for this is the period prior to 2015 when this station was still visible. And you can see. Still visible. And you can see that, hey, we're doing, you know, we're doing, it seems a reasonable job of predicting. I haven't talked about the prediction intervals yet. But the reason our prediction cuts off here is we're only predicting when the observations are large. So we're looking at essentially the top 5% of the predicted X hats. We can create prediction intervals. We have to do that in a very different geometry where we're Very different geometry. We're very much outside the elliptical geometry, which sort of underlies the prediction intervals that we're used to. And so we have to do it in a way that makes sense for regular variation. And essentially, what we do is we rely on a completely positive decomposition of the prediction TPDM. So if I have this, if I have my random vector x, all I have to work with is the TPDM itself, but I can from the Itself, but I can from the TPDM, I can get the two by two TPDM, which relates the strength of the dependence between x hat and xp plus one. And from there, I can do the completely positive decomposition, kind of like Anna did, to get a 95% confidence interval. So we get reasonable coverage rates better than we would get applying Krieging to only the large observations. Creaking to only the large observations. And this even works in 30-dimensional financial data where we get reasonable coverage rates. And because this is so much heavier-tailed, our Gaussian coverage rates are terrible. So let me only say that we've been also working to extend this to an idea of partial tail correlation, which kind of gets to an idea of causality. An idea of causality. And Jan Gong, who's going to talk tomorrow and who's been working with Raphael Huser out at Caust, has also been looking at that. And so I'm looking forward to her talk where you extend this idea of prediction, which allows you to define partial tail correlation. And I'll stop there.