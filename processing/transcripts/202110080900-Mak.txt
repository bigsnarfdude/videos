Everybody, I'm delighted to have Simon Mack, one of my brilliant young colleagues, speaking here today. He's going to be talking about TSEC, a framework for online experimentation under constraints. Thanks, David, for the kind introduction. So as mentioned, so I'm going to talk today about TSEP. It's a weird name. It stands for Thompson Sampling Under Experiment. Thompson sampling under experimental constraints. So, not very creative, I admit, but you know, it's a name that sort of works. Okay. And this is a way of using Thompson sampling, modifying Thompson's sampling to allow for online experimentation under what's called experimental constraints. And we're going to explore one type of very, you know, pretty common experimental constraints that we face in, you know, one type of a marketing problem, which is website optimization. Website optimization. So let me. There you go. My computer is lightning a little bit, so jumping a little bit between slides. So first of all, I'd like to sort of acknowledge and thank collaborators on this. So David has done a lot of work with regards to coding for this project. So he was a previous PhD student at Georgia Tech, and now he's working in Uber. So he has a lot of insights into these sort of web. Lot of insights into these sort of website optimization problems and the sort of the industrial sort of side of things. Okay. LaVon is an undergraduate student at Duke, was an undergraduate student at Duke who is now working at Citibank. And she has provided a lot of really interesting input and insights on the financial problems I'm going to talk a little bit about later on. And of course, Jeff Wu, who is a professor at Georgia Technique, he needs no introduction here, right? He needs no introduction here, right? So, right. So, a little bit of the talk outline here. I'm going to first talk a little bit about the motivation specifically in terms of this website optimization problem. And specifically motivating the fact that this is a challenging, and there's a lot of challenging problem. There's a lot of open issues there. And specifically, motivating this thing called an armed budget constraint, which is an Constraint, which is an experimental constraint that we would have when we're sort of trying to optimize these websites in practice. Then we're going to use this to motivate something called Thompson Sampling under experimental constraint, or TSAT. Talk a little bit about the methodology, the algorithm, and how it works. And then I'm going to illustrate this in a simulated website optimization study. And the reason why this is simulated is we couldn't find any data, any tangible data specifically on website optimization because a lot of the studies. On website optimization, because a lot of this stuff is proprietary, right? So, if folks have data on this, they would want to be making money off of it. Okay, so we simulated this based on a model in the literature, and we'll show a little bit of details on that. And then lastly, we're going to try and apply this thing in a more sort of a real-world setting in terms of portfolio optimization, specifically for exchange-traded funds. Okay, and one comment about the presentation. So, again, you know, everything is on Zoom right now, but it will be really Zoom right now, but you know, it would be really nice if you do have questions, just shout it out on the call. It's nice to get a little bit of interaction and I'll pause for questions periodically just to make sure that everyone is on the same page. Great. Okay, so let's start off with this website optimization problem, where the goal here is to find an optimal setting of a web page in order to maximize customer conversion. Conversion. So, you know, the first part of this is pretty clear, right? We wanted to find a way of customizing a web page. Now, what do we really mean by customer conversion? Okay, this is more of a technical term. So by conversion, what we mean is customers and individuals who respond positively to the website. And they would sort of, if they respond positively, they would perform a certain action. And this action that we collect in terms of the data is oftentimes binary. So this conversion data is. Oftentimes, binary. So, this conversion data is oftentimes binary. This can involve clicking a certain link, right? So, or installing a certain app or donating to a certain campaign. So, I'm going to use this particular example that David had, which is using the, I think Obama had in his re-election campaign, right? He was, you know, there's a lot of work in terms of designing an optimal website, you know, which can sort of funnel in a lot of the donations, right? A lot of the donations, right, uh, to maximize the donation that they get for the uh uh um um the campaign here, right? So, I'm going to use this example and sort of show you know some of the problems that you know they faced with regards to this design problem. Okay, so let's take this particular illustration. Okay, so we have you as an accomplished marketer, okay, deciding whether or not you want website A or website B. site A or website B. Okay, and if you look carefully at website A and website B, the difference here is that we're sort of switching the order of the map, right? The Google map on the left here, with this sort of panel describing some of the details of President Obama's prime. So the question is whether or not we should have the map on the left or the right. And the question is that you have to think about as marketers, which one is best? And this really goes back when we're deciding between one website. Back, you know, when we're deciding between one website or another website, right? Um, this really goes back to the problem of A-B testing, which is very widely studied in the literature. Uh, statistically, you can sort of view this thing as a two-sample hypothesis test, right? And you know, this is widely used in the industry in terms of making decisions, in terms of using the current website A or an alternate website B. But the problem gets more complicated than this, right? So, let's say you're, you know, you think a little bit. Let's say you think a little bit more about the website, and you think, well, I can indeed change the order of these two panels, but I can also change the location of this donate button, right, to be on the left or the right. Now, we have two factors that we can change in this website, right? Each with two levels. And the problem gets a little bit more difficult. So you're a little bit more nervous here. So which one is best? And how do we decide which one is best from data? Decide which one is best from data. And of course, in this case, in terms of website optimization, there can be a lot of things we can change. You can be creative about this. And this might be a headache for marketers, because you have to think about a lot of different choices to make. So for example, the font and the style of certain blocks of words. There's many colors that you can choose from, many themes that you can choose from. And you can sort of imagine. Then you have this website optimization problem where you have thousands or perhaps even millions of Or perhaps even millions of potential websites that you might want to test and see which one is optimal. And it quickly becomes this sort of very high-dimensional optimization problem. So that's where really this idea of multi-armed bandit comes into play. And we call these things MABs. There's a really good sort of review sort of article, well, paper, I guess, right, by Bubeck and co-authors here. Co-authors here. But the idea behind a multi-on-band is that you have capital N. So it's going to be, you know, I'm going to sort of introduce the notation bit by bit. Hopefully, in this case, there's not too much in each, but notation. But in this case, we have capital N competing choices or arms. And so you can think about, you know, just a person in perhaps Las Vegas trying to, you know, play different types of slot machines. And I want to find the slot machine, which gives me the Which gives me the greatest reward. Okay. So I have n slot machines or arms. And arm i will yield a random reward of x i of t at time t. So you can sort of think about, look, if I decide to play the first machine, it's going to have a certain distribution for the reward that I'm going to get out from this. Okay. And the goal in terms of MABs is what we want to do is we want to allocate the resources that we have, right, in terms of the number of runs. Have right in terms of the number of runs in such a way to maximize the expected reward that I'm going to get out, or equivalently to minimize the regret that I make in making the wrong choices. And so if you think about this person, you know, gambling in Las Vegas, right? You know, this type of decision really has to be made sequentially, right? Because we don't know which machines will give me the greatest rewards if we don't have data on it, if we don't try it out. Don't have data on it, if we don't try it out, and so this sort of sequential learning really goes back to an exploration-exploitation trade-off. Okay, uh, exploitation means, or exploration means, look, we want to try all of the machines to at least see which one, you know, get some data on each of the machines. But exploitation, you know, talks about we want to then play the machine, right, after observing some data, which gives us the greatest reward, right? At least according to the machine. Reward, right, at least according to what we estimate from data. So, this particular trade-up is key in terms of getting this optimal strategy. Okay, but of course, when we talk about optimality, it's always good to think about optimal in what sense. Okay, and so one common metric that we would use is this notion of regrets. Now, I put here the expected regret, there's also the pseudo-regret, which is a little bit different, but it's very much the same flavor. So, let's say in this case, I'm playing I'm playing these slot machines for t time periods. And at each time, I can play one machine. So what we have here in terms of this criteria is a couple of things. The first one here is talking about, well, let's take a look at this one first on the right. This is talking about the cumulative reward. Remember, X here is the reward. So I want IT. So I want IT will denote the arms that you played at time t, okay, the machine that you played at time t. So this thing right here will be your observed reward. And what I'm doing is I'm summing this up over all t time periods. So this will be your cumulative reward over the arms that you did sample. All right. So on the left right here is the cumulative reward from the optimal arms. So if you take a look at what we're doing here, we're summing up all of the rewards for our particular Of the rewards for a particular arm i, okay, just this part, and what we're doing then after is we're taking the maximum of this over all capital N arms. So this would give us the maximum reward that we can get by choosing the optimal arm, excuse me, every time. Okay. So you can think about the difference between this as sort of like the regret in your decision, right? And so when we take the expectation over this, this will give us the expected regret. Will give us the expected regret in choosing the wrong arm. Okay, and this is something that we want to minimize in terms of making decisions, in terms of choosing these I of Ts. Okay, so that's pretty sort of standard multi-on-banded stuff here. And this multi-on-banded problem has been explored quite a lot in the statistics and the machine learning literature. There's many types of methods. I sort of listed down three main ones here. First one is an epsilon greedy method. First one is an epsilon-greedy method, which is investigated by Sutton in 1998 and many papers afterwards. Second one is this sort of UCP upper confidence bound type of method, which was explored theoretically by Auer et al. And there's been many developments afterwards. Now, what we're going to focus on in this talk, and there's a reason for this, right, is this sort of Thompson sampling method. And there's a really interesting history behind this particular method. This method is inherently method is inherently a very simple approach if we view this thing from a Bayesian perspective and we're going to extend this sort of Bayesian modeling paradigm later on and I'll show why this will be useful in our context right but a little bit of history right so this approach was proposed a very long time ago in a biometric paper by Thompson in 1933 okay and you know it's been around in the literature for a long time but it was sort of revived very recently well not very recently a decade ago right by Chappelle Decade ago, right? But Chappelle and Lee in the Neuros period, where they said, look, for this multi-unbanded problem, this actually gives very, very competitive empirical results for minimizing regrets. And so this sparked a lot of interesting work. Agrawal and Goya, I think, was the first ones to think about theoretically what's going on in Thompson's sampling that allows it to minimize. There's been many, many papers afterwards, which explored the theoretical aspects of this. Which explored the theoretical aspects of this. But what they can show is that Thompson sampling is a very simple and very natural procedure from a Bayesian perspective, but it also achieves near optimality in terms of minimizing regret here. And so we're going to take this procedure and extend this for our problem. So a little bit more on what Thompson's sampling actually is. So we're going to take a simple case of this for what's called Bernoulli multi-arm bands. Multi-arm bandits. And the term Bernoulli is just really talking about the distribution of the reward. So, in this case, for Bernoulli bandits, the rewards will be binary, which fits in terms of the website optimization case. So, again, it's much easier to explain this thing in terms of like a Bayesian perspective. So, we first have a distribution for the rewards. Remember, XIT is the reward that we get from ARMI. And we're going to assume that this reward quantity is independent. reward quantity is independently distributed from a Bernoulli distribution with probability theta of i. So each arm has a probability of theta i. arm i has probability of theta i of yielding a reward essentially okay and this should be i just it's whenever i go through talks you know you always notice you know typos here and there this should be a capital n okay total number of arms okay and so from a basic perspective we And so, from a basic perspective, we don't really know what this probability is. So, we're going to have to assign a certain prior to this. And one perhaps prior that you might want to assign is just this flat prior, right, from a beta distribution with parameters one and one. So before we observe data, we're going to assume all of the arms have this sort of beta distribution, the beta prior for the probability. Very simple model. So, let's say in this case, we're going to perform. Now, we're going to Perform. Now we're going to change this problem a little bit because, in the website optimization problem, we have this thing called traffic. Okay. So let's say in this case, for each time period, we're going to present, we're going to perform little n experiments. And I'll sort of draw this out here. I have this sort of blank space that I'm going to try and draw. So you all can sort of see the drawings here. Yes. So let me just draw this up. So let me just draw this up. So let's say in this case, this is our experimental time period. Okay. And we start off with time one, time two, all the way to time capital T. Okay. Of course, we should have a time zero here. So the idea is that you can sort of think about this from the website optimization problem, right? At each of these time periods, you can think about this as sort of like daily experiments that we're going to perform. At each time period, I'm going to first, I might have, you know, a traffic. I might have, you know, a traffic of 5,000 people, okay? People that would visit my website, and I would have to funnel them into different websites. Okay, so we call, you know, within each of these time periods, we can perform little n experiments where little n here will be 5,000. And this can be sort of viewed as your website traffic. And for each of these time periods, I'm going to run this thing for 5,000 in each time period. So that's the setup here. So given this particular setup, what does Thompson sampling actually do? Actually, do okay. So let me go to the next slide. So, what it starts off with is, of course, it starts off by assigning beta priors to each of these, right, these flat priors. And afterwards, what it's going to do is for each of these time periods, right? And for each of these experiments, right, for each of the people coming into my website, I'm going to sample theta i. To I from the posterior distribution. So you can sort of imagine if maybe I have, you know, arm, uh, arm one, arm two, and arm three, right? Each of these will start off with a beta prior start, right? Beta one, one, beta one, one, beta one, one. Okay. So what I'm going to do is I'm going to first sample from the posterior distribution. Well, in this case, it's the prior, right? One sample of theta one prime. theta one prime, theta two prime, theta three prime from each of the arms. And then I will then compare which one of these values is the largest. Okay, so I would then experiment on the arm, which I call a star here, which has the largest sampled probabilities from the posterior distribution of the reward probabilities. So that's how I'm going to make my decision. So that's how I'm going to make my decision. And for each of these 5,000 people, I'm going to do this repeatedly, right? And then assign, you know, maybe 2,000 for this first website, 1,000 for the second one, and 2,000 for the third one. So that's the procedure that we're going to do. And this is essentially Thompson sampling. So any quick questions on what Thompson sampling? Any quick questions on what Thompson sampling is first? I'm going to have to try to. There's one thing with annotations is that it's either on PowerPoint or on Zoom. And I believe this is on PowerPoint, which means I don't have to delete it. Great. Yeah, that's right. So any questions on this so far? Yeah. Hi. Hi, Samantha. Yeah, so I see that in here, like observe binary reward. So after So, after the experiment. So, this in the real case has some delay. So, usually, because data aggregation needs time, and when the reward is reflected to the algorithm, it sometimes have delay in real case. So is this delay a problem in time-some sampling, or is it okay not to want to? That's a good question. So, let me go back to my again. Of course, I removed my visualization of the My visualization of the procedure here. So let me actually escape full screen, discard all of my annotations, and then essentially clear the screen here. Right. So let me be careful and just annotate on Zoom so I can actually delete this. So, right. So, in terms of delays, typically what happens in website optimization, this is from sort of David's perspective on this, right? If we go from time one, two, three, oftentimes One, two, three. Oftentimes, data is collected and compiled at the end of the period here. So maybe in this case, we run through a traffic of n is equal to 5,000 people. We would then run this Thompson sampling. And then at the end of this, we would collect all of the data and then update the posterior. So there is a little bit of a delay. And that's why in this case, we're sort of doing like a batched version. Sort of doing like a batched version. That's why I call this adaptation of Thompson's sampling that's used in the industry. This is a batched version where there's a little bit of delay, but the delay is controlled by each of these time steps. Does that sort of make sense? Yeah, yeah, thank you. Thank you. I got it. Yeah. It's actually an important point because later on, what we're going to do is we're going to talk a little bit about ARM constraints within each of these time periods. And that's the context in which we're going to frame the problem. Okay. Okay, so let's move on a little bit. So let's move on a little bit. So that's Thompson's answer. Okay, now, what about experimental constraints? Well, let's take that problem of you being the marketer here. And let's say you start getting a little bit creative. You start thinking about you have maybe thousands of Canada websites. I can change this font here. I can move Obama's image from left to right, et cetera. And then you think about, okay, this is really cool. I'm going to then talk with the person that's actually designing the websites. And oftentimes they will give you this freaked out face right here. Would give you this freaked out face right here, right? And they would say, Look, I can't design that many websites, and I can't maintain that many websites as well, right? Because every day, this you know has to be sort of maintained on the server. So he starts freaking out, okay? And, you know, after thinking a little bit, he sends you another email saying, look, I might be able to maintain maybe 10 websites at a time if you want. 10 websites. Okay. And this is something that we'll call an arm budget constraint. Okay. This is a budget. This is a budget, a constraint on the number of arms or the number of websites that you can test at a given time period. Because in a lot of these cases where we're sort of exploring very high-dimensional space for the arms, there's costs involved. And I really can't maintain a thousand different websites at a given time. There are costs. So you think a little bit about this. And let's say in this case, you really can't argue with them. So we have to pick 10 websites. We have to pick 10 websites. So let's say we start off by taking just like a random sample of some of the websites. Now, this is not really random, but you can sort of imagine this is random. So we're going to call this thing as the active arm set. The arms are the websites we're going to test in a given time period. Active arm set. And so we start off with choosing these particular websites. We give it to your programmer, and then they code this in, And then they code this in, you know, we run the traffic through these websites and we collect some data. And let's say afterwards, realize, you know, some of these websites are actually really, really bad. Maybe the donate button is all the way at the bottom where no one's going to see it. Okay. So you want to then maybe for the next time period, you want to choose a different active arm set. Okay. And we call this procedure switching. Okay. So this procedure of switching is adding promising arms. Adding promising arms, right, from arms that you didn't sample before, but also removing bad arms. And this often happens between the experimental time periods, right, when the traffic is slow. So you can sort of think about, you know, we're running this sort of website optimization at the end of the day, right? We might perform some switching. So that's sort of the context here. Now, clearly, there's sort of this sort of A problem with armed budget constraints, right? And this really sort of opens up a really interesting sort of a couple of objectives, design objectives for online experimentation. And so there's really two that are of interest here, right? So the first one is, how do we really select a good active arm set? And we have to do this thing sequentially. How do we choose a good active arm set, which we're going to denote as script A? Denote a script A. And of course, this is going to be a subset of the total arms that you have. And the size of the set is going to be capital K, right? And capital K here, the size of your active ARM set is going to be much less oftentimes than the total number of websites you want to explore. So there's clearly a constraint here. So how do we choose a good active ARM set in a sequential fashion? So that's goal number one, how to select the set. The second one is, let's say you already selected this particular One is, let's say we already selected this particular set and we're going to run the experiment. We have 5,000 traffic experimental runs to allocate. How do we then allocate these runs over this restricted ARM set to minimize regret? So there's two design questions here. And I think one sort of overarching challenge, which might not be obvious at the start here, right, is the fact that, look, if we wanted to explore a thousand or a million websites, Explore a thousand or a million websites. One challenge is this N can be very large and this K can be small. And we then have to leverage information from arms that we did observe, which are few, in order to make decisions on arms that we did not experiment on. So specifically, if you think about this particular problem, I have a million websites. How do we then identify promising arms? Promising arms out of these million websites, if we didn't sample maybe like 99.9% of these websites, okay? That's one question, because if we don't observe data on them, then we, you know, in terms of the traditional Thompson sample procedure, right, we would have to explore, we would actually have to observe data on a website in order to make a decision. But if, you know, we have a million websites, that's clearly not easy, okay? And a similar problem also holds for allocation of the arms, meaning Allocation of the arms, meaning if we have certain arms, right, which we didn't have observed data on, then how do we exploit them in an efficient fashion? Right? Because again, Thompson sampling requires us to observe data on all, on each arm in order to make a decision on whether or not the arm is good or bad. But we don't have enough resources in order to explore a million websites, right? So that's again a challenge. And I would argue that in this case, one way of addressing this. In this case, one way of addressing this is to use a Bayesian model which can learn the correlations between the different arms. We're going to try and model dependencies between the rewards of each arms and then exploit these dependencies in order to address these two design questions, one and two, right? With the goal of minimizing regret. So that's the goal here. Okay. Any questions so far? It makes total sense. In this case, you have covariant information on the bandits: donate button high or low, Obama left or right, stuff like that, which should be used. That's right. But it should be used in the context of arm constraints, which I don't think anyone in the literature has. And one small question about your switching. So, when switching, you completely like Completely abandon the previous arms, or you partially accept the previous good ones and then partially adopt the new arms? Yeah, that's a good question. So let's go back to switching here. So in this case, the illustration is not as good because it's going to be hard to sort of draw. I should perhaps have drawn the active arm set like right here, right? So you could, and I think it's of course a good idea, right? It depends on the procedure, but you might want to keep some arms which are good in terms of the activity. Arms, which are good in terms of the active arm set. So you're not abandoning the whole arm set. You might choose to, you might retain the good arms, right? But drop the bad ones and add in promising arms that have not been sampled on. Yeah, I understand. Thank you. Great. Okay, so let's move on. So that's that. We need a model, essentially. So the model here is nothing too new. It's not, it's nothing new. Okay. We're simply going to Okay, we're simply going to adopt a simple sort of Bayesian two-factor interaction model. We're going to assume that in this case we have capital M factors. So, factors here could be the font size, could be the font style, right? It could be the position of the donate button. And we're going to assume that factor M has L M factors. Again, this is a lot of notation just to sort of put the model down in a mathematical notation, but you know. Notation, but you know, it's really the idea is pretty simple. Okay, we have m factors, each one has that many levels, and we're going to assume that each arm can then be represented by a vector of the different levels for these factors. Okay, so one arm could be a website and you know, i1 dot dot dot i am could be the levels of each of these design factors that we have, okay. So, we're going to do a pretty simple model here. This looks complicated just to sort of write down the notations, really not. So, the probability. Really, not so the probability of the reward right for arm i is going to be controlled by these regression coefficients, beta, okay, and it's going to follow the sort of probit, probate regression model, right? Nothing too special. This thing right here is the main effects for each of the factors. This thing right here is the two-factor interactions, and of course, you can be flexible in terms of how you design this probability model, right? Okay, nothing too special. And of course, the reward will be based on a renewable. The reward will be based on a renewally on these probabilities. Okay, great. Okay, so we'll talk a little bit more about this model just briefly here. Of course, in this case, we need to maintain identifiability of the coefficients. So what we're going to do is employ these sort of baseline constraints, meaning the first level for the main effects and interactions, we assign an effect of zero. Okay, nothing too special here. For the priors, now we want to do this in a Bayesian way. And I'll show you why this is the case later. Amazing way. And I'll show you why this is the case later on. But we also need priors into regression coefficients. And what we're going to do is impose this prior proposed by Joseph 2006, which imposes this so-called effect hierarchy on the regression coefficients. So essentially what we have is the intercept here will be assigned some sort of normal prior with a variance tau squared. The main effects will have a decreasing effect, right? So this r factor will be between zero and one. Will be between zero and one. Okay. And so we're saying that the main effects a priori will be a little bit less influential than the intercept. And likewise, the two-factor interactions will be a little bit less likely to be active than the main effects. So we have the sort of hierarchy of effects as we increase interactions. You know, this is something nice that we would expect in terms of a lot of experiments. And we can sort of encode this in terms of the part. Nothing too new here, but this is just. Part again, nothing too new here, but this is just you know modeling decisions, okay. Okay, and in terms of posterior sampling, of course, we can uh tackle this with a lot of very standard samplers in the literature. One approach is the data augmentation algorithm in Albert and Chip. Again, you can use your favorite MCMC sampler. I think you're special here. But the idea is that we get these sort of posterior samples, beta one, beta p, which corresponds to the regression coefficients that we have in this model. Okay, uh, and this is sampled from the posterior of beta condition. Sampled from the posterior of beta conditional on data. Again, this is just the model part, nothing too speech. Okay, great. So we have the model. But then the real question, and I think the key thing that we thought about here is how do we then use this dependency? We have this Bayesian model, which leverages this low interaction model to model dependencies between the rewards of each other. Now, we can use the posterior samples we have here to learn the dependencies. Here to learn the dependencies. That's all nice and good. But how do we then use the dependencies to tackle the two design problems? Namely, to select the active arms set A, and then to allocate the runs over the active arms within each period. So let's take a look at the first question here, which is selecting the active arms set A. Now, keep in mind this particular image, right? Where we start off with a particular set of arms, but then we have to decide: look, Set of arms, but then we have to decide: look, do we throw away some arms from the current set, right? And then which arms do we sort of add in? Which websites do we sort of add in, which are promising? Okay, and this is related to an interesting problem in the multi-arm banded literature. It's really a subset of the best arm identification problem in MABs, where we're trying to identify which decision is the best, which arm is the best. Which arm is the best? Okay, it's related to the best K identification problem where we're trying to identify the best K arms. Okay, so the way that we're going to do it is we're going to actually extend an approach that's been used in terms of the best K identification problem. So we're going to let this thing right here, this quantity, be the posterior distribution of the reward probabilities for RMI. And notice that in this case, this can be easily computed, right? Is this can be easily computed right for each of these arms by using the posterior samples that we have beta one to beta p. Okay, we just fit it within uh take these samples here and then plug it within this particular model to get the probabilities pretty simple here. Okay, but now we have a posterior distribution for the reward probabilities. And the way we're going to select the next uh the next k arms is we're going to then choose the arms which have uh um A quantile, right? So the q of i minus uh one minus alpha here is the quantile function. We're going to choose um uh the arms which have the highest one minus alpha quantile right in terms of the conversion probabilities. Okay, so this ensures that the arms that we choose has with high probability, okay, high conversion probabilities, which we then sort of deem as promising. Okay, and this is again very much. Okay, and this is again very much related, and there's very nice theoretical properties in terms of the best arm identification literature, where they do this sort of quantile-based approach for selecting promising arms. Any questions on that? As a practical matter, I can imagine that companies will require, they want to have a minimum income stream. They want to have a minimum income stream for sure, and then they can explore above that level. That's a different type of constraint than the ones you're considering, but I can imagine that they would want to have sort of an expected return that is at least X. Yeah, yeah. So you can always modify this thing right here, such that you want to maybe even constrain this in a certain way that you know, you want to maximize the return, right? Maximize the probability, subject to. Probability subject to you know, maybe the lower quantile, right? Maybe with the one percent quantile being above a certain threshold as well. Yes, yeah, so you can always modify this procedure. It's very, very flexible. Thank you. Great. Okay, so again, yeah, just to sort of emphasize the point again, this again would leverage the learned dependencies to select promising arms because without this sort of dependency structure, right, we would. Dependency structure, right? We would have no way of identifying which arms, which websites here would be promising, right? And we have to perform experiments on them, which we don't have the resources to do. Okay, great. So the next one is to use this dependency structure to select, now, to allocate the end runs, given that we already selected the active arms. So if we take a look here, right? We already selected this set A, and we're going to decide, you know, with the 5,000 runs, how we're going to allocate these samples. How are we going to allocate these samples? So, if you can recall, you know, Thompson sampling can be sort of reduced down into two steps, you know, that procedure that I talked about previously. The first one, right, and keep in mind these are for independent arms, right? The original Thompson sampling algorithm. The first step is that we want to sample one sample from the posterior distribution of the reward probabilities. And then, using these samples, we then compare which arm gives Arm gives the largest sampled probability, and we then experiment. We play that particular arm. We can actually use a very straightforward extension of this for the dependent arm setting that we have here. If you think about it, the posterior distribution is going to be more complicated, but we still have access to this. So one step is to first sample, one sample, right, from the posterior distribution of the reward probabilities. Now, this is going to be indexed by the betas. Now, this is going to be indexed by the betas, which we're going to get from MCMC. We can do that in a similar fashion. And then we can similarly choose the ARMI, which yields the greatest sample probabilities. Very simple extension of this. But again, the key idea here is that we're going to leverage the learn dependencies between the arms to then allocate experimental runs, the promising arms. And we can then exploit ARMs without necessarily observing data on them, which we are great savings if we have millions of websites. We have millions of websites. Okay. So that's really the algorithm here. I mean, I put this algorithm here just for completeness' sake, but I would imagine that the idea should be pretty clear. So we can sort of go through this procedure within each time period for each experiment and then do the same type of procedure. Okay. So let's take a look at how this procedure performs. And again, you know, for Forms. And again, you know, for website optimization, we would like to try this thing on real data, but a lot of this is proprietary. So we're going to try to mimic this with a hopefully realistic website optimization experiment. So we're going to consider here 10 factors, each with two levels. And each of these factors can include the font family, font size, background color, et cetera. And we're going to deal with this ARM budget constraint of 16 websites. 16 websites. So, this K is the constraint that we have. And so, within each of these periods, we're only allowed to experiment on 16 websites at a time. So, this requires us to select the arms carefully and then this perform the allocation carefully as well. And in the industry, this sort of arm switching procedure oftentimes happens at the end of the day, because that's when the traffic is at the lowest and there's going to be little cost in terms of switching the arms, okay? Switching the arms. Okay. And so, what we're going to do here is we're going to run 500 experiments, 500 traffic, right? That's your daily traffic within each of these time periods, which is going to be a day. And we're going to run this thing for five days and see how this sort of sequential procedure performs. So that's the setup here. Let's take a look at some of the benchmarks. So I apologize in this case. I'll just explain this in terms of the ideas, but this is taken from the paper. Ideas, but this is taken through the paper. So, we're going to explore three different industry benchmarks. And this is really something that David sort of did a lot of work on in terms of trying to see what people actually did in the industry to deal with these constraints. So, this is some of the stuff that he's thought a little bit about in Uber in terms of discussions with colleagues and stuff like that. So, the first baseline is something very simple, which is, you know, what they would do is maybe just select an What they would do is maybe just select an active arm set at the start of this whole experimental period. You can select it in a smart way, maybe using a fractional factorial design, right? But then they're going to stick with this active arm set A throughout the whole period, okay? And not do any arm switching. So this is a pretty sort of perhaps naive baseline, but this is a baseline that's used in the industry. The second one is, so we start off with this sort of So we start off with this sort of active arm set, but we start discarding arms which are sub-optimal according to some sort of expected posterior probability. But the new arms are added randomly. So we're not going to leverage this correlation structure in terms of picking potential arms. So again, this is a test on how well our low interaction model actually picks out correlations and exploits correlations in terms of picking potential good arms. Arms. The third benchmark here is going to be very similar to the second one, except in this case, the top K arms will be selected based on the expected posterior probabilities from Tonton Santa. Again, there's no exploitation of underlying dependency structure in terms of selecting the arms. So, again, this is a test on how well our procedure does in leveraging our Bayesian model for decision-making. Simon, I have a question about. Simon, I have a question about that. It seems to me that what one ought to do is to select arms in such a way that you systematically explore all of the 10 factors that might have binary levels. And it looks to me as though that's not really happening here. Yeah, and that's exactly it. Because I think in the industry, the idea, even in the literature for multi-armed bandits, right? I mean, this notion of correlated bandits is still quite new. Of correlated bandits is still quite new in the literature. It's surprising to us, right, when we sort of took a look at the literature. I mean, there's probably a paper in 2020 which explored this idea of correlations between bandits, but a lot of the literature on this really explores this, not necessarily from a Bayesian perspective, right? They think about, I have some correlation, and you know, and then they do some sort of bound on this thing and then prove something, but it's not really clear what their motto is. And so I agree with you, you know. I agree with you. The benchmarks oftentimes in the literature, even in the literature itself, they don't really exploit the correlation structures to pick out potentially interesting arms. And that's really what our approach tries to do. So that's where these sort of benchmarks, you know, it's what's done in the literature, but it's not the best approach for sure. Thank you. Yeah. Good. Okay. So let's take a look at how this sort of performs here. And so what we have plotted out here. Uh, so what we have plotted out here, or maybe I'll talk a little bit about how the conversion data is generated. So, we used a model from Lee et al., right, where they had uh, you know, this is not the actual data, this is proprietary, but they fit a model and then they report the model on the conversion date. And we use this thing to simulate what the conversion data might look like. Okay, so what we plotted out here is time on the x-axis, and the y-axis is the cumulative regret, which we're trying to minimize. Okay, and so in this case. Okay, uh, and so in this case, we're comparing the T-Sec approach, our approach, with the three benchmarks. And we see that the start, right, the cumulative regret is actually quite similar between all of the methods. But as time progresses, we see that our approach right here does yield noticeably lower regret compared to some of these existing methods. And I mean, the reason behind this is intuitive, right? Our method takes a bit of time to sort of learn what interaction structure is. Learn what interaction structure is. But once it learns interaction structure, it can then exploit, right? Maybe after time 100 to pick the arms and also allocate traffic to promising arms, in which case we can notice the sort of noticeably decrease in terms of the regrets. So this again highlights the sort of exploration period at the start. But once it starts exploiting in terms of both the arm selection and allocation, our method seems to do a little bit better. Method seems to do a little bit better. So, we did a couple of other experiments here just in changing the total number of websites as well, right? So, right now, what we're trying, you know, the total number of websites is 2 to the 10, right? Which is still a lot, but we want to see what happens if we sort of vary this from 2 to the 8 to 2 to the 15. Okay, and what we plotted out here is the cumulative regret at the end of the experimental period. So, as you can see here, So, as you can see here, the previous results here will be 2 to the 10, which is this column right here. But as we sort of increase the number of arms, we see that at least we still get an improvement over many of the existing methods. And one can even argue that, and this is sort of more just visually looking at this, right? But the slopes for this line is a little bit less than maybe some of the benchmarks. So we do see a little bit of improvement as the dimensionality of our arm space increases. Of our arm space increases as the number of websites increases compared to some of the benchmarks. Um, I'll talk about one more figure and then I can sort of uh have some questions on this. So what we then try to do is then increase the arm budget constraint as well, right? Maybe in this case, you know, you hire another programmer and they can sort of maintain two times as many websites or four times as many websites, right? So we increase the constraint from 16 to 32. From 16 to 32 and 64. Okay. And then we plot the cumulative regret over time as well. So, as we can see here, right, as we sort of relax this constraint, some of these existing benchmarks actually can be quite comparable, right? In this case, benchmark one can be quite comparable to our approach, right? And in some sense, that sort of makes sense because our approach is probably likely to yield the greatest improvement when there is really. Improvement when there is really this sort of very tight budget, right, on the number of arms that you can test. So the tighter the budget typically are, the better our method performs. Any questions on that? Hi, Simon. So you mentioned that switching, the best time to switch is like the end of the day when the traffic is lowest. So can I consider Is lowest. So, can I consider the time in your like regret plot that as like the number of days? So, like your TSEC algorithm start to show its benefit after like around two months? Yeah, I'd say this is a so again, we're performing 50 experiments here. I think this should be 2,500, but I'll take a look at that. But I'll take a look at that, but yeah. So, in this case, this will be day one, day two, day three, day four, day five. So, as in terms of you know, maybe like the second date, we start seeing improvements. Okay, I see, I see. Of course, this depends on the problem. This depends on how many interactions are there, right? This depends on a lot of different things, but that's that's sort of sort of what we get from this. Okay, one can view this as a portfolio problem in which you do not pay. Problem in which you do not pay a price for switching from one investment to another. So, if I have a little bit of time, maybe five minutes, I can talk a little bit about that portfolio optimization, which is a little bit more of a complex problem. So, again, portfolio optimization, I think many of you know who have portfolios, you know, that optimizing this can be a challenge. And there are many established strategies. And I think the key idea, this is in conversation with a lot of people. This is in conversation with Levon, right? Is that under different market conditions and for different industries, right? Any of these strategies that we have in the literature can perform well or can perform very poorly. And so the question is then, you know, within a given time period, right, maybe a short time period where things don't change too much. How do we develop like an optimization procedure for optimizing a certain industry-specific investment strategy from Investment strategy from the market data. So I'll talk a little bit about how we sort of massage this thing into a multi-arm band with constraints. But I'll first talk a little bit quickly about different investment strategies. So these are pretty standard strategies. One is the equally weighted, where we place equal weights over all of our assets. Buy hold, which is we take the previous time period and we have the same proportions. Sold all means you sell everything. So the weights are zero. Value weighted. The weights are zero. Value weighted means you're weighing this by the returns in the previous period. And this is probably the most sophisticated one, which is the mean variance portfolio. This is published by Markowitz, who won a Nobel Prize for this particular allocation. But this is trying to balance the mean of the portfolio with the variance of its risk. So there's five different investment strategies that we're going to consider here. But then I'm going to start blowing things up in terms of the number of things that we can test. In terms of the number of things that we can test. So, Lavon sort of dug up a little bit on these sort of exchange traded funds, which she thought would be a good sort of test case for our method. So, we took the five largest industry here, tech, healthcare, finance, gold, real estate. And for each of these industries, I then think about, well, I could have five possible investment strategies within each industry. Now, this is already quite naive in terms of the financial perspective, right? Because, of course, Perspective, right? Because of course we can apply these sort of different approaches, but we can even apply them within each of these assets itself, right? So this is already a small subset of number of investment combinations that we do. But even with this, you can imagine the arm space that I have will be five to the five, which is 3,125, right? Possible investment combinations, which again are many arms, many portfolio combinations. Arms, right? Many portfolio combinations that we can potentially test within a given time period. So that's a challenge. But now talking a little bit, and so that goes back to David's point a little bit earlier, which is the armed budget constraints. Now, one issue to do with these portfolios is that if you want to experiment on a certain portfolio, you have to buy it. So transaction costs are an issue because transaction costs are charged. Because transaction costs are charged for each of the portfolios which are held. Okay. And so, you know, LaVon thought about this a little bit and she thought, well, if you have a certain cost budget for the transaction costs, that puts a constraint on the number of portfolios or the number of arms that you can play within a given time period. So this again sort of puts things back into the context of this MAP with cost constraints. Because within each of our investment period, we can only observe, we can only test out, right? We can only test out on a limited number of portfolios. Okay, so this is going to be the last cloud, I promise. I'm taking a look at time to make sure I don't go over here. What we tested out here was we used a time period of 20. Now, this is using ETF data that Levant scraped, starting from January 2010, right, and continuing on for, I think, 24 months. The rebalancing time will be. The rebalancing time will be taken monthly. Okay, so you're going to be allowed switching of the portfolios every single month. And we're going to allow, let's say in this case, we're going to allow a total number of portfolios, 75 portfolios within each time period, which is our ARM budget constraint. So we did a test on how TSEC performs in comparison to some of these other methods, which do not combine the sort of industry-specific. Industry-specific strategy, right? And also different combinations of these strategies as well. And so what we plotted out here is the months as well as the cumulative wealth in terms of our portfolio. So this will be a way of measuring the reward from our decisions. So as we can see here, the red one here is our method. And it's definitely not as volatile as the other ones, and perhaps not as good as the other ones. As good as the other ones, maybe up until the first year, right? We can see that it starts sort of learning something, I guess, right? After maybe 10 months, and this starts increasing in terms of cumulative wealth. And at the end, right, maybe at the end of the second year, we start getting noticeably higher cumulative wealth than some of the other methods, which then took a downturn. Okay, so in some sense, and this makes sense from a financial perspective, because And this makes sense from a financial perspective because the more we diversify in terms of our investment strategies over different industries, the more stable our returns are going to be. But we can also see that it's also increasing in terms of the cumulative wealth in a very steady fashion. So the multi-armed bandit, our procedure here, is able to sort of leverage correlations between different investment strategies and between different industries and use that, right? Right, to sort of maximize the cumulative wealth that we can get. And so that's one sort of application. Of course, we can try this thing for different years and it might sort of give varying results. But we tried this thing for maybe like, I think, the data was from 2010 to 2013. And we do see this sort of behavior, okay? Where at the start, we might not do as well as some of these existing methods. But as time progresses, right? And as time progresses, right, we start seeing noticeably improved performance. Okay, so I'll quickly end off with a conclusion slide and then we can open up for questions. So experimental constraints arise in many online experimentation problems, not only in computational advertising, but also in finance, et cetera, right? And so what our approach here does is it leverages a pretty simple model, but it uses this model to exploit correlations for ARM selection and run allocation. For ARM selection and run allocations under ARM budget constraints. And we see some promising results for website optimization and portfolio optimization as well. Okay. So thanks. If you have any questions, feel free to ask them. This is wonderful. I am really impressed, Simon. I think two slides back when you were comparing the cumulative wealth, it looked as though several of the methods were eventually doing about as well. Eventually, doing about as well as your method. It would be nicer if your method substantially outperformed the others in terms of accumulated wealth. Yeah, yeah. So I think one thing, and LaVon sort of thought about this a little bit, right? But I think one issue with like, especially financial data, I mean, I think website optimization is a much cleaner example in the sense that there's less noise and there's less model misspecification, right? Less model misspecification, right? In finance data, it could be very well be the case that things are non-stationary over time. It could very well be the case that this model is not a good fit, right? But I think one of the things that you see here is that, look, while our approach, at least at the end of the 24 period, 24-month period, I mean, it is better, but not noticeably better than some of the other ones in terms of Kim Lithwell. But I think one thing that would be of interest. Of interest investors, right? In addition to the return, is how stable this is. Yes. And so that's one of the things. And this is really related, this notion of mixing performance is related to diversification. And so what we're trying to do is to try and find an optimal diversification, which achieves this. And at least in this case, it does achieve what we want from diversification while achieving good returns as well. But it will be nice, but it's a difficult problem. Be nice, uh, but it's a difficult problem, yeah. It's certainly the safest, and that's that's a plus. That's right, yeah, yeah. Oh, Simon, it's interesting that you brought like industry into the problem, like not industry, like different market, a different industry market. So it makes the things more complicated. I heard in Maori armband, but I'm not sure about how the details work, there is a contextual arm bandit. Is that something can fit in this framework as well? That something can fit in this framework as well? Yeah, of course, there's a lot of bandits, and I sort of should have mentioned some of these at the start here. There's a lot of, well, I say there's a lot of there's some literature recently. Contextual bandits, I think, are a paper from 2013 or something like that. And there's been a couple of papers very recently on correlated bandits, right? So contextual bandits will be taking into account contextual information for modeling this particular band. So you can sort of think about what we're doing here as a way of leveraging contextual bandits. Contextual bandits for choosing the active arms and then to allocate resources among the active arms. So it's applying contextual bandits for the problem of armed budget constraints. Okay, does that sort of make sense? But the context here is specifically in terms of the factors that we use, right? Which is in terms of the sort of factorial form. Thank you. This is great. Thank you. I know Sammy is about to start speaking, but one more question. I would imagine that for an advertising company, you would learn early on that, say, one particular font times New Robin is better than Calibri or something like that. And so you should be learning across all these experiments rather than redoing everything for the Obama. Everything for the Obama ad, for the Toyota campaign, for each thing separately. Yeah, I agree. There could be some sort of transfer learning between different campaigns as well. And that's why a Bayesian framework, I think, would be really nice here, right? Because this allows you to say, look, if I have some sort of posterior, right, on what these betas should be, then I can easily transfer this in a different context without running the experiments again. Experiments again. That's why, you know, Bayesian sense here is not only good for leveraging Thompson's sampling, it's also good for transferring between different contexts as well. Simon, thank you so much. This was terrific and excellent. Really impressed, as always. Thank you.