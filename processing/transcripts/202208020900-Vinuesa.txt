Hello. Good morning, everybody. Welcome to the day two of our Berks Workshop, M2 The Core 5, which is mathematics of multi-scale, multi-physics, and multi-phase models. It's a great pleasure to introduce Professor Ricardo Vinuesa. He's going to be talking about modeling and controlling turbulent flows through deep learning. Looking for an exciting day on. Looking for an exciting day on machine learning and AI. Very good. Thank you very much for the introduction and for the kind invitation to this very nice event. I'm going to be talking about deep learning applications to fluid mechanics. My work is funded mainly by the ERC, but also by other organizations, and I'm based at FLOW at KTH in Stockholm. Flow at KTH in Stockholm. And one of the starting motivations for this work comes from this slide from Airbus, where we can look at the distribution for a commercial aircraft of the drag from different components of the airplane. And we know that the wings contribute by around 50% to the total drag of aircraft. So that's an important chunk. And within the wing, one can actually look at the friction drag, which is 50% of that, and the lift-induced drag, which is connected with the The use drug, which is connected with the win-tip vortices, that's around 40% of that. So, here with our simulations and our data-driven methods, we're going to try to look at this 90% of the drug contribution that comes from wings in aircraft. And we're going to try to leverage some of the capabilities in terms of high-performance computing. And to do that, we use the spectra encoding for 1000 developed by Paul Fisher and others in Argonne, which is basically combining the good. Basically, combining the good aspects of finite elements, which is the geometrical flexibility, and also some of the good aspects of high-order spectral methods in terms of convergence and accuracy of the simulations for high-Reynos member Turboen flows. So in that sense, we can actually distribute these finite elements within the domain to have quite some geometrical complexity, not extreme geometrical complexity, but we can do wings and shapes like that with a DNS sort of. That with a DNS sort of accuracy that we can try to use to understand our turbine features in these flow cases. So, we have been working on using this sort of method. And I'm going to show you a visualization of a DNS that we carried out some years ago now. This is a NACA 4412 win section, a Reynolds number of 400,000, an angle of attack of five degrees. And what you can see is that we can resolve the vertical structures, right? This is the Lambda 2 criterion. Right, this is the lambda two criterion with quite some level of accuracy. We can see how the suction side is subjected to a quite strong arbor special gradient, which really increases the boundary layer thickness, really enhances the energetic level of those structures. And you can actually see more or less the behavior. We have quite some backflow, 30% of backflow. So we don't have mean separation, but we have important flow structures that arise when you have such a strong R-special gradient, and we analyze them from various perspectives. them from various perspectives and what we can also see is that on the pressure side we have a subtle favorable pressure gradient that is accelerating the boundary layer so obviously we'll have a much thinner boundary layer here we will have a smaller and less energetic structures closer to the wall and what we will see is that the two boundary layers merge in a shear layer forming the typical von Parman type of shedding downstream of the airfoil and this is the type of level of detail that we can actually try to achieve That we can actually try to achieve to understand turbulence in these complex flow cases where many phenomena that are not present in channels, pipes, zero-process boundary layers are taking place. So we can actually try to push the limit even more and go to more extreme conditions. What I'm showing you here is a number of what we call well-resolved LES, which is basically a DNS with a bit of a coarser resolution and a stabilization. So we resolve 90% of the true kinetic energy. The trouble magnetic energy, uh, at the true magnetic dissipation, basically. And what we have here, you can see all these databases in the general heat of fluid flow 2018, is a range of Reynolds numbers from 100,000 up to a million. So you can actually see how the vertical structures are changing. We can have a much broader range of scales when we increase the Reynolds number. We actually see much more scale, well, disparity and interesting mechanisms when we increase the Reynolds number. When we create the Reynolds number, and we are currently running these cases at 1.5.6 million, so that's even higher Reynolds number with higher angles of attack. So, we actually have mean separation, so we can try to understand a little bit the physics when you actually have this massive separation close to the training. One thing that is important to highlight is that all these simulations were carried out in conformal measures. So, what we have is basically, and these are the characteristics of the 1 million simulation. Characteristics of the one million simulation. This is a quite high-ranos number case. This is actually larger than what we can do in the wind tunnel at the moment. So that's quite an interesting scenario. And this is 2 billion grid points, but our latest simulations are on the order of 10-15 billion grid points. So these are very, very large cases. But it has one limitation. As you can see here, these elements exhibit a very, very high aspiration because of the limitation that you have in terms of the boundary layer resolution and the conformant mesh. Resolution and the conformal mesh that we need to impose in these simulations. But more recently, there have been developments for non-conformal meshing and adaptive mesh refinement, which allow us to have much more flexibility in terms of domain size, in terms of being able to resolve the far-filled properly, and in terms of also automatically refining where we need the resolution, which, as you can see, is the boundary layers close to the airfoil. And then, of course, as we move farther away than the resolution. Of course, as we move farther away, then the resolution gets progressively coarser. What is interesting in this case is that we have a wing with a wing tip. So, this is not an airfoil anymore, this is actually a three-dimensional wing where we can understand the physics of the wing tip vortex that forms as a result of the pressure difference of both the pressure and section sites. We can see how on these blue planes here, the meso solution is able to track the evolution of the vortex. So, actually, we are able to adapt the meso solution wherever we need it. Wherever we need it, and we can very well assess the impact of the wintry vortex on the features of turbulence in a wide range of cases that we can list here. What I would like to highlight is that for all these cases, we have not only the mean flow and fluctuations, we have all the terms of the budgets, spectra, time series, instantaneous flow fields for developing reduced order models. So, some of the tools that I will be discussing from machine learning are also being used to try to understand the coupling of the wintry vortex. To understand the coupling of the wintry vortex and the wake, and try to see the impact on the aerodynamic quantities of these interesting flow cases. So, there's actually quite some work to be done and quite some availability of data that we can have now with these wings with wing tips. So, this is all about high-performance computing, very large-scale simulations. Obviously, that gives us a lot of data that we can actually try to exploit from the perspective of data-driven methods. And data-driven methods have been used in the context. Methods have been used in the context of fluid mechanics for a while, perhaps starting with the paper 20 years ago by Vilana and Kumosakos, where they were able to model the near-world region of truvel and channels via neural networks, and they were able to really assess a bit more the pros and cons of these methods compared to traditional model decompositions. But there's some more recent work on ACS modeling, development of in-flow and boundary conditions, being able to develop better flow control strategies or better range modeling. Strategies or better range modeling and also the context of PINS physics informed neural networks, which is helping in certain contexts and certain niche problems. Following on this summary, we have a review paper which Scott Paul is now in Nature Computational Science together with Steve Branton, who will be presenting later today, on how machine learning can help CFD in general. So we can actually see that on the one hand, we can use machine learning to accelerate DNS, so we can have more efficient DNS. So, we can have more efficient DNS configurations. We can improve the modeling of both LES and RANS, and we can, in principle, develop more robust reduced order models, leveraging some of the deep learning techniques that can go beyond what we can get with traditional linear decompositions, while trying to retain some of the physical properties of those decompositions. And I'll be talking a bit more about it later. So, here there is kind of like a summary of those potential applications, keeping in mind. Potential applications, keeping in mind that machine learning is not about to replace CFD or any traditional method. This is just about using these techniques to complement certain problems where we can try to improve the efficiency, basically. So there's quite some potential, and of course, I'm happy to discuss more about it. Talking also about the potential of machine learning, we need to be aware of the fact that deep learning methods are not interpretable, which means that you cannot really see how the input and the output are related. And you just get an input and then you get an output. Then you get an output. And what we want to do also is to open the black box. So, to really see what are the equations that one can derive from those relationships that are established by the neural network. And in that sense, there's quite some work on interpretability through symbolic regression. So there's some work from Princeton. We have an article also in Nature Machine Intelligence where we try to really understand how those inputs and outputs are connected. And that's essential if we want to really use these methods, not only for CFD, but for any. Thoughts not only for CFD, but for any application that is going to have an impact on everyone's lives. So, interpretability is going to be one of the biggest things so we can actually try to understand that a bit better. Okay, so what I'm going to do is that I'm going to discuss several examples of how we use machine learning for fluid mechanics. Okay, so here is a little list of examples. I will go through them. I'm going to start with non-intrusive sensing in a turbulent channel via CNNs, convolutional neural networks. And non-intrusive sensing is essentially being able Is essentially being able to describe what happens on the flow without directly measuring on the flow. Because when you put a probe on the flow, you're disturbing the fluctuations and you're introducing additional uncertainty. So the whole idea is that we're going to measure at the wall. So we're going to measure the wall shear stress components and the wall pressure. And we're going to use the wall information to predict what happens above the wall at different off-wall locations. The motivation of that being that if you know better what is happening on the front. Know better what is happening on the flow, you can design better closed loop control strategies for turbulence reduction or for optimization of aerodynamic quantities. So, that's a little bit the strategy that we will have here. So basically using the information at the blue plane to predict what happens at the yellow plane. And we're going to predict the three velocity fluctuations above the wall using convolutional neural networks. This was published in JFM last year, so all the details are here. But of course, you can also discuss if you have any questions. Also, discuss if you have any questions. So, what we do is that we create a DNS database to be able to establish training data that connects the wall information and information above the wall. We chose an open channel because the dynamics of the larger scales is going to be slightly simpler, although in practice the results are very similar to a closed channel. And we take Additaus of 180 and 550, so we can have quite a range of Reynolds numbers, and we use a Fourier THF code to produce that DNS database. That DNS database. So essentially, because turbulence inherently contains spatial information, you have coherent structures and you have a lot of spatial data that you can analyze, we will not use fully connected networks because those will not be able to efficiently extract the spatial correlations in the data. And instead, we will use CNNs, convolutional neural networks, which are a very standard tool in computer vision. We will be using more advanced tools later on, but we will start with CNNs. And the basic principle of CNN is a very important. And the basic principle of CNN is to apply convolution filters. So you have your image here that is your input. You have a kernel, which is a 2D filter, in this case, 3x3. And by applying convolution operations, what we will do is that each output point will receive information from nine points of the input, essentially. So you sweep through the image with this kernel, and then you can try to minimize the number of parameters of the network to be able to understand the spatial correlation. Understand the spatial correlations in the data. Using CNNs, what we can do is that we can establish this architecture, and this is one of the first architectures that we use. Now we have even more complicated ones. What we have is three inputs on the left, which are the two wall shear stress components and the wall pressure. Three outputs on the right, which are the three velocity fluctuations at the target plane that we want to predict. And then we have six hidden layers with the number of filters that you have on each of those layers. With the number of filters that you have on each of those layers. And essentially, each filter is going to identify one or several features on the input data. And the input data is going to be images from the Turing flow. So the first layers are going to be associated with features that are simpler, more abstract, like edges, lines, circles. And through the concatenated application of convolutions, this is going to be able to create more complicated shapes, which are the larger scales and more complex to create. Are the larger scales and more complex structures that we know? So, we're going to progressively build these more complex features from simpler ones that are identified in the first layers. And what is nice is that, of course, this hierarchical way in which the neural network is learning is resembling the hierarchical structure of turbulence. So, in principle, these type of networks are going to be suitable for these predictions. And as a side comment, now what we are doing, this has six hidden layers, but now we have much deeper networks on the order of 20. Much deeper networks on the order of 20, 30 hidden layers, but the same number of parameters. So we reduce the number of filters per layer, we make them much deeper, and in such a way we can actually exploit even more that hierarchical nature of these networks for predictions. So this is actually something that works interestingly well for this type of identification of coherent structure. And this is how it looks like, right? So we have the three inputs on the left, these are the type of spatial features I'm talking about, and the three. I'm talking about, and the three outputs on the right, which would be the U, V, and W fluctuations instantaneously at a certain wall normal location. Okay, so this is the sort of prediction that we want to make using CNNs. And what I explained to you so far is what we call the FCN network, fully convolutional network, in which we take all the wall information and we predict all the information above the wall. But there is another way to do this, and it's what we call the FCMPUD. And it's what we call the FCM POD in such a way that we take the domain, we divide it into these smaller subdomains, and we apply POD on each of those subdomains to be able to predict just the temporal coefficients of the POD. So we don't really have to predict the whole signal, the spatial modes are fixed. What we do is that we predict the temporal information. And that's what we will call the FCM POD method. And well, this will have pros and cons depending on the type of prediction that we want to make in this case. Okay? Okay. So these are instantaneous visualizations of the velocity fluctuations at y plus 15 in the first column, y plus 100 on the second column. Our reference is the DNS, obviously, which is the third row. This is what we want to compare with. And the first row is a prediction based on what is called the extended POD. The extended POD is formally equivalent to a linear stochastic estimation. So the linear stochastic estimation is starting. Estimation is establishing a linear relationship between the wall information and the information above the wall. And of course, we know that turbulence is a multi-scale phenomenon in which the inter-scale interactions are, on the one hand, linear, and that's the linear superposition, but there's also the non-linear modulation, which is a multi-frequency interaction. And of course, the extended POD cannot capture the non-linearity because it's just a linear model. What we see as a result is that the strict Is that the streaks predicted by the extended POD are much smoother, they're filtered, and the intensities of the fluctuations are attenuated compared to the reference. So, extended POD or LSE are not an ideal method to make these predictions, although it can be used to learn something about the linear dynamics of turbulence, basically. Now, on the last row, we have the FCN, which is the case where we predict the whole plane directly. And we can see that close to the wall at web plus 15, we have perfect agreement. 15, we have perfect agreement, and the flow. I will show you some of the metrics later on and the statistics. The flow is predicted extremely well, very close to the wall. And the FCM POD, when we use the POD step later on, it has good agreement far away from the wall. Of course, when we're trying to predict larger scales far away from the wall, the data is less correlated. So there's going to be inherently less information to make the prediction. But it turns out that it's better to use the FCMPOD file. To use the FCM POD far away from the wall, where most of the physics is dominated by fewer or larger scales that you can encapsulate efficiently into the POD modes, rather than trying to predict the whole plane using the FCN directly. So close to the wall, the FCN method works very well. Far away from the wall, the FCN POD works reasonably well. That's a little bit the message here. And this is corroborated when we look at the turbulence statistics. So these are the U, V, and W fluctuations. U, V, and W fluctuations. These are the RMS profiles. The black line is the DNS. The triangles are the FCM, the dots are the FCM POD, and the squares are the linear method, which is the extended POD. What we can see is that the near wall peak of U prime, we get less than 1% error with the FCM method, which is actually pretty good. We get worse as we go far away from the wall. And far away from the wall, the FCM POD gets better. So it can overtake the FCM POD. Better. So it can overtake the FCM prediction at y plus 100, where we can have 26% error, which in principle is still quite good. I mean, if we want to do large-scale control, and there's, of course, some debate in the community about reactive control in terms of smaller scales or larger scales, if we want to tackle the larger scales, we will be able to know where the streets are based on this accuracy of this prediction. And clearly, these methods are better than the extended POD for all the world normal allocations, basically. Wall for all the wall normal locations, basically. So, we are able to obtain very good predictions close to the wall and reasonably good predictions a bit far away from the wall based on these non-linear methods. And this is something that we can try to exploit when we know these CNNs, right? And I mentioned a little bit about the hierarchical way in which these networks are trained and are learning. The thing here is that we can exploit something called transfer learning. And transfer learning is a method in which you can Is a method in which you can take what a network learned in one case and use it in another one that is not too similar or not too different, right? So it's kind of a different application, essentially. And what we're going to do with this transfer learning is that we're going to take a network that was trained at y plus 15, and we want to develop a new network to predict at y plus 15, so farther away from the wall. And to do that, we will transfer the weights of the three first layers. The weights of the three first layers. So those will be frozen. And we will only retrain the three last layers. The hypothesis behind this is that, of course, the smaller features are going to be more similar, closer and far away from the wall, whereas the larger ones are going to be different. So we probably only need to train the last part of the network that takes care of those more precise and complicated features far away from the wall. And what we can see with this transfer learning approach is that we can obtain the same level of accuracy. Is that we can obtain the same level of accuracy, but reducing the amount of training time by a factor of more than four. So, actually, we need much less data to make the new predictions because we're exploiting the information that we had at a lower wall normal location in this case. And this is important because we need to be able to develop deep learning models that are efficient and sustainable in such a way that you're not wasting computer time when you can actually reuse some of the things that you have already learned. And perhaps a more interesting example is A more interesting example is what happens from low to high Reynolds number. So, what happens if I take the network train at Aditau 180 and now I try to use what I learned there for Aditau 550? So, I'm starting the training of the Aditau 550 case with the weights of the Aditau 180. So, I'm not starting from random weights, I'm just transferring those weights and then continuing the learning. And what we can see is that compared with the case where the random weights are used for The case where the random weights are used for initialization, we can reduce the amount of sharing data by a factor of four again, which is then very cool because, of course, the simulation at Aritau 550 is much more expensive than the one at 180, right? So we can do most of our training with data that we can generate on a laptop. And then the larger scale data that one needs is a bit of a bigger HPC center for that, then you can minimize that amount of data and keeping the same level of accuracy, essentially. So this is a quite important aspect, because we can again maximize the We can again maximize the well, not only the saving in the CPU time and the GPU time of training, but also in terms of the data generation, which is quite important also for these machine learning applications. So, transfer learning is a very important tool that we have now, and we should really exploit it in the context of fluid mechanics. Okay. Now, everything that I've told you so far was based on the So far, it was based on the fact that we know all the information at the wall, right? Because I was reducing the NS data. But in an experiment, in practice, you don't have that. You have fewer measurements and you need to account for that reduced resolution. So what we consider is a different architecture, also very widely used in computer vision, called generative adversarial networks. This stands for GANS, basically. That's the acronym for it. So in GANS, what we have is two different networks working together. Networks working together. One network, the generator, that is producing high-resolution images based on low-resolution ones, trying to follow the statistical properties of the reference data set. So the generator is trying to produce very statistically realistic images from the low-resolution ones. And the discriminator has the job of differentiating if a high-resolution image is real or it was produced by the generator. And then these two networks obviously have antagonic roles. They're trained together using game. Rules, they're trained together using game theory, and they progressively get much better at their own tasks. So, the result is a generator that is able to produce very realistic high-resolution images from low-resolution ones. And we want to downsample basically the DNS data to mimic the resolution that you will have in an experiment. And essentially, the way that this network is set up is like this: so, we have the low-resolution inputs here. In a first step, we produce a high-resolution reconstruction of the wall data. Resolution reconstruction of the wall data using the generator of the guns. And in a second step, we use the high-resolution data at the wall to produce high-resolution data above the wall. So that's a little bit idea. So we can go from coarse inputs to high-resolution velocity fluctuations above the wall. And what I'm showing you at the bottom, all these results are published in Physics of Fluids as well, so you can find all the details of these architectures. What you can see in the bottom is the instantaneous streamwise Wall-Share stress. Streamwise wall-share stress. So, this is the DNS. On the first row, I'm showing you down sample versions of that data. So, I'm making it coarser and coarser and coarser. So, what you can see here is something similar to what you could get in an experiment. So, basically a bunch of point measurements where you're losing all the spatial information that you had in the original data. And on the second row, I'm reconstructing the world stress stress field. So, what we can see, first of all, is that, of course, when I First of all, is that of course when I'm increasing the downsampling, the predictions get worse. But even in the case with a lot of downsampling, which is this one, if we compare this panel with that panel over here, what we can see is that the streaks are where they should be. They are with the right length, the right spacing. So the results look physical. I can actually locate the streaks and I can actually try to do something with them for prediction and then for control. So in principle, this is encouraging. And as we Encouraging, and as we move farther away from the wall, what one can do, what I'm showing you here on the first row is the DNS. First row is web plus 15, 30, 50, and 100. And as I move down, I'm increasing the downsampling from 4, 8, and 16. So these are results above the wall. Obviously, this is the easiest prediction because it has the lowest downsampling and it's very close to the input. And actually, the results are pretty good. And this is the most challenging prediction, which we are comparing with this one. Prediction, which we are comparing with this one over here, what we can see, of course, is that there is quite a strong attenuation. We lose most of the details of the non-linear interactions. However, the stricts are very well located again. So, in principle, if we're tackling large-scale control, this could be something that could be helpful for us. So, this is a direction that can be, in principle, promising in the context of predictions for non-intrusive sensing, even with the coarser solutions that you will have in experiments. Will have in experiments. So, this is something that we can try to leverage later on. And in the next example, what I'm going to do is instead of predictions from the wall, to above the wall, is I'm going to try to use the information from far away from the wall to determine information closer to the wall that could be maybe helpful for wall models. We're going to see how helpful it can be and what directions can be interesting. But in principle, there is also some potential using CNNs for this sort of prediction. For this sort of prediction. So essentially, we try to establish in the same data set, in the turbulent open channel data set, such a relationship between outer layer variables. And what we do is that we try to mimic what Nituno-Jemeneth did some years ago, where they just took the overlap region and they took a plane far away from the wall and rescale it to another plane closer to the wall inside the overlap layer, so they could actually exploit the subsimilarity of structures there. There's some similarity of structures there. We try to do something similar but with CNNs. So, in our case, the input is at y plus 100, and these are the u, v, and w instantaneous fluctuations. The second row is the target, which is the data at y plus 50, so a bit closer to the wall, but still in the Odola player. And the third row is the prediction from the CNN. So, we can actually see that the predictions in principle look reasonable. I mean, the structures are there. Reasonable, I mean, the structures are there. The V fluctuations that are a bit attenuated, but still, the most relevant features are also there. Turns out that it's very difficult to make this prediction compared to the previous one. There are two main differences. The first one is that when you're using predictions from the wall to above the wall, all the small scale information is already in your input. But now we are using large-scale information to try to reconstruct missing scales that were not in that input data. So that's something challenging, that's something that we don't. Something challenging, that's something that we don't have in principle. And the other thing is that our receptive field, which is basically the little window that I showed before, when you predict from the wall to above the wall, the receptive field is occupying most of the scale that you have as an input. But when you have very large vortices, your receptive field is a very small part of it. So you don't really exploit much of the spatial nature of that large scale, essentially. So this is why this prediction is more difficult, but still. Prediction is more difficult, but still we get 20% error in the reconstruction, which is okay. You can see in the power spectral densities at y plus 50 that the orange line, which is the prediction, misses the small scales and there is some displacement of the large scale energy. But in principle, the results look more or less reasonable. Now, here, there's two things, right? One is, do I have a good prediction or not? And another question is, what is required for a good prediction? Question: Is what is required for a good world model? And in principle, even a perfect prediction of this would not be what you need for a good world model. You probably would need something else. There is some work using reinforcement learning to go in that direction by Petros, and there are some other approaches. But as an exercise of prediction, this is also interesting, and perhaps some of these methods could be leveraged for world model development at some point. Two possible solutions to the problems that I mentioned before are using guns to reconstruct the missing small scales, which could The missing small scales, which could be in principle working. And the other solution for the small receptive field is to play with the max pooling in such a way that you can collapse the large scales into fewer data points and then use the receptive field in a more effective way to apply the convolutions. So that's also something that we are looking at, and that perhaps could be something promising as well. So again, this is just ongoing work for inspiration for whole model development, but this could be something that in principle could be used at some point. In principle, it could be used at some point. Now, another example is what I mentioned before about reduced order models. So, is it possible to use deep learning to try to develop better or at least to establish new strategies for reduced order model development? And in this case, we're going to use auto-encoders. I will talk to them about them in a second. What we want to do is to embed non-linear What we want to do is to embed non-linearity, because of course POD, for example, is linear, while retaining the nice properties of POD, namely the orthogonality and the optimality, right? So we want modes that retain the good physical properties that POD has, but while being more compact, leveraging the non-linearity that we have in the autoencoders in such a way that we can have nonlinear modes which are interpretable. So we are actually able to get some physical insight from them. And this work has been. From them, and this work has been published this year in expert systems with applications. This is an applied machine learning journal that is quite interesting. So, you can see all the details here. There's a lot of work on these ideas applied on a cylinder flow at Reynolds number 100, right? Which is a good first step, but essentially everything works in that case. We try to go to more complicated flow cases. So, we use this simplified ARBA environment, these are just two obstacles in a screening. Obstacles in a streaming flow configuration. We have more cases with more buildings and more arrangements, but we just took the simplest one to start with. This is a very well-resolved LES with NEC 5000. So you can see the level of detail that we achieve in this urban environment simulations. We take the cross-section, the horizontal plane at the middle of the height of the obstacle, just as a reference to prove the feasibility of this approach. And we have over a thousand instantaneous snapshots to be able to. Instantaneous snapshots to be able to train our autoencoders to develop deep learning models. So we are going to present different types of autoencoders. And for the ones who are not familiar with them, autoencoders are a deep learning model in which you take an input data, in this case an image, an instantaneous flow field from the database, and what you want to do is progressively reduce its dimensions. And this is a CNN auto-encoder, so we will exploit the convolution. So, we will exploit the convolution operations that I mentioned before in the because we have, of course, a spatial information that we want to learn from. And we apply convolutions with a reduced dimension until we shrink the original data into a latent space, which is the bottleneck here, which contains the reduced representation of the data set. So that's going to be our reduced order model is going to live in the latent space. And this first part is what we call encoder. The second part is what we call the decoder. And the decoder will progressively. Decover and the decoder will progressively increase the dimension of the data until we recover the original snapshot, essentially. So, this model is trained to recover the original data in such a way that we can reduce its dimension as much as possible in this latent space. And essentially, what we can do is that we can learn about these nonlinear modes from the latent variables that we have in this latent space, essentially. Okay, so this is what Okay, so this is what we call the CNN autoencoder. It has been used in many problems already, but this is not optimal, nor orthogonal. So this is in principle something that doesn't have the properties that we want to look at. And one way to be able to establish the optimality, so meaning that the modes are ranked in the sending energetic contribution to the reconstruction, is to use what is called the hierarchical autoencoders, no? The hierarchical autoencoders, which were proposed by Foucami and others. And in this hierarchical approach, what we do is that we start with an autoencoder of dimension one in the latent space. So we first obtain one latent variable. Then we create another autoencoder with dimension two in the latent space. And we fix the first latent variable. So we train to get a second one. And we do this recursively in such a way that we're getting more and more modes with. Getting more and more modes with progressively less contribution towards the reconstruction. And this is interesting because then that gives us a non-linear model decomposition that is orthogonal, no, but it's optimal. So it still doesn't have the orthogonality property because we are not doing anything about it, but we are having an optimal representation. We have less and less contribution towards the reconstruction, which is good. What we propose in our article is an approach to be able to obtain orthonal non- Able to obtain orthogonal nonlinear modes. And to do that, we're going to use what is called the variational autoencoders with a penalty. So it's going to be the beta variation of autoencoders. And this is how they work. Let's assume that the P of X is the distribution of the regional data and P of R is the distribution of the data in the latent space where the producer model lives, basically. What I want to do is to maximize the marginal likelihood, which is an approximation of the original data given the parameters of the network. Data given the parameters of the network. And that's a little bit the constraint that we're going to impose. Keeping in mind that variational autoencoders are based on distributions in the latent space. And basically, we will have a decoder, which is a generative model. The decoder will take us from the latent space to the original data. And an encoder, which is a recognition model, that takes us from the original data to the data in the latent space. Okay, so these are the two probabilistic distributions that we. Realistic distributions that we want to play with in our beta variation autoencoder. So essentially, we assume that the encoder is a Gaussian distribution. This is the starting point of variation autoencoders. And we're going to add some noise, some Gaussian noise, to be able to sample from this latent space, essentially. And what we can do, what can be shown, is that we can define a loss function like this with a reconstruction term. With a reconstruction term and a penalty, beta. This penalty is the one that is going to promote the orthogonality through learning statistically independent variables in the latent space. So the key is that we want to minimize the distance between the distribution in the latent space and the product of its marginals in such a way that we keep a small dimension of the latent space and orthogonal, or in other words, statistically independent variables in that latent space. We want to obtain a disentangled We want to obtain a disentangled and parsimonious latent space, which is convenient from a perspective of storage, but also from the perspective of being able to interpret those modes later on. Okay. So this would be the decoder, again, going from the latent space to the original data. This would be the encoder, which takes us from the original data to the latent space with the Gaussian distribution that I mentioned plus the noise that we're introducing for sampling from it. From it. So, this is basically a way to introduce stochasticity into that latent space of our autoencoder. So, some results. Let's look at a reconstruction of a snapshot, and then I will show you some statistical quantities. This is the reference data from the DNS, or almost DNS, that we consider. If we reconstruct that with five POD modes, we obtain this. So, basically, quite a filtered version of the Quite a filtered version of the regional data with only 32% of the energy, right? That's expected, of course. You need several hundreds of modes from the POD reconstruction to go to 90 or 95% of the energy. And these are the various auto-encoder approaches, which all of them are about 90% of the reconstruction with only five modes, right? So, in principle, from that perspective, the auto-encoders are able to encapsulate and compress the data much better than POD in principle. You can actually see. You can actually see the details of the flow field and this large region of high velocity. This is basically reconstructed by all the auto-encoder models, but the POD doesn't really capture it so well. So there is some features that actually can be more efficiently reconstructed with the non-linearity of the auto-encoder. But there are some statistical properties that are important also in the context of the auto-encoders. For example, we can look at the For example, we can look at the determinant of the cross-correlation matrix that gives us the orthogonality. Of course, POD is orthogonal, 100%, but the beta variation of the encoder is 99.2% orthogonal, right? So in principle, we are able to learn statistically independent variables in the latent space, which produce orthogonal modes later on. The other two auto-encoder approaches are below 90% orthogonality, which means that, of course, a particular physical phenomenon will be mapped onto many of these modes. Onto many of these modes, and it will be very difficult to interpret them. So it's not so convenient, not from the perspective of being able to get something physically sound and relevant. Another aspect that is important is the penalization factor beta. So of course, the larger the value of beta, that you can see here on the horizontal axis, that will give us more orthogonality, which is the red line, but also less accuracy in the reconstruction, which is the blue line. So of course, this is a penalty. Because this is a penalization, a regularization in the training. If you're trying to solve two problems at once, you will not be as good at solving those two problems together. But in principle, one can get a quite good balance with this larger value of beta, where you get 99% orthogonality while getting a reasonable reconstruction of the data. And of course, the larger the value of beta, the more zero variables that you will have. So the more parsimonious the model will be. Parsimonious the model will become as well. This is another important aspect to consider when we're trying to interpret how to develop such a redisorder model using autoencoders. And there is an interesting approach. I mean, we can actually impose the optimality as well in the beta variation of autoencoders through recursively reconstructing the modes. So basically, take one mode and make the other zero, and we can try to. Other zero, and we can try to build an algorithm as you can see here to be able to rank them in terms of energy reconstruction. So we can actually have orthogonal and optimal modes with the beta variation of encoder. Perhaps the most interesting is the interpretability. I mean, why did we do all of this in the first place? So here are the POD modes. That's the second column. What you can see is that, of course, we can observe quite some prominent shedding in these POD modes. The two auto-encoded approaches that The two auto-encoder approaches that do not involve the orthogonality, they're just showing us a bunch of large and small-scale fluctuations, basically, a bunch of entangled turbulent contributions there. But the beta variation of encoder, which is the first column, this one gives us a very good representation of the large-scale shading that is represented by the TOD. So you see that the modes look similar, but you have much more high-frequency information associated with those turbulent fluctuations that would be Turbulent fluctuations that would be present in very high-order POD modes, you can actually encapsulate them into the first modes in such a way that with five or ten auto-encoder modes, you can get most of the energy. So it's an interesting approach in terms of the compactness of the real server model. So if we want to use this for control applications or for trying to modify and optimize the system, then Autoencoders can be an approach. I mean, this is something that we're developing and is starting, but And it's starting, but we believe that there could be some promise in following up on this route over here. So, interpretable nonlinear modes through beta variational outer curves. Okay. Good. And the last application is flow control. So, we also want to see how we can use data-driven techniques, in this case, deep reinforcement learning, to be able to control the flow. To be able to control the flow in certain configurations. The advantage of deep enforcement learning being that it could, in principle, provide us alternative strategies to flow control, to apply flow control, beyond what is classically known in established theories. And in deep reinforcement learning, what you have is something like this. You have an agent that interacts with an environment, and the environment will be the flow solver, basically. The flow environment will have a The flow environment will have a certain state, and the environment, the agent interacts with the environment through actions. So, the idea is that those actions will change the flow state and will produce a certain quantity that we'll call reward, right? A certain property of the flow that we want to maximize or minimize. And the goal of the PPO, the policy development strategy, is to define a policy that, given the state of the flow, gives us the Of the flow gives us the optimal action to be able to achieve the reward that we're trying to tackle. Research. This we presented last year in the APS. We're having a couple of articles now coming on it. And the whole point is that we started with a first example where the reward is to minimize the separation length of a separated flow, a 2D flow, but we want to use this for opposition control into roulettes, right? So we can actually try to devise and discover novel control. And discover mobile control strategies in cases that are more challenging. So, basically, in our first case, we consider a 2D channel, so a very simple flow. We impose suction and blowing at the top boundary in such a way that we induce a 2D separation bubble, which is this one over here. We want to actuate upstream of the bubble and want to minimize the separation length through the reinforcement learning-based approach. And the idea, this is how the flow looks like, basically. Flow looks like basically. So the green region here is the negative velocity, basically. So this is the recirculation bubble. These dots that you can see here are the probes. So these are the states that I can observe about my system. The actuation will be in this square region that you see here. And my reward is this arrow. So I want to make the arrow shorter. I want to reduce the separation length in this flow. And what I'm training with the DRL is to, I have a body force. The body force is. I have a body force. The body force is mimicking the effect of blowing and suction in a so you could have in a channel basically. And I'm trying to learn the amplitude as a function of time with a certain spatial distribution to be able to maximize the action of the control. And I'm going to show you here on the left a little visualization of what the control is doing. You will see that we have basically two dominant frequencies. One is associated with the spacing of. One is associated with the spacing of these wave packets that are being sent. Another frequency is associated with the length of the bubble. So this DRL is actually learning a control that is more sophisticated than periodic forcing. In periodic forcing, you would take the dominant frequency and force at that one. Here you get a more complete, richer spectrum. And what we can see by comparing some of the bubble reduction results is that we can outperform classical methods, for example, with periodic force. So for a set amplitude of the So for a set amplitude of the control, predict forcing would give me 20% bubble reduction, whereas the PPO, the different forcement learning-based approach, would give us 30% bubble reduction, which in principle is promising for this simple flow case. We're currently applying this to a turbulent channel flow where we're doing opposition control in such a way that we are basically sensing the flow and trying to oppose the fluctuations. In classical opposition control, you just have an empirical constant to oppose the fluctuations. Empirical constant to oppose the fluctuations in the near-world region or in the outer region, and here we're trying also to use DRL to devise a more sophisticated approach to that control. And the results are quite promising as well in the context of turbulence. Later on, of course, our goal would be to optimize the aerodynamic efficiency on a wing, so we can not only reduce the drag, but perhaps optimize the lift over drag. That's kind of like the goal with these methods that we're trying to develop here. And just to sum up, And just to summarize a little bit what I have presented, we use CNNs for reconstruction in turbulent flows. We showed that it's very, it's really feasible to reconstruct with very high accuracy at WebPlus 15. And also later on in the outer region, GANs are very interesting for super resolution and for reconstructing missing scales, which can be another interesting approach in the context of world model development. We use autoencoders to produce non-linear produce-order models, basically. Produce other models basically that can help us to produce these drums in a more efficient and compact way. And we showed the feasibility of using deep reinforcement learning for basically for flow control in simpler cases, but going towards more complicated cases. And I would like to thank you very much for your attention. We have plenty of time for questions, so I'm happy to take them. Thank you. All right, we'll walk around with this. Questions? All right. Thank you. Very, very interesting work. I have a lot of questions. I don't know which one to get started. I guess I will start with maybe the auto-encoder. I think that's a very interesting one. Encoder. I think that's a very interesting one. I'm an experimentalist, so the POD is sometimes a sort of a lot of a relatively standard method. This seems to be capture a lot of the sort of the high frequency things the POD failed to do. I just wonder, you know, you mentioned the interpretability and when I see the data, I want to interpret it. I think you made a great point that the beta V autoencoder. The autoencoder method sort of captures a lot of large-scale structures. But how do I interpret the high-frequency? For example, in Mo1, you see a lot of high-frequency things. Are those like small-scale structures being produced by those large-scale structures? Or how do I make sense of that, I guess? Yeah, thank you for the question. That's not necessarily the case. They're not necessarily produced by. They're not necessarily produced by those large scales. They're just higher frequencies, higher turbulent fluctuations that are consistent with that mode in a statistical sense, but that's purely statistical. We don't know from a physical point of view, they're not necessarily together. And that's also something that perhaps one could try to dig into a bit further and trying to explore this latent space to learn some of those dependencies. But in principle, at the moment, it's just a statistical reconstruction. As a statistical reconstruction, so those higher frequency components are in a way coupled with the large-scale ones to produce a better reconstruction. That's the only thing that is imposing the laws. We cannot learn anything about the production mechanisms from this. All right, the second question is related to the super resolution you mentioned using GANS. That's a part I may not fully understood. Fully understood the method here. So, how does the method capture the small scale without? Does the method, does it already have an understanding of Navistok's equation? How does it know the small scale details? Yeah, let's have a question. So basically, what GANS does is what we're showing this schematic here, is a supervised learning approach. So we have high-resolution data that we filter, and basically we're trying to teach a net. And basically, we're trying to teach a network to go from the low-resolution data to the high-resolution one. So the Navier stokes are encoded in a way in the statistical properties of this original high-resolution data set. So the GANS is learning to go from here to there in a supervised manner. It doesn't know about the neural stores. What it knows is that it needs to follow those statistical properties of that data set. There are unsupervised approaches with guns, like cyclic guns, for example, that in a way, For example, that in a way it can help us to bring in some physics into this reconstruction. This is not what is done here, but it could be also something promising to go in that direction. So does that mean that, for example, if I have a DNS data set, I can train against, and then I can use an experimental data set and try to determine the results? But I guess the question is: does that mean I have to use it for the exact same configuration? Like, say, whole body turbulence has to be. Wobounded terminals have to be. I had to use a DNS from strictly the same configuration, same Reynolds number, or I can slightly change my training data set for a different Reynolds number, different configuration. Yeah, so what you're mentioning is something that we're currently doing. So we have a 3D obstacle, an urban environment, and then we have the DNS, then we have the PIV measurements, which are 2D planes, basically. And we can reconstruct from the 2D planes the 3D data from the DNS. And that works pretty well, actually. Pretty well, actually, using GANS, but a bit of a more sophisticated GANS approach. But in that case, it is the same, the same flow configuration, basically. However, if you want to go to other configurations, one could use transfer learning with a framework of continual learning in such a way that when you do transfer learning in the way that I did it, you go from 180 to 550, and then the network works much better at 550, but it doesn't work so well at 180. If you want a network that works well in all the cases, then you can use continual learning and the network. Can use continual learning, and the network will be performing very well in all the cases that it has seen. And it can interpolate across cases much better. So, one could think of this transfer learning-based approach in such a way that you could have several geometric configurations, and then the network would be able to interpolate among those. So, that's one possibility. But another possibility that could be interesting to explore is physics in four neural networks, PINs. We have some work on PINS. I didn't show it today because it gets a bit too long, but I'm happy also to. A bit too long, but I'm happy also to discuss it. And there, you can leverage some of the governing equations information in your predictions. So, that could also be maybe an approach if you want more generality to add some pins basically component into your model. Thank you. Thanks, Ricardo. Very nice, complete overview of a lot of topics. Mine is. Of topics. Mine is maybe kind of a very sort of detailed question on one item that I've noticed quite a few times, and we've run into this in the similar problem. I think in a lot of times when you're, you know, for example, this example, your prediction compared to the DNS, it looks very good. And all that's missing perhaps is a little bit of the variance. So, you know, and you showed the predictions of the RMS, you know, it's always, it seems most of the times it's just under predicted by maybe 20%, 30%, you just get. 20, 30, you just get less RMS, but everything else works very well. So, I'm curious, that seems like an easy problem to fix. All you would need is to learn a coefficient to just multiply your prediction, you know, just subtract the mean, take what you got, multiply what you got by 1.3 or something, and then the variance works as well. So, why is it, it seems very difficult somehow. We've run into the same problem to kind of learn that just rescale. Kind of learn that just rescaling factor because you have the data, you know what the truth is. Why can't we simply learn a rescaling factor? Yeah, thank you for the question. There are several points there that are interesting. And the first one is that networks tend to go, they're biased towards the mean, right? I mean, that's why the mean is very good and that's how they are trained. There's ways to, well, if you want to promote learning fluctuations, you could have a loss in such a way that you try to reproduce the Way that you try to reproduce the RMS or the spectra, so to try to impose that. Something that is also maybe interesting, we have a very recent paper on probabilistic neural networks in such a way that you can learn through stochastic weight averaging. So you can train different networks and you can really see the variance of the predictions. So you can try to learn how different networks are focusing on different parts of the space, basically, that you're trying to predict and encapsulate the information from several of them. Captured information from several of them, try to get a better reproduction of the variants. So that's a venue that could be promising as well. This probabilistic neural network approach. Here, I mean, of course, all our results, we're including also the uncertainty of those predictions. So we don't only show one model, we train many of them, and then we try to look at the statistical assessments. And then one last thought about learning the difference, basically. Something that we have done here, we haven't published it, but we're working on it, is to use an is to use a network that first does the extended POD. So it basically learns what we know about the physics, which is linear and is obvious. And then the network, so the non-linearity, is focused on the remainder, right? So you're training to learn what is missing from the linear contribution that you have. And that, of course, works much better because you're able to focus all your parameters on that, the non-linear parameters, on the non-linear part of the problem. So that's also promising as well. And something that we're also working on is on. We're also working on is on recursively. So you are predicting with a network at well plus 15, you use that prediction to predict at 30, then at 50, instead of trying to directly predict at 100, that recursive prediction also seems to work quite well in the spirit of what you're mentioning, of being able to, when you still have good variance predictions, use that prediction to really try to go to the more difficult ones. So those are different directions that could be in principle promising. We are looking at them. But yeah, I mean, it's not straightforward. It's not straightforward. I agree. Ricardo, excellent talk. You had a question? Yeah, sure, sure. So, yeah, thanks. This is Samir Khanna from BP. So, very fascinating talk, very interesting. So, I'm just thinking about practical applications in the industry of all this fascinating work, right? So, one of the questions I have is mostly on. Is mostly on turbulence models. So, you've done all this machine learning, and are there some more universal, better turbulence models in some Python libraries that we can use for our industrial applications? And you can extend that to multi-phase flows, too, better drag models, and things that we can use in the industry instead of the K epsilon models that we have been using. Thank you for the question. Yeah, I mean, machine learning is helping to develop better models in LES and in runs, as we discussed in our review paper. In my taste, it's still a little bit too much coefficient fitting. So you can fit more coefficients with more data, basically. There is some work going into neural operators, for example, or in super-resolution that could be helpful in that direction. There is also Direction. There is also a framework that I think could be interesting, and it's that of PINS. So I didn't talk about it because of lack of time. This physics infer neural networks. So we have a paper in Physics of Fluids where what we do is to use the PINS framework to solve the Reins equations. So we don't need any model for the Reynolds stresses. What we need is boundary conditions for those Reynolds stresses. So one thing that we do and we show is that one can, I can show you some more challenging cases. So actually in the This. So, actually, in the this one, so the periodic hill, if you have RANS data or measurements of the flow that you want to solve in certain locations, inside the domain, you can use the PINS framework to get very, very good reconstruction at a cost that is similar to that of a normal runs, but with much better accuracy. So, in this case, the periodic heel, which is a flow with separation and reattachment, we can capture very well the separation bubble. Basically, the mean has less than 3% error, the RANS stresses have around 20%. Error, the Reynolds stresses have around 20% error in turbulent wings like this one. We get very good results in the Reynolds stresses as well. So, in principle, this is a direction that I think could be promising, but it requires that you have some information on the boundary. So, either some measurements or some coarser estimates that can guide your pins into the solution inside the domain. But in industry, we have some projects with industry where this is actually quite. This is actually quite adopted, being adopted quite a bit. So I think that it could be promising as well. But I would not restrict only to this application. I think there are frameworks that allows us to get symbolic regression models for runs that are interpretable and that are more general than the classical coefficient fiddling, basically. So I believe there are some applications that could be benefiting from machine learning, and we are trying to explore them. Thank you for the question. All right. There's like just one minute. I was going to ask just a comment on when you are trying to guess from the outer region, the inner part, while this is a good exercise. Do you have some comments about causality of this particular process? Because it's the inner boundary layer that really drives the other one. Now we are. One. Now we are trying to create a model that is looking at the outside the boundary layer, trying to predict something inside. You have some comments on that? Yeah, that's a very good question. I mean, of course, there is an additional challenge. Here we're inside the overlap layer. So it's going to be mostly the same structures that are being predicted. But if you want to extend that prediction to even closer to the wall, you need to shift the larger scales because, of course, they're inclined and the advection is taken care of. Advection is taking care of some of that causality that you're mentioning. You need to shift the large scales in such a way that the predictions are causal. So there is some stuff to be done there and even incorporating several snapshots as an input in such a way that you can understand the temporal dependencies there to again bring in the causality. I don't talk about it today, but those are things that are part of the model that one should also take care of. Yeah. Good question. We're going to be having a lot of discussion. So let's thank Ricardo one more time. Uh one more time.