Very much for the introduction, and I hope you hear me well. If not, just let me know. But I would like also to thank the organizers of this workshop for this really exciting program. And yeah, the very interesting talk so far. So the work I'm going to present today is joint work with my postdoc at Chaust, Jordan Richards, and this has to do with extreme quanta regression. Extreme quanta regression. And so here the methodology will actually rely on using neural networks for performing this extreme quanta regression. So this is the outline of my talk. I will start with a brief applied motivation, then introduce this framework of extreme quantile regression. Then we will actually present this framework that we propose. This framework that we propose based on partially interpretable neural networks, which we call the pin network. Then I will also introduce a new point process model that we develop for this task. And then we will go to the application and discuss this application on US wildfire risk assessment. And then I will conclude at the end. So as you know, wildfires cause significant deaths and damages across the world. Death and damages across the world. And especially recent years have seen devastating wildfires, particularly in the US, with hundreds of deaths and millions of acres of destroyed lands. And in fact, wildfires are especially vicious in the sense that they are both aggravated by climate change because temperature increases and increases the risk of wildfires. But wildfires themselves also contribute to aggravating climate change itself because Climate change itself because they actually contributed to about two gigatons of carbon emissions in 2021 alone. And a high proportion of that actually comes from the US. So to mitigate risk and to be able to identify high risk areas, we actually need also to understand what are the drivers of risk. And for that, a natural way to quantify risk is to look at quantiles of burnt area. At quantiles of burnt area in a specific spatial temporal grid box. And so this is why we actually turn to extreme quantile regression. So here, again, the application we have in mind is wildfires. And so the response that we will try to model is aggregated burnt area for a specific spatial temporal grid box. And we will be interested really in the upper tails because that's where the most dangerous wildfires actually occur. Wildfires actually occur. So, as you can imagine, the typical quantiles of interest will be perhaps larger than previously observed. So, we need to do quantile extrapolation. And for that task, non-parametric quantile regression will very likely perform very poorly. And so that's why we instead turn to parametric quantile regression using models that are asymptotically justified based on extreme value theory. Value theory, and then we will actually describe the parameters of these models in terms of covariates, and these covariates can be pretty high-dimensional. So, there are essentially three different classics in extreme value statistics, which have been also mentioned by Anthony in his overview talk. So, the GV distribution for block maxima, the GPD, so generalized Pareto distribution for peaks over thresholds, and the point process framework. And the point process framework that kind of unifies these two approaches. So, here in this work, we will focus mainly on the point process approach. Although, of course, this framework also extends to the other distributions for extremes. So, extreme quantile regression is, of course, not new. I mean, other people have done it in the past. And the framework here is Is again that usually we assume a certain parametric model for the data, and this model actually depends on some parameters. And classical approaches in the literature have mainly focused on specifying theta, the parameter, in terms of linear or additive functions. So additive in terms of splines. And so here theta will denote the parameter, which is a vector, and x will be x. Which is a vector, and x will denote the different predictors or coverage that we have, which line R D, and D, the number of predictors might be very large. And so, what we do is to assume that theta depends on X. Now, the problem with linear models, by definition, is that they are not able to capture non-linear structures. And so, they might, as you can imagine, perform very poorly in complex problems, which is the case in our application. Instead, you can turn to Instead, you can turn to additive models based on splines. And this actually goes back to this very popular and very well-cited paper by Valerie Chavez-DeMuller and Anthony Davison, the GRSS C 2005 paper, if I'm correct, where they propose to actually generalize additive models for non-stationary extremes. But the problem is that with these additive models, again, well, Well, they can capture nonlinear relationships, but they cannot capture interruptions by definition because they are additive. And so, again, they might actually scale very poorly to high dimensions. And here in our case, we actually consider 30 predictors, which is quite a lot. So, to deal with these issues, we propose using deep learning based on neural networks, artificial neural networks, because these models can capture complex structures. So it Complex structures, so in particular, interactions in the predictors, they scale well to high dimensions, and they also facilitate high predictive accuracy. Now, usually, I mean, statisticians don't really completely like neural networks because they are kind of a black box, and you can, I mean, it's difficult to make sense of the results, it's difficult to interpret the results. Results. It's difficult to interpret the results. And this is something that, as statisticians, we really care a lot about. And in our context of Wi-Fi risk assessment, it's really important to actually be able to interpret the results of the model because we actually want to identify some key drivers and to understand their effects. So what we propose is actually a novel framework based on partially interpreted. Based on partially interpretable neural networks. And so, what we do here is, again, so we assume that the response value sum distribution could be an extreme value distribution or something else. It doesn't really matter. Again, we have the parameter depending on predictors. And now what we do is that we actually split the predictor set X into two complementary subsets, which comprise a part of interpreted or interpretable predictors and another. Predictors and another part of non-interpreted or non-interpretable predictors. So the interpretable part is comprised of coverage that we really care about, that we want to understand their effect. Okay, that we want to understand how to, I mean, their effect on wildfire risk. And the non-interpretable part is comprised of all the other predictors. All the other predictors that we don't really care about, but we still want to use them to boost the model's predictive accuracy. And so the model will actually look like this. So the ith parameter, theta i, will be a function, I mean, a potentially non-linear function. So this h i is a link function. And then inside that link function, we have an intercept, and we have a function of the linear. And we have a function of the linear or the sorry, a function of the interpretable predictors plus another function of the non-interpretable predictors. Now, for the interpretable part, we'll actually model these cover it either linearly or additively in terms of splines, so we can further express that formulation in this way. And the non-interpretable predictors here, all the others, will actually feed a neural network. And again, so. And again, so we will not really interpret that part. We will only interpret the first part, but we still use these coverts that we don't interpret in the model in order to boost its predictive accuracy. So the way we actually fit that model is by minimizing a loss function, which is here taken as a penalized log likelihood. And we exploit variants of stochastic gradient descent using the R-interface. Stochastic gradient descent using the R interface to Keras TensorFlow. Now, let me go back to the beginning and let me introduce some basics on extreme value theory. So, as you probably all know by now, so if you have a sequence of IID random variables y1 up to yn, and if you assume that you can find sequences a n and b n that can renormalize the block maximum in this way. Normalize the block maximum in this way such that the limit is non-degenerate, then this limiting distribution has to be the generalized extreme value distribution. So, this is the motivation for using the GEV distribution for block maxima in practice. And as has been explained several times during this workshop, this GV distribution depends on three parameters, location, scale, and shape. And in particular, the shape parameter is very important, determines the type of distribution that you get, either fresh a type. Distribution that you get, either Frechet type with heavy tails when Ïˆ is positive, or a Gumbel distribution with light tails when xi is equal to zero, tends to zero, and the reverse variable distribution when xi is negative with a bounded upper tail. Now, in our case with wildfires, we will actually assume that psi is non-negative. And so in that case, it turns out that the upper bound is infinity and the lower bound is finite. So it's equal to mu minus sigma of x. It's equal to mu minus sigma over xi. And this is a problem when training neural networks. And I will come back to this in a bit. So if this result for blocked maxima holds, so if this limiting G V distribution holds, then you can also look at this two-dimensional or this sequence of two-dimensional point processes where you rescaled the times of the observations. The times of the observations and the observations themselves, exactly in the same way as you renormalize maxima. And it turns out that this point process converges to a Poisson point process on regions of this form for any threshold U that is bigger than the Laurent point. And this limiting point process has a mean measure that can be written in this way. So you see here that it depends, in fact, on the, or it can be written in terms of this limiting G V distribution G. Limiting GV distribution G. And you can write down the intensity, and from that result, you can also write down the point process likelihood, which has this form. So in practice, what you have to do typically is to fix a high threshold U, can be typically a high quantile. And then if you were to use maximum likelihood inference, you would then just estimate mu. You would then just estimate mu sigma psi by just maximizing this likelihood function in terms of mu sigma psi over the space, the parameter space. So here, what we do is to actually also choose, I mean, we actually rescale the intensity by this number, n y, which is the number of years of observation, so that mu sigma psi can be interpreted as as if you were actually fitting the G V distribution to yearly maximum. Distribution to yearly maximum. Now, in our context of extreme contact regression, of course, potentially all these parameters, mu sigma and psi, as well as the threshold u may actually depend on coverheads, potentially a large number of coverheads. And so we want to model all of them potentially within our pin framework. As I mentioned before, we assume that the shape parameters We assume that the shape parameter is non-negative. And so this means that the lower bound Z minus will actually depend on mu sigma xi. And because we model mu sigma xi in terms of predictors, in fact, the lower bound also depend on these predictors. And this is a major issue for training neural networks because as the algorithm proceeds, if you actually jump, If you actually jump, if you propose a new set of parameter values that lie outside of the parameter space, then training will fail and then everything will stop. And so this is very annoying. And this is due to the fact that the lower bound actually depends on the parameters. And there is no very obvious, robust way to actually. Robust way to actually deal with that problem, I think. So, what we did here to do this, to solve that issue, is to propose a new point process framework that relies on this so-called blended GV point process, blended GV distribution, sorry, which was proposed in this recent paper where actually the first author is Daniela Castro Camillo. Danila Castro Camillo, and I'm also a co-author, and together with Hover Group. And so, this paper is to appear in Envirometrics. So, what we did in that paper was to propose this blended GEV distribution, the BGEV, which essentially blends the heavy-tailed Frechet distribution with Xi positive with the light-tailed Gumbel distribution where Xi is equal to zero. And we do the mixing in a small region in the lower tail. Fixing in a small region in the lower tail, in a smooth way, in such a way that the density will essentially be continuous and will have first and second derivatives. But also this means that, in fact, the bulk and the upper tail will be like the Fresher distribution, but the lower tail will actually resemble the Gumbel distribution. And this means that we will now have an unbounded support with infinite lower and upper endpoints. In that paper, In that paper, we also propose a reparametrization of the GeV distribution, which we find more convenient and more easy to interpret. And so we reparametrize mu and sigma in terms of new location and spread parameters. So the new location parameter is an alpha quantile, which we take here to be the median. And the new spread parameter is a difference between two quantiles. And so in our case, we take beta to be 0.5, which means that the spread parameter will be. means that the spread parameter will be the interquartile range. And we believe that these new parameters are somewhat easier to interpret than the old mu and sigma parameters. Now, how do we actually create a new point process based on that BGV distribution? Well, the only thing we do is to replace G, the GV distribution here in this expression in the measure by the BGV distribution. We just have to make sure this is a valid measure, but this is the case. And so we can actually use. And so we can actually use it to define a new point process that we can fit to extremes. So, this figure just illustrates the difference or the similarity between the GV and the BGV distributions. So, you see the Freichet distribution in yellow here. You see the Gumbel distribution in red. And you see the resulting BGV distribution in blue. And so you can see that it exactly matches the Frischer distribution. Matches the Frischer distribution in the bulk and the upper tail, and it actually matches exactly the Gumbel distribution in the lower tail, and it does a smooth transition in between. So, there is almost no difference, but the main crucial difference is that the support now is the whole real line. Okay, so let me now describe the application. So, what we looked at is monthly burnt area for the contiguous US. For the contiguous US from 1993 to 2015, for the months of March to September. So we have in total 161 random fields, each containing 3,500 locations. So you can see how they look like on these maps. And so here, these two maps actually plot the burnt area for two different months, July 2007 and July 2012, which were especially extremes. 2012, which were especially extremes for the Western US, and they are plotted on the log scale. As predictors, we have 30 of them. So we have land cover maps from Copernicus with proportion of grid cells consisting of one of 18 types, for example, water, urban areas, grassland. So grassland is illustrated on the bottom right figure here. We also have 10 meteorological variables from the ERA5 reanalysis on land surface. Reanalysis on land surface, including temperature illustrated on the bottom left figure, wind speed components, precipitation, and other things. And we also use the mean and the standard deviation of altitude from NASA. So in total, we have 30 predictors. The models that we fit, so we have different models. We have first model for the occurrence of wildfire, so whether it's positive or zero. And then And then we have another model that describes the extremes of the burnt area given that it's positive. And for that, we also need to estimate the threshold. So we have, in fact, three models, one for the occurrence, one for the threshold, which we take to be the 80% quantile. And the last one is the burnt area given that is positive. Now, we here don't model the burnt area itself, but we model the square root. Itself, but we model the square root of the strictly positive burnt area. The reason for that is that burnt area is actually very heavy-tailed, and so by taking the square root, we make it a bit less heavy-tailed. So the training is a bit more stable. And also, it's very natural, in my opinion, to take the square root because burnt area is an area, right? So taking the square root means that we can interpret the square root of the burnt area somehow as a proxy for the diameter. Somehow, as a proxy for the diameter of the burn area. And so here we have 30 predictors, but we choose to interpret only seven of them, which include east and north wind speed components, temperature, precipitation, evaporation, proportion of urban and grassland coverage. And all the others, the 23 other predictors, we still use them in the model, as I said, and they feed the neural network. To assess the uncertainty, which is also critical. To assess the uncertainty, which is also crucial here, what we do is to use a stationary bootstrap. And so we present the results as the average over 250 bootstrap samples. In our case, we choose to fix the shape parameter over space and time because this is an approach that is often taken in statistical analysis of extremes. And we have also found that this was a good idea in our case. Also, found that this was a good idea in our case, and we find that the shape parameter was 0.35, so indeed it's quite heavy-tailed. You can imagine that the shape parameter for the burn area itself would be about twice as big. So, before discussing the results, let me also explain a few comparison studies that we did. So, this table shows you a comparison between different A comparison between different formulations for the model. So, here what we took was a very simple neural network, but densely connected neural network with only a few layers. But we compare different model formulations, either fully linear or additive or fully neural networks. And the one at the bottom here is our proposed model. And we have also other models in between. Model, and we have also other models in between. So we can see the number of parameters in each of the models. That's quite a lot, but we also have quite a lot of data. And here we report the training loss, the validation loss, the training AIC, and this is another measure of accuracy. And the one on the right-hand side here is the threshold-weighted CRPS, which is a proper scoring rule, which focuses on extreme values. So let's just have a look. So let's just have a look at the last column here, the threshold-weighted CRPS. What you can see is that the best model by far is the model that includes all covets into the neural network. So we don't interpret any of them. We use them all in the neural network. And so this is really the best across all the models that we looked at. But the second best is the partially interpretable model that we proposed. So there is a Proposed, right? So there is a gap of 63 in terms of the threshold-weighted CRPS, which arguably might be large, or I don't know if it's that large or not, but there is a gap. But at least with that model, we can interpret the parameters that we care about, the predictors that we care about. And there is still a huge gain with respect to models that are fully linear or fully additive. Now, in this different model comparison, what we did was to Comparison: What we did was to fix the model that we proposed. So, this partially linear, partially additive, and partially linear network. But we compare different architectures for the neural network. So either densely connected neural networks, so with two, three, four, and six layers at the top here, or convolutional neural networks with two, three, four, and six layers. So these CNNs are widely used and wide. And recognize architectures that can extract information, I mean, spatial information from the data. And so from this study, what we can see is that the best model is actually a CNN model with four layers in terms of the threshold-weighted CRPS. But we also see that the one with three layers actually performs quite well. It's also kind of close in terms of the threshold-weighted CRPS and has. Weighted CRPS and has the best validation loss. So, in the end, I believe what we did was to actually consider this three-layer CNN as the model from which we make inferences. So, again, so from that table first, we see a huge gain when using neural networks in the model. And from that other table, we see that we can actually gain even more if we use an appropriate architecture. Architecture based on CNNs. So, let me now briefly describe the results that we got. I don't have a lot of time for this, so I'll be quick. So, here, these are four predictors that we use linearly in the location parameter, which I recall is the median of the annual maximum distribution for the square root of the burned area, given that it's positive. And so we can see a very significant and strong. Can see a very significant and strong effect of temperature and evaporation. We don't see any effect of precipitation and urban coverage, as well as wind speed, which is a little bit counterintuitive at first. But we interpret this as, I mean, we think this is actually natural because of the the temporal scale of the analysis that that we did. So we have only monthly data, so maybe using wind speed at this temporal scale is is not really informative. Is not really informative. So these are some of the spline results again. So this is the proportion of grassland in the location parameter that we modeled in terms of splines. It's very non-linear and very significant. This is the effect of precipitation in the interquartile range of the annual maximum distribution. Again, also very non-linear. And the last one is the proportion of urban coverage, also in the spread parameter. Spread parameter. So at the end, what we get is extreme quantile maps. So we can actually combine all these models together. Again, so the model for the occurrence of wildfires together with the model for the extreme, I mean, extreme burnt area, given that it's positive. And so we can compute unconditional quantiles, which are shown here at the bottom. So the bottom maps. Bottom maps here represent the unconditional quantiles, Q quantiles, for the log of the square root of the burned area when we combine these models together for July 2007 on the left panel and September 2012 on the right panel. And you have the corresponding observations at the top. And you can see that it seems that the model is actually quite flexible and quite accurate at capturing. Accurate at capturing the patterns that we see in the data, and we can actually use it to make extrapolations beyond the observed data. So to conclude, we have here proposed a novel framework based on partially interpretable neural networks that is quite flexible for fitting these extreme value models using deep learning. And this framework combines the high predictive accuracy of neural networks with the interpretability of linear and Interpretability of linear and additive models that we like as statisticians. We also developed a novel extension of the extreme value point process model that bypasses this low bound problem. And we have seen that the model fits very well to wildfire data and reveals new insights into the drivers of extreme wildfires. Now, in terms of theory, theory is quite difficult to get when you have new networks, but you can actually check that preprint here, which is an archive where they actually do something quite similar, but for Something quite similar, but for classical quanti regression, and they have some theory about this. I believe overall there is a really wide research avenue for combining machine learning methods, which are usually very good at prediction and are computationally very efficient, together with spatial extreme value methods that have the theoretical strength and robustness of asymptotically justified models. Now, in terms of future research, I think one Future research, I think one should actually extend that framework for doing like proper causal analysis. What we have done here is really regression, but I think it would be also very interesting to look at really causal effects. So, with that, I thank you very much, and I'm happy to take any questions. Thank you, Rafael, for this nice talk. I think we can take maybe one question. Maybe one question if somebody so is there somebody on site who would like to ask a question? There are two questions here, so I'm just going to pass the microphone. Rafael, thank you for the nice talk. Just a quick question about how do you combine these splines and neural network? For the linear part, I can imagine you just I can imagine you just it's part of the architecture, but what about splines? How do you learn this model? Okay, I didn't implement it myself, but I believe everything is part of the architecture. So everything is implemented in Keras, in the R interface to Keras. And so I believe what we do here is to Yes, both the linear part and the spline-based part can actually be written in terms of these layers like you would do for neural networks. Yeah, thank you. And maybe a question about CNNs. That looked a bit surprising to me. This is usually about, you know, it's good to apply those things for time series, image. It's good to apply those things for time series images, right? When you expect that smoothing effects will have a nice performance. But here it should depend on the ordering of your covariates somehow. It should depend on the ordering of your covariates, depending on how you order your covariates, CNN. Well, because you have kind of effect from the neighboring covariates in this CNN architecture. Is that not a problem? Is that not a problem? I don't think this was a problem. I thought maybe it's somehow overfitting if you use this fully connected neural network. Maybe it's good to try with some kind of other techniques like, I don't know, dropping some edges at random or right. You mean drop out? Right. Right. Yeah, I mean, what we did here was to really take the simplest architecture that we could think about. So, this densely connected, that's the reason why we actually took that. So, just the simplest thing that we could think about. And then we used CNNs because they are known to capture this spatial structure in the data, right? So, I mean, I should ask Jordan to make sure. To make sure we don't have the issues that you mentioned, but I don't think we, I mean, yeah, I will ask him. I will ask him, but that's a good point. Okay, thank you.