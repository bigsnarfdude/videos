Let's talk about gradient descent and attention model. So, hi everyone. I will be talking about some very recent work that we just submitted about a month ago to NeurIps on gradient descent and attention models with a focus on challenges that are posed by the SoftMax function. And this is joint work with Luckland, who is here, and Hensheng and Ziqin, who are. And Hansheng and Ziqin, who are attending remotely, and with Renee, my advisor. So, in recent years, transformers have become ubiquitous in modern machine learning. I think in the past six or seven years, we've seen more architectures and variations of the original vanilla transformer than we can keep track of, with some very interesting names for some of them. And so, this figure is from last year, so it's probably already very outdated now. Probably already very outdated now. So, although they've proven to be very successful, their training dynamics remain mysterious and poorly understood. In fact, not just their training dynamics, but a lot of their properties are still very poorly understood from a theoretical perspective. And so, a lot of progress has been made in theoretically understanding other deep learning architectures, such as feed-forward networks and CNNs. And I think Renee's talk did a very good job. And I think Renee's talk did a very good job of positioning the different works and highlighting some of their strengths and weaknesses. So, I mean, some of us are probably very familiar with a lot of these works. This is just a subsample of them. But the list is very long, and most of them focus on linear networks, regular networks, very few with other non-linearities. So, although we have all of these results, Although we have all of these results for other architectures, it is believed that the lost landscape in Transformers presents some unique challenges due to the attention-based components that are not present in these other architectures. And why do we think this? Well, for one, there's this mysterious poor performance of SGD that has been observed in practice. So in this figure, we have on the left here some traditional art. The left here, some traditional architectures, Lennet 5, ResNet 18, and we see that SGD with and without momentum performs comparatively at the same level as Atom. This is from a training perspective. Obviously, I think we know that when it comes to generalization, there is a difference between the two. But for transformers, we see something very different, which is that Atom does relatively well, but SGD tends to really underperform. And in some of the work, Underperform and in some of the works, the results with SED are not even presented due to its poor performance. So, why does this happen? Some hypotheses have been proposed in prior works. So, the first one, I'm not getting the pointer to work. So, the first one was that the stochastic gradient noise is very different in language tasks for transformers. It is heavy-tailed and is non-Gaussian. Detailed and is non-Gaussian. That was soon more or less debunked. So, in this paper in 2023, it was shown that the problem persists even with full batch gradient descent training. So, the stochasticity is not the issue. We could think that maybe it's still related to the task. So mostly people have looked at language tasks. So maybe it's the data modality. But in another paper in 2024, it was shown that the problem still persists with vision transfer. Still persists with vision transformers. So, clearly, it is not stochasticity or the data modality. There were some other ideas that were proposed. For example, it has been observed that there is SGD shows vanishing gradients or imbalanced gradients. Some people have shown that, you know, on the path taken by SGD, there is a large condition number that is observed. So, that could be causing this poor performance. Others also. Performance. Others also pointed to high directional sharpness. So these mostly looked at the gradients, Hessians, along the trajectories of SCD. But an important point about all of these works is that they're mostly empirical. So there isn't really any theory explaining why this happens. So there was another flavor of results that look at the convergence analysis of gradient descent for transformers. So here people will not really look into comparing. People will not really look into compare transformers to gradient descent to other adaptive methods or explain its poor performance. They mostly focused on doing the traditional analysis of convergence of gradient descent. Most of them focused on the over-parametrized setting. And some of them, even though they did not perhaps focus on that setting, did not really provide convergence rates or explain this poor performance. And mostly focused on things like implicit bias or what transformer or what transformer. Or what transformers are learning. And so, what I'm going to be talking about, and our main contributions here, is we try to look at the dynamics of gradient descent for a very simplified one-layer softmax attention model. And we propose one possible explanation for this poor performance of gradient descent on transformers, which is the severe ill-conditioning caused by the Jacobian of the SOFMAX. And we show that the over-parametrized setting that is often Overparametrized setting that is often considered in the literature actually fails to capture this phenomenon. So, reminder on transformers and attention, I think most people are already familiar with it, but essentially this is the traditional or the original architecture in the attention is all-you need paper where we have some inputs, we add some positional encoders to them, and then the basic block has some multi-attention head operations. Multi-attention head operation, then followed by some concatenation or addition of the outputs and normalization, then we feed it to a feed-forward network, and so on. But essentially, the main or the main feature of Transformers is this attention operation that we highlight here. So, the idea is that we're looking at, we're given three different types of tokens, queries, keys, and values. And we look We look at the similarities using some, maybe dot product, for example, is a very popular one. So that's what we have here: the QK transpose. And then we apply the softmax to get some probability that sort of captures to what extent we should pay attention to a certain key. And then we multiply that by the value. So essentially, we get a convex combination of the value tokens. And the way we obtain these query key. The way we obtain these query, key, and value matrices is just through linear transformation of our inputs. So in the case of self-attention, it's the same data matrix. I'm sorry, I can't use the pointer. That would be very helpful here. But yeah, I'll just. So here we have in the self-attention case, all of the matrices would be the same, but here we have sort of cross-attention where the query. Cross-attention where the query and keys do not need to come from the same matrix. Okay, so in practice, the query tokens are actually sometimes also learned. So not just these weights, WQ, WK, WV here. So for example, this is the case when transformers are used for classification, where we add some CLS token that is tuned, or in the case of prompt tuning, which has become popular recently and that's And that's when instead of fine-tuning by training the weights, the W's, we essentially freeze those and we add some prompt that is being tuned. And it's been shown that there are some preliminary results showing that it can be more expressive than fine-tuning. But for us, the reason why we consider this is that it just gives us a simpler model to analyze. And so we're considering this model here where for some data matrix S, Data matrix S. The next slide will show the dimensions of all these quantities. We take the softmax of X multiplied by some frozen weight matrix W. And then this is query or prompt token that is learned. So all the quantities in red are the ones that are going to be learned. And then again, X and times the value vector. So another aspect is that we're assuming that we're working with vectors instead of matrices. So the V here. Instead of matrices, so the V here and P are vectors, but the matrix case should also have the same properties. Okay, so to be more precise, the task is that we are given a training set of Xi's Yi's, where Xi's are matrices of size N by D. So N is the number of tokens, and D is the dimension of each token, and the output, or the label is YI, which is just a scalar. Which is just a scalar. And the model, as I just said, is this one where P and V are both d-dimensional. W is a square matrix. So the quantities that are good to keep in mind are the dimensions of P and V and the number of data samples because we're going to be comparing those to say whether we're in the over-parameterized or under-parametrized setting. And we're considering an empirical minimization problem where, as I said, we're training P and V only. Said we're training P and V only, and we're assuming some loss that we're going to make additional assumptions later, but for now, let's just say that it's differentiable in the first argument. Typically, it'll be convex, but we'll see those assumptions in the different cases later on. And the algorithm that we're considering is gradient descent or gradient flow. So, the first part of the talk is going to be with gradient descent, and then in the second part, I'll be using gradient flow. Okay, so the first setting is going to be an over-parameterized setting where the number of data samples is smaller than the dimensions of the weights. So D is greater than capital N. And so here, in order to get results in this over-parameterized setting, there is a known recipe where essentially the ingredients are that we have some L2 loss or strongly convex loss. We make some assumptions on the initial. We make some assumptions on the initialization that allow us to get a uniform lower bound on the PL constant. That's the mu constant here, which gives us this gradient dominance property. And we show the ellipsis smoothness along the trajectories. And so, yes. Oh, no, no, this is just for a general gradient. So, in general, for gradient, the So, in general, for gradient descent, it is known that if it satisfies this PL plus lifted smoothness, then you get linear conversions, even if the loss is non-convex. And so, people try to show that for these highly non-convex losses. And so, if you have all these ingredients, the result is that we get linear convergence of the loss with a small enough step size. So, now, if we want to see what the theorem looks like in our setting. It's a lot harder to parse, but essentially, we still have the over-parametrization d greater than n. We have a bunch of quantities that we define, and our condition on the assumption sort of boils down to this mu that represents the PL constant being greater than some quantity, which allows us to relate it to the initialization. So, essentially, think of it as an assumption on the initialization that guarantees this linear convergence. Convergence. So if this condition holds, then gradient descent on the loss capital L with step size less than two over this constant L, which represents the Lipschitz constant along the trajectories, converges at a linear rate. And one important property in this type of results is that it relies on the weights being bounded. So that's something that we're going to care a lot about here. Going to care a lot about here. So, both P and V remain bounded along the trajectory. So, what are the limitations of this type of analysis? So, as I just said, the norms remain bounded, specifically the norm of P remains bounded. This means that we cannot learn sparse or approximately sparse attention maps, because, in order to do that, the norm of p must approach infinity. If you remember, we had softmax of We had softmax of some matrix times p. So, in order to get the soft max output to be approximately sparse, the norm of p has to grow. Another very important aspect is that the assumption of d being greater than n is very unrealistic. So in practice, n is the number of data samples. It's of the order of millions. D is any, we can think of d as the dimension of the tokens, which, for example, in d. The tokens, which, for example, in the original Transformer paper, it was of the order of 500. But all of the dimensions are essentially much smaller than capital N. So we need to understand settings that are beyond this over-parameterized setting, which allow us to capture this phenomenon of poor performance of gradient descent. The problem is that we lack technical tools to analyze this under-parameterized setting. This underparameterized setting. So, we're going to make some additional assumptions to simplify the problem. So, the first one is that now we're going to freeze V, and we're also going to be considering gradient flow instead of gradient descent. So, now that we've frozen V, the only part that is being learned is the vector P, the Prunk vector, and its dynamics are. And its dynamics are just minus the gradient with respect to p, which is this expression here. Sure. Yes. Yeah, this is P. This is the only part that is being trained now. Okay. So it's a sum over all the data sample. So it's a sum over all the data samples of these Ki's, where we just combine the Xi with W since both of these terms are frozen, times this J matrix, which is going to be very important, which is the Jacobian of the softmax. It has this very interesting structure. The J matrix of some vector Z is just the diagonal matrix that has Z on the diagonal minus the outer product. And we sort of combine all of, again, all of the terms that are frozen by defining this L tilde loss and just saying that we have the gradient of this L tilde that depends on the data point. So this still looks kind of hard to understand. So what else can we do? Well, we try to look. Yeah. Is there a question? Oh. Uh so here the softmax is just one vector because this is right is the different rows ah yes, so the KIs have the data matrix but Matrix, but j is a square matrix, so whatever the dimension of this vector is, it's a square matrix about each data matrix. So there are capital N of the J's. There are capital N of the J's. No, no, they're not the same. So, capital N is the number of data samples. Number of tokens, it's the dimension of XI. Right, but here we only have one row. So, as I said here, I mean, we could have p that is a matrix, in which case softmax would be applied row-wise. But to make it easier, I mean, in that case, we would get like a big block diagonal matrix with the different j's. But to make it easier, we take p that is a vector. So, the output of softmax is a vector. Right. Yeah, I mean, in experiments with self-attention, for example, even though it's a big softmax with different rows, these observations will. Different rows of these observations hold, at least in experiments. But I'm not making any claims in the theoretical part. Okay. Right, so now we look at what the dynamics of the softmax for each. So the J's correspond to the different indices of the data sample. So for each one, we have some. For each one, we have some attention vector, and the dynamics of this follow this equation. So, the nice thing about writing this is that if we look here in red, we have all the terms that depend on p only depend on it through the softmax. So, although it might look like we didn't really gain much, it'll turn out that this can be helpful. So, the annoying thing about this equation is having all of these cross terms. So, because So because now we're looking at the, initially we had the J for just the data sample I, now we're looking at the cross terms for I with all of the J's. So in order to get rid of that, we make this assumption, which is pretty strong, but these results would hold even if it's approximately true. So essentially, we assume that ki kj transpose is zero if i is different from j. If i is different from j, and it's a kth identity if otherwise. So, what this allows us to do is essentially get rid of all the cross terms and just keep this one term here. So, this is a homily made in prior work. It is a pretty strong assumption. If we assume that. Assumption: If we assume that these terms are very small, so if it's approximately true, then we expect the results to hold because all the cross terms would not be having a huge influence on the dynamics. But if not, then the dynamics would look very different. I don't think we can draw the same conclusions. Right, it is technically an orthogonal data assumption. Yes, right, after multiplying by W. Right, yeah. Okay, so yeah, so now we have a simpler equation. Equation. We just replace the softmax vectors with some x superscript j. And as long as we make sure that the initialization is in the interior of the simplex, then these dynamics are well defined because the softmax is always going to remain in the interior. And this equation guarantees that as long as you start in the interior, you remain there. So moving forward. So, moving forward, we just get rid of the superscript j just to make the notation a bit cleaner. But essentially, for each data matrix, we have the same sort of dynamics up to different l tildes here and different initializations, obviously. So, now we're just going to analyze this one equation to see what we can draw from it. So, as I said, although X is technically our Although X is technically our softmax vector, it's always going to remain in the interior of the simplex, but it can converge to a boundary of the simplex asymptotically. And that's a case that we would care about if we wanted to converge to something that is sparse. So one interesting feature of this equation is that it actually has a name. It's a replicator equation. So for those familiar with evolutionary game theory, might know some additional properties of this type of equation. Of this type of equation. Essentially, any equation where we have this same J matrix that is essentially the Jacobian of the softmax times some vector-valued function, then it's referred to as a replicator equation. And in our case, this capital F here is just this entire term. So this leads to a lot of nice interpretations that I don't have time to go into. But although there are a lot of existing results coming from this theory, they can. Coming from this theory, they cannot be applied directly because this J here is rank deficient, and so we cannot really translate those into conditions on the loss itself. But we'll still be able to get some results, and that's this theorem. So for a stationary point x star that is assumed to be in the interior of the simplex, we can show a lot of things. So the first one is that. We can show a lot of things. So, the first one is that it is a KKT point of this constrained optimization problem. So, minimizing L tilde on the unit simplex. This is not true if it's on the boundary of the simplex, so it's only something that we can show if it's an interior point. The second one is that we can linearize the system around x star. So, we get this linearization here. And then we can analyze this to get some information about the stability. information about the stability. So for instance, if the Hessian of L tilde is positive definite, then X star is linearly stable. And if we have additional information about that Hessian, meaning that we have its smallest and largest eigenvalues, then as long as the smallest eigenvalue is greater than one, we can get some lower bound on the condition number of this matrix here when we're constrained to the tangent space of the unit. To the tangent space of the unit simplex. And our dynamics, our linearized dynamics, are always going to stay in the tangent space of the unit simplex. So that's all we need to look at. And so this lower bound looks like this ratio here where we have the mu and L coming from the Hessian of L tilde. And we have the max over min squared of the ith coordinate of x star. So essentially the largest, the smallest one. Smallest one. And so, this is why we believe that this can lead to very severe ill conditioning. Because if you want to get very close to the boundary of the simplex, then we expect this number to be very, very large. And this corollary illustrates that. So, if the smallest coordinate of x star is epsilon, then we have a lower bound on the largest one because we're in the unit symbol. One because we're in the unit simplex. And so this gives us this lower bound on kappa, our condition number, meaning that if epsilon goes to zero, kappa goes to infinity. So now I'm just going to show some experiments sort of validating this on this model, showing that we indeed observe this phenomenon. So I have two figures on the right, overparametrized on the left. On the right, over-parametrized, on the left, under-parametrized. My number of samples is 100. So in the under-parametrized case, I'm considering a dimension D that is 20. So that's smaller than capital N. In the over-parametrized case, I'm considering different dimensions, 150, 400. And I'm assuming that the number of tokens is the same as D, so n is equal to D, just to avoid some random. So n is equal to d just to avoid some rank deficiency issues that could occur. So we're taking full batch gradient descent. So this is not, this shows that some of these results hold for gradient descent as well. We're using some standard initialization. And so the quantity that we saw in both the theorem and the corollary, this ratio of largest to smallest coordinate of x star is the thing that we were going to vary to some extent. We were going to vary to some extent. And because here we have this is true for we have an x star for each data point, so we're going to take an average of those ratios over all the tokens at the end of training. So we're assuming that the labels for all these models are generated using teacher models, meaning that this problem is realizable. And in the overparameterized setting, we observe indeed fast linear convergence. Fast linear convergence. But the points that we converge to, or our stationary points here, have relatively smaller ratios than the ones that we used for the teacher model. So the teacher model for both of these had a ratio of 10 to the 25. And this shows that, as we saw in the theorem, in the overparameterized case, our weight remained bounded away from the boundary. So we're not going to get. So, we're not going to get arbitrarily close to the boundary. However, in the underparameterized case, we see that there are two cases. We try a teacher model with a ratio that is small, 10, and another one with 10 to the 25. In both cases, we convert to something that is of the same order of magnitude, but we see that gradient descent does pretty well in the case of small r. In the case of small r, but it really struggles when r is very large. And so basically, this sort of confirms that the idea that the condition number or this ratio affects the conversions of gradient descent. So, some final remarks. The main takeaway of this talk is that the over-parameterized analysis. That the over-parameterized analysis may not be sufficient to capture the behavior of gradient descent that is observed in practice, and that the softmax, the Jacobian of the softmax function, plays an important role in the training dynamics and could be one possible cause of the poor performance of gradient descent. And obviously, there are still a lot of open problems here and a lot of future work that can be done when it comes to doing a global analysis of the training dynamics. Analysis of the training dynamics in the under-parameterized setting, as well as analyzing adaptive methods and other training heuristics to see how they avoid this problem. Thank you. I'm not a little bit more than a little bit of a data. Right. Yeah, I think with additional heads, it would depend on whether we can once again sort of keep the dynamics decoupled or whether the different heads will interact with each other. So if there is some strong interaction, then it might be harder to analyze. But if you decouple everything, we'd get similar results. So, this is a reason why gradient descent might be bad. But are there any intuitions for why something like Adam might be okay, why it might be able to get around these sorts of issues? Yeah, so people have several hypotheses for why this might be true. I mean, Atom is very difficult to analyze from a theoretical standpoint, but there is this idea that it does some kind of Does some kind of, it is similar to sine descent. So it's just looking at the sign of this gradient rather than the actual gradient. So that could be one reason that allows it to avoid this poor conditioning problem. But it's not really. I don't think there's a definitive answer right now. There is not a question, but the for mine is a microphone. You know, when we consider things like a convolutional neural networks, the parameter that we consider studied over parameters is like the the number of features that in this case would be In this case, it would be analyzed that it felt because it didn't occur to you because look into that and it didn't um yeah I had looked at it but didn't really spend that much on it because even this one was was very hard to tackle like we only have a local analysis in this case Analysis in this case, nothing that is global. Again, as I said, if we can decouple things, we get similar results, but otherwise, there could be different phenomena that lead to this. I mean, we're not saying that this is the only reason why gradient descent might fail, but it is one possible reason. And obviously in the more complicated problem, there are so many elements interacting. So many elements interacting. But yeah, overparametrization here is not referring to the overall number of parameters in like a very big model. So it's mainly referring to, in this case, the number of parameters that capture the dimension of the tokens compared to the number of data samples. Typically, this is similar to other types of analyses that consider very large width for like two-layer linear RELU networks. Linear ReLU networks. So it's that type of over-parametrization. With additional layers, I cannot predict what would happen.