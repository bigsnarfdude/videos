Okay, so thanks a lot. Okay, let so yesterday I stated the theorem. Let me just recall the theorem that recall theorem proved by Oliveira in person. The person myself that for all alpha less than a half, there exists C alpha and C alpha prime greater than zero, positive constants, so that for all finite reversible lazy mark of chains. lazy mark of chains. T mix is upper bounded by C alpha times th of alpha and lower bounded by C alpha prime times TH of alpha. And recall that TH of alpha was defined to be the maximum over all starting states X and sets A with measure at least alpha of the expected time to hit A, starting from X. To hit A, starting from X. Okay, so yesterday I showed you how we related added geometric mixing time to this quantity th of alpha. And I just want to emphasize, in case I didn't emphasize it yesterday, that the proof that I showed relating the geometric mixing to this TH of alpha holds for all chains and there is no reversibility assumption needed. Is no reversibility assumption needed. But then reversibility becomes important in order to relate the geometric mixing to the total variation mixing time. And I didn't explain exactly how we prove that, but I just gave some ideas using the stopping rule. And then on the website, I posted a short note explaining how one can construct a stopping time that achieves the minimum in the definition of. Achieves the minimum in the definition of this top. Okay, so the other thing I mentioned yesterday is that the reversibility assumption here is essential. And I gave a counterexample, which was the biased random mock on the cycles again. So the first thing that I want to discuss today is what happens if we remove the reversibility assumption. And of course, the theorem fails, but I would like to see. But I would like to see what else one could say. So, removing the reversibility assumption. Okay, so as we saw yesterday, let me just draw it again. Let me just draw it again. A biased random walk on the cycle with two-thirds and one-third. I said that the mixing time is of order n squared, but th of alpha is of order n for any alpha. And actually, for any point, the maximum over all, for any points x and y, the maximum over all x and y of the expectation starting from x to hit y is also further end. Y is also for the n because the walk has positive speed to the right. So the theorem fails in this case, but what could we say instead? So for any fixed point, the heating time is of order n, but now if instead of having a fixed point, we think of the point moving at the same speed as the walk, then the time to heat it is going to be of order n squared. So in this case, we could relate the mixing time. Could relate the mixing time to this new quantity where instead of asking to hit a specific fixed set, we want to hit a moving set. So let me set up some notation for that. So for alpha in 0, 1, I'm going to define a collection of sets A of alpha to be sequences of sets. So A is going to be set. Sequences of sets, so A is going to denote a sequence of sets A t, t greater than or equal to zero, with the property that all of the sets have measured at least alpha. So pi of A t is greater than or equal to alpha for all t greater than or equal to zero. So here I'm looking at sequences of sets where every set has measure greater than or equal to alpha. And I'm going to define also Tau V, by abuse of notation. So yesterday tau v was the first hitting time of A, now it's going to be the first hitting time of the sequence. So what do we mean by that? It's the first time that Xt belongs to AT or A which is equal to 80. And now I can state the theorem. So this is a theorem that we proved with Peter Winkler a few years ago. So now the assumption we don't require the chain to be reversible anymore. Anymore. So fix alpha strictly less than a half, then sorry. There exists, I'm sorry, there exist two positive constants C alpha and C alpha. alpha and C alpha prime so that for all irreducible finite Markov chains if I'm going to write now the move of alpha to be the su over all starting states Over all starting states X and all sequences of sets A in this collection A of alpha of the expectation starting from X to hit A, then C alpha times this T move of alpha is going to be upper bounded by T mix and it's going to be upper bounded by C alpha prime times the same quantity T move of alpha. So so here this let me just go over the definition of T move of alpha. Here before we were just looking at maximum over all starting states X and all sets A of measure at least alpha of the expectation of the first hitting time of A. Now instead of looking at fixed sets we are looking at sequences of sets. We are looking at sequences of sets of big size, and we are waiting for the warp to hit this sequence of sets. And because now we allow the sets to move, we can drop the reversibility assumption. So I'm not going to prove this theorem, but the proof is quite similar to what we saw yesterday. It used similar ideas. Okay, so that was the first thing. So, that was the first thing regarding the reversibility assumption in the theorem. And the second thing I want to discuss today is what happens when alpha is equal to a half. So, as I mentioned yesterday in the overview, this is going to be, I'm going to talk today about only heating times, so it's not going to involve mixing times anymore. So, for today. So for today, for the first part of today. So heating times. And this is to close the gap for alpha equal to half, but the statement that I will mention is only going to involve heating times. And then using the results that we have, we can just go back to the mixing results. I just see a question. The chain doesn't have to be lazy because if it's not lazy, then Because if it's not lazy, then the mixing time could be, so if it is periodic, then the mixing time would be infinite. But also, you can find the sequence of sets that you would never meet, you would never hit. So then there is no contradiction to the theorem. So the chain, so yesterday I said most of the time I will not be writing the assumption, but here I didn't write it because it doesn't have to be laser. Okay, so today I'm going to present this result about heating pipe. Present this result about healing pains, which is due to Simon Griffiths, Ross Kank, Roberto Oliira and Virez Patel. So everything I'll be talking from now on is going to be so. So, okay, I'm just responding to the chat. The proof doesn't go through the geometric mixing time. No, one can just do it directly for the moving targets. So, I have put a reference on the website for the paper with Peter Winkler, and you can, it's at the beginning of the paper, and it's just a one-page proof. So you can it just uses similar ideas to defining a set B that you first want to hit B and then from there go to A. And then from there go to A. Okay, so let me go back now to this result, which closes the gap for alpha equal to half, but it's also an interesting result in general. So let me see. Okay, so I recall here the definition of the age of alpha. So here. So the theorem. That I will prove. It's a very elegant proof. It says the following: let zero less than alpha less than beta less than or equal to half, then for every irreducible finite Markov chain. finite mark of chain we have that th of alpha is upper bounded by th of beta plus one over alpha minus one times th of one minus beta and all of this is upper bounded by one over alpha times th of beta So let's just look at the statement. So th of alpha, recall, let me just write it here. Recall th of alpha is the maximum over all x and a with pi of a greater than or equal to alpha of the expectation to hit A. Now we're just talking about fixed sets, forget about the moving targets. So what these theorems So, what this theorem says is that we can compare the maximum heating time of sets of size at least alpha to the maximum heating time of size of size at least beta in this way. And so, first of all, because alpha is less than beta, it's immediate that th of alpha is going to be greater than th of beta. And so this gives the opposite inequality. So, it's upper bound by th of beta, and we have to add this th of 1 minus beta. And because th. one minus beta and because th of one minus beta is greater than th of beta because of the assumption that beta is less than or equal to half we can upper bound this this sum by one over alpha times th of beta so before I jump into the proof I just want to write one remark That these two inequalities are sharp. So what this means is that for any alpha less than beta less than or equal to half, there exists a Markov chain, an irreducible finite Markov chain, for which all three terms are equal. So i for For all zero less than alpha less than beta less than or equal to half, there exists an irreducible finite mark of term for which all terms, all three terms. Are you cool? And actually, it's only a three-state Markov chain. I'm not going to describe it, but once you see the transition matrix, then it's really easy to check that indeed it satisfies equalities on both sides. And also, the condition β less than or equal to half is also sharp. is also sharp. So beta is equal to a half is a boundary case in the sense that for any beta greater than a half, there exists a class of irreducible irreducible finite Markov chains such that T alpha of sorry TH of alpha divided by TH of beta can be made arbitrarily large. And again, this is, I think it's a two-state Markov chain. So I'm not going to discuss these examples, but what I'm going to present today is the proof of this theorem. But before I do so, I just want to explain how this completes the Completes the previous theorem that I mentioned here. So if alpha is equal to half, sorry, let's take Vt equal to half, then th of alpha is going to be upper bound by one over alpha times th of a half. And so we immediately get the upper bound here that we wanted. And remember, the lower bound in this theorem was true for any value of alpha. And actually for the lower bound, we don't even need the reversibility assumption. Even need the reversibility assumption. It's true for any Markov chain, and also I want to emphasize that in this theorem, the only assumption is that the Markov chain has to be irreducible and of course a finite Markov chain. So there is no reversibility assumption in the theorem. Okay, so today I'm going to prove this theorem. First, I'm going to state a lemma. I'm going to defer the proof until the end. I'm going to defer the proof until the end of the proof of the theorem. So, this lemma is going to be used in the proof of the theorem, and the proof of the theorem will be quite short once we have it. So, I'll state the lemma, and I will explain the intuition behind it, and I'll come back to prove it afterwards. So, lemma. So, let me call this theorem one so that we can go back to it. So, lemma one. So we have an irreducible mark of chain. So same assumptions as before. So for an irreducible finite Markov chain for any AB subsets of S, S is the state space. We have a okay. First of all, let me just define two quantities. So define d plus of ab to be the maximum over all x in A of Ex tau B and T minus of A B Of AB, a minimum overall X in A of the expectation starting from X to go to B. Okay, so D plus of AB is the maximum over all starting states in A of the expect time to go to B, and D minus is the minimum over all states in A of the time to go to B. So, okay, they're not going to be empty sets. Then biova is upper bounded by d plus of AB divided by d plus of AB plus B minus Plus B minus So as I said, I'm going to prove it later, but let me just give a non-rigorous explanation of why the lemma is true. So So let me rearrange. So, what I mean is I just multiply out. So, pi A times D plus of AB plus D minus of B A. We want to show that it's upper bound by D plus of AB. So, let me take X, let X be such that. x be such that E x tau a sorry E x tau B is equal to so let X B let X in A be such that E X tau B is equal to D plus of AB so I take the X that achieves the maximum Then what we have, what we want to show is that pi of A times Ex of tau B plus the minus of BA, which is the minimum, is upper bounded by the plus of AB, which is this expectation. So because the Markov chain is irreducible and on a finite state space, it has an invariant distribution, but Space it has an invariant distribution by the ergodic theorem. We know that by time t the chain visits the set A by A times T times. So the long run proportion of time this the chain spends in the Of time the change spends in the set A is pi of A. So by time T if T is large, we expect to visit it pi A times T times. Now let's take define tau B A to be the first time after time tau B that X T is in A. So we A. So we wait to first hit B and then starting from there we want to go to A. So I said it's non-rigorous because of what I'm going to say now. So by time tau BA, the chain, so let me put in quotation marks, spends time by A times A. Pi A times the expectation, starting from X, times the expectation of tau P A. So, of course, this is not a correct statement, but we're going to make this correct in the proof by defining the right distribution. Instead of starting from x, we're going to start from the correct distribution. But if we suppose that tau b A is a large time, then by this time, the chain is going to spend pi A times. Chain is going to spend pi A times this expectation in A. That's the amount of time that it spends in A. Now, if we wait up until tau B A, then the number of steps that the chain spends in A are only going to happen between time zero and tau B, because after time tau B, we just wait until we go to A, so we don't count any more visits to A. Also. Also the time spent when I say time I mean expected time is upper bounded by the expectation starting from X to go to B after tau B no more visits to A and so And so this implies that pi A times Ex tau B A is upper bounded by EX of tau B, but EX of tau BA is at least EX tau B plus D minus BA. Because first we want to hit B, so we definitely need to take EX tau B steps and then starting. Take EX tau B steps, and then starting from there, I just take the minimum overall starting states in B of the time to hit A. And so that's the end of the non-rigorous explanation. So we did, because ex dot D is equal to D plus of AB. That's how we chose A. Okay, so now armed with this lemma, and we'll present the proof afterwards, we can go back to the proof. We can go back to the proof of the theorem. But actually, I'm thinking maybe it's a good time to take the two-minute break now so that I don't interrupt the proof of the theorem. Is that okay? Yes. Or if you want, the proof of the theorem actually is quite short, so I can just do that and then take a break. As you prefer. Okay, it doesn't matter if it's not at half past the break, I guess. Okay, so let me present the proof of the theorem now. Proof of theorem one. So what we want to show, let me go back to the theorem here, is that th of alpha is upper bounded by this quantity. So what we are going to do is fix a starting state x, a set A with big mass. X, a set A with big measure, and show that the expectation starting from X to hit A is upper bounded by this quantity. So fix X and the set A with pi of A greater than or equal to alpha want to show that the expectation starting from X to equal to A Starting from x to go to a is upper bounded by th of theta plus one over alpha minus one th of one minus beta and so ah thank you Luigi so why is that after tau b um so between tau b and the first return to a so I define tau b a to be the first time that we are in a after having Time that we are in A after having first hit B, and I don't count the last time that we are in A. So I'm going to define it properly afterwards what I mean by the amount of time spent. But it's between tau B and tau B A minus one. So then you don't visit A at all. Okay, so that's what we want to prove, that this expectation here is upper bounded by the right-hand side. So how So, how are we going to prove that? So, the idea in all of these heating estimates, heating bounds, is always the same. We want to define the right set and we want to use the following rule to go to A. We first want to hit the set that we define, let's call it B, and we are going to define a set B so that the measure of B is at least beta. And so the time to hit B is going to be upper bounded by th of beta, and then the remaining. th of beta and then the remaining time play can be controlled by this quantity here so want to define a set b with pi of b greater than or equal to beta so that we first wait to heat beam B and then starting from there, the time to hit A is controlled by the second term by this term here. This term here. Oops. Okay, so since we want the second part to be controlled by this term, this gives us a unique choice. Yeah, that's right. So I'm going to define all of this carefully later when I prove the lemma. So don't worry about the non-rigorous explanation if you didn't understand it. I just want to give the idea why this is correct. Give the idea why this is correct so it doesn't look like a strange lemma. Okay, so since we want the second part to be controlled by the second term, this gives us a unique choice for the set B. So define B to be the set of points y so that starting from y, the time to go to a is upper bounded by 1 over alpha minus 1 times the h of 1 minus v. TH of one minus beta. So if we show pi of b is greater than or equal to beta, then we are done because the expectation to go to A is going to be upper bounded by EX tau B. Tau V plus the maximum starting in V of the expectation to go to A. And so this is going to be upper bounded by th of beta plus 1 over alpha minus 1 th of 1 minus beta. Because this is how B was defined. It's the set from starting from here, the expectation to hit A is upper. The expectation to hit A is upper bound by this quantity. Okay, so all we need to do is to prove that pi of B is greater than or equal to beta. So we'll prove it by contradiction. Suppose not i pi of b is less than beta and let c be the complement of. C be the complement of B. Then we assume that pi of C is greater than one minus beta. How we can see the first inequality which inequality Are you talking about the dilemma or the first boundaries here? You're talking about here, right? This point here. Yes, I think that's right. Okay, okay. So all I'm saying is that we have a set A. We start from somewhere and we want to hit A. So first, I'm taking a set B. Set B and then waiting to first hit B and then starting from B, we want to go to A. So this is going to take longer if we do this because we could have already hit A on the way to B, but it's just an upper bound. So starting from X, we want to hit A, and we are saying, okay, I first wait to go to B, maybe I hit A on the way. It doesn't matter, it's just an upper bound. And then starting from B, I count how long it takes starting from there to go to A. To go to A. Okay, thank you. Okay, so and now the rest here is just by definition. Because if B has large measure, then we can upper bound this by th of beta. Okay, so now we want to solve a pi, we want to arrive at contradiction. So supposing that pi of c has measure at least one minus beta, we want to We want to get a contradiction, and the contradiction is going to be the assumption on A that pi of A is not going to so assume that groups are going to be able to So, okay, so by obey, we can upper bound it by D plus of A C So using the lemma We have the pi of A is upper bounded by, using the lemma for A and C, we can upper bound pi of A by D plus of A C divided by D plus of A C plus D minus of C A. So now D plus of A C is the maximum over all x in A of the expect time to go to C and if C has measured at least And if C has measured at least 1 minus beta, this is going to be at least, sorry, it's going to be at most th of 1 minus beta. And d minus of C A is going to be, so it's the minimum over all starting points in C of the time to hit A, but C is the complement of B. So this is going to be at least greater than 1 over alpha minus 1. than one over alpha minus one times th of one minus beta so if we substitute here we get that pi of a is going to be upper bounded by one over one plus one over alpha minus one which is actually it's going to be a strict inequality because this is a strict inequality here this one here is a strict inequality is going to be upper bounded by this quantity which is going to be equal to alpha This quantity, which is going to be equal to alpha, and this is a contradiction because I of A was assumed to be greater than or equal to alpha. So, let me just go over the proof quickly. Go over the proof quickly. So, we define the set B to be the set of points from where, starting from there, we can control the heating time of A. So, D plus of A C A so how could it okay? Sure, if D plus is zero. Okay, sure. If D plus is zero, then D plus, I mean, C could contain, no, actually, C cannot contain A because C is defined as a set of points from where the expectation is at least this pointed here. So the expectation starting from A to hit. No, actually, no, it could be the case, right? Because A could be included in C, but if D plus is zero, then you're done immediately. Then pi of A is going to be zero, so then we arrive at. A is going to be zero, so then we arrive at contradiction as well. So, of course, here we cannot divide if it was zero, but if it's zero, then we just get the contradiction straight away. Okay, so that's the end of the proof. So, we can just take the two-minute break now and then come back to the proof of the lemma. This lemma here. Okay, so we take a short. Okay, so we take a short break now. Feel free to ask any questions on the chat.  So is D plus with no they're not symmetric d plus and d minus are not symmetric. In the moving set, could the A D be random but independent of the macro chain? Sure. Are we allowed to take constants? Um yes you are. Yes, you are. So you are allowed to take a constant sequence. But let me just scroll up. Here in the moving target, we're taking the worst sequence of sets. So if you take, so it's going to give you a bound, it's a lower bound always. But a question that arises is: in which cases is there actually equality? Is there actually equality between this quantity and the maximum heating time, or between a moving point and a fixed point? So, this is not, they're not in general equal, they're related up to constants, but for instance, in the case of the cycles at end then, they are both equal for a simple random walk. So, let me just see because I will have a few. So let me just speak with a web view. Yeah, so in the moving targets, we just take the worst sequence of sets. So it could be a random sequence. So D plus and D? No, they're not symmetric. I mean, even in the reversible case, take, for instance, you can take X and A, sorry, A and B just to be singled on X and Y, then the expectation starting from X to go to Y is not equal to the expectation starting from Y to go to X for anomaly. It goes to x for a random graph, it doesn't have to be, it's not symmetric. Okay, so okay, so let's continue. So, um, okay, so now let's go back to the proof of the lemma. Proof of lemma. So I'm going to show you the proof of this lemma using the idea that I explained here. But first, I want to state another lemma and I'm going to leave. Okay, I'm not going to look at the chat now. I'm going to leave the proof of this lemma for. Going to leave the proof of this lemma for as an exercise. So, lemma 2. So, let X be an irreducible finite mark of chain with With values in the set S Let mu be a probability distribution and tau a stopping time with a With the property that starting from mu, x tau is equal to x is equal to mu of x. So mu is stationary under tau by being stationary under tau, I mean exactly that. But if we start from mu, then at time tau, we are distributed according to mu again. Okay, then the expectation starting from mu Starting from you of the number of visits up to tau minus one to x is going to be, sorry, not to x, the number of times that we spent in a set A is going to be pi way times the expectation of the random time tau started from u for any A. A subset of S. Okay, so what is this lemma saying? It says that we have an irreducible finite Markov chain and we have some probability distribution with the property that if we start according to mu, then the probability that we are at a point x at time tau is given by mu of x. So then the statement is that So, then the statement is that if we start from u and we count how many times the mark of change spends in A, this is proportional to the stationary measure of the set A, and the proportionality constant is exactly this expectation of the time tau. So, I'm not going to prove it. So, proof. I will leave it as an exercise, and I want to give a hint. Exercise. So hint. So this is similar. So let me just take a step back. So when one takes a first course in Markov chains, then under the assumptions of positive recurrence, but here irreducibility and finite Markov chain assumption is enough to ensure that there is a unique infinite distribution. Invariant distribution. And so then, how does one prove that there is a unique invariant distribution? The proof goes via defining nu of x. We fix a point x0, and then one defines new of x to be the total number of visits up to the first return time. So this is the first t greater than or equal to 1, so that xt is equal to x0. It's the first return time. equal to x0, it's the first return time, the number of times that the Markov chain spends at the state x. So one defines this mu of x and then one shows that mu of x, sorry, that mu is invariant under p. So mu is equal to mu of p and under the assumption of positive recurrence or here that the chain is finite, one can normalize nu. So mu is now an invariant measure, but one can An invariant measure, but one can normalize it, and this gives the invariant distribution π. So, this lemma here is quite similar. So, instead of starting from a fixed point x0, we start from a distribution mu, and then we run up until this random time, instead of running up until the first return time to x0. But this random time has this important property. So, now define. So now define for to prove it, define mu tilde of x to be the expectation starting from u of the number of times up to time tau minus one of the indicator that xi is equal to x. And so that mu tilde is equal to mu tilde p. And so deduce that mu tilde has to be a multiple. Of the environment distribution pi. Okay, so that was the lemma. And now, using the lemma, we can go back to the proof of the lemma, of lemma one. So proof of lemma one. Okay, so as I said before, we want to define. So, as I said before, we want to define this time, define tau B A to be the first time after time tau B that X T is in the set A. And now we want to count the number of visits to A, as I said before, up until this time. But before I said that the number of times we are in A is The number of times you were in a is pi of a times expectation. This is not exactly right, but we want to get to the setting of lemma 2. So let me define that and an auxiliary Markov chain with a transition matrix queue. Given by Qxy is equal to the probability starting from X that at time tau BA, the chain is at Y for X and Y in the set A. Now, this is now a finite Irreducible Markov chain and so this implies that it possesses an invariant distribution that we call mu. So, what I did so far is just to define a new Markov chain with transition matrix Q and now because this is a finite And now, because this is a finite Markov chain, irreducible, irreducible because the original Markov chain is irreducible, it follows that this Markov chain has an invariant distribution that I'm going to call mu. And let now mu, mu y, be the probability starting from mu of the location of the walk at the first heating time on B for y. For Y in B So let me write that and now count the number of visits to weigh up until thousands of people. Up until tau BA starting from you. So what we want is imu the sum i from zero up to let me call for simplicity I'm going to call tau equal to tau b A. Tau minus one, the indicator xi is in A. So this is now going to be upper bounded by E mu of tau B for the reason that Luigi explained earlier, because here I'm looking up until time tau minus one. So tau is the first time that we are back in A after having hit B first. So all of the visits to A. So, all of the visits to A are going to happen between time 0 and tau b. So, that's why we just upper bounded by the expectation starting from u of tau b. These are all of the visits that could happen to the set A. Now, because μ is invariant for the matrix Q, it follows that starting from mu, the probability that x tau is equal to x is equal to mu of x for all x. So the conditions of lemma of lemma two are satisfied, and so this implies that This implies that this expectation here is equal to pi of A times the expectation starting from u of tau, which is equal to pi of a times the expectation starting from u to go to B plus the expectation starting from u. Expectation starting from to go to A because remember that was defined here as the heating distribution of B. So starting from mu, what is the distribution of the first heating time that we hit B? Yeah, the support of Q is not all away. I think that's an important point. But Q is irreducible. Okay, so I'll come back to that afterwards. Let me just finish the proof because it's distracting. So, okay, so So okay, so um the expectation starting from u of the amount of time to hit A is equal to pi of A times this expect the sum of the two expectations on the right hand side. And so pi of A times this expectation is upper bar. Is upper bounded by because of star it's upper bounded by E mu of double B and so if we rearrange we get that pi of A times E nu of tau A is upper bounded by one minus pi of A. pi of A times E mu of tau B and this actually completes the proof this now completes the proof because what we want to show was that pi of A was upper bound by V plus of A V AB divided by D plus of AB plus D minus of BA, which rearranging we get pi A times D minus of B A is upper bounded by 1 minus BA times V plus of AB. And here we found a distribution new support. Distribution supported on the set B for which pi A times this expectation, sorry, this expectation here is upper bound by 1 minus pi of A times this expectation, but this is upper bounded by D plus of AB, and this is lower bounded by the minimum over all points in B of the time to go to A. So it's lower bounded by this. And okay. And okay, let me write and E mu tau A is at least D minus B A and E mu tau B is at least B plus of AB because mu is supported on B and mu is supported on the base. Is supported on A by definition. And this completes the proof. And I think, yeah, so this completes the proof of the lemma. And so that's the end of my lecture today. Okay, so thank you. So, thank you, and let us all unmute and thank Paola.