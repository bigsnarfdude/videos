Um I can set a time just just give it 30 minutes. I'll tell the coffee people that we're running lately. I'm gonna try to show this video. Okay, cool. So hence we're uh really happy to have Akash telling us about planted cleave. Telling us about planning the agreements? Yeah, so thanks to the organizers for the opportunity to come. It's been really exciting so far. I'm looking forward to hearing all the talks. Yeah, so I wanted to tell you about some recent work with Reza Ghazari, who's at Northwestern, and Iming Shu, who's currently a postdoc with me at Waterloo, on the problem of sort of solving the plant-to-click problem using sort of the dumbest black box algorithm we can come up with, which is. Box algorithm come up with, which is gradient descent, down to the sort of order optimal scale for elements, down to the root inscription. So, my motivation for this is twofold. On the one hand, you know, there's no getting around the fact that people use gradient descent as a black box way to solve important statistical problems. And in some sense, at least to us, it felt worthwhile to understand how something so commonly used works on arguably the central problem. On arguably the central problem in statistical computational complexity. But more generally, the question we were thinking about was something that's sort of been haunting, I think, at least the probability side of the community since Jerem's early work on the Planton Clique, which is that somehow there's this whole line of really nice work on understanding the landscape of optimization problems that has this sort of nice interplay between the Has this sort of nice interplay between the geometry of the objective and the underlying randomness of the space. But when it comes to sort of going from this to giving hardness, usually the first heuristic we pass through is by showing something like gradient descent or Markov chains fail. And this is, of course, natural because whenever you have energy entropy trade-offs, Markov chains are sort of designed around this. Sort of designed around this sort of picture. But sort of, you know, okay, so Jeremy was writing this paper in the early 90s, simulated annealing had just come around. There was all this fantastic work on sort of the intersection of optimization, statistical physics. And he sort of asked, you know, is this actually reasonable evidence of hardness? And in particular, what he was saying was, you know, if I can't solve planted clique using the standard move. Using the standard move, I don't know about this. This seems maybe it's a bit too much of an ask. And so, what I'd like to explain today is if you use, I would argue, the first method you learn to solve constrained optimization problems, it will work down to the root n scale. And that's just the method of Lagrange multipliers. Because Planted Clique at the end of the day is a constrained optimization problem on the Boolean hydrogen. Yes. Yeah, so okay, so what is splinted leak? So we've already had a few slides on this, but just a quick reminder. Right, so really just to introduce the notation, we say a graph G is an instance of the Plenty-Clique problem, which I'll call Gn half K. So step one is you sample a graph G prime from the Erdogani random graph with equal edge probability, and then you go to this guy and you pick And then you go to this guy and you pick some collection of vertices of size k and make it a collection. And then, you know, the basic task is given your graph G, find the clique. Okay? Right, so plant a clique problem. So, so yeah, so our So, yeah, so arguably it's sort of, as Alex said earlier, the central problem for many reasons. So, on the one hand, as we saw in Alex's talk, it's a very well-studied example of a problem that has statistical computational gap, at least in terms of evidence. We have so much now. And so, just as a reminder, right, so we have this phase diagram where, so here I'm plotting the size of the clique. The size of the clique, when the clique is less than 2 log n, then your planted clique is going to be buried in just the insane number of cliques of that size that are going to be there for the Irish level. So in some sense, there's not really much of a hope to think about something here. Now, okay, so between 2 log n and omega of root n, the problem is Is there is a ton of evidence by like half of the people in this room that this problem is really hard, right? We have the you know the early work with Flag and Krautheimer on the Lewis Schruer hierarchy, the work by Sleal and Sam and Passad and Pravesh and Steuer and so many people. There's this huge flurry of work around 2015-2016 showing that the sum of squares hierarchy, at least for degrees that At least for degrees that's sort of sub-logarithmic, can't beat the scale. Then there's statistical query bounds, there's sort of really nice new work on Boolean circuits, AC0 type stuff, and the list goes on and on. So in some sense, there's compelling reasons to believe that, yes, it should be hard below the scale. But beyond this, firstly, if you give me log n room, just look at the Room, just look at the largest degrees. But if you want to go down to root n, so firstly, the right scale is not constant root n, it's in some sense omega root n. Sort of this original observation of Alon, Krivolevich, and Sudakov is that as soon as you can get down to constant root n, you can have it by just a small subsample. But sort of beyond the omega root n scale, this problem is easy. There's, again, loads of algorithms. There's the spectra algorithm of Valen, Providence, and Sudakov. There is these, there's a There is these, there's a semi-different program due to Feigen-Karlheimer. There's this nice two-stage algorithm called Feigen-Run. And then there's other things like there's AMP, Dude-Dishpande and Montanari, and again, amongst many other. And we can solve this problem in essentially linear time, and we heard about beating that as well. And so, again, conspicuously missing from that list is great. Is gradient descent and Markov chains. And this is for a good reason, right? This was Jerem's original attempt at thinking about this problem. What he says, well, let's look at this thing he called the metropolis process, which is sort of a natural metropolis-Hastings-style algorithm on the space of cliques. And what he showed. And what he showed was that this has a bottleneck if the glee size is little O of root M. And then he sort of asked this whole question, you know, you know, you know, maybe things work beyond root n? Maybe not. If it doesn't work, if you have bottlenecks beyond root n, that's really scary. But okay, you know, bottlenecks are related to spectral gaps, and so going back. Are related to spectral gaps. And so, going back to that sort of joke from earlier, this is an average case, worst case kind of statement. You might also like to argue from sort of uninformed starts, it would be really, even really scary if from the natural uninformed start, if you own the space of cliques, that would be the empty clique, if you couldn't solve it a little bit beyond the root n scale. And so then very recently there was this nice work at last year's SODA of Chen Mussel. Maso and Sadiq, who said, you know, the situation is even worse than that. What they showed is that this process, it fails. You have bottlenecks even for sublinear sized cliques. So, unless your clique is a macroscopic fraction of the graph, there's going to be bottlenecks. And not just there going to be bottlenecks. And not just there are going to be bottlenecks, it fails even when initialized from the empty clip. So, and not just for low-temperature metropolis, metropolis at positive temperature, for simulated tempering, everything just gets completely wrecked as soon as the clique isn't a macroscopic fraction of the graph. So, okay, so this is kind of scary, but one thing I want to remind you is a little bit. One thing I want to remind you is a little bit before this, there was this really nice work by David Aenilius in David's, I mean, in Ilias' PhD thesis, where they looked at this problem from the landscape complexity perspective, and they gave evidence of hardness in terms of the OGP, but in their evidence of hardness, they already But in their evidence of hardness, they already said, oh, it looks like it should actually be harder than the scale. But the really interesting thing, at least for us, was built into their argument was, okay, so you always do these sorts of moment computations, and moment computations when you restrict to cliques is a pain. So relax it a little. And then when you do these computations, the kind of threshold you pick off seems to depend on how much you relax. And so this is where you can sort of already. You can sort of already see something's going to happen. So let's just roll up our sleeves and sort of think from scratch about the problem ourselves. So let's think about, okay, we have planted clique, and let's just go beyond the two log n scale, because below that it's kind of hopeless. Beyond the two log scale, two log n scale, the planted clique is the largest clique. So there's the optimization form of it, which is the maximum clique problem. So what is max clique? So, what is max clique? I maximize the size of my subgraph, constrained to the fact that this subgraph is a clique. How do you measure the size of clique? You could do vertices, you could do edges, these are the same thing. So we'll just take edge. And so there is my problem. Maximize the size, the number of edges in my subgraph constrained to the fact that it's a clique. The optimizer of this is the max clique. Of this is the Max Klik. So, this is a constrained optimization problem. If you think about these things in terms of adjacency major, this is a constrained optimization problem with a Boolean hypercube dimensional. So, how do you solve constraint optimization problems? So, option one, if your constraint is nice enough, it's sort of a connected, it's a manifold or sort of a nice sort of connected subgraph within your configuration space. Within your configuration space, you could do a walk on the constraints then. Right? This is like radiant descent on the sub-manifold of the constraint. The metropolis process lives in this world. These sort of simulated tempering things live in this world. But then sort of it's very easy to convince yourself that there's sort of an obviously scary thing right from the get-go. So if you're going to start from the empty clique, From the empty clique, your very first move is going to pick a, you're just going to pick a random edge, a random vertex. That's all you're going to do. And if I pick a vertex at random, the chance that it's in the clique is little of one, unless k is log f, I mean, is of the size of f. So as soon as you have a sublinear size clique, your first move is not going to put you in the clique. And so wherever you're going to end up is definitely not going to be the clunky clique. Up is definitely not going to be the functive clique because there's one vertex that's not in it from the beginning, and I'm walking on space cliques. And then work going from that to saying, well, actually, it's not even going to have to intersection with the clique. There's some work there. But this is already right off the bat something that might give you a pause. So just to understand what Kamari and Zadik said, so the problem is that there are many locally maximal peaks. Locally maximal things of size about 2 log n, and these tend to get stuck at them, and they have very little or no interest. Yeah, so I'll show you, there's a very intuitive explanation of what's going to happen. It's exactly like, so basically, as you go up, there's just so much going on, and most of the ways up don't go to the right chill point. So, just one thing you've done. If you could find anything bigger than 2 log n, then you're Then you're I mean, so for example, if as soon as your clique is, say, of size k, k is like n to the alpha, and alpha is a bit bigger than half. If you had some magical way of uniform at randomly sampling the log n size cliques, you would also. Because most of the cliques of size log n, when your clique is of size n to the alpha, are actually children of the Planck clique. But we'll get to that in a sec. But okay, so option one is you walk on the constraints that. But there's option two, right? But there's option two, right? So you teach a class on multivariate calculus. The first time students learn about constraint optimization, the first thing you teach them is the method of Lagrange multipliers. So, okay, so how does this work? So you go to your constraint, and then you say, okay, so what's my constraint here? Say, okay, so what's my constraint here? u is a clique. So u is a clique if and only if all of the edges that need to be there or could have been there, which is the number of vertices to shoot, is the total number of edges. That's what it means to be a clique. So what's the method of Lagrange multipliers? Instead of optimizing my objective, I optimize the Lagrangian, which is the objective plus lambda times the constraint. The constraint. Okay, so here is a Lagrangian. I like balls to roll downhill. I do gradient descent, so I'm going to put minus v of u. So that's the objective. And then lambda times the constraint. The number of vertices choose two minus the number of edges. So you can think about this as a penalty for the number of edges that should have been. So is it the same as So is it same as uh trying to think of max, like denser subcraft kind of problem and then doing Gary design to solve denser subcraft? Uh yeah, density subcraft. Sure. Yeah, I um density is number of edges. I mean number of edges normalized by the number of nodes, but yeah, keep the number of nodes fixed. It's it's yeah something else which I apparently considered in my thesis. I apparently considered my thesis while I think we also looked at it, changing the size of the click from the click which is the target. The target click is size K. We also were looking at K prime and it was something... Is it also something you're missing? Are you looking only of the click of graphs, Lagrange multipliers on graphs of the same size as the platted? No, right now I'm just saying, okay, no, I'm looking at all. Just saying, okay, now I'm looking at all, so that now I've relaxed the space of all subgraphs, and this is my Lagrangian. But it's sort of the standard move, it's a constraint optimization problem on the Boolean hypercube. So I write down the Lagrangian on the Boolean hypercube, and then I optimize over the Boolean hypercube, where my Lagrangian is literally just lambda times the constraint. This is just literally Lagrange multipliers one and one. I'm not trying to do any extra restrictions or anything. No, no, I understand, but you're relaxing the number of nodes as well. No. Number of notes as well. Notes? Notes. Number of notes? Yeah, I'm working on U is, H is now a function on all possible subgraphs. Good, good, okay. Yeah, yeah. I'm just, yeah, I'm looking at every possible subgraph. Okay. Yeah, yeah. So there are no more constraints at all. And then, so, so, okay, so there's my Lagrangian, and then my algorithm is just gradient descent. Right, so how does gradient descent work? Okay, step one, you initialize by your favorite method. And then step two, you go from your time step t to your time step t plus one by the steepest move. And if there are multiple moves that are the steepest move, just do uniform. So, okay, so. Sorry, can I? So you're going to initialize, say, with a random click of the desired size, because. No, no, no. Yeah, so I'm going to talk about that right now. That's exactly what I'm just about to say. So, okay, so now when we did this problem, when we walked on this sort of constraint set, the natural and informed start is the empty click. But now, when I look at all possible subgraphs, there's two. Two dumb ways to start. There's the empty clique, and there's everything. And so what I'll tell you about is the performance of gradient descent on this Lagrangian for those two initializations. So the first thing is a positive result as promised. So the positive result. So the positive result. Can I ask what do you mean by a move? These are the steepest move or something. So it's gradient descent. So I look at all the possible ways of just changing. So I take one step in the space of subgraphs. But what is one step mean? So I look at the set of subgraphs and I look at everything that's connected to it. So by adding or removing a vertex, look at all the possible moves that are of the form. I add one vertex or remove and look at. And then take the move that decreases the energy, the Lagrange, the most. And it doesn't matter how you put ties. No, I don't think so, no, because ties don't help. I don't know. Jasic. Basic. So the first theorem is as follows. So if you take your Lagrange multiplier large enough, Large enough, and then you have your error level that you want, epsilon. Then there is some constant depending on epsilon and lambda, such that if your clique is of size at least constant times ruin, then radiant descent initialized from the full graph succeeds. And it succeeds in linear time with probability 1 minus epsilon. So you can get down to the constant root n scale with probability 1 minus epsilon. There's a price here in terms of the constant that we didn't try to optimize. But if you just start from the full graph and do gradient descent with that, you get the right answer. Yeah? Like for all C, there exists lambda and one, such that this is true. I mean, so depends on what you want. If you want, if you're allowing me to modify it, then yes, because I could do the Allan Kribble and Sudikov trick and just resample. But if I just want to do this, yeah, I don't know. I didn't try that. I think it's believed that, like, uh square root of n over e uh because that's where I ended up. I could totally constantly. That's what I think so. Leave it for linear time about the whole thing with whatever constant. Yeah. All right. Thanks. Yeah, I didn't try to fidget with that constant. But that's, yeah, thanks a bit. So that's the positive result. It will succeed. And now for the hair and the soup. So if your Lagrange multiplier is again large enough, and you have a sub-linear sized clique. Sublinear sized clique, then with probability tending to one, gradient descent initialized from the empty clique on this Lagrangian will fail. It will, in polylog time, get absorbed by a state that very much doesn't intersect the point. Now, before I get to the proofs, Before I get to the proofs, I just wanted to throw in an observation we thought was interesting. It might be interesting for those of you who like Markov chains. So you could ask, okay, so gradient descent. Jam was talking about Markov chains. Gradient descent is a Markov chain, but what about some sort of positive temperature chain? Could you do something? And so I'll just talk. So, I'll just talk about the positive result. I won't talk about the negative one. So, the thing with this is we're using gradient design. Yeah, good question. So, if lambda is infinite, then it's... So, is your result subsume, Yes, Rosso, and others? Because it's what they're doing effectively is very in the sense starting from empty with lambda infinity. Yes. I think the argument we gave here should also apply to there for gradient descent and metropolis. But we didn't try to push back. But the negative result, I'll just talk about gradient descent. But yeah, so for finite bit, if you wanted to do some sort of process with respect to a Gibbs measure at finite temperature, At finite temperature. So here's a sort of funny fact. So the intuition, at least when I was an undergrad, we learned about simulated kneeling. The intuition behind it is: you know, as temperature goes to zero, your natural dynamics converges to gradient psi. And in the continuous world, this is true. The zero temperature limit of Langevin dynamics is greater than sine. In the discrete world, this is not necessarily true. The zero temperature limit, it's not, it's very It's very easy to see. The zero temperature limit of natural dynamics, like Glauber, Hermetropolis, et cetera, is not radiant the sun. And the reason why is that there's an extra layer of randomness in the sort of, the physicists would say, the micro-canonical randomness, the mathematician would say there is this base chain behind things. And it's like, there's the one step where you basically pick an edge and then assign, pick a vertex and then assign it, which is not their ingredient design. not there in gradient descent. So you could ask what is the natural, what is a dynamics whose zero temperature limit is gradient descent. And so one thing you could construct is this thing we call the low temperature chain. So you pick your beta, and then it's an edge-weighted random walk with transitions. I go from subgraph W to subgraph U with probability e to the minus beta h. e to the minus beta h of u over the partition function restricted to the one neighborhood of w in species subgraphs, and is zero otherwise. And so you can sort of check that the zero temperature limit of this guy is gradient descent. It is a reversible marked out chain. Marked on the chain, but the cute fact I wanted to point out is that it is reversible, not with respect to the Gibbs measure you might try to write down, but there is a sort of entropic correction, a free energy correction, to account for the fact that your sort of zero temperature limit doesn't have this extra entropy. So you take the Gibbs measure you would have wanted, but you need to add to it the sort of free energy of the one neighborhood of the configuration you're at. So for beta large, So, for beta large, this correction is kind of negligible, but it's still technically there if you're going to try to check detail balance. Yeah? Fendments, fine. That's all anything. Okay, so proofs. So, I'll sort of give the idea at a high level for both of these two results in a picture. So, first, I'll start with the negative result and hopefully answer Chris's question. question so the idea that sort of captures thing the picture that sort of captures things for me is is the following saw mini plant sort of the landscape of cliques so here's PC here's the log n size guys here's the empty clique and you can think about paths that try to go up to Think about paths that try to go up to the peak. And so, like I was saying earlier, if your clique is a bit larger than root n, most of the cliques at this scale end up being children of the Planty clique. But the problem is you have to go up. And looking from below, all you see is the mass of the Erdős-Renier random graph, essentially. Most of the paths up want to go up to things that aren't children. To things that aren't children of the planter group. So, in some sense, in fact, the measure that you're going to be sampling from here is going to be quite inequivalent from the uniform measure there. But sort of this is sort of the real issue. When started from the bottom, if you run, for example, this gradient descent, it just cannot see the way to the planet cleave because there are just so, so many ways to mess up coming from below. So, in particular, one way of So, in particular, one way of making this rigorous, what we did is we say, okay, so we can think about the same algorithm, but where I ran, my base graph is actually this GPRAM, just the Erdős-Rani random graph, no planet. And then I can, given how we constructed the plantic leak, I can couple this directly to the dynamics on the true graph that we wanted with the planet. And sort of the point is that these two guys are the same. These two guys are the same in all polylab time scales. But then you know, well, if I just do gradient descent on the Aerodish Reni, it's going to get absorbed in polylog time, and specifically log n to the power constant with probability 10 into 1. So these guys are equal in polylog time, and this guy gets absorbed in polylog time, so we're done. Time sort of done. So, in terms of the positive side of things, now sort of the point is instead of sort of trying to take the gondola up, you're kind of like helisking or dropping down from the top. And now there's plenty of spaces to go. And so, just sort of in a quick picture, I'll sort of explain what the algorithm is doing. And I can explain more direct questions, but the idea is. But the idea is this. So I'll look at the fraction of my graph that's of my subgraph that I'm at, that's intersected with the plan and clique. The fraction of my subgraph that I'm at intersected with the rest. So at time one, you get everything from everyone. And so essentially what's going to happen is for the first most, for actually most of the running time, what's going to happen is you're just going to dump vertices. But you're not just going to dump, not click vertices. Not just going to dump non-click vertices, you're going to dump clique vertices as well. And you're just going to keep dumping, dumping, dumping, dumping until you get to the constant log n scale for the intersection with the non-quantic clique. And then once you reach this scale, you can argue that the point you're at will have most of its vertices, in a certain quantitative sense, in the plantic cloud. And then the point is that if most of your vertices... And the point is that if most of your vertices are in the plantic leak, you can show that if you look at the Lyapunov function, which is just the handing distance between your sort of where you are at the plantic leak, it's strictly decreasing once you have that inequality. And that's sort of how the algorithm works. So yeah, that's about all I wanted to say. Questions? Can you say anything about maybe planted or planted dance now? Dance instead of yeah, yeah. Maybe? I haven't got some other questions. So, do you expect, for example, So, do you expect, for example, that say we have a stochastic blockboard with two or three communities, gradients and like similar gradients and find the communities? Because they're what would be initialized. Yeah, I mean, so I don't know what to predict, but the point I guess we're trying to make with this is this sort of physics-based approach where you initialize from random and you understand the landscape, and this somehow tells you where gradient is going to go. Gradient descent actually does it. The picture you predict from doing The picture you predict from doing this is actually not unreasonable. Gradient descent does seem like we now know, okay, so like actually, Jerem's original motivation was to do, to study this question of cart, where maximum independent set can be solved on the Erdős-Reni random graph by greedy up to half optimal. And there was the question at the time of could you do any better? Now, the answer we know is no for increasingly larger families of algorithms. We have this really nice work of We have this really nice work of Bryce and Mark Selke saying the similar story holds for the P-Spin models on the sphere, gradient descent for peace bin models on the sphere we know can get you nearly the right answer due to the recent wonderful work of Mark Selka. And then Bryce and Mark said, well, Lipschitz algorithms, amongst other things, won't be able to do past that. So, yeah, I guess the point I'm just trying to make is: you know, gradient descent initialized from a dumb way isn't such a bad idea, but there might be more than one way to be dumb. And don't forget layer onto multiples. I mean, if you think about the sorry, if you think about the spectral algorithm and you do think about it doing, you know, how you would compute the eigenvectors and the CPU power method, I mean, this is sort of similar to grain descent on the Sort of similar to green design on the grainetic form. And I guess. Sort of starts looking a little bit similar to your multiplier. Do you have a sense how these things are written? I mean, I mean, I guess one big difference is that you, like, you know, here you walk in a very particular way that you have zero one vectors, and I guess you only flip one vertex at a time. And for how method you would update all the values, and it would be fractional values? Yeah, I don't know. I could totally imagine that there is some, because yeah, like you said, it's like if you're. Because, yeah, like you said, it's like if you think about the objective, it's like you can write it in terms of a quadratic form and creating a sentence. In a way, AFP sort of does it, right? Because it has a power, it sort of takes powers and then non-linear trash moving, and that keeps you a little n over e, right? So, in a way, and I guess I mean I think. I mean I guess I mean I think in an optimization literature this might be called coordinate design. Coordinate design? Yeah, exactly. Yeah, if you think of it as design of X transpose A, it's copying one coordinate at a time. Okay, cool. Thanks, Akash. Yeah, three forty five. Three forty five. Yeah. Can you explain this local, like, again, the feet back chain? Yeah, it's it was like, yeah, I was hoping one of the things that I've seen. I don't know. When do you say that it's not trying to keep detailed? Is it the gift of detail? It is that you could just do detailed balance. And if you try to write down the detailed balance, you will in respect to the natural gates measure, which is just this. So it will just fail. You need to add in this term to make it helpful. The physical interpretation of this, I don't understand. Let's keep that change, which means equilibrated according to its neighbors. Oh, you're fast. So the heat that has references choose now consider all plastic values on the different parts and choose from among them with this thing. So yeah, because this is just so what this update is, I take where I am, and then I will propose moving to state U, which is connected to me with probability. Which is connected to me with probability that just exactly take the actual Gibbs measure that you want to, but condition on the one neighborhood of backup of where you started. And so if you let pi be the real Gibbs measure, so to speak, then you could write this as the probability of going from W to U is the probability with respect to its bad notation, but it's like the probability of sampling U. Of sampling U from the measure, which is the Gibbs measure, condition on the one neighborhood of W. So it sounds like what you're saying. But that's the effects of it. So I mean. Oh, so this is the balance. So that obeys detailed balance by itself. You don't need a correction factor. There must be something I'm not quite getting. In other words, the stationary distribution of what we call the EPF algorithm is the Gibbs. Is the Gibbs distribution. It's not self-primary. Are you aligned self-loops? No. That's the thing that is. There's no self-loops distribution. So because he has no self-loops, if there should be a huge self-loop, then that's the issue that's going to be. This is EFW's normalizing. Oh, so it never stays. It never stays. That's the difference. That wouldn't change the discussion. Yeah, I have to move. And my other question is: so, suppose a planet fleet is size. So suppose a planet fleet is size root n, and now I initialize your process or a process like it with a random set of size root n times 100 log n or something. Then probably my set intersects the planted clique on a set of size 100 log n, which is a clique. So then So then, would your greedy process then just add the other stuff in the planted clique, pretty much? I'm going to guess. So, there are too many ways. So, you're saying that even if I start with a larger than expected sub-clique of the kind of Once you have enough of the plant to click in you, this dynamics will send you the plant to click on the microphone. Even if you're the login scale that everything is. But how small is the set U? How small is the set U? Set U is going to be... Don't you want it to be pretty small or it can be Yeah, it's gonna be quite small at this gas. So maybe rephrasing Chris's question some of these is wrong, but let's just initialize it at at root out. Initialize it at root end size. But with the promise that it has 100 login size click in it, is that going to be quite a bit? Where in your diagram is that? I was guessing that you were going to prune to you quite a bit by the performance. Right, so if I just look at all subgraphs of size root n that have a