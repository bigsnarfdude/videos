I'd like to really thank the organizers for the invitation to come to this meeting. I was going to start by apologizing that my talk was not dispersive, but then I listened to the full title of the workshop that we just had. And the word fluids was in there somewhere. So I guess it's okay. This is a talk about the Euler equations. And it's joint work with my colleagues Taipei and Sein. With my colleagues Typling Tsai and postdoc Evan Zeller, IEPC. Okay, so we start with the familiar Euler equations. I'm going to allow any space dimension here which is three integrators. So we have a vector field, fluid velocity vector field in Rd. Of course, three dimensions is the most. Three dimensions is the most interesting case physically, but it's also reasonable, I suppose, to consider general dimension if you're interested in seeing how phenomena depend on dimension. So in any case, we're going to allow any dimension 3 and higher. Here are the Einer equations, which are supposed to describe incompressible and inviscid fluid dynamics in such fluids. Dynamics in such fluids in terms of this fluid velocity vector. So, right away we're going to specialize to a very nice particular setting of axisymmetric and swirl-free vector fields. So, this means that we make some cylindrical coordinates on our space. So, we have one direction called here z. Called here z, which is a horizontal direction. And then all the other directions we consider, we combine into a radial direction. And the fluid velocity components are only allowed to depend on the radial and horizontal directions. And moreover, they only have components in the radial direction and the horizontal direction. So this is axisymmetric and swirl-free. Free. And we're not really going to work with the Euler equations as written, but rather with a vorticity formulation of the Euler equations. And more precisely here, in this axisymmetric setting, this scalar vorticity, which is defined as this function here. So in three dimensions, this would be the actual vorticity, or rather, the theta. Or rather, the theta component of the actual vorticity. But this generalizes to any dimension. And then the dynamics can be written in a very nice way in terms of the scalar vorticity. So there's this quantity here, which is the scalar vorticity divided by some power of the radial variable, which is simply transported by the velocity vector field. Have a simple transport equation here, which is why this is such a nice setting to work with when looking at Euler flows. But of course, the velocity has to be recovered in some non-local way from the vorticity itself. So that's where things become interesting. Okay, and then we're going to specialize even more. We're going to specialize even more here. So, I want to look at vorticity of a certain type, which is inspired by anti-parallel vortex rings in three dimensions. So, the idea is we have radial direction, horizontal direction, and above the And above the plane, I want to have some positive vorticity or positive scalar vorticity. Maybe some region like this. And then I'm going to assume that the scalar vorticity is on under reflection, z goes to minus z, so I have a corresponding negative batch down here. And then if we were in three dimensions, then we would have one further dimension. We would be rotating around this axis, and that would produce these vortex rings, which are anti-parallels. But this can generalize to any higher dimension. The reason for looking at this kind Looking at this kind of configuration. So, it's very popular thing to study. In particular, there's been a lot of numerics on situations like this, where you take anti-parallel vortex rings. And what happens is that they collide and then they stretch out in a way that produces very rapid growth. So, we're expecting some. Expecting some behavior like this. So, collision and stretching, at least from numerics and formal asymptotics and so on. So, the idea is to it's an interesting problem to see if you can make this rigorous. And there'll be a few further standing assignments. There'll be a few further standing assumptions on the initial scalar vorticity is to ensure lots of things are bounded. So I want the vorticity to be bounded. I want the transported quantity to be bounded. And then I want some integrability at infinity as well, so that some moments are finite. So those are standing assumptions. Okay, so I want to state the result. But first, I have to introduce. But first, I have to introduce this radial moment. So, the way I'm trying to capture this, or we're trying to capture this behavior, is by looking at the radial moment. So, I'm taking the vorticity, I'm multiplying by the radial variable, and I'm integrating just in the upper half space. If I did, vortices odd in Z, so if I did the whole space, I would just get zero. So, that's you can. So that's, you could think of that as the first radial moment of the vorticity, or maybe even better is to look at the transport equity. So if I take the transport equity together with dx, that's a measure which preserves its mass because of the transport, and so it's like a probability measure. So this is really the d minus first radial moment of the transported quantity is the result. Here is the results that I want to present. Okay, so there's a lot here. It's essentially upper and lower bounds for this radial moment, depending on the dimension. So let's maybe look first at the upper bounds. So the upper bounds are reasonably nice in dimensions 3 and 4. In dimensions 3 and 4, and bad in higher dimensions. So, more precisely, in dimensions 3 and 4, we have global solutions, as long as you're willing maybe to assume a little more regularity in the four-dimensional case. And we have an upper bound on how fast this moment can grow. It's a polynomial rate in three dimensions and an exponential in four. And then when we try to do the same thing in higher dimensions, the result is actually a singular growth. So there's some capital T such that if the solution were in fact to exist up to that time T, it couldn't blow up any faster than this. So things, in terms of the upper balance, things go bad in higher dimension. Maybe more interesting is the lower balance. Maybe more interesting is the lower bounds. So these are polynomial rate lower bounds on the growth of this radial moment in all dimensions, which is supposed to somehow capture some aspect of this behavior, at least maybe in a weak sense. And we have different powers depending on the dimension, which they get worse as the dimension gets higher. As the dimension gets higher. In fact, the lower bounds get worse and the upper bounds also get worse. And of course, there's a big gap between lower and upper bound in everything. Okay, so that's the main result. Let me say a few words to give a little context. So the main inspiration for this work was a recent paper of Troy and Drum. I studied exactly this situation in three dimensions and gave a lower bound on the growth rate of the moment, three dimensions. So what we're doing here really is, well, we have some improvement on the lower bound and the generalization, higher dimensions. But in many ways, we're using similar methods to Choi and John's work. Another remark is that maybe you're more interested in LP norms rather than moments if you want to track growth. And so the claim here is that you can, if you want, you can convert these moment growth rates into L P norm growth rates in certain situations. So the simplest one, for example, would be if you had a vortex patch. If you had a vortex hash, so you want your transported quantity to be just, say, a characteristic function of some set. So in that case, it's easy to see that uh moment growth of this of this quantity would correspond to growth of, for example, the L infinity norm of quartic functions. And if you're worried that this is not so smooth, you can also make things a little bit smoother if you want. Instead of having a pure characteristic function, you could smooth it out a little bit and get some slightly more regular vorticities for which the moment of growth implies a LP north growth. LP North Cool. Okay, a couple other remarks. So these are relatively smooth solutions of the Euler equations we're looking at here. It's well known that if you allow rougher solutions, then you can make things blue. For example, in dimension three. So here in particular our solutions are very smooth on the axis. And then the final remark. So the upper bounds here are relatively straightforward. So in what remains in the talk, I'm just going to say a little bit about where the lower bounds come from. So let me put that now. So your result suggests something about the shape of The shape of omega of t, or if you really have a tea spoon going to the right? Yeah, so not really saying much about a precise shape, but the general southeast direction of the movement. So the radial moment is increasing, and as I'll explain in a little bit, the horizontal moment is also decreasing. So in that crude sense, it's saying that things are moving in this way. Much more or less you would think something's maybe same. Yeah, I I don't really have any uh insights into into what the page is that you get. Maybe you would have more insights uh well it decreases uh it's not clear that it goes to zero. Back to yeah. I can yeah. I'll click on the bottom. Okay. Ah, okay. So just a little bit about where these bounds come from. All the computations are based on the BSABR law. So we can recover the velocity components from a stream function and then the stream function from the scalar vorticity by Vorticity by inverting some kind of Laplacian. And you get a pretty nice expression as an integral in cylindrical coordinates with a nice-ish kernel for this recovering the stream function. So notationally, when I write omega bar, this means the vorticity evaluated at the bar variables together combined into a measure. This S is This s is some kind of dist distance to the diagonal in cylindrical coordinates, and this f is some elliptic type integral, which we have to analyze a little bit. Anyway, there's a nice formula for the stream function, and therefore you can get nice formulas for the velocity components. Okay, so the first uh important step, which is uh we got from Troy and John. We got from Choi and John is to, as I said before, to look at this horizontal moment. So instead of radial, I put the horizontal variable there. Okay, so we can do a computation using the formula in the previous slide to compute the time derivative of this horizontal moment. So we just take time derivative, take time derivative. Take time derivative, use the transport equation, do some integration by parts, use the oddness symmetry, and we can express this z dot as an integral over two copies of the cylindrical coordinates. The vorticities in there and some kernel, which is somewhat elaborate, can be expressed in terms of these elliptic integrals. And it turns out with some And it turns out with some effort, you can check that the appropriate elliptic integral combinations here are decreasing in order to make this thing less than this thing. So we have a manifestly positive quantity there. Tells me that indeed the horizontal moment is decreasing. So that at least captures some aspect of this anticipated identity. Of this anticipated dynamics. And more importantly, for the analysis, it gives us an a priori bound on the horizontal moment. Since it's finite, it's positive, finite, and non-increasing. What was capital F? And calligraphic F? Ah, well, it's on the previous slide. It's this integral here. Okay. So now we can turn to the radial moment, which is the thing we're interested in. There it is again. We can do a similar computation using the Biosavar law. Compute the time derivative of the radial moment. It has an even simpler kernel. In fact, it's simple enough that you can see immediately the sign. So the fact that the radio model is. So, the fact that the radial moment is increasing is immediate from this computation. But in order to get some bound on the rate, lower bound on the rate, we need to understand this expression a little bit more carefully. So we just need to look at the behavior of this elliptic integral, essentially, to determine that, from the point of view of upper and lower bounds, we have an expression. We have an expression which looks like this for the time dependence of the regular moment. Again, that's an integral over two sets of cylindrical coordinates. Okay, so we'll come back to that expression in a minute. The last step is to look at kinetic energy, which is a conserved quantity. And again, can be expressed in a similar way to the other quantities can be expressed in terms of with an integral kernel involving these elliptic integrals. And with a bit of work, one can cook up an upper bound for this energy, kinetic energy has this slightly elaborate looking form. form and the thing I want to point out is that it's not so different than the R dot expression. So the expression for the integral kernel of the kinetic energy is very similar to the one for the time dependence of the radial moment. There's a couple of differences. One is you have this log factor to some pain. And the other, the real pain is that you have the wrong sign Of z and z bar. The reason that's a pain is that what you want to do, if you could bound the kinetic energy, which is a positive constant essentially, by r dot, then that would give you nice linear growth of the radial moment. So that's kind of the idea, but of course you can't do that. Of the idea, but of course, you can't do that, you have to do something fancier. So, I'm running short of time, so I won't belabor the details. Essentially, you want to split up the region in the integration into places where this can be controlled by this, and where that fails with some parameter epsilon. Parameter epsilon. In the failure region, in the sigma epsilon region, we can use a Holder inequality to control the integral of the kinetic energy kernel by essentially by the moment itself, some power of the radial moment. And so we can choose this parameter epsilon in a time-dependent way in order to get. Dependent way in order to keep this component small, small enough that the full kinetic energy is really bounded by the remaining component where I can control this one by this one. That's accomplished by another holder inequality. And I should emphasize when we're doing these holder inequalities, we need this a priori bound on the horizontal moment, which came from the monophysics. The monitor is okay, so you run that through, and it gives you an ODE lower bound for the radial moment, which if you integrate the ODE gives you some power lower bound algebraic rate of increase, which is given here. Just to finish off first time through this, I First time through this, I ignored the log factor, so that's going to cost us an epsilon in each of these exponents if you handle it correctly. And then the last thing is there's a small improvement beyond what I described here in the three-dimensional case, very small improvement, to get a slightly better lower bound. Okay, I think that's all I want to say now, so thanks for your attention. Thank you very much for the first talk of this morning. Do you have any questions? Do you expect they should be the same or what? Yeah, that's a great question. It's a huge gap currently between our It's a huge gap currently between upper bound and lower bound. And some people, I think there are some conjectures as to what it should really be, which is somewhere in between, not close to either the lower bound or the upper bound. But I don't really have any great insights. Are there questions? I have one minute. So you said that you were considering this higher dimension, so from tree up. So there are technical difficulties or physical reasons why two dimension or lower well two dimensional case is actually uh it's very interesting and very well studied and Well, studied, and I guess the phenomenology is similar. So, people have studied this situation in two dimensions, and it's similar. And I guess you can say even sharper. Maybe in that case, you have a sharp growth rate. I think you have linear growth in that case. Some other work where you get where, for example, work of Uh well, okay, I won't cite any particular words, but you can show very uh not low, but very rapid growth of the uh the velocity in two dimensions. Um so that that that's a better understood what would be done for bounded domains? Uh that would be for bounded domains. Uh that would be for bounded domains. That's right. Yeah, yeah. So here this is a situation where things are going out to infinity, so it's a good reason. Do you believe that the vorticity in some sense tends to zero on compact sets? The vorticity should grow. Sorry, you want it to leave? Want it to. Wanted to? No, I don't think that's. Can we say anything about the minimum? I think it's not clear, actually, because it stretches out. So, yeah, I'm not sure about that. Like a filament? And it becomes becomes, I s I suppose it should become thin. Not have any precise Precisely. Close supports closer and at the same time shrinking, I suppose. The supports are shrinking of your picture. Okay, there's a transport, so there's a preserved volume. That's right. But yes, they're getting. The volume is preserved, so it should be shrinking. Right. So I think that it's time to thank our speaker again. Here again. So let's start with the second talk of this morning by David Pollato and he will talk about mean field limit of not exchangeable multi-agent systems called the great person. Thank you. Should we press here something? I'm not sure if we have to record it. Thank you. Okay, thank you. Thank you for the introduction, and also thank you so much to the organizer for giving me the opportunity to speak in this great place, which I really enjoy. So today I'm going to talk about this joint work with Emmanuel Javan from Penn State. And Juan Sorrejo is in the audience. He was my PC advisor. And I'd also like to And I'd also like to start by apologizing a little bit about the token. I mean, it's not compute key in the spirit of dispersive interable and non-interable model, but as we will see later, many models, some of them coming, for instance, from flu mechanics can be at least formally derived through this type of methods. So, what I would like to speak about is a method to try to A method to try to obtain macroscopic equations from an underlying microscopic description. So, something that mathematics we call mean-field limits. And in this case, I would like to talk about mean-field limits of what we will call non-exchangeable systems. We will see in a minute. So, essentially, the goal of my talk, as I was saying, is to describe some quantitative and qualitative properties of large multi-agent systems. So, these are systems of many particles. Systems of many particles who interact. And initially, we observed that the motion can be very disorganized. But after a while, essentially, we see that there's an expontaneous emergence of some large-scale self-organized structures. More specifically, in this type of system, we observe that pairwise interactions between agents are strong enough to push or drive. To push or drive all the population to a global consensus, to a global emergent dynamics. And so this theory lies somehow in the interface of many, many theories, in particular kinetic theory and statistical mechanics. But we also employ a lot of tools from PEES, as is the goal of this session. And as we will see in this topic, we also employ some methods coming from. We also deploy some methods coming from graph theory. So let me see if I manage. Okay, working. So let me introduce a very general class of non-exchangeable systems and we will understand what this word non-exchangeable system means. So here we have an agents with position xit. It can be whatever I'm calling a position, but it can be any state variable, for instance. A state variable, for instance, some activity of neurons in the brain. Actually, many of the things that I will be speaking about are inspiring models of neuroscience. So, you describe the dynamics of the activity of the different neurons in the brain. And they are interacting pairwise through some interaction kernel, which I call K. And there are some links connecting those agents, which state how strong the connection between the two agents are. Connection between the two agents are, I call them WYJ, and eventually we may include some Brunian motion. So, what we are reading here is just the law of motions. This is a first-order system, including the pairwise interactions and eventually the effect of noise. And we have to know it with some initial data. So, along the talk, I will focus on the case without any noise, but as we will see, actually. But as we will see, actually, results are even better or stronger in the case with noise, as usually happens. So let me just list a list of or a group of equations which embedded in this type of description. And many of them, as I was saying, are inspiring neurodynamics. So just for those who are familiar with these models, we can think about the Uramoto. We can think about the URA motto: hatching hatch leaves, hala human model, or integrated fire models. But essentially, in any of them, the main question, which is question Q1, is how to derive a macroscopic description, so an accurate average description of my system of particles in terms of some probability density. So I could like find an equation for the probability density of finding particles in a specific state. And there are two different scenarios. And there are two different scenarios. So, if we have uniform coupling, so all agents are connected to each other with the same strength 1 over n, which is the mi-fuel scaling, then this is classical and in fact particles are exchangeable in the case that they are indistinguishable. They are all identical. They interact in the same way each other. And just to read some group of people who worked already in this area, let me put the names of Dobrujam, Meliab, Noiser. So, the Brusham, Melia, noisy. But the opposite case, so the case where we don't have a uniform distribution of weights, has put the attention of this community in the last year, and there are already some works, currently some of the researchers, Chiva, Mentez, Juan, Bogas, Chu, among others. There are some others. So, this is question one. Okay. Question two. Question two could be once we have a microscopic equation, we would like to understand how the emerging dynamics of the food population happens. Again, many works have already addressed the case of uniform weights of 1 over n, particularly in some of the previous models. But essentially, the question of what is the emerging dynamics in the case of non-uniform weights is just Uniform weights is generally unknown, and there are some people working on this. But in this topic, in this presentation, I will focus mainly on the first question. So, the rigorous derivation of the microscopic equation from the underlying microscopic dynamics of the agents. So, this is the outline of the talk. So, first, I would like to start with the exchangeable case, so the case we call uniform coupling, which is already classical. And I would like to introduce or recall very briefly some Recall very briefly some results in the literature in the non-extangible case, so we're not uniform weights. I wouldn't like to be very precise in all the results, I would just like to give you some overview of these methods and this particular problem. And then I would like to introduce our contribution again in the non-exchangeable case of non-uniform coupling. If time allows, I would like to sketch very quickly the Like to sketch very quickly the proof, and I will end with some conclusions and perspectives. Okay, so let's start with the exchangeable case. So the exchangeable case, as I said before, is the case where particles interact each other with the same strength, one over n. So couplings are identical, one over n. And the classical result here, again, I list the same names as before, who started to work in this topic in the 80s and 90s. They proved. They proved essentially using different methods, but all of them led to the same result, the following. So, if we have an interaction candle which is literally, the Rigorous Minfield equation, so the macroscopic equation for the probability of finding particles at some time t and position x, is the Blazov equation. And the Vladsov equation is a hyperbolic equation, it's actually a transport equation where the velocity field is self-generated by all the Field is self-generated by all the agents in my system, so by the whole distribution F itself. So, in order to understand properly what do we say by rigorous mean field limit, let's try to understand the Means you're limited, let's try to understand the proof or the idea behind their proof and what the proof is. Therefore, when they have what they call the classical W stability estimate, and more specifically, if we take any solution to the Blassoff equation and we take this specific solution, which is muen, we call this the empirical measure, it's just atomic measure localized over the position of the particle system itself. The boundary literature distance, which is a distance between the two projective measures, at time Probably the measures at time t can be bounded by the initial bounded Lipstick distance times an exponentially growing term, which depends on the Leptrus norm of the kernel. So in particular, if we use the Lofland of large numbers, our data which are random are initially independent and identically distributed, my initial empirical measure will converge to the common law S0. Then the right-hand side converts to zero, and uniformly over a compact set of time I will. And the uniformity of a compact set of time I will have that my particle system gets well approximated by the Plus of equation when the amount of particle grows to infinity. But this is all in the case in this changeable case, so we stop working networks. Sorry. So excuse me, what is DBN? Uh, where? Where? The distance of dvl is the bounded leapage distance. It's that distance which you can build on the space of probability measures and it's actually the distance which metric is the narrow convergence. But it's essentially the induced dual Leech's norm over the probability measures. So you compute the dual Lipschitz norm on the dual space and you induce it over the probability measures. But you can quantify actually. But you can quantify actually this stability estimate in many other distances, in particular the usual parser time distances for priority measures. Okay, so in the non-exchangeable case, we have non-uniform weights, and then the main question is how to obtain a large graph limit. So we have at the same time the amount of nodes is growing, also the graph is growing, and the amount of edges is growing. We would like to have some continuum objects describing the weights of my continuum system when I have infinitely many particles. So I think that the more instructive thing is to come back to the origins of this theory, which is the theories of graphons or dens graph limits. And essentially the theory is quite intuitive. And what it says is the following: so, imagine we have a finite graph with vertices, the numbers from 1 to n, for instance, and some And some edges, which we call E, and the adjacency matrix, which is actually the matrix with 0 and 1, saying whether my nodes are connected or not. And then I can embed this display object into a continuum object. The way to do is embedding it into a step-wise function. So it's a step function, which is built as follows. So I take the unit square and I divide into pixels. So I divide both sides. So, I divide both sides with pixels and exact pixels of the same size, one over n. So, what I do is over each pixel ij, I'm going to put the constant value of my weight coming from the adjacency matrix. And then this is a function of two variables. And intuitively, if we make them out of vertices, so n goes to infinity, the pixels are going to be thinner and thinner. And eventually, if everything behaves properly, we expect to have some continuous, or at least some linear. Continuous, or at least a limiting function describing my continuous distribution of weights over my system. And this is essentially what was proved in the 2006 by Lovas and Sejeti. What they did is they considered the bounded function on the unit square, which are these uh continuous continuous objects, which are symmetric because they um prefer to work with symmetric uh graphs or uh undirected graphs. And directed graphs. This space can be endowed with a specific distance. I don't want to enter into the details. They call it the cut distance. And what is interesting is actually that this space endode with the gap distance is compact and the space of graphons, so these functions which are associated to finite graphs is actually a dense subset. So actually, any graphon can be approximated. Any graphon can be approximated in an appropriate distance or an appropriate topology by finite graphs. I don't want to enter into many details, but something interesting is that this class is large enough to obtain continuous limits of graphs which are dense. So if I have a sequence of finite graphs where the amount of edges is essentially quadratic with the amount of vertices, this means that the graphs are dense. The limiting object is typically non-trivial. Typically non-trivial, so it captures really a non-trivial limiting of a continuous graph with a dense behavior. Once this theory was known by people in infield theory, so infield limits, they tried to employ it to sort of combine with the previous result by no insert on the mean field of exchangeable. On the mean field of the exchangeable system to obtain a continuous limit of the system of particles with non-identical weights, the non-exchangeable system. Essentially, what they did is the opposite process, so they depart from a continuous object, so we think that we know the limiting object already, which is the graphon. And what we do is to cut the unit squared into pixels, compute the average of Two pixels, compute the average of my graphon over every pixel, and then I compute weight for each single cell, each single pixel. And then this describes a finite graph, and it's quite easy to see that the associated graphon to those finite graphs converge, in particular in the L1 norm, towards the linking graphon, and in particular, it also converges in the cat metrics. Also converging the cat metric, so the natural metric in the space of graphons. And what they did, I mean, these people infield limits, which I list here, is the follow-up. So assume that we have a uraphon, refined at some appropriate continuity conditions, and let's discretize in the previous ways. And we already have also an underlying microscopic system whose weights are precisely the discretization of the microphone. And then what they could prove is if the kernel Is if the kernel is litches, then we also have the mean field limit. So we have the rigorous equation for the probability density of finding particles in a specific state. And the trick is that we have to attach a new variable which is playing the role of a continuous index. So index i, which are discrete, are going to be replaced by a continuous index g. And then the equation is again a, we call it the generalized equation, it's sort of a transport equation. Local equation is sort of a transport equation whose velocity is again generated by the population itself. But here, not only the kernel is going to play a role, but also the limiting graph one is taking how agents with different labels, G and Clinta, are going to interact. So now I would like to identify questions for the moment. Otherwise, if there are no questions, I will. Otherwise, if there are no questions, I would like to show very quickly that this theory, which only works for graphons, so L-infinity bounded functions in the uni square, which describe a limit of dense finite graphs, would be generalized if we generalize the limiting objects. So if we augment our class from graphons to a broader enough Broader enough class, we can still capture limits, non-trivial limits, of a sequence of graphs which are not necessarily dense. So we may arrive at some sparse distribution of weights where a specific node is very sparsely connected to their networks. And it was not contained in this initial paper. So So uh there are many theories I I would like to be very quick, but here I I'm again restating the theory of dense graph limits by Lobas and Geohevi, which works fine for dense graphs, so graph whose amount of edges is quadratic with amount of vertices. But as I was saying, it can be generalized and there was a first A first theory of large graph limits in the regime where edges is just linear with the amount of vertices, so it's a sparse sequence of graphs. The continuous object was called a graphon. And in the regime where we have that the amount of edges is superlinear with the amount of vertices, but still quadratic with the amount of vertices, there's a different theory by Borg, Christian Borg, Jennifer Chase. Christian Burke, Jennifer Chase, and some other collaborators, which they call the LP graphons. Essentially, they are no longer L infinity functions, but LP functions. And what's interesting is that the two regimes could be recast in a different way using different continuous objects, which are called graph operators or graphs, and SRAFs. And as we are fonts. So, more specifically, if we have a sequence of graphs, find a graph whose amount of edges is superlinear with the amount of vertices and subquadratic with the amount of vertices. Then there's a naturally meeting object which is non-trivial and which was found by Bachox and Segheddi. And it's what's called a graph operator. Graphox. It's just a bounded linear operator from N infinity to. Linear operator from negative 150 to L1, which is positivity preserving, this is somehow weights are positive. And SQ symmetric, so in a sense we are admitting only limits of symmetric weights milliamp. There's a slightly more general mm class of graphons or limited graph graphs which is called uh S graphons and essentially we have a a probability measure on the unit square. A probability measure on the unit square. And any of these two theories could be used in the spirit of the previous publication in order to produce a rigorous convergence from a microscopic dynamics to a macroscopic dynamics from a mean field equation, leading to a very similar Plus of equation, but where we have this new continuous object, the graph operator, either the graph operator or the S-graph on appearing in the equation, right? But in any of these theories, But in any of these theories, the idea was essentially the same, and it's actually the same as in the first paper by Chiba, Medeved, and the other collaborators. What they did is we depart from a non-limiting object, either a graphone, S-GraphOp, or whatever, they discretize in an appropriate way, in an appropriate way. They put those weights in the multi-agent system, and then they perform the NICU limits. Performed an infinite limit. For sure, we already know the continuous objects to which our weights are going to converge because we are precisely discretizing such a continuous object. And what would be more interesting is actually to perform in a simultaneous way, a MIFI limit, an a large graph limit, when we do not know which is the continuous object appearing as a limit of our graphs. So I don't know exactly. Limit of our graphs. So I don't know exactly where our graphs are going to converge to. They must converge to something. And at the same time, I would like to find the continuous object for the weights and the continuous object for my particle distribution. And this is essentially what we did in our paper, which I recall is a paper with Pierre Manor, Javon, and Juan, Councillor, in the audience. And let me try to explain at least the hypothesis. At least the hypothesis. So, again, we have a very similar hypothesis. We have a leaky skirt for some technical reasons. We need actually W11 regularity on K. And the reason is actually that we are not going to work simply with measure-value solutions in the classical result by NOISER, but we actually need some stronger solutions. So, we need some solutions with solar regularities. So, we also have to put some extra integrability of my k and. Some extra integrability of my k and its derivative so that I can ensure that those solutions exist. And okay, so then we have some mild condition on my weights. I don't know where my weights are going to converge. I only have some my growth condition. Essentially, they are saying that this mixed norm are uniformly bounded with respect to n. So by any mixed norm, I'm saying that the sum with respect to j or my maximum with respect to i is uniformly bounded with respect to n. And since we are not With respect to M. And since we are not going to reduce to the class of symmetric weights, we need a similar symmetric condition. So, some with respect to I, maximum with respect to J, is going to be uniformly bounded with respect to M. Now I need also some decay of my weights, because in the mean-field limit I require that those weights are going to go to zero at an appropriate rate, like in the classical uh uniform case, where this is actually just one over n, which the case usually impacts. N, which decays to see in facts. And then I need some conditions on my particle system. So let's consider x1, xn. This is the solution to the multi-agent system with non-identical weights. With those non-identical weights, our annual initial data are independent but not necessarily identically distributed, as in the paper of in the previous references. And the reason is this assumption is actually pointless. Pointless. If we put this initial assumption due to the non-exchangeability of my system, the particles are distinguishable, this is going to get broken instantaneously. So actually, we don't need it, but we cannot use it neither, you know? And then I'm going to assume that their laws, the law of each of the particles, Fi0, verifies some mild conditions as well. I'm assuming that they have uniformly bounded second-order moments and uniformly bounded norms. And uniformly bounded norms in my regularity class, which is again w11, w1 infinity. And then in this setting, what we proved is that we can derive the Mi-Field limit, a Large Graph limit, simultaneously. And what we know is that there's a couple of continuous objects. So on the one hand, there's this W, which we call an extended graphon, it's an extension of graphons and these S-graphones graph operators and S-graphons. And it is just a bounded function. And it is just a bounded function with respect to the variable g, which is a random measure, finite measure with respect to the variable theta. And again, I need a symmetric condition. So I need it, it can be regarded as a bounded function with respect to the variable theta, which is a finite measure with respect to the variable g. This is the continuous object for my waves, which I will find in the limits. And I will also find some continuous objects for my particles. So this is the distribution of any particle at time. This is the distribution of any particle at time t, position x with label g is a bounded function of t and g, which has some within a regularity in space. So, again, we have the same regularity assumptions as my initial data. And what we did is to show that those limiting continuous objects verify essentially the same general Leibla equation, except for the fact that here we don't have a bounded function. We have a function with respect to chi is bounded, but with respect to theta is a measure. So I'm integrating. A measure. So I'm integrating first with respect to theta, which is a measure. It's a function of theta. This gives me a function of G. The way of computing how the microscopic equations, the macroscopic system converts to the macroscopic one can be in particular be quantified. Actually, we have convergent rates, but I didn't want to write as follows. So again, if I compute my empirical measure and then compute the And then compute the analog continuous object. I'm just averaging with respect to the variable g, that's to say the labels. I'm just summing over i say at the continuous level. Then these two things are going to get closer and closer in the Varsalstein distance over a uniform compact set of times as the amount of agents go to infinity. So here I'm subtracting an appropriate subsequence. This is a compactness result. Results. Here I have to put expectation because naturally these measures are random measure. But still the general life velocity equation looks pretty much the same with a more generalized object, which in particular is able to capture limits of sparse graphs. So essentially, I think I can skip this part because I already mentioned almost everything except for maybe. Almost everything, except for maybe the last part. Here we forgot about the noisy case without not putting any noise on our particles, but in fact, if we put it, we even have better results. And then I think I don't have too much time to go over the proof, also it's a bit technical, but I would like to mention the main steps. So the main steps are the following. So, on the one hand, the first thing is my particle system xi is going to be composed of random variables, which are dependent random variables, because of the dynamics. It's going to make them dependent. And as in the speed of noiser, we have to approximate those particles by a new system. New system which consists of independent random variables. So, what I'm saying is there's a way to simplify a little bit the dynamics because I can approximate each single particle xi by a different new particle xi bar so that the new particles are independent of each other. And essentially, the particles must verify what we call the non-exchangeable version of the mapping SD, the natural or exchangeable version of the Or exchange allocation of the MacLean SD or Stochastic differential equations, actually a tool which was used by Noniter in the classical proof with uniform weights. And I was saying, if I look at the corresponding loss of the two systems, so the initial system of my Xi and the new system of the Xi bar, I can compute their distances, for instance, the W1 distance, so the Rasustine distance, and they are closed. They are closed. So, if my weights become equal to zero, both systems are essentially giving me the same information in the limits. So, why not working only with this system of independent random variable, which is easier? So, that's interesting, but still not enough because I have n of them. In the classical setting with exchangeable system, I could see that all the xi-bar are actually the same, but here we have different xi-bar, and the reason is we have. Different xi bar, and the reason is we have non-uniform weights. And then it's quite complicated to pass to the limit in all my particles xi bar at the same time. So what we do is to reformulate the system in a slightly different way, which is the graphon representation. So regarding my weights, I can just associate the corresponding graphon. So again, I compute some partition of my unique square, and I embed my weights into a My weights into a sequence of refunds. Okay, and here I do a similar argument for the loss of my particles. So what I do is over each small cell, this means I have to finish. Over each small cell, I'm going to put the constant law of my Fi bar. And this is essentially a parametrization of the different laws Fi bar of my particles. So as I move shift from one cell to another, this is going to jump. To another, this is going to jump from one particle to another. And in a sense, I'm parametrizing in a continuous way all of them simultaneously. And what is clear is that in this formulation, W and Fn already verifies the general Lieblazov equation. So the final question would be how to pass to the limit when N goes to infinity here. And the problem is that it is not that easy. We cannot pass to the limit. We cannot simply simply pass to the limiting Fn and W n in strong norms because typically the W n is not going to converge in the classical strong norms. It must converge in some distance which is similar to the cat distance and it's not really the one norm. And then we require some compactness arguments jointly for Wn and Fn. We also require identifying the limits and showing that the limit again I'm showing that the limit again verifies the Vlasov equation. So, in order to do so, let me be quite quick. So, what we did is to first simplify a little bit the system. So, we proved that if I have a have a capital solution to the general life loss of equation, so now the step is do I have a similar uh stability property as in the case of uh exchangeable system? In the case of HNO system, the stability property by diversion? And the answer is sort of yes, but in a slightly weaker version. So if I take any couple of solutions to the general Leiblazov equation and I average over the I-indices, so I integrate with respect to G, they are going to stay close at time t. If initially some observables, this I don't want to enter into the details, but these are objects associated to the initial data, are Associated to the initial data are close enough. So, if some observables of my initial data are close enough, I can also show that my solution at time t will stay close enough. And then the proof is essentially done if we also know how to pass through the limit on the initial data. So, with respect to the initial data, we have a compactness argument. So, if we have any couple of, if we have any sequence of graphons, wnth, or extended graphons in our new class. In our new class, and Fn in our regularity class, which have uniformly bounded norms in their natural norms. Then, what we can prove is that there exists a couple of limits, W and F, still lying in the same class, so that they are observable, which I miss, I didn't want to introduce the exact expression because they are arbitrary, but they converge. And then we can put everything together. So I have my particle system, I compute first. Compute first a limit through the compactness argument before for my sequence of graphs and my sequence of particle system. Then the initial observables are going to converge and then if my initial observables are going to converge through the stability estimate I can prove that the average with respect to g of my initial f and my average with respect to g of my particle system essentially is going to converge to zero and then I use the The triangle inequality. So, since I want to compute the varsity standard distant spherical measure on my average with respect to the continuous variable cheek, then I can just use, as I was saying, triangle inequality. I just compare first the distance from the empirical measures to the average of my FI bar, and then I compare the average of my FI bar with the average of my continuous limit by F. This first part is going to converge to zero thanks to the previous. converge to zero thanks to the previous result. The two systems of the xi and the xi bar essentially are identical when the amount of particle goes to infinity. And this second part is also going to converge to zero because of the stability estimates. I have that two couple of solutions to the general Leiblassov equation are close enough if my initial data were close enough. And this was what I proved at the very beginning. So I'm sorry that I went a bit quick over the proof. I was understanding a bit a different I also understand a bit different from the type of methods that you may be used to, but I'll be happy to actually answer this, I can't skip because actually I already mentioned, I'll be happy to answer 20 questions. So thank you so much. Thank you very much. So time for maybe one short question. Mark? R? The limiting equation this way so for the limiting equation for when n goes to infinity are our new equation, right? Yes, and the reason is we have this stability testing. So let me come back. I went very quick through it so I couldn't emphasize on the full information that we can get from the stability estimation. We can get from the stability estimate. So here, what I'm saying is that I can compare some appropriate LP norm, in fact, it's sort of L1, L2 norm, between any couple of solutions to my general aglosov equation, if I know how close the initial data they are. So if the initial data are in fact identical here, it's actually I'm getting zero, and the two solutions are exactly the same. So not only we have uniqueness. We have uniqueness. We also have this full stability estimate, and then if I put as f my continuous solution to the general Aiblasov equation, and here I put my fn, so the displayed approximation, I also have convergence, as long as I have it initially. This is what I was trying to explain. I I could reduce or we could reduce our problem to just knowing whether those observables initially converge. Robots initially converge, and this is done through a compactness resource. So I think that it's time to thank our speaker again. And now there will be a coffee break here in the main hall, and we will resume in personal 30 minutes with our very small live talk. So  I guess I can talk about it.  