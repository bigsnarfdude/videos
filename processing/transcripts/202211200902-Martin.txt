It turns out that there's a lot of comparative prime number theory we can do before even talking about arithmetic progressions. And in fact, some of the ideas are a little bit easier, some of the mathematical techniques and ideas are a little bit easier to see before we get there. So the first third of this talk, well, for some value of one-third, is actually going to avoid arithmetic progressions altogether. We're just going to talk about the prime number theorem and the error in the prime number theorem and the relationship to the Riemann Zeta function and a couple of other related problems that I think are definitely part of comparative prime number. I think they are definitely part of comparative prime number theory. And then we'll talk about primes and arithmetic progressions in this second third here. And then future goals is the third third. I have no idea how long this talk is going to take. So either this will be the talk or this part will be the talk and we'll save this for discussion. I'm just going to see how it goes. So don't be scared that you don't see atomic progressions right away. They're coming, I promise. But we're going to start in probably a very familiar place with humans. Humans like to count primes, and so when we count primes, we count count counts. And so when we count primes, we can use this function pi of x, literally just the number of primes up to x, the sum of 1 over all the primes, the sum of the indicator function of the primes. But my philosophy, and not just mine, is that nature likes to count prime powers. Nature prefers this function, psi of x, which is the sum of log p over all numbers of the form p to the k up to x. Or we can write this using the von Mengel lambda function, the sum of lambda of n of n of x. And that function is what makes this an equal. And that function is what makes this an equality. Lambda of n is log of p if n is some power of the prime p, including p itself. And it's zero if lambda of n has at least two distinct prime factors. And actually, since we're going to see the notation capital omega of n later, I might as well give us a chance to remember. Capital omega, oh, sorry, I actually mean little omega in this case. Well, then it's your size. Okay, never mind. Well, I might as well write it up. Little omega of n is greater than or equal to 2 in this case. Is greater than equal to 2 in this case. But we'll take capital omega later. Why do we say that nature likes this function psi of x better or this function lambda of n better? One big reason is that if we look at the Dirichlet series corresponding to the von Mengel function, this is a really nice Dirichlet series. It's related to the zeta function, which just makes us emotionally happy, but also it's negative zeta prime over zeta. It's the logarithmic derivative, and that has really nice properties. It's got simple poles at all the zeros and poles of zeta. Poles at all the zeros and poles of zeta, and it's got residue one at all the zeros. More precisely, if it's a zero of multiplicity k, the residue is k. I guess with the negative sign, it's negative k. But we usually write the lists of zeros and sums over zeros, and we just say count it with multiplicity. So that zero occurs k times and has residue one will get us to the right answer. So this is a nice function to analyze with nature, but we really kind of care about pipex in our hearts. And so when we go from side to side, Our hearts. And so when we go from psi of x to pi of x, we have to do two things. One of them is we have to remove the weight log of p. Partial summation is the way we do this. And this affects the formulas we write down quantitatively, but maybe not in a horrible way. Often we have some formula involving psi, and we just end up dividing both sides by log of something like that. So that's fine. But we also have to take into account that psi of x is counting not just primes, but squares of primes, cubes of primes, and so on. And we want to remove those because we care about primes. Because we care about primes. And in particular, it turns out that removing the squares of primes has a really big impact. And this has a qualitative impact on what happens in comparative prime number theory. It causes many of the biases that we see. So this one is just like, ah, you know, we make the analysis work out, it's fine. But this one changes the actual qualitative answer, and we'll talk about that as we go. Nevertheless, it's useful to start talking about how can we analyze psi of x. I'm skipping a very long story. I'm skipping a very long story here. Let's jump straight to the explicit formula. So the explicit formula relates psi of x, this prime counting function with weights, prime power counting function with weights, relates it to the zeros of the Riemann zeta function. So in fact, this main term x comes from the pole of zeta das at s equals 1. And then every non-trivial 0, so those are zeros in the critical strip with real part between 0 and 1, gives us maybe a 1 times x. 1 times x to the rho over rho. x to the rho over rho is some Mellon transform weight. And 1 is the residue of that 0 with the negative sign outside. And then there are some trivial zeros as well, which are completely irrelevant to the story I'm telling. So we're just going to ignore the universe. So this is an explicit formula. And if one makes this, if one writes this exactly out, you really get an equality here, which is amazing. It's one of the most magical things in this whole subject. But for us, this approximation is fine. So the psi of x minus x is x. The psi of x minus x, the error term in the prime number theorem, is an explicit formula. It's a sum over zeros. And let's try to look at how that sum behaves just in gross terms. If we write the generic zero row of zeta of s as beta plus i gamma, so beta is the real part, beta then will be between zero and one because we're talking about non-trivial zeros. And x to the rho is then x to the beta times x to the i gamma. Here's x to the beta, and here's x to the i gamma. We can write it to e to the i gamma log x. e to the i gamma log x. e to the i something is just some number on the unit circle. And in fact, if we think of log x as the variable, then this thing is just running around the unit circle at constant speed as log x increases, which will be important on the next slide. But the size of this term here is dictated by the real parts of the zeros. And so one way we can measure that is we can define theta, capital theta, to be the supremum of all the real parts of all the zeros of zeta s. Of all the zeros of zeta s. There are definitely zeros on the half line, Riemann tolest that. So this is at least one half, and it's at most one by the definite, well, the definition of critical step, but also all the stuff we know about the zeros of zeta function. So this capital theta will always denote the supremum of the real parts of the zeros. And one thing we know, for example, is that the error term psi of x minus x gets as large as x to the capital theta. Well, let's throw a minus epsilon in just to be safe. But it gets both that large with positive sign and with negative sign infinity. With positive sign and with negative sign, infinitely often. So, this was Landau's theorem is the typical tool that we would use to derive this. And if we have this collection of zeros, which a priori might have lots of different real parts, the ones that matter for the error term are the ones that are farthest to the right. So if we wanted to, we won't see this equation again, you'll have to memorize it, but just to give an idea, if we say, let's look at all the zeros up to capital theta, but then cut off at some smaller theta, which might be very close to capital theta. Which might be very close to capital theta, but strictly smaller. So here's a little theta, then we can ignore all of the zeros with real part less than or equal to little theta at the cost of an error term that looks like this. So this thing is going to have size x to the theta, which might be near x to the capital theta, and this thing is a little bit smaller. So great, but of course what this, even this on the left, this might even, violently many zeros out actually be kind of amazing in some ways. Scary, but amazing. This might, this. This is still somewhat hard to analyze because the behavior of this depends on whether these real parts sometimes equal capital theta or not, or whether there's a sequence of betas that increase to capital theta or not. At least one of those must be the case by the definition of supremum, or both, but how you analyze it and what the answer you get depends on the situation. Kind of complicated. People have done work like this. Good for them, that's hard. But a lot of the work that we do in comparative prime number theory, we just start out assuming the Riemann hypothesis. Just start out assuming the Riemann hypothesis. And if we assume the Riemann hypothesis, this gets a lot simpler because beta is always equal to 1 half. So we can literally factor this out. And in fact, on the next slide, we're going to subtract x from both sides and divide by x to the 1 half and we're going to leave ourselves with a normalized error term, which we'll call capital E sub psi of x. Normalized error term in the prime number theorem pi of x minus x over root x. And this is just the rest of the explicit formula, so the each of the. Rest of the explicit formula, so the e to the i gamma log x, the ordinates, the imaginary parts of those zeros pop up there, and then some error term that won't concern us for this story. This particular form is quite small. And one of the philosophy that one can take in comparative prime number theory, it's not necessary, but I like to take it, is to model this function, which is a real deterministic function of x, by a random variable. The idea being that this term here just looks like it's running around the Term here just looks like it's running around the unit circle lots and lots of times. Why don't we replace that by a random variable that literally samples a point uniformly from the unit circle? So that's what x sub gamma does. If we literally replace this by x sub gamma, we get something like this. We turn on our brains a little bit to note that if gamma is an ordinance of a zero, then negative gamma is as well, like Schwartz reflection principle. And the fact that e to the negative i gamma log x is the complex conjugate of e to the i positive gamma. conjugate of e to the i positive gamma log x means that we probably shouldn't have two different random variables. x sub negative gamma should be the conjugate of x sub gamma. And so when we change this to a random model, we get a random variable x sub psi, which becomes this, but as soon as we pair gamma with negative gamma, we get a sum over all positive ordinates of the Riemann Zeta function, twice the real part of this uniform S on S1 random variable, and then the magnitude. Note that instead of rho here, we just have absolute value. Instead of rho, here we just have absolute value of rho because the argument doesn't care. We're sampling uniformly from this is a random model for x sub psi. And of course, you can make models all day long, but we want to say that they actually have similar behaviors. And one thing that we should all say, and our probability students would be quick to point out, is that we haven't actually specified what this means yet, because we haven't said the relationships among the X sub gammas. The default assumptions that things are probably The default assumption is that things are probably independent if no one told me, but we should say so. But this actually encodes some non-trivial information. If we imagine, for example, that we have gamma from one term and two gamma in another term, then these sines and cosines are very tightly correlated. Cosine of two theta, cosine square of theta minus one. Choose cosine square of theta minus one. Yes? Why do we choose uniform distribution? Why do we choose uniform distribution? Imagine for a second that log x is our variable, then this, let's call it y, e to the i gamma y, for e to the i gamma y for a gamma fixed, as y increases, it's literally going around the unit circle at constant speed. And if we take larger and larger intervals of y, we're going to go around the entire unit circle a bazillion times and maybe a little bit extra, but that's approximating a uniform distribution as we take larger and larger intervals of y. That answer the question. Right, right. So it has to do with the fact that. Right, right. So it has to do with the fact that if we literally look at the values of this function as x increases, log of x increases, in real time it goes over the unit circle at constant speed. Yeah. So it's constrained to the unit circle rather than a two-dimensional thing because of the form. And then the constant speed is what tells us we get a limit in the limit. In the limit, we get a uniform distribution. Yeah, I think that's the motivation. Okay, we were talking about independence of these variables, and we were talking about how if there was a gamma and a two-gamma, the A gamma and a two-gamma, they would not be independent, but we suspect that that doesn't happen. Our suspicion might just be based on what we believe an orderly universe would do. It's kind of hard to test these things, but we believe that there are no rational linear relations among the ordinates of the Riemann zeta function and many other L functions, but we'll get there. It's a hard thing to test, even for like the first two zeros, right? It's an infinite calculation to decide whether they have their. To decide whether they have rational multiples of each other. But this is something that we believe. We believe, I don't know, maybe that's kind of a strong statement. You all can believe whatever you want. But it is a conjecture, the linear independence conjecture, so we'll just call that LI in this talk, that the set of positive ordinates of the Riemann zeta function is linearly independent over the rational numbers. And what this corresponds to is saying, yes, these different x gammas for different positive gamma. X gammas for different positive gamma are really independent random variables. The whole collection is independent random variables. So this is the kind of sum that of an infinite sum of independent random variables that one would see in probability, and it will behave like we want. If a wise alien came down and gave us a list of every single rational linear combination among the zeros, we could figure out what the random variable is, but it's very hard generically to sort of incorporate maybe those linear relations. So this is what we're going to do. So, this is what we do. And it turns out, through tools like the Kroniker-Wau theorem, that we really can say if we assume, first of all, the Riemann hypothesis to factor out the square root of x, but also the linear dependence conjecture that tells us that the e to the i gamma log x's act independently in the limit. That's what the Kronik-Raval theorem says. We really can say that the limiting logarithmic distribution of this normalized error term, by the way, logarithmic distribution, because log x is the variable that. Because log x is the variable that we have to do to make this conversion I say. The limiting log distribution of the actual prime number error term is exactly the same as the distribution of this random variable. And this is nice because we know things about infinite sums of random variables. Yeah, question. No, so we'll actually come back to this, but this is, you know, how when we have conjectures and it's almost a little fun. And it's almost a little fun how embarrassingly little we know about some of them. This is an incredibly interesting embarrassment that we know basically nothing about the algebraic character of these ordinates. So we'll come back to that a little bit, but the answer is kind of like if you want to know something, we don't know it yet. We have a CRG, and one of the things we get to say is, what would we like to see proved in three years? Let's just add that to us. So we'll come back to that later in the talk at some point. That's a good observation. Okay, so here we have an infinite sum of random variables that are independent. There's lots of tools nowadays at the beginning of the subject that are making this all up as they went. But nowadays we can just say, oh, look, compute the Fourier transform of that limiting distribution, or in probability terms, just compute the characteristic function of this random variable. It's going to be a product of a bunch of individual, a product of a bunch of individual characteristic functions. This is twice the real part of an S1 uniform thing. The real part of an S1 uniform thing gives us Bessel functions if we care. So, if you spot them in the literature, that's where it comes from. But the point is, we can write down the Fourier transform, and in principle, that has all information we'd ever want to know about the limiting distribution, and in practice, we can actually extract a lot from it. So, for example, and this isn't exactly what Montgomery did, but this is a nice opportunity to mention that one of the conjectures in this field of comparative prime number theory is: how big should this normalized error term get? Square root of x times something, right? But how big should it get? Thing, right, but how big should it get? And Montgomery, there's a Montgomery's conjecture where we use tail estimates for this particular random variable. What's the probability that it's bigger than k for some very large k? And then Montgomery used that information to conjecture that here's the normalized error term. And if you divide by log log log x squared, then that should be the maximal order of the oscillations of the error term. And in fact, with even specific Limb C and Limit. Limpsi than Limit. This would be a wonderful conjecture to be able to prove even conditionally. We'll come back to that in our aspirational sections. But in principle, if we assume RH and Li, we have access to a whole lot of information about the limiting distribution, which is really nice. Okay. Well, this was all about psi of x. What happens if we go back to pi of x? Remember, we have to do two things. We have to remove the logarithm factor. That's actually partial summation, and it doesn't really affect much on this term here. Doesn't really affect much on this term here. There's a slight typo in the post-it slide, which I apologize for you following along. But we get the exact same sum in the explicit formula. Sorry, let me look at the left-hand side first. Instead of psi of x minus x, we get pi of x minus the logarithmic integral, which we saw in the people's talk yesterday. And then instead of dividing by root x, we divide by root x over log x. This is the normalized error term e pi of x. And most of it looks exactly the same as the normalized error term for e psi. Normalized error terms for E psi of x, this error becomes a lot worse, but it's still the world of one. It's good enough for us. Except, so that partial summation did that for us. But then we remove the squares of primes. And the number of squares of primes up to x is the same as the number of primes up to square root of x, which is asymptotically exactly root x over log x by the prime number theorem. So when we remove the squares of primes, that gives us a negative one, a constant bias in this random variable. Now, the random variable. Now, the random variable x is just the same random variable you saw, except minus 1. I didn't sort of describe qualitatively what the random variable's distribution looks like, what the density function looks like. It's roughly bump-shaped, symmetric round zero. It's not normal, but it's some specific distribution. It's roughly bump-shaped. It actually decays faster than normal at the tails. And this negative one causes the axis to shift, and most of the mass of x sub pi is negative. In other words, the limiting distribution of pi minus lie is heavily. Pi minus li is heavily skewed towards negative numbers. And then back in the day, people noticed that pi of x looked, it was asymptotic to lie of x, prime number theorem, gay, 19th century math, but it seemed to be less than li of x for as far as we could compute. We learned yesterday, I think 10 to the 19 is how far we can compute it. 10 to the 19? Oh, yeah. Back in the day, it was like, what, 10 to the 3 or something? But anyway, they said this is enough information for conjecture. So people are like, you know, maybe pi of x even knows asymptotes. You know, maybe pi of x, even though it's asymptotic to live x, it's always less than li of x. Turns out that that's not the case, but I mean, even if they've gone to 10 to the 19, they could be forgiven for thinking so. But that's caused by, the prevalence is caused by negative one. The fact that you don't see any counterexamples right away is just a little bit how the universe is organized. There's a famous theorem of Littlewood that I think was a surprise to a lot of people in the field: that you can use diaphragm approximation to show that. Diophantine approximation to show that this sum takes macroscopically large values, so greater than constant times log log log x. And roughly speaking, what you do is you use, again, use Diophantine log approximation to find a value of log x so that lots of these pi log x's are close to a multiple of 2 pi or something slightly different than that. They all point in the same direction, and they give a large contribution that the tail of the sum doesn't always cancel. And so Little would actually prove that pi of x, first of all, this already says something interesting about the psi of x minus. this already says something interesting about the psi of x minus x, but even more surprisingly, more interestingly, it says that pi of x is greater than li of x infinitely often, because log log log of x doesn't grow that fast, but it grows faster than negative one. Maybe, I've been told. So this really does become positive. And Rubenstein Starnack had this seminal paper in 1994 called Chebyshev's Bias. And they actually used the random, they didn't say random variable, but mathematically equivalent to the random variable, and they literally To the random variable. And they literally said, How can we estimate the mass of this probability distribution that's less than zero? And it turns out to be a number that to eight decimal places, rounded to eight decimal places is 0.99999974, something like that. And this was a rigorous calculation in the sense that when we take the continuous thing to the discrete thing, they bounded all the errors and everything. So if we trust our computers, we trust this number. So the probability that x sub pi is negative is this, and what that means is. Negative is this, and what that means is if we assume, by the way, this is an unconditional statement about a random variable, but if we assume the Riemann hypothesis and the linear independence conjecture, that tells us something about the set of positive real numbers x such that pi of x is greater than li of x. It actually has a positive logarithmic density, thinking of log x as a variable, and that density is like 2.6 times 10 to the minus 7. Pretty darn small. And we talked about skews number in regions where this actually flips. So a whole So, a whole other interesting topic. A lot of you know this already, but just to emphasize everyone who's watching, the bias is caused by removing squares and points. That's where it came from. There is no bias for psi of x minus x. That's the 50-50 positive and negative. The same techniques we'll show that. Ruben-science starting to show that. Before we get to arithmetic progressions, there's a couple other things related to the Riemann zeta function that we might as well talk about. There are some classical conjectures in the literature, which I think are part of comparative prime number theory. Which I think are part of comparative point number three. And they involve at least these three sums here: the Merton sum, m of x, is the summatory function of the Mobius function, mu of n. What is it, the Liuville sum, I guess? The sum of, sometimes called lambda of n, but negative 1 to the number of prime factors of n counted with multiplicity, which is kind of like mu of n, but for all numbers, not just square 3 numbers. And then also the same function, but weighted by 1 over n. And people had made computations of this. And people had made computations of this, not 10 to the 19, but and observed certain numerical observations of these that they wondered if they continued. So Merton's conjectured, for example, that the absolute value of m of x is always less than or equal to the square root of x. Roughly, maybe square root cancellation between the ones and minus ones, reasonable thing to think. The initial values of l are all non-positive. In fact, whether equal to zero has something to do with the class numbers, which I Equal to zero has something to do with the class numbers, which I've heard many times in my life and I don't understand. Maybe one day I'll understand it. And Paulier wrote a paper that said if this is actually true, then we get some implications, which I'll say in just a second. And then also, Turan noticed that if, so numerically, this particular function with the reciprocal weight always seems to be non-negative at first. And Turan also noticed some implications of that. And often in the literature, this statement, the yes. This statement, the yes answer to this question, would be called Polya's conjecture and Turan's conjecture. They did not conjecture this. They noted implications of it and they said, so it's interesting to study it. And I think they had a good instinct that they should actually be false, so it's a little mean to call their conjectures. So what is the relationship of these quantities to things we care about? Any one of these three yes answers would imply the Riemann hypothesis, basically by Lambda's theorem. It also would imply that all the zeros are simple zeros, which is... All the zeros are simple zeros, which is interesting. But Poya and Turan pointed out that they would also imply something else that maybe we don't think is true: that there would be infinitely many rational relations, rational linear combinations of ordinates of the zeros of zeta that equal zero. So infinitely many q rational linear relations. And that seems a bit unlikely. And not even just one or two, but infinitely many. So if we believe Li, then we don't believe any of these conjectures. These are all now known to be false. These are all now known to be false. Hazelgrove, I believe, disproved the first two, and Adlis go into reala the second. No, not the second turn. The last one, please. Oh, sorry, you're right. Yeah. So M of X, thank you. So Edlisco and Turila did M of X, and it has a growth earlier did the polio M to Ram problems. Thanks. The slight weakening of the Merden's conjecture that this is less than less than root X, this is still an open problem, but if you assume RH and Li, it's not. Assume RH and Li, it's not true. I mean, we know that it's inconsistent with RH and Li, and so we still believe it to be false, but that actually hasn't been resolved yet. And one can say, you know, what's the biggest constant we can get, like, in terms of the oscillations, and that's the subject of ongoing research. There are explicit, so okay, why is this in this section? Is that there are explicit formulas for each of these functions that are quite a bit like the explicit formula for psi of x. These Dirichlet series all have something to do with zeta of s, the Mogus function, of course. To do with zeta of s, the movie's function, of course, is just one over zeta of s. This one turns into zeta of 2s over zeta of s, and then this one is just that shifted by 1. And each of those gets an explicit formula, which comes from contour integration. We're picking up residues at various poles. There's always an x to the rho over rho floating around, or after the shift change of variables. But then instead of the log derivative that we saw earlier, having a residue of 1 at every pole, now we have a residue of, well, what do we get here? It's actually 1 over zeta prime of rho. It's actually 1 over zeta prime of rho, or that times zeta of 2 rho, and so on. So all these explicit formulas, again, just come from all the residues of the non-trivial zeros, even the trivial zeros, if you want to write it this way. And then zeta of 2s has a pole of s equals 1 half, and that actually leads to a main term for l of x, of size x to the 1 half, and then this denominator is actually a negative number. zeta of 1 half is negative. So L of X does have a negative bias. This is a distribution that's symmetric. Is a distribution that's symmetric around the origin with a negative bias. So there is some reason for this to happen a lot, but we don't think it happened. Well, now we know it doesn't happen all the time. And similar for the reciprocal one, now with the shift there's a pull at negative one half and it turns back into a positive bias when we look at the residue of the pull. And maybe it's important to point when we get psi of x equals x minus stuff, that x itself is a residue of the pulse, x to the one over one. That rho equals one. So, these are the same explicit formulas, just written in larger font. So, if we want to analyze these distributions and say what's the, you know, what's the density of values for which these are positive or negative, or how big do we think the tail estimates get, we're going to use the same techniques. There's one additional complication, which is a reasonably serious complication. We don't know as much as we'd like about it, is in addition to Riemann hypothesis and linear dependence hypothesis, we also have these quantities L prime of We also have these quantities L prime of rho, so the values of the derivative of the zeta function at the zeros of the zeta function. And the behavior of that is something we'd like to understand better. We have reasonably good conjectures, but we haven't proved everything we'd like to prove. So if we talk about discrete moments of zeta prime, especially one over zeta prime of rho, this is one of the contexts in which we care quite a lot. I also wanted to point out that there was a paper of Mossinghoff and Trudge, one or two papers, where they said, you know what, this is the Liouville function. The Liouville function with over n to the 0, and this is the same function over n to the 1. What if we look over n to the alpha for all the alphas between 0 and 1? There are similar explicit formulas. Alpha equals 1 half is slightly special, I think, because there's a double, zero, double pole somewhere. But they've studied this whole family of interpolating sums. And I'm going to refer to this at some point in the talk, so I just thought I'd mention it. So this is an incomplete description of the kinds of things we can do in comparative. Of the kinds of things we can do in comparative prime number three before even getting to primes and arithmetic progressions. We're about to get to primes and arithmetic progressions, but let me pause and say: are there any questions or comments? Just a five. All right. Is there a quick intuition for why they imply an infinite number of vibrations of Li? I'll try. I think I might be able to answer that. So if there are no vibrations, So, if there are no violations of Li, then all these random variables are independent. Let's say there are 12 violations of li that involve 92 of the zeros. So, we take those 92 zeros out, and there's some complex, I mean, they're still just the sum of these x gammas. There's a complicated dependencies between them, but that whole thing is just some bounded random variable that if we really want to, we try to understand. But then the whole rest of it looks like an LI-type random variable. So, we have something a little hard to understand, and then the convolution of that distribution. And then the convolution of that distribution with something that's nice and bum-shaped and has infinite support or whatever, and the convolution actually makes things nicer in general. So that's basically the reason. And this is what people like Paula and Turan observed. So that kind of means like LI, just need LI on sort of. Absolutely, yeah. And that's also something we're going to talk about later in this as well. That's exactly correct. I mean, we don't want to need any assumptions, but yeah, can we, we can li based on this idea that we don't need. LI based on this idea that we don't need a lot and we're going to definitely get there for sure. That's a nice observation. Any other questions or comments when you all check? You know, how many, how often on SATA2? How often, sorry? SATA2 merches? Yeah, that's so this is, I guess, you even mentioned this, I think, Nathan, unpublished work with Micah and Bill. I mentioned it yesterday. Yeah, so there's some unpublished work which we'd love to. Unpublished work, which we'd love to make better and have published and bring people on in the spirit of CRG. But yeah, to understand, take a specific linear relation like gamma 1 plus 2, gamma 2 equals 0 or something like that, and try to say at least it doesn't happen all the time or something like that. So we've got a little bit of unpublished progress in that direction, but I think there's a lot that could be done there. I don't know if you want to add that. Data two rows on the one line should never be seen. So I guess that would be the same. Probably what we mean is one half plus two i gamma, not literally two rows. But there is an older result. But there is an order resulting actually on this. Yeah, so that's talking about a fixed arithmetic progression on the line, and we'll mention that as well. Yeah, for sure. So we're doing a nice job of anticipating some of our future goals. It's like, can we prove anything that's kind of in the direction of LI? And I think that's exactly what we'd like. Great. It's good to see with energy. I'm going to call on volunteers to take this comes. All right, primes and arithmetic progressions. So one can give a whole hour. One can give a whole hour-long talk just on the numerical observations. That's a fun thing. It's fun to talk to undergrads. I'll give you the boring version. I'll give you the quick version because you're all very advanced. So, Chebyshev in 1853 or something noticed that if you just look at small primes, there seem to be more that are 3 mod 4 than a 1 mod 4. If you have two different counting functions and just plot them with step functions on the same axis, sometimes they're tied, but there was never a point where there were more 1 mod 4 primes than 3 mod 4 primes. Now we know in the 50s they discovered the first time. Know in the 50s, they discovered the first time that that's the opposite, that there are more one mod four than three month four primes. It's two, six, eight, six, one is the first place, and that's the first of a pair of twin primes, so it immediately gets tied again and then this is fair. So that's a whole great story. But this kind of observation that even though, I mean, we now know the prime number theorem and arithmetic regressions, that these are asymptotically the same, but that doesn't rule out maybe an inequality. I mean, they're asymptotically as many odd images as even images. There's a pretty good inequality you can prove. And that's noticed not just to. And that's noticed not just to the modulus 4, but to lots of small moduli. So, if you just look at the data, and this, again, all of us and undergrads would come to the same conclusions. There are more 2 mod 3 primes than 1 mod 3 primes, at least with finite things that we can get our hands on. Mod 7, 3, 5, and 6, those three residue classes seem to have more primes than 1, 2, and 4. If we look at mod 8, poor 1 mod 8 is just in fourth place in this race, and these three are dropped. Place in this race, and these three are jockeying for the podium positions. So, what's wrong with that? Mod 10 is a great one because it's just the last digit of primes. There seem to be more primes that end in three and seven than primes that end in one and nine if you look at finite amount of data. Mod 12, it's a similar thing. There are four contestants, but only one mod 12 seems to have some burden that it's carrying. And if one is sufficiently versed in elementary number theory, we notice that all of these green numbers here, the ones that seem to have more primes, these are the non-squares to their respective modules. These are the non-squares to their respective moduli. And these red burgundy numbers that seem to lag in these races are the ones with squares. And that certainly includes one mod 4 and 3 mod 4. All the odd squares are 1 mod 4. And given what we've talked about before, we maybe think we know a reason why, in hindsight, that wasn't the first thing that people thought of, but now I think we know the story. And it's fun to just do this, look at some of this data. And it really, it's not subtle when we see this, especially for the A. We see this, especially for the 8 and 12, which is super not subtle. That poor 1 mod 8 and 1 mod 12 and just feel pity for it. Anyway, let's have some notation. We're going to count primes in arithmetic progressions. So pi x qa is just the number of such primes up to x, the ones that are congruent to a mod q. Again, just summing the indicator function like us humans would like to do. Or we can do psi of x qa, which is summing the von Mangelt lambda function over all the integers in that arithmetic progression, which is anytime I have a prime power that happens to be congruent to A mod q. prime power that happens to be congruent to a mod q, I give myself a log of p, similar relationship. And now if we're going to go from nature's psi to our pi, because the partial summation doesn't really affect too much, but we're removing squares of primes, but the primes whose squares we're removing, those primes are not congruent to A mod q. They're congruent to some other b mod q where b squared is congruent to a mod q. And sometimes there's none of those, right? If a is a non-square, that's the definition, but there are no such. Sometimes there's a lot of them. Sometimes there's a lot of them. Not if it's a prime modulus, but in general, there might be a lot. So we actually are going to want to count the number of square roots of that residue class, the number of B mod Q such that B squared is congruent to A mod Q. And then for normalization reasons of how we write this down, this is usually defined with a negative 1, negative 1 plus the number of square roots. There's only two possible values for this. If I'm a non-square, I get a negative one. And if I'm a square, all of the squares have the same number of square roots. It's the same as the square root. Same number of square roots. It's the same as the square roots of negative, oh, it's the square roots of one. They're all actually cosets of that subgroup. So there's only two possible values for this CQA. But using this, we're going to get a contribution of like negative CQA when we go down to primes. Okay. And then there's also an explicit formula for primes and arithmetic progressions. Dirichlet said, hey, name these L functions after me and then use them to detect the arithmetic progression. And so there's going to be phi of q L function. And so there's going to be phi of q l functions mod q, and we have some linear combination of explicit formulas that all looks like the same x to the rho over rho. So we can use them to detect the residue class a mod q, and this is an explicit formula we get. The main term is x over 5q. All 5q qualified residue classes get the same number of grounds asymptotically. And then the normalized error term is an explicit formula, which for our purposes is really not that much more complicated than the original one, which is more symbols to write down. Symbols to write down. And if we're interested in inequalities between two of these prime counting functions, say resb classes to the same modulus, if we look at their difference, the main term cancels out, which is why it's surprising that there might be a bias. And then we just get some coefficient. Well, we multiply out by phi of q, and we get some coefficient like chi bar of b minus chi bar of a, something like that. Well, not something like that, we get exactly that. And so this may be e psi of xqab, the normalized error term up to x, if we're counting primes congruent to a mod q and b mod q is prime. I'm going to A mod Q and B mod Q is quite. And this has a random model which is every bit as trustworthy as the other random model. You can decide how trustworthy that is. Replace each of these on the Riemann hypothesis. I guess now it's the generalized Riemann hypothesis for Dirichlet L functions. We can factor out an x to the beta and x to the 1 half. And then the remaining x to the i gammas, we can just model by these random variables here. And now to get the twice the real part, I guess, if gamma is an ordinate of the 0 of ls chi, then minus gamma is. Of the 0 of ls chi, then minus gamma is an ordinate for ls chi bar, but we have them in the sum in the right way, so they do combine. So this is the random variable for x sub psi, and like we don't know what the zeros are, so it kind of looks the same to us. For x sub pi, we would need to add a constant to the right-hand side. And note that if a is a non-square and b is a square, this constant is positive. So that means if we're asking if pi qx a is bigger than pi x q b, there's a positive bias. And if contrary wise, And if you know, contrary-wise, there's a negative bias if we're trying to make a non-square residue class who's bigger than a non-square. So, this is kind of the analytic reason why Chebyshev's bias exists: is nature is 50-50 races, but when we throw away squares of primes, we prefer non-square residue classes. We didn't hurt them, but we hurt the square residue classes. All right. One way we can measure the strengths of these bias, and we actually saw one of these with pi versus live, is we With pi versus y, is we look at the set of real numbers, say the set of positive real numbers, or set of real numbers greater than one. No, this is a typo as well. Too many letters floating around. The set of positive real numbers that satisfy the inequality we care about. So pi of t qa is greater than pi of t qb. And then we can look at that set between 1 and x, and then integrate it against 1 over t, because secretly we understand that log of x is really the variable, or you could make that change of variables explicit, or you could just put in this log of the metric. But you could just put in this logarithmic measure if you want. And then that gives you a measure on a finite interval from 1 to x and take x to infinity. So this is literally the logarithmic density of a set of fields. It's the natural density of the log of that set. Maybe it exists, maybe it doesn't, but let's call it delta sub pi, pi for the prime time function, pi qab. So this is, if it exists, it's the probability in the number theorest sense that we can't ever do probability, we have to take limits, the probability that a randomly chosen real number will see more primes congruent to A. Real number, we'll see more primes congruent to A than congruent to B1Q. And Rubenstein and Starnack treated this case as well. They named their paper after it. Still, assuming the generalized Riemann hypothesis and the linear dependence hypothesis for ordinance of these zeros of Dier Schley L functions, all these densities, logarithmic densities exist. The natural densities actually don't. The logarithmic densities do. And they're strictly between 0 and 1. So each of them occurs sometime. Also, if you do the A and B and Also, if you do the A and B and B and A, they add to one, which actually does have some content. It says that ties have density zero under these hypotheses. And they also showed that this density is strictly greater than one half if and only if we're racing a non-square versus a square. So this is really a justification, conditional, but a complete justification of Chebyshev's bias, the way we observe it. They also pointed out that we have an idiosyncratic random variable for every prime. syncratic random variable for every prime number race, but if we let this family of random variables, as we take it as q goes to infinity, we really do have a central limit. If we suitably normalize it by a particular function of q, it goes to a standard normal random variable. And in particular, these densities do tend to one half as q goes to infinity. And if you're someone like me, and Daniel, if you're really, you like how fast does this happen, and we actually, this is the next slide, we actually investigate. We actually investigated this issue. So, this is, except Danielle and myself, still under the same assumptions. We have to assume GRH and LI. I mean, you can prove whatever you want about the random variable, but it takes GRH and LI to say this is the same as the actual prime number in data. We actually gave an asymptotic formula for how fast this logarithmic density approaches 1 half. Let's assume that A is a non-square and B is a square, so it's going to be 1 half plus something. And we gave an asymptotic formula. And we give an asymptotic formula, which is the number of square roots of 1 mod q divided by 2 root pi, and then the square root of phi of q mod. So it goes roughly like 1 over q to the half. It's the speed at which these densities approach 1 half. And we actually did a little bit of explicit, I mean it's conditional, but a little bit of explicit number theory. We actually calculated of all the two wave prime number races in the world, there's exactly 117 densities that are greater than 0.9, just as a proof of concept kind of thing. Proof of concept kind of thing. Now, there are some symmetries among these densities. For example, if you race A versus B mod Q and then multiply both residue classes by the same square, it turns out you get the same density. And how do you prove it? You look at the random variable and you just note that there's some symmetries. The absolute value of chi of B minus chi of A doesn't change. So this 107 is up to symmetries, equivalence class of densities, but this is really a list of all of them. If you want to know the worst case, or if you would like to gamble and you want to know the best horse to bet on, bet on five miles. Forced to bet on, bet on five mod 24 over 1 mod 24. That race is 99.9988% likely to come in screw through. You might not get a huge payoff, I guess, but those are the options. And we also, you know, collaborations can be really great when he's like, you have some ideas, you have some ideas, and then when you get together, you have more ideas than you had separately, right? So when we got together and started trying to finish off these ideas, we came up with some other ideas, and it's really a great collaboration. There's some secondary terms. There are some secondary terms, so this is the main term of the asymptotic phone, but there are secondary main terms that help us distinguish between different races modulo of the same Q. So what I'm going to say out loud is almost completely true. One can make it true if you just say more words. But let's imagine we look at moduli for which negative 1, 2, 3, and 5 are all non-squids. So that it makes sense to race them against 1 and ask about the answers. It turns out that for all but finitely many q, when we race negative 1 against 1, that's the least biased of all the races. One, that's the least biased of all the races, followed by racing three against one, two against one, and five against one, and so on. And three comes before two because the relevant weight is, what is it, log of p over p as it turns out. Log of 3 over 3 is bigger than log of 2 over 2 and you can see this numerically if you calculate these densities. If I might say so myself, there's a very pretty picture in our paper, and you can actually observe density. So if you look at moduli where some of these are not squares, you can't look at those. Where some of these are not squares, you can't look at those densities and won't have. But the ones that do exist, there's an ordering to the, there's a, the secondary main term causes the bias to relax a little bit, and that's we're kind of racing races now in this internet. Crack? Yes. Crack? Yes. Is the density missing block t in the denominator? Block x? Oh, yeah, yeah. Thank you. Yeah. Thanks so much. That's completely correct. So this thing itself, if I had the entire set of real numbers, would look like one of the log X, so I got a normal. Numbers would look like one of the loggers, so I gotta monopolize that. Thank you. All right. And I guess we're gonna, okay, so now we've told myself the answer. We're gonna save this stuff for the discussion, so we've got a little bit more primes and arithmetic regressions to go. The Chebyshev's observations were always about two residue classes, right? We have this and we have this, because literally, if you subtract them, you're asking about the sine of a one-variable. Asking about the sign of a one-variable function. But if we think of it as a contestant of a race with lots of contestants, we can say, let's throw more contestants in there. And it turns out there's a lot of interesting stuff one can say about this. So we can look at multi-way races. We take the K different residue classes, mod Q, and we can say, what if I want these K horses to come in this specific order? Most primes A1 mod Q, then A2 mod Q, and so on. Some real numbers will satisfy these inequalities and some won't. Let's write down the logarithm. Let's write down the logarithmic density of that set. So here's the notation: pi for counting primes, delta for density, and then the modulus Q and the K residue classes in order A1 up to AK, where A1 has the most primes and AK has the least primes. There are K factorial possible orderings of that fixed step from A1 to AK. So if life were completely fair, well, we're not assuming it's not, but if it were completely fair, we would expect all of these to be like 1 over k factorial. So that's what we should compare these to. First of all, do they exist or not? Well, Rubenstein and Starnex analysis. Do they exist or not? Well, Rubenstein and Sarnex analysis works just as well for these. These all exist and they're strictly between zero and one. And if you sum all k factorial, then you get exactly one, I guess, as well. My first paper in this subject with Andrei Feuerg at the University of Tirana, we confirmed a suggestion that Rubenstein and Sarnat made in their paper that once we get to three-way races or higher, even if we just stick with racing non-squares, we would stick with racing squares, that where it seems like Stick with racing squares, that where it seems like it might be fair, there can still be inequities in the densities. And they're not actually caused by cubes of primes, they're just caused by multi-dimensional random variables. Normal random variables are weirder than moving. So for example, if you look at the mod 8 race between 3, 5, and 7, mod 8, any two of those is a 50-50 race because they're non-squares, but the three-way race is not a 6-6 to 6-6. The densities range from something like 0.12 to 0.19, something like that. And which actually surprised me. Which is actually surprised to me. I set out to prove that they were. So, how could they not be? We learned things. And then, Gunes, thank you for being here, gave asymptotic formulas for these multi-way races compared when we subtract 1 over k factorial, how quickly do they approach 1 over k factorial? Do they approach 1 over k factorial at all, first of all? So, if we think of k fixed, then again, there is a central limit there, and they're going to approach 1 over k factorial. Now, the relevant distribution is. That now the relevant distribution is a multi-dimensional distribution. Probably k-dimensional is the most natural way to write it down. So we're talking about a bump-shaped function in Rk. But there is a central limit theorem, and Eunice gave asymptotics for how big the difference between the density for a finite Q can be compared to when we subtract more we factorial. And unlike the decay of one over square root of Q roughly that we saw with two erases, here the decay can be quite a bit slower. I mean, it is not every Slower. I mean, not every error term is this big, but there are definitely error terms that are this big, as large as like 1 over log q. So once we even get to three contestants, there's a lot slower convergence to the densities of 1 over k factoil. And then one can ask the interesting question, this is with k fixed. What if we let the number of contestants grow with q? And so Nunos teamed up with Adam Harper later on to show that we still get this asymptotic formula if k is almost as large as log q. As large as log q. But if you let k go faster than log q, I'm quite simplifying what they proved here, and the three of them showed that k factorial times the density can be really, really tiny or really, really big. And they have very explicit quantitative statements to this effect. So even if you have like log squared residue classes, which you might think is quite small to q, the densities can now vary really widely away from the size of 1 over k factorial, which is very, very interesting stuff. Interesting stuff. What might we demand of our time number races? I guess what we kind of feel like from these, both from these results, these conditional results, but also from our sense of fairness or whatever. It's like, all right, we're throwing weight squares at primes. We can't imagine to be completely fair. But we should think that any of these prime counting functions should eventually take the lead sometimes. Take the lead sometimes. In fact, all of these k-factorial orderings, maybe they should all occur. And so there's different levels of demand we might place on these k-wave prime number. The first one is, which we call exhaustive, is literally that each one of the k-factorial orderings happens infinitely often. There's no point beyond which we never get pi xq, a1 bigger than pi xq2, and so on. We can say that the prime number race is weakly inclusive if the density actually Inclusive if the density actually exists. It's a limit, so maybe it doesn't exist. But if it exists, it's weakly inclusive. We'd really like it to exist and be strictly positive. So that's what the inclusive is. Sometimes the methods will allow you to just prove that it exists, and you're like, well, I've got to use something else to prove that they're positive. And then strongly inclusive is actually kind of something about the underlying distribution rather than prime number rates. The underlying distribution is a k-dimensional distribution bump-shaped on r sub k. Does it have finite support? Is it supported in a hyperplane? Is it discrete support or whatever? Is it discrete support or whatever? We think that it should sort of be bump-shaped and be supported everywhere. And that's the property we call strongly inclusive: that the limiting logarithmic distribution of this vector-valued function, this deterministic function, has full support in R to the k. And one of the reasons we give this a name is that usually when we end up proving things are inclusive or exhaustive, it's because we prove it strongly inclusive. So this is something that we end up shooting for, like our tools are going to give us that. And one way to interpret the So, one way to interpret the results of Rubenstein and Sarnak is that every K-wave print number race is strongly inclusive. But can we weaken the linear independence hypothesis? By the way, you can also ask, can we weaken the generalized Riemann hypothesis? And Eunice and Ford and Kanyagin and then joining forces with Eunice gave some really nice constructions to show that it is possible for GRH to be false and it completely breaks prime number erases. Like instead of k factorial orderings happening infinitely often, it's like k squared. Happening infinitely often, it's like k squared or something like that. So you could really break prime number erases if you put grh violating zeros in special places. So, but let's say we think grh is true, can we weaken li? Or in fact, one of the things Nathan wanted to do is to kind of do something similar to what Ford and Condovin and Us did is, can we break prime number races under GRH? So, can we put in some linear relations that make some things come true? It turns out that there's a lot of linear relations the math consideration. There's a lot of linear relations that math can support without breaking, so we ended up proving something in the opposite direction. And the easiest way to state what we proved, and this is the last slide before we take a little break, but the next one's the last slide before we take a break. The easiest way to describe the results is in terms of a self-sufficient ordinate of a zero-voice al-function. So basically it says that this one is linearly independent from the rest. That's not really grammatically correct. But what it means is that this particular ordinate is not involved in Q linear relations of all the other ordinates. It's not in the Q span of the other. Other ordinates. It's not in the Q-span of the other ordinance. So that's what we mean by a self-sufficient ordinate. So if we have a set of 12 self-sufficient ordinates, it is a linearly independent set, but it's stronger than that. It's completely independent in a specific way from all the other ordinates. And this is kind of this idea of if we have some stuff we don't understand, but some stuff we do understand, maybe the part we do understand is enough to give us the results we want. And so Nathan and I wrote a paper. This appeared in 2020, and it took us 9 million years to write. Under GRH, we can get weakly inclusive prime number rates. In other words, we get the existence of these log densities, even if every Dirchley L-function has only three self-sufficient zeros. That's enough to say there's a bunch of stuff we don't understand, but this stuff we're convolving with something we do understand. So the densities exist. Now, weakly inclusive means that it could be zero, and we'd like to say more than that. We want to be strictly positive. And so if we sum the 1 over gamma over only the 1 over gamma over only the self-sufficient coordinates of zeros of a particular, well, I guess here we're summing over all the Dirch L functions mod q. If that sum diverges, then we actually get strongly inclusive. That's enough to give the Lending distribution full support in whatever dimensional space we're living in. And I'd like to point out that if we sum this up to t for all ordinates, we get something that's like log t squared, ignoring the dependence on q. So diverging, there's actually a lot of room to diverging. Diverging, there's actually a lot of room to diverge and not have full density. So, this is actually consistent with 100% of the zeros being involved in linear relations, and you can still get the support being the full support. So, one thing that says that it's really hard to break prime number erases if you assume GRH, but you still get a cool thing out of it. And then Lucille Duvin came along and said, oh, that's pretty cool. Let me see. I'm going to do this better. So, one of the things she did, by the way, was extend all of this to the entire cell bird class of functions. I think it's exactly. Selbert class of functions. I think it's exactly the Selberg class, but if not, it's something very Selberg classy. She also said, even if you don't assume Rh, you can still get limiting distribution statements. Instead of normalizing by x to the one-half, you'd want to normalize by x to the supremum here. Now, depending on whether there's a sequence or whether supremum is attained, this distribution might be degenerate or something. But the point is that she proved unconditionally that this distribution exists, which is nice. And then we thought we were cool, only three self-sufficient ordinates to get. Three self-sufficient ordinates to get weakly inclusive. Well, she cut that by a factor of approximately three. All you need is one self-sufficient ordinate, and you get an absolutely continuous distribution. And in particular, that means the density exists. Any questions or comments about these multi-wave prime number races? Or that would be used as a central limit theorem? A central limit theorem? So yeah, so we have this maybe the easiest place to see it. Have this, maybe the easiest place to see it is we have a sequence of random variables, maybe a little bit weirder than a sequence. We have a sequence of random variables that are indexed by q and then by pairs of residue classes, mod q. We can think of that as a bunch of random variables, mod q, and then q going to infinity. And so each of these random variables, if we look at the, they converge in distribution to the standard normal as q goes to infinity. As Q goes to infinity, uniform in whatever A's and B's that you pick. That's literally true for this one. It's centered at zero. And then it's true for the, well, I guess it's still true for the other ones because the mean is non-zero, but the limit kind of swamps that when we normalize it quickly. So it's the random variables converge to the standard normal distribution, but that also allows us to extract a lot of information, quantitative information, about the size of these densities, or about tail estimates in the limit or things like that. Things like that. If the limit is normal, then when Q is large, it's approximately normal, and that's actually quite a fruitful way to look at these distributions. Great paper of Eunice and Adam Harper that really takes this probabilistic approach to really great heights. So comparing the true prime number distributions to normal distributions, the comparison involves analytic number theory, but then analyzing the normal variable is just probability. I mean, I'm not probability, but and somehow if we separate those two steps, we If we separate those two steps, we can really see where the errors are coming from and where the strong information is coming from. Yeah, I mean, just a quick comment. So, the difficulty here when you have many A's and B's is that each of them is Gaussian. But if you take the joint distribution, then they do correlate. You can see it from here, because you have Kb minus KA, but you have the sum over all zeros. But you have the sum over all zeros. So you might have like zeros in the same set as another random variable with AB different. They are summing over all directly. Difficulties to understand the correlations. That's why the probability was not as easy. It's just. Absolutely, yeah. And in fact, the true central limit. And in fact, the true central limit theorem washes away all these difficulties that we actually have to address. We have to sort of deal with the finite key case for sure. Yeah, so this idea that in three-way races are higher, you can get biases even when they're all non-squares or non-squares. Is this sort of the idea something, the difference between independence and pairwise independence from the matrix? Yeah, absolutely. And first of all, I wouldn't call it naive because I set to prove out it didn't happen. Because I set to prove out it didn't happen, and I was well, I learned something, that's the good news. But yeah, it's if we have, say, a three-dimensional normal variable, a standard, not standard, a three-dimensional normal variable with correlations, it just happens to be true that every two of the coordinates is a 50-50 ray, but the three-way is not 1-6-1-6. In fact, it's a cute little calculus exercise. You can actually get an exact formula for those six probabilities in terms of R10 and something. And I wrote a paper with Jewe-Lin, who And I wrote a paper with Joe and Lin where we do that quite explicitly in some special cases, and it's appeared in other places as well. So it's kind of well known to probabilists that in dimensions three and four, you can get exact formulas for these orthant probabilities, which is kind of cool. Known in the sense it's kind of like lost knowledge over the generations, like elimination theories and things like that. Any other questions or comments before we take a little stretch break? Then let's do that. Remember, if you need to check out, please do check out. Well, it's more than a spectrum, it's a full coffee break. Yeah, full coffee break. So we'll reconvene right at 10.30. Let's be super on time because I want to make sure we don't go over at the end. I'll start by talking about these future goals, but again, I want this to be very interactive. And thank you for engaging so far. 10:30, yeah. 