A slide, and I was going, you know, I'm certain there will be one, and I can't see it. So, yay, thank you for pointing that out for me. Yes, and thank you to the organizers for inviting me to be here. So, I am going to, I had a bit of a challenge when I was preparing the slides for this, which is that a lot of Slides for this, which is that a lot of my results with Cavita on this project are rather technical. And I thought that, to be honest, it would be really unpleasant to actually show them in kind of the full detail they have. And so instead, I'm going to try and overview some of Visa's project and sort of highlight my own results within that. So, oh, that's interesting. Okay. Yeah. Interesting. Okay. Yeah, so this is working fine. So let's start by considering some kind of Alan Kahn equation. So we're going to have this collection of n interacting SDs. So we've got three tens here. So we've got this dual well potential. This is basically saying that each of the particles should be pushed with a strong force towards the point minus one. The point minus one or one, and there's also an unstable point when x is equal to zero. We have this second term. So this is like an attractive force. It says that particle i should be attracted towards particle i plus one, and similarly, i and i minus one. We have this third term, which is going to be our independent Gaussian noise. Our independent Gaussian noise. So, what we would expect if we were thinking about these processes is we're going to have two clusters that are going to center at minus one and one. And somehow the particle on the edge of these clusters have some kind of attraction towards one particle that is in its same well as it and one that is in the opposite well to it. So there's a certain Motivation for certain particles to jump between the wells. Now, if we want to take the limit as n goes to infinity, then, and defining this function phi accordingly, then what we would expect is that the interactive term is actually going to confirm, converge to a Laplacian term. So that Term so that the dynamics, this collection of SDEs in the limit is going to converge to this SPDE. Great, so this is FIFO. I think a lot of people probably recognize this. And what this, I put this up here to illustrate, is that there is a very close link between large numbers of these locally interacting equations and as the Equations and as the number of vertices taken to infinity, these SPDEs. So let's look at some other kinds of interactions that we might be interested in. So, oh, wow, seeing this on the big screen, it rendered very differently from my screen. So this would be like, say, a lattice, right? So instead of like just having a set of equations, say n or indexed by integers, we might sort of index them by some kind of lattice. Of index them by some kind of lattice, okay. Natural. The extension is then just going to be instead of one-dimensional spatial variables, we've got like two-dimensional. Okay, this is quite natural. But we could also consider some kind of different graph. So this is a graphic that I think you can probably guess where I stopped drawing it, where this is some kind of deregular. So instead, you know, each equation is interacting with three other equations. Three other interactions, and well, that is the root is interaction with three, and each other one is three new vertices and its parent. Okay, so this is like some different kind of graph that we could consider, and this would also be perfectly within the scope of what we're going to think about. But let's let's also try and think about like random graphs as well. So, to motivate that, let's consider a Gautam-Watson process. Uh, so uh, Gautam-Watson process is uh Gauss and Watson processes are just defined recursively using the identity that, like, the process m at zero is one, and then we simply generate a number of independent random variables equal where the number of random variables is equal to the value of the previous step and carry. And then we sum over all of these random variables. Now, these would be like, for instance, Poisson distributed. So, this would, you know, there would be. There would be a probability that this would be zero, but equally, it's quite reasonable to think that, like, okay, well, like, if enough random variables get large, there's actually quite a large probability that this thing will blow up to infinity as we take n to infinity. Well, then the Galton-Watson tree has exactly the same structure, where we're going to start with a single vertex, which we can see in this set here, and then progressively we're going to Progressively, we're going to generate a collection of offspring of each vertex, and we're going to take a random number of offspring. And then the edges, these are just saying each newly generated vertex is connected to its parent. And then just by summing over all of these, you know, random vertex, random vertex. Sects random vertex sects, we're going to get a random tree. And again, there's going to be some probability that this is a finite graph, and there's going to be some probability that this is an infinite graph. And we could also consider all kinds of interesting examples where the distributions for different parents within the graph are all different. That just gives you a couple of different examples of some of the graphs that we could be considering here. Considering here. Now, what we're going to want to do is given some graph, which may possibly be random, we're going to want to allocate some marks to it. Those marks are going to represent the information about our setting we're putting into our problem. So for each vertex of our graph, we're going to have a scalar, we're going to have a path, and we're going to have some function here. This doesn't totally matter for the purposes of this point. So we're going to have this. At this point. So we're going to have this x naught, which we should think of as being an initial condition for some stochastic process. We're going to have w, which we think of as like a driving signal process, and b, which we should think of as being like a drift term, right? So that we can look at this and say, okay, we've got some kind of random graph, and then I'm initially, and then after that, I'm allocating some kind of randomness to each of these. Each of these vertices of this random graph. And then we're going to have one further step of randomness that we might want to keep track of. So for each vertex, we have the neighborhood of that vertex, which I'm denoting by this n u, that's n for neighborhood. Then for each vertex, we're going to have a measure over the neighborhood. And you know, remembering that these are going to be Remembering that these are going to be discrete sets. This is just basically a collection of scalars that will sum up to one in practice. But what we're doing is we're allocating weights to the edges of each of this random graph. And that's going to tell us kind of how important the interactions of each equation are going to be. So you can already see there's like quite a lot of randomness going on in here. And our goal is. And our goal is to try and solve this random collection of SD. So we're going to have our drift term. And when I said, oh, you don't really need to understand what this set is, here's what's going on at that point. So our drift term is going to have three variables. So the first variable, time. The second variable is on path space. This is going to be really important. So instead of considering it like some spatial variable, this is actually the path up to time. The path up to time t. And this final variable is a measure. We can see it's a weighted sum of deltas that are taking their values on paths. So again, the weightings here coming from the graph, and we can see it's summing over neighborhoods. So each equation is going to be interacting with the path of its neighbors. Of its neighbors and additionally, some additive noise just to kind of keep everything churning. Great. Well, oh, yes. It would, but there is, as I will get to, a really good reason why these are dependent on the whole path, whole previous path. So let's think about a really natural problem if we're looking at these locally interacting equations. So we're writing down the same equations before, but previously we had only n equations. Now I've got an infinite collection of equations, right? And suppose that the question that I want to understand is what is the dynamics of a single one of these equations, a marginal of this huge collection? A marginal of this huge collection of equations. Well, that's a bit of a pickle, right? Because in order to understand a marginal, I need to know the dynamics and the laws of adjacent equations. And we actually need to solve that before we can start running these equations. It's very good people say the exact spatial. Yes, exactly. The tag, exactly. So let's suppose that we're going to consider. Let's suppose that we're going to consider a tagged particle, but here it's not a single particle we're going to tag, but it's a collection of three particles together. Okay. Then what we would observe is, let's suppose that we're taking conditional expectations of each of these equations, then we would observe that this term conditioned on x minus 1, x 0, and x 1. Minus one, x0, and x1 will just be itself. And this term here, we're not going to worry about for the moment. But this term here, well, this middle term here, this is just explicit. We can solve exactly what this is. And this term here and this term here are going to be the law of the process x2 conditioned on these three processes and x minus 2 conditioned on these three processes. Processes. What's the next slide? Right, well, we would hope, in general, that in this setting, there's a lot of symmetry going on. And we might even hope that these two laws might actually be the same. It's a reasonable aspiration we might make a guess at. And oh, the computer is being scheduled to go. The computer is being scheduled to go to sleep automatically. Okay, I will click cancel and we'll see what happens. Cancel. No, it doesn't like me saying cancel. Oh, there we go. Okay. So if we could get some information about these two distributions, then, and we could re-express these distributions in terms of the law. Of the law of the equations minus one, zero, and one, then we might be able to re-express this expectation in terms of an integral with respect to the law of this marginal equation. That would be just some kind of McKeem-Vlazov equation. So that would be super ideal. Now, what's going on in all of the equations? What's going on in all of this? We can see these terms here, just as before, everything's worked out fine. These terms here, in the middle, I've got this term is as before. And these two functions here are going to be defined to be in terms of this conditional expectation. Let's just take a moment and look at this because there's a lot going on here. So I am saying that I'm interested in In the function gamma evaluated when I put in the path x minus one and x naught. And gamma is defined to be this conditional expectation when the inputs are x naught and x one. So if I'm putting in this path and this path into this measurable function, then this is equal to the expectation of this term conditioned on the fact that this conditioned on the fact that this x naught is equal to separately a different process x minus one and this x one process is equal to a separate path which is the input x naught right so this is a very intricate functional that is by no means uh trivial to evaluate but uh if we were If we were able to perform a mimicking theorem that allowed us to work with these conditional expectations, which we're able to do in the classical setting where these are Brownian motions, then the resulting dynamics, this equation here, describes the marginal of just these three equations within the wider collective. So that's a really big deal computationally. Oh, wow, we're running out of time. So, with that sort of motivation in mind, I'm going to throw out a couple of results I've been looking at, which are existence uniqueness, continuity of the solution with respect to the underlying graph, and finally, a Markov random field process. And I think I'm going to have no time to do proofs. I will have time to do proofs. Wow, okay. Well, you know, I'm keeping you away from lunch. So thank you. Thank you. So, thank you. Thank you. So, first of all, let's think about some kind of setting where we could prove existence and uniqueness. Now, I want to just pause for a moment and say this expectation is, again, rather intricate, because what's going on here is we have this conditional expectation with respect to the graph. What am I trying to say here? Well, let's suppose that we have some kind of random graph, and additionally, that the driving system. That the driving signals, these w, the initial conditions x and the drift terms, were independent. I mean, that is quite a sort of physical and natural assumption. And what we would see is what I'm saying is expectation of, for instance, let's say Brownian motion, that would be a fixed constant in time. Take a supremum over all of the possible vertices. Well, this would be a supremum of a constant, but it's a random supremum. And then take a summary. And then take an expectation. So, you know, that would be finite. But it's also quite easy to start thinking of examples where there might be correlation between the driving signals and the underlying graph. And in that setting, suddenly these equations get really, really terrible because you typically want to control an expectation of a supremum of all of the Brownian motions. And so if you think this is an infinite set, infinite. This is an infinite set, infinite, a supremum, an infinite, countably infinite collection of Brownian motions. This is there's just no way that that is going to be integrable. So this is quite intricate. But let's additionally assume that we have some kind of Lipschitz assumption as coming back to Sammy's question, right? Like we saw in the local equation, the resulting local equation, that the coefficients were path dependent. So even though we started off with something that was just spatial variables. Know just spatial variables. So, what we've done in all this is try and think about: okay, well, let's suppose that these functionals are path-dependent. And we see that in this Lipschitz statement, right? Because, you know, input path x and y taking a supremum over up to time t, these two different paths. And then under these assumptions, we're able to get existence uniqueness. This is quite reasonable. We're also able to weaken this Lipschitz assumption to something more likely. Lipschitz assumption to something more like linear growth and get weak solutions, but that requires a bit more care because, again, if you start like thinking for a moment, the classical strategies, you want to have a sequence of measures apply prokhorov, right? So you need some kind of meaningful Polish space to embed them into. And like a kind of these, if you think like, okay, so this is like the This is like the set of integers, set of, you know, some supremums of integers is not a particularly reasonable binary space to attempt to embed into. So what I would say is weak solutions, there's a lot more sort of intricate details that sort of come in. But this is, so let's look at what do I mean by like continuity of the solution? Well, let's say that suppose. Let's say that suppose that we've got two graphs, and we want to say, like, can we say that two graphs are similar or different? Well, one way of doing that was to say, let's fix a vertex in each graph. So these are, so we'll say that these are rooted graphs, right? And then let's consider some ball of radius k, all of the vertices that are within distance k from the root. Uh, from the root, uh, then we could uh compare these two subgraphs and say, Okay, well, these two subgraphs are isomorphic to one another. And then what we would be saying is, okay, but like there's a vicinity in which these two graphs are similar, and everything outside of that vicinity is far away. We don't really care what happens far away, just in these two localities. Then we're able to define this metric on space of growth. On a space of graphs. So, what's going on here? So, this is a weighted sum. And so, if we and then we can see a one inf. So, let's suppose for a moment this term here happens to be infinity or large or whatever. I'm not worrying about it. Then what we would observe is that when these are all infinite, this is just k equals 1 to infinity, 2 to the minus k. This would be 1, right? So, when two graphs and there's just nothing. When two graphs and there's just nothing they're nothing alike, then this distance is one. Suppose instead we've got two graphs and we're saying they're somewhat alike, right? So there's probably some vicinity in which the two graphs are isomorphic. And on that vicinity, we're saying that these are marked graphs. We're saying that the supremum of all values of the marks up to the marks. Uh, up to you know, on this uh, on this vicinity, that is going to these values are going to be small, right? Uh, and we're taking here this in FEMUM just means that we're saying, okay, well, maybe you've got two graphs and there's a way of rotating them so that there are multiple isomorphisms. So, this really just captures the idea that, like, within convergence in this Convergence in this metric of a sequence of graphs, for instance, basically says, okay, for any ball that I can draw around my root, I'm saying that eventually the distance between all the marks on that ball will be small, but I'm not worried about everything outside of the ball. But, like, you know, the ball can be chosen arbitrarily large in order to get convergence. So let's suppose that we've got some. Suppose that we've got some graph, rest spolars, and we've got then the resulting dynamics that derive from this graph, then we're going to define this capital phi, which just maps this marked graph to this other marked graph. And we're going to call this the solution map. So we can see that the input graph has all of our Input graph has all of our initial conditions we need, and the output graph is just dependent on the output SDE. Well, this is the solution map, and what we were able to prove is a continuity result under certain strong assumptions. So, if we just look at this, this is convergence and probability of a sequence of graphs g n going to g, then the resulting solution map, the graph g n is also going to graph. map the graph G n is also converging to G and probability. And we can similarly prove this in a couple of different forms, either convergence in probability, convergence in distribution. This assumption is particularly interesting, I think, for a couple of reasons. So first of all, we're not just saying like, if you ignore this term for a moment, this isn't just like a supremum over all the vertices of the process X. This is actually like the neighborhood of X. You really need to be able to control not just like X. To control not just like X, but also everything that's around it. And separately, this is the number of vertices in a ball of radius L. So if it's a deterministic graph, then that's just a constant. You can move it out of the expectations, everything's fine. But if it's a random graph, then you really need to take a lot of care to make sure that the correlation between this and this term doesn't blow up. But again, there are lots of very reasonable settings. Reasonable settings, Gautam-Watson graphs, and then independent initial conditions give you exactly this. I'm not going to talk about a proof. So the next part of the kind of setup that I described at the beginning was this idea that I could have like my process x minus two and conditional on minus one, minus zero, one and x. 0, 1, and x, 2. What we would ideally like to say is that those two laws are independent of one another. That's what we're going to be looking at here in this section. So suppose you've got some like undirected graph and a collection of random variables marking that graph. Then a Markov random field just says that for any two vertices conditional on everything else on the graph, these two Else on the graph, these two random variables are independent. Now that's like not a terribly great assumption to be working with because you're conditioning on a lot. So typically people want to work with global Markov random fields. So that says that suppose that you've got a set A and a set B, and you've also got a separating set. The separating sets just means that on the graph, any path between the vertex of A and the vertex of A. Of A and the vertex of B must go through this set S. Right, so yeah, so conditioning on a separating set, we can get two sets A and B are independent. Now, if instead we had a two-separating set, that means any path between a vertex A and a vertex B must go through S and B of length two. So this is a slightly more complicated assumption, but okay. Complicated assumption, but okay, we're going to be working with two Markov random fields here, and I'm going to justify that for you. But the way that we go about proving that is working with something called the Hammersley-Clifford theorem. This is just a really neat way of characterizing Markov random fields because it says that suppose that we've got a measure of new that's a two-markov random field, and then separately. And then separately, we've got the radon nicotine derivative measure nu with respect to measure new star has this clique factorization. That means that we can express it in terms of all of the cliques. That is, a clique is the two cliques here are all of the it's it's a vertex and a second vertex that are completely connected. So that really Completely connected. So, that in practice looks like a vertex and all of its neighbors. Suppose that we can represent this in terms of some kind of fleek factorization. Then Hammersley Clifford tells us that two implies one, and if the radon-nicotin derivative is strictly positive, then one implies two. So we should really think: okay, to mark our random field, what we actually want to do is prove a pleat factorization. This is just like. Factorization. This is just like a neat way of getting simple proofs. Perfect. Well, let's work on the following assumption on the drift term. And we should have in mind that now we're going to be thinking about our driving signals all being Gaussians. So if they're all Gaussians and independent, then there's going to be some kind of, then we're going to have a collection of these abstract Wiener spaces. And what we want to think is that our drift. And what we want to think is that our drift term is going to be embedded into the reproducing kernel Hilbert space so that we can do Gersonov transformations. Now, remember that we've got all the interactions to keep track of so that it is not completely trivial to just say, oh, yes, of course, the drift term is in the reproducing kernel Hilbert space. But under the assumption that it is, then. Then, suppose that we have initial conditions are a two-Markov random field. We have a collection of independent Gaussian signals driving our equation. Then this collection of stochastic processes do form two Markov random fields. Why is it a two-markov random field rather than a one-markov random field? This isn't actually a really important question that I struggled with a lot when I was trying to understand this. So, let's look at this system of interacting. This system of interacting equations, and I'm coming to the end of my talk. So, the information about the dynamics of equation two is feeding into the dynamics of equation one. And in exactly the same way, information about one is feeding into two. So there's this flow of information going back and forward between each of the vertices. Now, if I condition just on equation two, this becomes This becomes, there's no information going in anymore, but there is still information going out. In particular, the information going out from two is going into one and it's going into three. So there's still going to be some correlation between one and three. However, if instead I fixed two and a condition on two and three, right, then there's information about two going out into one. About two going out into one, and there's information about three going out into four, but there's no information about two going into four, and there's no information about three going into one. So that all of the information in this side here and all the information over here are completely separate. That's why these dynamics end up forming the Markov random field property. And what we can sort of And what we can sort of step back and remind ourselves what that means is that the law of 2 conditioned on 1, 0, minus 1 is going to be independent of the law of minus 2 conditioned on minus 1, 0, and 1. I had a rough draft of the proof left here, but I think I will skip that and I'll just go to my conclusions. So there are a couple of other things that I allude to. There are a couple of other things that I alluded to when I was giving this talk, and that was mimicking theorems. So, a mimicking theorem is just the idea that suppose that you've got some stochastic process and it's got some drift term, it's got some diffusion term, then it's the question of whether you can express that stochastic process in terms of an SDE. Now, for eto processes, the answer is actually yes, you can, and the drift term, the diffusion term are just conditional. The drift term, the diffusion term are just conditional expectations. Now, that result holds just for Brownian motion. And it's absolutely that the proof relies on the assumption that you're working with semi-martingales. But we've actually been able to extend that to a couple of different Gaussian additive noises. And that revolves some really interesting research that I've been doing with PhD student Kevin Hugh. PhD student Kevin Hugh on the fundamental martingale, which is, I think, super interesting, but I didn't think I'd have time today. Given that resulting mimicking theorem, you then have a derivation of the local equation. Now, the local equation, when you've got fractional Brownian motions driving these dynamics, doesn't come out to be exactly the same form as you might guess. You end up having these Volterra kernels popping up all over the place, which just All over the place, which just compensate for the fact that the drift terms are being embedded into a reproducing kernel Hilbert space and then mapped back out of it again. So that these dynamics are actually quite fiddly, but you can still do it. And another question which I've been looking at at the moment, which is terribly interesting, is actually showing the uniqueness of the law of the local equation. Of the local equation, because in terms of like, actually, oh, look, let's look at the drift term diffusion term. Okay, well, the diffusion term is quite fine, but the drift term is a measurable function. No one ever wants to solve ST's room by some measurable function where you don't even have something like linear growth, right? It's an utterly awful thing. Kavita has a proof that relies on the assumption that the drift term is bounded. This is a face value. At face value, like a really, really strong assumption. There's no intuitive reason I can see why you actually need it to be bounded. And yet, you just try and change the drift term, even a slight bit, and the proof completely fails. So, this is something that we're looking at at the moment with some really interesting optimal transport stuff. And I think that's the end of my talk. Thank you so much, everyone, for listening. Thank you very much, Brian. Do you have a question? It's a lot and lunch is soon, so I get it. Yes. I'll make sure. Yeah, thank you. 