The first speaker, we're very happy to have Eliza from Caltech to give a talk. So Elisa is currently a postdoc scholar fellowship in computing and mathematical sciences, an NSF postal fellow at Caltech. He got her degree at UTLC and her research license. And her research lies broadly in mathematics of data science, specifically the intersection of the stochastic convex geometry, high-dimensional probability, and the stochastic process. Today, she's going to talk about the random tessellation features and the forest. Please. Great. Thanks so much for a nice introduction and for giving me opportunity to speak in this workshop. I've really enjoyed the talk. The talk so far. So, yeah, today I'm going to talk to you about some joint work I've done with my collaborator, Doc Chan at UT Austin. And it's on some new applications of stochastic geometry to applications in data science and machine learning, specifically in random tessellation features in forests. And so what I want to convey to the talk is both some results we've obtained so far, but also Results we've obtained so far, but also that you know, we're really excited about this new intersection of stochastic geometry and machine learning. And there are lots and lots of open questions. And so we're excited about sort of future directions as well. So right at this intersection are random partitions. So random partitions are used in machine learning. Are used in machine learning for random features, for kernel approximations, and for building random forests for regression and classification. So I'm going to talk more specifically about these applications in sort of the second part of the talk. But the key idea is that the quality and sort of the key theme throughout the talk is that the quality of these approximations depend on the geometry of the cells of the partition. Cells of the partition. And so, because of this, and also for computational reasons, the geometry of the random partitions often used in these applications are restricted to these axis-aligned partitions where the cells are these rectangular boxes. So, in particular, on the right-hand side here are two examples of random tessellations. Of random tessellations used for these applications. The one on the top here is generated by this random grid where the boxes are, the width of the boxes in each dimension are independent random variables. So it's very easy to access the distribution of these cells. This partition on the bottom here is generated through a more complicated process. Process. And it's an example of what's called a Maundrian process. So this was a stochastic process introduced by Roy and Tay that recursively builds a random axis-aligned hierarchical partition in an axis-aligned box in Rd. So I've only shown pictures in R2, but these partitions are defined for any dimension D. And the way that they're constructed yields efficient algorithms for Yields efficient algorithms for things like kernel width learning, classification, and regression. And so, again, I'll get more into those applications later. And I also wanted to mention, if you're curious where the name comes from, they are indeed named after the painter, the artist, whose paintings are somewhat reminiscent of the tessellations you get with this process. Tessellations you get with this process. And this is one that's sort of painted in to look a little bit like a Madrium painting. So, like I said, the way this is constructed yields these efficient algorithms. So the first thing I want to do is describe how these rending testellations are constructed. So we fix a lifetime parameter, lambda, and then given a sort of rectangular window. Sort of rectangular window, we draw, we start an exponential clock. And the parameter of the clock is the sum of the widths of the window in each dimension. When the clock rings, if we've already passed our lifetime parameter, then we stop the process. Otherwise, we sample a hyperplane cut by first choosing a dimension with probability proportional to the width of the box in that dimension. And then we choose. And then we choose conditional on that dimension. We choose a cut location uniformly in the interval defined by the width of the box in that dimension. And then we recurse independently on each subrectangle that was generated by this cut with a new lifetime parameter given by the initial lifetime minus the time that's already passed. The time that's already passed. So, a few more cuts might look like this. And so, in particular, this is this construction is Markovian. And so, what this allows us to do is if you run the process up to a certain lifetime, lambda, if you instead, after doing that, want a sample. Doing that, want a sample from the process with a longer lifetime. You don't have to resample, you just have to, you can start from what you have and then run an independent process in each of the cells of the current tessellation for however much longer you want the process to run for. And this is going to have the same distribution as if you ran the process for that longer lifetime initially. Similarly, if you want a shorter lifetime, Similarly, if you want a shorter lifetime, you can just sort of remove the cuts, the latest cuts you made sort of after that lifetime. And so this is the structure that's really appealing from a computational point of view in applications. Of course, a drawback, though, is that this process is restricted to access align cuts, which doesn't necessarily capture dependencies between features of the data, between dimensions. Between dimensions. So, can we generalize this process to non-axis line types? And the answer is absolutely, because in fact, the Mondrian process is a special case of what's called the stable under iteration process in stochastic geometry. So this was introduced back in 2003 by Nagel and Weiss. It's a much more general class of random tessellation processes. And in fact, it was initially. And in fact, it was initially introduced to sort of model like crack patterns in materials. And said processes are indexed by a directional distribution, which is an even probability measure on the unit sphere that characterizes Le Cut direction. So in particular, for any even probability measure on the unit sphere, there is a corresponding SNP process. And the picture on the right here is a sample from a SNP process where this directional distribution is uniform. This directional distribution is uniform as the uniform probability measure on the unit sphere. And so you see, you get much more complicated cell geometries. And the name stable under iteration comes from this process or comes from this property of a tessellation, which if you consider the operation of the iteration. Operation of the iteration of a tessellation is to take a random tessellation, a random partition, and subdivide each cell by the intersection of an independent copy of your random tessellation with that cell. This is a process called iteration, and a random testellation is stable under iteration. If for all n, if you iterate n times. n times and then scale all of the cell boundaries by n, you recover the distribution of your original testellation. So sort of the self-similarity property that the SIP process and correspondingly the Maundering process have. So how can we generalize the construction of the Maundering into this stit? So if we're given a directional distribution, Distribution. Oh, I also want to mention that this construction is more general. We're not restricted to constructing this on an access aligned box. This construction is well defined for any compact window. So if we have a directional distribution phi, then we parametrize hyperplanes by their sort of normal direction, a mean sphere, and a display. Direction, a mean sphere, and a displacement t. It's a real number. And we define, using the directional distribution, we define a locally finite and translation invariant intense, what we call intensity measure, which is a measure on the space of hyperplanes in Rd. So, in particular, for a compact window W and R D, we define the set bracket W. W as the collection of hyperplanes that have non-empty intersection with W. Then the measure lambda of this set is given by the integral over the sphere with respect to the directional distribution of the width of W in a given direction. So given that, how can we construct our statement? How can we construct our STIP process? So we start, as we did before, we fix a lifetime parameter. She also called it intensity in discussed geometry literature. And we start an exponential clock. And the parameter is now the measure lambda of the collection of hyperplanes that hit W. And if the clock rings. If the clock rings and we're already past our lifetime, we stop. Otherwise, we sample a hyperplane cut that has this conditional distribution determined by the restriction of lambda to the set of hyperplanes that have that hit W. So more specifically, we sample a direction for the hyperplane from this distribution that is the directional distribution weighted by the widths of W in a given direction. In a given direction. And then condition on this, condition on a direction u, we sample the displacement uniformly in the interval determined by the width of the width of W in that direction. And then again, recurse independently in the new cells generated by this cut with. Cut with a new lifetime parameter. And on the right here, I just so the first two pictures are if we have given like if our directional distribution is the uniform distribution over three directions, these are the first two steps on the top. And then the third is if we run it for like run it for a bit and have more cuts. Are there any questions before I continue? I do have a very naive, very, very naive question. So, when you show that the beautiful picture in the right in the general case, you have a general tessellation. So those domain you participate participate. So, so always like a convex or just it can be more general than the convex? It can be more general, actually. Yeah. Yeah, just to be compact. Yeah. I see. I see. Thanks. I see. Yeah, it's nice. And you'll see maybe sort of why that's true in a minute. So what's sort of some additional theory about these tessellations is that they have this consistency property. So if you construct them on a larger window, W2, and restrict them to a smaller window, W1. W1 that has the same distribution as if you just ran the tessellation on the smaller or constructed the tessellation on the smaller window. And what this consistency property gives us is the existence of a random tessellation on all of our D that is stationary. It has the property that if you intersect this tessellation on RD with any compact window, it has the same distribution as if you had just run Just run this construction on that window. And it has the stable under iteration property, which also implies this self-similarity or scaling property, which is if you run the STIP process corresponding to parameter lambda scaled by lambda has the same distribution as a STIP process, a SIP tessellation with parameter one. So this all of these nice things were proved by Nagel and Weiss a few years after introducing the STIP process. And so here is just a sort of an illustration of in the Maudrean case, how you, if you construct the stesellation on some compact window, you can view it as you're looking. You're looking at the stationary random tessellation. So, stationary meaning its distribution is invariant under translations. You can view this as the stationary tessellation viewed through some compact window, which can be a really nice viewpoint in sort of analysis of these random tessellations. And in fact, And in fact, a stationary random tessellation that satisfies these properties, that satisfies the stable under iteration property, it necessarily has to be sort of a STIP process. It necessarily has to sort of correspond to this STIT process construction that I just described. So in that sense, it's sort of the most general class of random tessellations that have this sort of nice sort of Markovian construction. Nice sort of Markovian construction, Markovian construction and iteration property that sort of underlies the appeal of these tessellations in practice. But of course, there's a natural question, sort of what do you gain in these applications, specifically with looking at more general SIP processes instead of the Mondrian? And so as I mentioned, And so, as I mentioned, you know, one thing is that it allows the process to capture dependencies between features of the data. There's also some empirical evidence that like the uniform stit, which is where the directional distribution is the uniform distribution on the unit sphere, performs better than the Mondrian. It performed better in the sort of a classification task in this one paper. But I want to first. But I want to first, and I'll discuss more about how there's, you know, I really think the theory of random tessellations and these stationary stit tessellations can be of use in these applications and even say more things about the Mondrian. But I first want to address sort of a big drawback, which is that STIP presses are more difficult to simulate in practice. If you recall the If you recall the construction, you know, there's a lot more sort of you need to keep track of when making these hyperplane cuts. And so, and it's still definitely, this is like still an open question to sort of come up with general, more efficient algorithms to simulate STIP processes that sort of Nock and I are discussing in ongoing work. But one sort of a small step of progress we did make in this direction is that in certain cases, you don't have to do anything more than sample, then generate a Mondrian. Because say if you want to generate a stit with a directional distribution that's uniform over a fixed number of n directions, instead of generating the stit process in R D, you can instead look You can instead lift your data up to Rn and run a monitoring process, and you get the same partition as if you had run the STIT in Rd. So, in particular, like this top tessellation over here has three directions and an appropriate lifting to R3 gives that this. Gives that this tessellation can be obtained as intersecting this monitoring process with some subspace. And so in this case, besides having to go to higher dimensions, you don't need to do anything more than generate the Maundrian process. So, but back to sort of the more the nice theoretical properties. The nice theoretical properties. What I'd really like to stress is that, you know, with this viewpoint of the monogen process as a stationary STIT process comes with a lot of theory from stochastic geometry on stationary random tessellations. And so in stochastic geometry, we view a stationary random tessellation as sort of As sort of a point process on the space of compact convex cells. And so we're sort of, we can look at a stationary random pesolation as basically the collection of cells that partition R D. And it's the stationary point process in this space, which gives us all these tools from the theory of stationary point processes. And so I won't go. And so I won't go more into detail there, but just to say that there's a few specific random polytopes that we can define through this viewpoint that are associated to a stationary random tessellation. One is the zero cell. So stationarity implies that every location X and R D almost surely belongs to a unique cell. surely belongs to a unique cell which will denote by z sub x because with probability zero x is going to lie on a on a boundary the zero cell is we define as the unique cell containing the origin and by stationarity the zero cell has the same distribution as the cell containing x translated so that x is at the origin so in particular any sort of geometric property that's translation invariant like That's translation invariant, like volume or diameter. If we want that for a certain cell containing a specific point X, this is the same as the distribution of that property of the zero cell. We also define what's called the typical cell, which to define this, we consider some center function that assigns to each cell some center. Some center that's compatible with translations. And examples include like the center of the in-ball or the center of mass of the cells. And if we define this space K-naught, which is the collection of compact, convex cells with center at zero, there exists a random polytope in this space. So a random polytope whose center is at the origin, which is called the typical cell. Origin, which is called the typical cell, such that the expected number of cells with a given property can be written in terms of just of the distribution of this random polytope. So, sort of like first moment properties of this tessellation can be computed just in terms of the distribution of this one random polytope. And the distribution of this cell can be, it also can be defined directly. Be defined directly. And intuitively, you can think of it as like a uniformly random cell from the tessellation, which is you have to define carefully because there are an infinite number of cells. But you can think of like, you take a really large ball and you pick a cell uniformly at random from that ball, and then you take sort of an appropriate limit as the ball goes to infinity, the distribution of that uniformly chosen cell converges to the distribution of. Cell converges to the distribution of this typical cell. And this is what, you know, in stochastic geometry, you know, there's a lot of theory on the distributional properties of zero cells and typical cells of stationary tessellations. In particular, the STIT tessellation has a close relationship to a more classical model of random tessellations. Model of random tessellations in stochastic geometry that are induced by a Poisson hyperplane process, which is a Poisson point process on the space of hyperplanes in Rd. And so what does that mean? It means that this is a collection of hyperplanes, a random collection of hyperplanes in Rd, such that the number of hyperplanes that intersect any compact set has a Poisson distribution. And these processes are indexed also by a directional distribution and a parameter lambda called the intensity. And in 2013, Schreiber and Taylor showed this really beautiful fact that a Poisson hyperplane tessellation and astit tessellation, the distributions of their typical cells for, they showed that the typical cell of They showed that the typical cell of the Poisson hyperplane testsellation and a spit tessellation, with matching parameters with the same directional distribution and intensity or corresponding lifetime parameter, are equal in distribution. And the typical cell actually also determines the distribution of the zero cell. So the distribution of the zero cells are the same. So this is a one, just a really beautiful fact that you have these tessellations with very different. These tessellations with very different global distributions and global arrangements. But locally, the distribution of a given cell is the same. This is also really useful in studying stit testellations because there's a lot of literature on the zero cells and typical cells of both on hyperplane testylations that all can be used. Can be used, all sort of can then be applied to the typical cell and zero cell as digit tessellations. So like I said, you know, in these applications, what ends up being the case is that sort of the The error, the approximation error, or sort of guarantees of these estimators that are generated, I mean, approximations that are generated from random tessellations depend on the geometry of the cells, which is exactly what stochastic geometry has a whole tool set to address. So, before I move on, are there any questions? Okay, so now let's shift to back to the applications in machine learning. So the first application of random testellations in machine learning are random feature approximations. So learning algorithms using kernels involve choosing Involve choosing a kernel function k that acts as a similarity measure between inputs x and y. And kernel methods access a collection of n data points through this n by n kernel matrix, which when n is really large, this becomes really inefficient and these methods have very high, high costs because you have to store this n by n matrix and you may have to invert this n by n matrix. And so in 2007, Brahimi and Recht proposed this really nice idea for approximating kernels using a random feature map, which is a map from Rd to capital D Z, such that the inner product of Z of X and Z of Y approximates the value of the kernel at inputs X and Y. So if we can obtain So, if we can obtain such a random feature map, this greatly decreases computational storage costs because we only need to store these random feature vectors for each data point. And the computational costs decrease a lot as well when capital D is much smaller than N. So, one example of a random feature map can be obtained from random partitions. So, given a partition, So, given a partition, you can define this map, a feature map Z that takes an input and outputs a vector of length given by the number of cells in the partition. And the ith coordinate is one if x is in cell i and zero otherwise. So it's a vector that indicates which cell of the tessellation the point is in. And in particular, You can point us in, and in particular, the inner product of z of x and z of y is going to be one if they're in the same cell, and if x and y are in the same cell, and zero otherwise. And here's just a basic example of feature maps like this of length four. So the idea for the random feature map is to take M IID copies of a random partition and then concatenate all M associated random. M associated random feature maps generated from these random partitions and scale them so that when you take the inner product, the resulting kernel approximation is given by the average number of times x and y are in the same cell of these partitions. And so as m goes. And so as m goes to infinity, what's the kernel we're approximating? Well, just by the strong love large numbers, the limit of this kernel is the probability that x and y are in the same cell. And so in particular, if our random tessellation was generated by a Mondrian process with lifetime parameter lambda, this limiting kernel is this Laplace kernel. And you get the same kernel using Poisson. Poisson hyperplane tessellations to generate this or axis, you get the same kernel if you use access aligned Poisson hyperplane tessellation. And an important point here is that the lifetime parameter is the inverse of the usual sort of kernel bandwidth parameter, which in practice, often you don't know beforehand what sort of Don't know beforehand what sort of the best bandwidth is to choose. And so often people want to learn this parameter. And again, the Maundering process construction, because it retains the history over the entire lifetime, sort of moving between different lifetimes is much easier and more efficient. Much easier and more efficient than having to resample the tessellation or the random features for any sort of kernel bandwidth. And so even in particular, if anyone in the audience is familiar with like random feature, random Fourier features, which I think is a much more popular sort of random feature map, in that case, you do have to resample for each new bandwidth. So this is an advantage. So, this is an advantage of the Mondiu process. Of course, this Mondi process only approximates this one kernel, but we showed that if you generalize to STIP processes, you get a much wider class of kernels, sort of increasing their utility. And so, in particular, you have a Laplace kernel, but also from like the sit with a To sit with a uniform directional distribution, you get the exponential kernel. And here are a couple other images from kernels with more directions than just the basis vectors. And in fact, we can characterize the entire class of kernels that can be approximated with STIP processes using a parameter associated to STIP processes called the associated. Associated system processes called the associated zonoid. So, in particular, we can generally write down a formula for the probability, this limiting kernel, which is the probability that X and Y are in the same cell. So this is a bit actually easier to see in the case this probability is going to be the same for a Poisson hyperplane tessellation. This is like the probability that x minus y are in the zero. probability that x minus y are in the zero cell. So since the zero cells have the same tessellation, we can think of the Poisson hyperplane tessellation. And this probability is then the same as the probability that there's no hyperplane in the Poisson hyperplane process that intersects the line segment between X and Y. And by the distribution of this Poisson hyperplane process, that is a Poisson random variable. The number of hyperplanes that hit that line segment is a Poisson. Hyperplanes that hit that line segment is a Poisson hyperplane or is a Poisson random variable. So the probability that that random variable is zero is given, is probability that this Poisson random variable is zero is given by each and negative lambda times sort of the intensity measure of the collection of hyperplanes that intersect the line segment. line segment between X and Y. And that measure or the intensity measure of that set can be written in terms of the support function of what's called a centered zonoid, which is a convex body that has this, it's a special class of convex bodies that has this integral representation for a support function. And so what we can show is that What we can show is that we can only approximate a kernel with random features from a stationary SIP process if it has this form as e to the negative two times the support function of some centered zonoid. And sort of the course, this corresponding, the SIP process that generates this specific kernel, this centered zonoid is actually called the associated zonoid to this random tessellation. To this random tessellation. And this associated xenoid reflects the spherical directional distribution and sort of is connected to the average shape of the cells. So just sort of intuitively, you know, for the Mondrian, the associated zonoid is the L infinity ball, which makes sense. So it matches the shapes of the cells. And for the uniform stit, the associated zonoid is an L2 ball. Is an L2 ball. And so, in addition, what we showed is a sort of uniform approximation rate for how well a stick kernel approximates sort of the limiting kernel. And so we showed that this uniform rate decays exponentially. Rate decays exponentially in M, the number of partitions that you average. And the rate depends on the geometry of this associated zonoid. So in particular, it depends on this ratio between the minimum value of the support function of the associated zynoid to the maximum value of the support function. So it's going to be smaller if your cells are like are thinner. Are like are thinner. So, this is sort of, I guess, one of the first illustrations of that principle I mentioned at the beginning: that it's sort of the guarantees of these approximations depend on the geometry of the cells or this pace, this parameter associated to the distribution of the cells of the tessellations. Are there any questions before I move on to the second? questions before I move on to the second application? Great. Okay, so the second application I mentioned are random force methods. So these sort of hierarchical random partitions generate random decision trees that's sort of illustrated here, which can be used to build estimators for Estimators for classification and regression tasks. And so, as I mentioned, the Markovian construction, it yields efficient online algorithms for these tasks because online meaning you sort of add more data as you go. And because we can just we don't have to resample a Tesla Maundrian process for a new. Process for a longer lifetime. We can add data and then just continue running from our current, continue running the process for a finer tessellation from the sort of current state, which ends up giving sort of these online algorithms that resolved in estimators that had the same distribution as if you started with all of your data at the beginning and just ran one. and just ran one Monday in process. And so this is sort of a real advantage of this process in practice, but also they have, they seem to really have this theoretical advantage too, because pretty recently, Maundrian random forests specifically were shown to obtain mini-max rates. Minimax rates for certain classes of functions in arbitrary dimension. And these are the first results for random forest methods, for minimax rates for random forest methods in arbitrary dimension. Previously, they'd only been obtained in dimension one. And so we can also define more generally stit random forests. Random forests in the same way. So, I'll be a bit more specific. So, our set, our regression setting that we'll consider is if we're given a collection of data points or a collection of inputs and outputs, where each pair are their IID samples from a random pair XY, such that X has some distribution mu, and Y is f of X plus some centered noise with variance. With variance sigma squared. And then the goal is to obtain a randomized estimate of f. And so how sort of random trees do this. So specifically, if we consider a stit process in the domain W in the input space, then the STIT tree estimator at Estimator at a given point X is the average value of the outputs that correspond to the inputs that lie within the same cell of the random tessellation or in the same leaf of the tree as X. And then the STIP forest estimator is obtained by averaging M copies of the tree estimator. And And we were able to basically extend these results on for mini-max rates to the stit random forests. So specifically, so this F hat will be the random forest estimator of F corresponding to a stit with parameter lambda. And we'll let And we'll let capital PI be the associated zonoid, Z not be the zero cell, and Z be the typical cell of the sit-tessellation with the same directional distribution, but with parameter one, the constant parameter. And then we see that we can bound the risk of this random forest estimator in terms of geometric properties of the zero cell, the associated zonoid. Associated zonoid and the typical cell. So, basically, where this upper bound comes from is you can sort of do a standard bias variance decomposition of the risk. And the bias is controlled by the diameter of the zero cell. And the variance is actually controlled by the expected number of cells that intersect your window W, your domain W, which main w, which you can which equals sort of this polynomial in lambda. And these coefficients are actually mixed, they're called mixed volumes that relate the typical cell to the window. And so in particular, we can pick a specific, we can sort of optimize the lifetime parameter. Of optimize the lifetime parameter. And plugging this in gives an upper bound on the risk that matches the minimax rate for this function class of L Lipschitz functions. And but I actually want to point out here that we haven't optimized over. We haven't optimized over M. In practice, bound doesn't depend on M. And so this minimax rate is achieved even for just tree estimators. But in practice, you know, random forests often perform better. And this is illustrated in sort of a second result, which is that if we assume our true function is twice differentiable and make an additional assumption, And make an additional additional assumption that the distribution of x mu has a positive, strictly positive, and Lipschitz density on the input space. Then, if we optimize both lambda and m, the number of trees in our forest, then we obtain this upper bound, which is the minimax rate for this class of twice-differentiable functions. And I also want to note that these random forests, you know, the way that they're defined, they can be defined for any random tessellation. So we could define the same, we could in the same way define random forests based on stationary Poisson hyperplane tessellations, for instance. Tessellations, for instance. And because the cells, because these upper bounds on the risk depend on the geometry of the cells, and the cells have the same distribution in both testellations, these exact same rates hold for those random estimators. So the theory itself is much more general than just for SIP processes, which is sort of a Of a nice sort of generalization of these ideas because, in particular, you know, we have similar similar statements were proved for the Mondrian process, but the proof techniques were really relied very heavily on the rectangular geometry of the cells and structure of the tessellation. And so we, you know, bringing in know uh bringing in this the theory of branded tessellations from stochastic geometry really really generalizes um uh sort of the analysis of these of these kinds of um random forest estimators so um that is about it i just want to end um on some you know there's lots and lots of open questions you know this is um our work uh really introduced Really introduced this statistic geometric theory to these applications. And I think there could be a lot more applications. One, there's a lot more theory and also, you know, a lot more open questions in these applications and other applications in data science that I think this theory could really provide some insight to. So one thing is general, basically. Is general basically how much more general can we get with the distribution of these STIP processes and hyperplane processes? And one direction of generalization are Cox random tessellations, which are Poisson. So a Cox-Poisson process or a Cox-Point process, sorry, is a Poisson point process, but within a random intensity measure. So this is just introducing more randomness sort of into the distribution. Sort of into the distribution. And in particular, you get even a larger class of kernels if you sort of use a random lifetime parameter for the random features. Another direction is, you know, these random, with these random force estimators, with the classes that we show mining max rates for, the rates suffer from the curse of dimensionality. You know, as dimension grows, these rates get. dimension grows these rates get get um much worse and uh so exploring sort of the interplay of dimension reduction techniques or and assuming some structure on the data um uh i think is an interesting open question sort of what might be the the um the best directional distribution to use um uh for a given sort of data structure and um And also a really interesting open question is, so the random forests I described are a class of purely random forests where the trees are built totally independent of the data. But the original random forest algorithm introduced by Bremen was very highly data dependent. So the cuts and the partition that were made really depended on the data itself. On the data itself. And these random forests perform really well in practice, but they were difficult to analyze, which is why people sort of moved to studying these purely random forests. But there is, you know, if you have a stationary random tessellation, that's inherently going to be data independent. But there is some literature in schizogeometry on non-stationary tessellation. Tessellations built from sort of non-stationary intensity measures. And so, you know, incorporating the distribution of the data into these intensity measures, I think it's a really interesting line of future work. And then, of course, as I mentioned, the computational questions of sort of increasing the efficiency of sampling from SIP processes. And also, you know, theoretically, these sorts of Theoretically, these sort of theoretical guarantees and these kernel approximations are sort of equivalent for STIP processes in Poisson hyperplanes. The difference, I think, comes up in sort of when you're sampling from them and like computing things with these testellations. And so there's sort of this open question of are there maybe there are scenarios. are there maybe there are other scenarios where using Poisson hyperplanes is actually more efficient than using a SIP process and and um so anyway just this this is just a few uh open questions we have there's plenty more um and uh but I'll stop there these are the the two papers um on archive that SORC is based on and thank you so much for for coming through my talk and I'm happy to take any questions. And I'm happy to take any questions or discuss open questions. Thanks so much for let's give this very nice talk. Any questions so far? This is wonderful how you have these partitions in R2 or in RN. They look really beautiful as well. I was just wondering, you know, I don't know much about this subject area, but Know much about this subject area at all, but how would this be doable within, say, the hyperbolic plane? Does having some like negative curvature and separation of geodesics give you potentially different or different interesting tessellations? That would be pretty cool to try to see the analogy there. Definitely. So I don't, I'm not super familiar with this, but I do know. Super familiar with this, but I do know there are people looking at random testellations in hyperbolic space. I don't know if they've looked at the Poisson hyperplanes specifically, or certainly not, but I don't think the STIT tessellations, but like Borneo tessellations, I think. You know, there's sort of a whole zoo of random tessellation models. The Borneo testellation is also a Poisson Bourne testation, also very classical. And yes, I recall seeing some talk at a stochastic geometry conference on these things in hyperbolic space. I don't know the details, but you should, if you, if you Google it, I'm sure I think some things will come up. Thanks. And maybe another question. I think so. Random forest techniques are super useful and some of the most powerful and accurate machine learning techniques within, I guess, the traditional. Techniques within, I guess, the traditional machine learning spectrum. So, do you have any specific application in mind that you think that your methods could be used for? Just because it's like such a workhorse and such an important tool. I don't know what you thought. I don't know if we have a specific application in mind. In mind. I think generally, I'm sorry if that sounds like the annoying question of like, what is this useful for? It's not intended to be that. It's just like, you know, general curiosity. Yeah, yeah. I mean, generally, yeah, I think, I mean, to be honest, the real hurdle right now, which I would really like to make progress on in the near future, is the fact that generating That generating stit random forests is harder than module random forests, and I'm not sure like it's desirable like yet. But I think there could be progress made in that direction. Maybe for a small data set or like some small regime, which could make sense to me. Yeah, yeah, certainly. And And I mean, there's also like, we also had discussions about, you know, they can also be used for like density estimation. And again, there's some nice, like theoretically, they're really nice because you sort of understand the distribution of the cells. And that's kind of a block or that's kind of a hurdle, I think, when using them for density estimation. Um, using them for density estimation previously, um, and but yeah, I mean, really, we just hope to make yeah, make them more useful by making them more, I would really like to make them more data dependent and sort of so providing basically a random forest method that both achieves sort of the performance of data-dependent random forests, but is also. Data-dependent random forests, but is also sort of analytically tractable and interpretable. And that's what I would really like to have. And then, yeah, using, I mean, if anyone has an application, you know, I'd love to try it on real data sets and see how they perform. I mean, that's also, you know, what we plan to do in the near future. That's great. Thanks for the answers. Yeah, thanks. I actually have a naive question. I'm not sure if I'm not. I'm not even sure if it does make sense. So, when you show this beautiful picture of the partitioning or tessellation in the 2D plan, this sort of reminds me, I mean, people use the reload networks and after training is done, right, it will tessellate the space like this from. Yeah, which is the maybe the possible hyperplane types. I'm not sure. So, no, can we guide or understand or you know? Or understand, or you know, but over there, the variable is not random, right? It's because you're training and there might be technique challenging. But do you have any comments about that? So not too much, except that like I've had this exact same thought. And I'm like, I'm excited that you also had this thought. And so absolutely, like, I, this is a sort of with under my umbrella of sort of future reachers goals is to see if. If, so, right, it's hard because for neural networks, you know, how well do these things model train neural networks? That might be more iffy, but there's also these like random feature models that sort of have more of the flavor of random forest estimators and are built from random features like random ReLU features. And I do think that this theory, like the same sort of theory would come up when. Would come up when computing the generalization error of sort of a specific random feature model. And so I'm really excited to kind of explore that as well. Yeah, yeah, I definitely think. But the only trickiest thing is the randomness over there. I mean, here at random, you have some nice distribution. You can do some estimation. Over there, I don't know. Yeah, I think it's not exactly random. Initial addition could be random, but after training is done, there might be some mysterious distribution which. Some mysterious distribution, which is not easy to handle, I guess. Yeah, yeah. And I guess, yeah, absolutely. The only thoughts I've had in that direction are like, again, like incorporating, you know, taking sort of a non-stationary intensity measure and sort of parametrizing that and sort of then sort of learning the parameters of that. And so maybe understanding, okay, how, at least for this model, how do those parameters adapt to the data and stuff? So there's just like maybe. If there's just like maybe simple ways to kind of just maybe slightly illuminate more what the train model is doing, that you know, maybe that's possible. I don't know, but I'm excited to think about that more. Cool, cool. Thanks. Thanks for the very interesting talk. Thank you so much for the question. Yeah. Yeah. Any more question? Yeah, if no one understands. Yeah, if no one understands