All right, here they are. Okay, so what I'm going to do today is I'm going to talk you, or more precisely, yeah, more or less talk you back to back through the physics intuition and the mathematical corroboration of the two-sat result. So, what we are going to look at is We are going to look at is a random two-side formula, like we talked about in the previous lecture. So we have n Boolean variables and clauses that look like this, propositional clauses that look like this. And so we are looking at a random two-sat formula with a dn on two number of clauses. So the average number of clauses in which a variable appears. Of clauses in which a variable appears is just d. And what we would like to determine is the number of satisfying assignment that such a formula typically has, to be more precise, because that number scales exponentially. We would like to compute the limit as n goes to infinity of one on n log of number of satisfying assignments. And we saw last time that unfortunately something as simple as a first and a second. As simple as a first and a second moment calculation won't do the trick, so we need to dig a bit deeper. And this is where the physics intuition comes in, and in particular the favorite toy of our very good friends from physics called belief propagation. We already saw that we can turn this random two-sat problem into a graph. On the one side, we have the propositional Boolean variables. The Boolean variables. And on the other side, we have the clauses, which are constraints that bind variables. And the colors of the edges between them indicate whether a variable appears in a clause positively or negatively. And we already spoke a little bit about the structure of this resulting random graph, namely, locally, this random graph resembles a Gordon-Watson tree. And in particular, this graph doesn't contain Particular, this graph doesn't contain a whole lot of short cycles. Locally, it is tree-like, and the degrees are bounded. Obviously, the degrees of the clauses are always precisely two, and the number of neighbors of a specific variable is approximately cos or d. Now, we already also spoke about the Boltzmann distribution, which unsurprisingly Distribution, which unsurprisingly is the protagonist of this problem from the physics perspective, and that's just the uniform distribution on the set of satisfying assignments. And I'm going to write this build sigma for a sample from this Boltzmann distribution. So build sigma is just a uniformly random satisfying assignment of our formula. Now, the main challenge in this problem is to actually get a handle on the ball. Is to actually get a handle on the Boltzmann distribution, and in particular to understand the marginals of the Boltzmann distribution, that is, the probability that under a randomly chosen satisfying assignment, a specific variable is set to the value true. And the device that physicists came up with in order to get a grip on the Boltzmann distribution and its marginals is a message passing process called. Passing process called belief propagation. The messages in this case are associated with the edges of the graph. And more precisely, for any pair of adjacent variables and clauses, for any edge of the factor graph, there's two messages traveling across that edge. One message goes in the direction from the variable to the clause, and the other message is going to go in the reverse. Other message is going to go in the reverse direction from the clause to the variable. What's the combinatorial meaning of the message from a variable to a clause? Well, the combinatorial meaning is that this is going to be the marginal of that variable in the absence of this particular clause. So, what would this variable do if I were to remove this clause from the formula? So the message is simply the probability that this variable here is set to plus one or to minus one if I remove this clause here from the graph. So in particular, it's a probability distribution on plus minus one on the two Boolean values. How about the message in the reverse direction from a clause to a variable? The idea is similar, but this time the message from A to X But this time the message from a to x, from for example a1 to x1, is simply going to be the marginal of this variable if I remove all the other clauses apart from a where x appears in. So in this specific case, the message from a1 to x1 would be the marginal of x1 if I remove a2. So in other words, this is effectively the Is effectively the influence of A1 on X1 if I forget everybody else. So, this is essentially what A1 would like X to do if there was nobody else interfering. And the nice thing about these messages by comparison to plain old marginals is that we can actually write a system of self-consistent equations for them. Equations for them. So we can actually relate these messages to one another through a reasonably conceivable system of equations. And these are the so-called belief propagation equations. These equations essentially express our intuition that in this problem there's no long-range correlations. So the gist of it is no long-range correlations, there's short-range correlations only. As short-range correlations only, and these short-range correlations can be expressed because of this particular structure that we introduced for the messages. And to be more precise, what these equations express is the following. So suppose I look at a variable x, like this one here, and I want to calculate the message that x sends to some clause A that sits up here. Here. Then I can hope that there's no correlations between these different influences that come to X from down below, from these three branches down here. Why can I hope that? Well, I can hope that because my formula is locally tree-like. So it is very unlikely that these different branches down here will meet within a short period of time. Within a short period of time, other than through x itself. So if I were to remove x from the graph, then there would be a large distance between these different clauses here and these different variables. So the only way they interact is presumably, unless there's long-range interactions, through this variable right up here. And so this notion, this idea of independence of these different branches. Of these different branches is precisely what we can express in terms of messages. And this intuition, if you work it out a little bit, tells you that the message that X sends to A has a product structure. So it's essentially the product of the influences on X that result from these different branches down below. To be precise, because we want our messages to be probability distributions, Our messages to be probability distributions on plus minus one, we have to do a normalization. And I didn't write the normalization, it's hidden in this proportional symbol here. And so, like I said, these self-consistent equations that link the messages to each other essentially express the independence of these different branches. That's for the messages from x, from this variable up here to some other. Variable up here to some other clause that may be somewhere up there. And we can write even simpler equations, even simpler recurrence relations for the clauses because each clause only has a single child. So for the clauses, it turns out that we can actually write the message in this particular simple form. Again, there's a normalization missing. And what this expresses is simply the probability that this variable here. That this variable here is going to satisfy the clause if I set the variable up here to some particular value, sigma. So I don't expect you to fully grasp these equations in an instant. You can read up on them later if you care about it. But like I said, the gist of it is: no long-range correlations means I can write down a recurrence relation that expresses short-range correlations. And so, actually, coming back to the previous slide again, actually, if my formula would be completely acyclic, if there weren't any cycles in the graph at all, then a simple inductive argument tells you that these equations are actually 100% spot on. They are 100% accurate. And in fact, if you do a bit of induction, you find that. Do a bit of induction, you find that on an acyclic formula, on an acyclic graph, you can even write a formula for the partition function purely in terms of these messages. And this formula for the partition function is something called the beta-free entropy in physics language. And so this formula is, like I said, 100% accurate if your formula happens to be acyclic. The graph doesn't contain any cycles at all. Contain any cycles at all. And again, I don't expect you to grasp this completely right here on the spot, but just bear with me. It is something that you can work out essentially as an exercise if there's no cycles. Now, like I said, locally, our formula is acyclic. Locally, it looks like a tree. There's few short cycles. And so we might hope that in the absence of long-range correlations, maybe we are in luck. Correlations, maybe we are in luck. And the same formula for the partition function, like on a tree, still applies in this scenario here. And this is what physicists would call the replica symmetric answers. Now, one drawback of this formula here is that, unfortunately, it speaks about the specific messages that whiz along the edges of a specific two side formula. Specific to SAT formula. And so it seems that in order to calculate this monster here, you would actually have to first generate your formula and then maybe try and find the fixed point or the solution to these equations somehow and plug that into this slightly awe-inspiring formula here. And so, obviously, that's not what we want. What we would like is we would like to be able to write a function that comes just in terms of the model. Just in terms of the model parameters, in this case, just in terms of the average number of clauses that a variable appears in. And so our good friends from physics have a trick up their sleeves for this too. That is something called density evolution. And the big idea is to grossly simplify the belief propagation equations along the following lines. So suppose I forget about Forget about the identities of the edges of my graph. I don't care anymore which variable precisely is connected to which particular clause and so on and so forth. What I do is I just take the messages, the belief propagation messages of my graph, and I put them into one big bag. And so mathematically, this bag is of course called an empirical distribution. So pi phi is just the empirical distribution. Is just the empirical distribution of all the messages from variables to clauses that actually travel along my random formula. Now, given that locally the structure of the graph resembles a Gordon-Watson tree, I can try and actually write down a fixed point equation not for individual messages one by one, but kind of statistically in distribution for all of this. For all of this, I mean, for this entire empirical distribution in one sweep. And so the intuition behind this would be that, well, if I forget about the identities of the variables and clauses, then maybe I can do the following. A variable in my formula has Poisson D children. So I just generate a Poisson D variable. To be more precise, I generate two Poisson D on two variables. On D on two variables, one representing the plus children, the children where my variable appears as a positive literal, and one representing the remaining minus children. And then I labor under the hypothesis that these messages that come in from down here below are, well, any old messages that travel along the edges of my factor graph. That is, I'm going to presume that these messages that come in from below are actually Come in from below are actually independent samples from this empirical distribution. So I'm going to presume that actually, in some sense, what matters is only the structure of the graph, only the geometry, how the geometry is distributed, and not so much precisely which variable talks to which particular clause. And so this is again something where this construction of messages comes in handy because the message from In handy because the message from x to a only essentially cares about the structure that lies on the other side of x that doesn't involve a. So maybe because of this particular message construction, this is a sensible thing to write down. And so what I'm writing down is, well, the message up here is essentially what I get if I generate these Poisson variables and stick in samples from my empirical distribution down below here. Distribution down below here. And this means that I'm arriving at this simplified belief propagation version. That's something called the density evolution equation in physics. So this is kind of like the belief propagation equation with the numerator and the normalizing term. But this time, instead of meticulously tracing the incoming messages, I just take independent samples from this empirical distribution. From this empirical distribution up here. And what this equation expresses is essentially the self-consistency of belief propagation, namely that the distribution of the message that I get out when I perform this operation is precisely pi phi, is precisely the empirical distribution of messages that I stick in. So you may find this a bit adventurous, and in fact, that's how many of us felt when we first saw this. Many of us felt when we first saw this, but if you, I mean, it will grow on you. It's not as outlandish as it might at first seem. And so the big plus of this is that we get one equation, I mean one fixed point equation, admittedly a more complicated fixed point equation because now the objects that we are talking about in this equation are random variables. But we get one single equation whose only parameters Equation whose only parameter is a number, namely d. And so while you're in this mindset, you can also write along similar lines a simplified version of the beta free entropy in terms of this empirical distribution. And along similar lines, applying similar reasoning, you see that this complicated free line formula with the various different logarithms and sums and products and so on and so forth actually simplifies. Actually, it simplifies in terms of this empirical distribution and boils down to this one-line formula here, where these μi's are independent samples from a probability distribution. And again, these D plus and D minus are independent Poisson, D on 2 variables. And so this finally leads to the so-called replicasometric solution, or mathematically speaking, replicasometric prediction. Prediction for the number of satisfying assignments first derived by Monasson and Zikina in 96. And so this prediction holds that there's precisely one probability distribution, there's precisely one distribution on the unit interval that satisfies this stochastic fixed point equation, that satisfies this equation that if you draw independent samples mu. if you draw independent samples μi from pi d you will get a random and apply this formula you will get something out that is distributed as pi d and the prediction is that log z is actually in the limit as n goes to infinity given by the beta free entropy that you get if you plug independent samples from this fixed point distribution into the beta free entropy so So, I mean, again, let me emphasize that this is quite a bit more complicated than what we would get from the plain first moment calculation. And so the big plus of this more elaborate prediction is that it takes into account precisely the short-range effects that take place on this random TUSA formula, and it labors under the assumption that there's no long-range effects going on. Effects going on. And what we are going to do next is we are going to turn this prediction into a rigorous theorem. So we are going to prove, in fact, that this stochastic fixed point equation has a unique solution, and that if you draw a bunch of independent samples from this distribution, from this fixed point distribution, you get the number of satisfying assignments out. Before we prove that, let me show you what the result looks like. Me show you what the result looks like. On the right-hand side here, you see on the x-axis d, which ranges between 0 and 2. Remember that the satisfiability threshold occurs at d equals 2. So there's no satisfying assignments left once d is greater than 2. The red line that you see here is this function b of d from the previous slide. So the red line is precisely The red line is precisely the beta-free entropy applied to our wondrous unique fixed point distribution. This blue dotted line here is the formula that you would get from the first moment. You can see that the blue line and the red line are reasonably close together, but remember this is on an exponential scale, on a logarithmic scale. So a small difference here actually translates into an exponential scale. Actually, it translates into an exponential factor between the first moment and the actual number of satisfying assignments. And something that you can maybe not quite as easily see in this plot is that the blue dotted line and the red line are never identical for any positive d. So the first moment is always for any specific d, an overestimate of the actual number of solutions. So another thing to observe. So, another thing to observe is that, like I said, the satisfiability threshold occurs at d equals 2. At d equals 2, our beta-free entropy still has a positive value. So right the moment before your formula becomes unsatisfiable, it still has an exponential number of satisfying assignments. So it falls right off a cliff at the satisfiability threshold. On the left here, what you see is the CDF. Is the CDF of this fixed point distribution that comes out of this equation here for various values of d for d equals 1.2 up to 1.9. And what you can see is that this fixed point distribution is something that is rather non-trivial. So, this is not something that this is not presumably the thing you would first guess. And the reason is that this. And the reason is that this probability distribution here captures precisely the possible or the statistics of the short-range correlations in your random two-set formula up to any finite correlation length. And so we obtained this result using a computer calculation, using a numerical calculation. And it turns out that actually this equation here. Actually, this equation here, this fixed-point equation, converges rapidly, so you can get very good estimates using relatively little computational resources. Anyway, so this much about the physics prediction. Let's now see how we can prove such a thing, how we can turn this prediction into a theorem. And the proof strategy has four items. The first item is something that something that is called the contraction method. Something that is called the contraction method. And this is a trick that we are going to use in order to prove that this density evolution equation possesses a unique solution, that there's a unique way of solving this distributional equation. The second stop is going to be spatial mixing. So we are going to actually prove that in this random two-set problem, there's no long-range correlations. And this part of the proof is And this part of the proof is the core of the matter. This is where the meat is, and where we will have to work the most. The next step is a coupling argument known as the Eisenman-Sim star scheme. And here we are going to combine the results of the first two steps in order to prove that the beta-free, the beta formula, the beta-free entropy, actually correctly gives the expectation of log z phi. Now, in this problem, because we are looking at zero temperature, at hard constraints that all simultaneously have to be satisfied, we are going to have to follow this up by another argument known as the interpolation method that will allow us to prove concentration of log Z. So, I think it's a good idea to maybe take a break at this point before we launch. Before we launch into the proof, before we actually begin with the contraction method, so why don't we take five minutes off? That sounds good. So yeah, if you have any questions, feel free to ask them in the chat and I think we'll restart at 2 p.m. Do you have access to the chat, Amin? Yeah, I'm working on it. Okay, I can just read notification. Would you briefly say the meaning of the expectation of a random probability measure in the last two lines of the two slides ago? This slide right here, so this the expectation. The expectation right here at the bottom in the beta-free entropy formula. Right, okay. So, what this means is this means the expectation over everything. So, this probability distribution pi d here is actually not a random probability measure, but it's a fixed probability measure, namely the unique probability measure, so that if you draw a sample from this measure, which is mu 0. Sample from this measure, which is mu zero, that sample is distributed precisely like the number that you would obtain if you were to draw independent samples mu i, multiply them up up to an independent Poisson d over 2 variable d plus, and then normalize accordingly. Now, this expectation, oops, this expectation down here is the expectation over everything. So, you generate an infinite sequence of An infinite sequence of independent samples from this distribution. So these μi are an infinite family of samples from pi d, and d plus and d minus are again independent samples from the Poisson d over 2 distribution. And so you just plug in the numbers that you get and you average over everything. Any other questions? So, should I? Yeah, it's looking good. I think you can go to the proof. I'm looking a little. I'm looking a little dark. I can try to maybe switch. That might brighten up matters a little.  So, did I manage to completely lose everybody? No, I think it's good. You had just had the one question. That's it. Okay. Louilla, are you still there? Yes, sure. Good.  Feel free to restart whatever you yeah, I'm happy to ramble on if you want me to. Okay. Right. Okay, so we will try to now carry out this program. Yeah, so there's, I should, in principle, I should say a few words about how this strategy compares to some prior work, not all of which deals with the two-sub problem. But let me just cut to the Let me just cut to the chase. I mean, the main difference really is that here we are talking about hard constraints. So you want to count assignments that satisfy every single clause simultaneously. And so that's basically the main difference between this and, for example, work on the easing model on random graphs. So, like I said, the first step on the agenda is going to be the contraction method. So, we want to show that there. Contraction method. So, we want to show that there that this stochastic fixed-point equation here. So, just out of curiosity, it would be interesting to see how many of you have dealt with stochastic fixed-point equations before. I don't know if there's a way of indicating that in the chat, but I would be curious about this. So, if you know them from some other context, maybe drop a line in the chat. Anyway, so we want to show that this stochastic fixed point equation. That this stochastic fixed point equation, where these μi are independent random variables and also the d plus and d minus are random variables, that this stochastic fixed point equation possesses a unique solution. And so one thing that is a bit unpleasant about this equation, I mean, even though it's one of the more amenable specimens that you will encounter in the context of the cavity method, In the context of the cavity method, one thing that is a bit unpleasant about it is that we have this complicated sub-product structure here in the denominator. And so one trick that is often useful to get rid of this kind of nasty denominators is to look at log likelihood ratios. So this means that instead of studying the mu i's themselves, the messages themselves, which are The messages themselves, which are numbers between 0 and 1, what we are going to do is we are going to look at these log likelihood ratios, log μ i on 1 minus mu i, and that's a positive or negative real number. And by going to mu i on 1 minus mu i, we immediately get rid of this denominator and we are left with a product of quotients. And so, upon taking the log, this product turns into a sum. So, our fixed-point equation from the previous slide turns into an equation about sums of independent random variables, namely that the random variable eta zero is distributed precisely as a sum of a Poisson number of independent summons, where each summand has two Rademacher variables. Rademacher variables si and si prime. And then, okay, admittedly, this log and tang, which are an acquired taste. But at least we have a sum of independent things. So that's maybe something that we can work with. And this equation now is precisely equivalent up to this transformation to the equation that we had on the previous slide. Now, what we are going to do is we are going to look at this equation here, this fixed. Equation here, this fixed point problem here. And we are going to use some tools from analysis in order to solve this fixed point equation. And in particular, what we are going to do is we are going to turn the space of probability measures into a complete metric space so that we can apply the fixed point theorems about metric spaces that we know and love. And it turns out the right space to look in this example. out the right space to look in this example is the Wasserstein space and the Wassersteinstein space W2 of R is defined as the space of all probability measures on the real line with a finite second moment. The metric on this space is this Masserstein metric delta 2 of rho and rho prime and this metric is defined as follows. You first take You first take a coupling of these two probability measures. So you take one random variable x that is distributed as rho and another random variable x prime that is distributed as rho prime. And now these two random variables need not be independent. Instead, they live on the same probability space. And you try to align them as best as you can so as to minimize the resulting. The resulting L2 distance. And the minimum possible L2 distance that you can accomplish in such a way is the Wasserstein distance of these two probability measures. It's a simple exercise to show that this turns the W2 space into a complete separable space. And on this space, therefore, we have our favorite Bannock fixed point. Bannock fixed point theorem, which says the following: if you have a contraction from this complete metric space into itself, that is a map that decreases distances. So if you put two probability distributions, rho and rho prime in, and apply your map, then the resulting distance is going to be smaller than the distance that you had previously, that you started from. Started from. If you have such a map that is a contraction, then this map possesses a unique fixed point. So that's going to be our tool in order to show that there's a unique solution to this stochastic fixed point equation. And we will apply this map to our fixed point equation for log likelihood ratios. So, what we are going to show is that this map that takes a probability distribution rho and Distribution rho and maps it to the distribution of this beautiful random variable down here actually is a contraction on the W2 space. So effectively, the uniqueness of the solution to this density evolution equation follows from the Banach fixed point theory. The proof of this fact, the proof of this contraction fact is given on Fact is given on this slide. I'm not going to talk you through it. It's not rocket science. It's just, as you can see, one, two, three, four, five steps. Six, okay, six steps. And none of them is difficult. About the most sophisticated tool that you use is the Cauchy-Schwarz inequality. And at the end of this calculation, you're convinced. This calculation, you're convinced that your map is a contraction so long as d is less than 2, and in fact, it is. So, in fact, we are assuming that d is less than 2, because for d greater than 2, there's no satisfying assignments left. Okay, so using the Banner fixed point theorem and the Spasser-Stein space, we are now sure that there's only one single solution to our density evolution fixed point equation to this simple. point equation to this simplified version of belief propagation on the random formula. Let's move on to step two. Step two is that we are going to show spatial mixing. We are going to show that there's no long-range correlations in our formula. And as an application of this, we will prove this proposition displayed here. If you take any d less than 2, Less than two, and you look at the empirical distribution of your Boltzmann marginals on your random formula. So you look at the, so you take all the Boltzmann marginals and you put them into one big bag. And so that gives you a probability distribution, a discrete probability distribution with support up to n points. Then we will prove that this distribution here We will prove that this distribution here converges in the limit as n goes to infinity to the unique solution of your density evolution equation. So this pi d is the unique solution to the stochastic fixed point equation that we just derived using the Banach fixed point theorem. And what we prove is that as you make n bigger and bigger, as you take the number of variables in your formula to infinity, Variables in your formula to infinity, the empirical distribution of the Boltzmann marginals actually converges to this magical fixed point distribution. Okay, like I said, the main step is going to be spatial mixing. The main step is going to be to prove that if I take two variables in my formula that are far apart from each other, then their joint distribution factorizes. Factorizes. The true value here doesn't tell me a whole lot about the true value over here. And in order to prove the spatial mixing property, it turns out that fortunately, we can exclusively work on tree formulas, on two-side formulas that don't contain any cycles. Specifically, we can exclusively work on the two-side formulas that are described by this Gordon. That are described by this Gordon-Watson tree that captures the local structure of my random formula. So, we are going to look at a Gordon-Watson tree that starts at a root variable x0. Every variable in this tree spawns a pos or d number of clauses as offspring. For each clause, we choose with probability one-half whether the variable appears positively or negatively. Positively or negatively in that clause. And every clause spawns, obviously, a single variable as its child, which again spawns course or declauses and so on. And what we are going to do is we are going to look at this Gordon-Watson tree and truncate it after two L steps for some large number L. And then we are going to investigate how the true value of the root correlates with the true values at the bottom. Values at the boundary of the tree at the variables at level 2L. And yeah, so what we are going to show is a very strong spatial mixing property known as the Gibbs uniqueness property. Namely, here's your tree, here is your tree expanded up to level 2L, and now the Gibbs uniqueness property provides the following. Suppose I Suppose I put in any consistent boundary condition on this tree. So I write down specific truth values, blue representing true, I think, and red representing false. I write down any collection of truth values for the leaves or for the boundary of this tree that can be extended to a satisfying assignment of the entire tree. So there must be a way of actually satisfying. Way of actually satisfying all the clauses in such a way that the boundary variables get these particular truth values. So suppose I take any reasonable, any conceivable boundary condition, and I plug it in down here. And now I ask myself, given this boundary condition, what is the distribution of the truth value of the root? So subject to So, subject to this boundary condition, draw a random satisfying assignment of the tree and inspect the distribution of the truth value assigned to the root. So that's some distribution on the true values plus 1, minus 1, true, and false. And now I would like you to compare this distribution that you get for a particular boundary. Get for a particular boundary condition with the free boundary distribution, with the distribution that you would get if you wouldn't plug in any boundary condition at all. If you leave the boundary free, you just take a uniformly random satisfying assignment of this entire tree formula and inspect the distribution of the truth value of x0. And what this gibbs uniqueness property says is that these two He says is that these two numbers, these two marginals are close to each other, and actually their distance goes to zero as I make the tree taller and taller. So for in the limit of large L, in the limit of large trees, you cannot influence the value of the root by plugging in any boundary condition that you like. You cannot nudge the root out. Nudge the root out of place with any possible boundary condition. It's simply not possible. Now observe that this is a property that purely speaks about trees, but because the local structure of my random two-sat formula is described precisely by this tree, this property directly translates to the random two-sat formula. To the random two-side formula. It directly implies that if I take a variable x0 in my two-side formula and carve out a large depth L neighborhood around this formula, I draw a circle of radius to L around this variable and now ask what is the impact if I write down specific truth values on that on level 2L of this circle, that that has a diminishing influence. That has a diminishing influence on x0 itself. And like I said, this is simply because the local structure of the formula is described precisely by this tree here. And right, so what we are going to show is that, in fact, in the random two-sat problem, this Gibbs uniqueness property always holds. So this strong spatial mixing property here is always satisfied. Satisfied. Now, the proof of this is really the core of the matter, and it's not a trivial argument. So, maybe your first step at this might be that you write down the recurrence that, given the boundary condition, actually gives you the root marginal. And in fact, you can do that. In fact, you can write this down meticulously. And what you end up writing down this way is precisely belief propagation. And then you could start and maybe try to. And then you could start and maybe try to take derivatives. So if I change the boundary values, how much does that actually influence the root? You could try and trace this by using calculus. And we tried that for quite a while, but it doesn't work very well. So you simply hit the ball and it turns out you hit the ball at d equals one, not d equals two, like we want to accomplish. Another thing you could try is you could simply try and take a union bound over all. And take a union bound over all the possible boundary conditions, but unfortunately, there's quite a few of them. Namely, in a Gordon-Watson tree like this, normally the boundary dominates everything. So the number of possible boundary conditions is rather enormous. So that doesn't work either. So what we did instead is, or what you can try instead is maybe you can try and recurse, try and take recourse to. Try and take recourse to a combinatorial construction. Namely, it turns out that in this problem, you can actually construct an extremal boundary condition. So you can construct a boundary condition that will maximize the probability, for example, that the root gets set to plus one. How do you do that? Well, it turns out to be more complicated than in many other problems. Than in many other problems, like, for example, the easing model or the hardcore model. Namely, what you do in this case is you actually have to look at the tree itself in order to come up with this extremal boundary condition. Here's how it works. Suppose I want to nudge the root variable as much as possible towards plus one. So I want to maximize the probability that the root is assigned plus one. How do I do that? How do I do that? Well, I first look at all the clauses where the root appears positively, all the clauses where the root appears with a plus sign. And now these clauses have another child, and I'm going to go to that other child, and I'm going to try and nudge it away from the value that will satisfy this clause as much as I can. So, in order to nudge the root, In order to nudge the root, I have to nudge its children. I have to tell them: try as much as you can to steer clear of the value that will satisfy this clause. Because if this clause is unsatisfied by the child, it will ask the root, can you please satisfy me? And that will encourage the root to take the value plus one. On the other hand, I can look at the minus one children of the root. Of the root. And I don't want the root dragged into the minus one direction by this clause here. So I'm going to look at the child of that clause and I'm going to try and tell this child, well, you know what? Whatever you can do to satisfy this clause, do it. Try and satisfy this clause as much as you can. So I'm going to try and mudge this guy here in the plus one direction simply so that it tries. Simply so that it tries to satisfy this clause, and simply so that this clause doesn't drag plus one in the wrong direction, leads plus one up here astray. And so you can now iterate this process. You can now recurse the same process on these child variables. We have some particular direction in mind where we want to nudge them. So we are going to pass to their children in turn. Children in turn and try and nudge them in some particular direction, and so on, until we reach the boundary of the tree. And you can prove by an induction that this will actually work, and this process will actually give you the boundary condition that maximizes the probability of plus one at the root. So now in the second step, what we would like to do is we would like to take this particular boundary condition that we meticulously constructed. That we meticulously constructed in this top-down fashion, and now calculate, given this extremal boundary condition, the probability that actually the root is going to be plus one. Now, at first glance, this seems rather challenging because in the process of constructing the boundary condition, we actually had to expose the entire tree. We actually had to walk down the tree, top to bottom, look at every sign in the tree, all the Tree, all the children and all the clauses, and so on, in order to reach the extremal boundary condition. So it seems that at this point we are actually stuck. There's no randomness left that could aid us pass this boundary condition back up. There's no stochastic process anymore, but we are just facing a deterministic problem. So at first glance, this seems rather hopeless. But at second glance, it turns out that actually it isn't. Actually, it isn't. Actually, you can fold the top-down process and the bottom-up process into a single recursion, into a single message passing equation. So you can solve these two problems in one sweep. And you can express these two processes that go hand in hand by a modified version of the density evolution equation that encodes. Density evolution equation that encoded leaf propagation. And so, again, formulating this equation in log likelihood ratios, we end up with this equation here. So, this is again a stochastic fixed point equation. It again has the same form that we saw previously, namely that you have a sum of independent random variables. And the only difference between the previous equation that we had and this one here is that here. One here is that here we have the same sign as i occurring twice. So previously this was an independent random sign. Now these two signs are identical. And if you work for maybe half an hour or so, you will see that this actually corresponds precisely to this construction of the extremal boundary condition. So all we have to do, in other words, is we have to show that this This fixed point equation here has a unique solution, and that this unique solution is identical to the solution that we got from the plane density evolution equation, where this one here was an independent sign independent of this other guy over here. And it turns out that this is actually correct. And you can use the contraction method again, just like we did before. We did before. A slightly modified version of the same calculation is going to show you that this guy also has a unique fixed point. And actually, for symmetry reasons, it turns out that you can easily check that pi d, the solution of the old density evolution equation, also is the unique solution to this fixed point equation here. So the consequence of this is that this nudging here, even though it gives you the extremal It gives you the extremal probability, it maximizes the probability that the root is going to be plus one, still doesn't move you away from the distribution that you had previously. So as a conclusion, we can derive the fact that this problem here, in fact, has the Gibbs uniqueness property. So if you make your tree taller and taller, there's Tree taller and taller, there's no way to construct a boundary condition that will match the root marginal away from the free boundary condition. Okay, so one corollary, one immediate application of this fact is this asymptotic independence statement here. So, what is this saying? This is saying that if Is this saying? This is saying that if you take any k specific variables in your random formula, for example, the first k variables, or more generally, some you choose k variables in your formula at random. And this is for any fixed k, for example, k is 500. And you look at the joint probability that these variables, these Boolean variables, are assigned. These Boolean variables are assigned specific values, then this probability here is approximately equal to the product of the individual marginal probability. So the truth values assigned to a bunch of faraway variables in your propositional formula is essentially a product distribution. How do we see that? How do we see that? Well, we take a large number L, we draw a large radius L around each of our variables. If we choose either the first k variables in our formula or otherwise k random variables, it is very unlikely that they have distance less than l, that there's a pairwise distance less than l. So we can draw these circles around x1 up to xk. k and we can actually not just fix the value of let's say x1 and x2 but we can in order to study x3 but we can actually fix the entire depth L boundary here and it's not going to influence x3 so that's a stronger statement than saying I'm going to fix x1 and x2 to specific values and look at the impact on x3 in fact something stronger is true even if i fix this entire Even if I fix this entire ball of radius L, it's not going to influence X3. So, this is an immediate consequence of the Gibbs uniqueness property. Okay, so now let's move on to the next item towards the proof of the main theory, namely the Eisenman-Simstar coupling argument. So, here's the core of the matter. Here's the core of the matter. What we are going to do is we are going to look at the change in Z, the change in the number of satisfying assignments, if we move from a formula with n variables, a random formula with n variables, to a random formula with n plus 1 variables. So how does the number of satisfying assignments change if I make my system? If I make my system a bit bigger, if I add one more variable and, of course, its adjacent clauses to my random formula, how much is my number of satisfying assignments going to change? And what we are going to prove is that this change is given precisely by the beta free entropy, precisely by this crazy expectation of log of some product and so on and so forth. Some product, and so on, and so forth. And in fact, in the course of this, we are going to see very precisely the combinatorial meaning of this beta-free entropy formula. Now, one or two words about this. I put this maximum of partition function n1 here to avoid that, by accident, the small minority of unsatisfiable formulas, formulas without any satisfying assignments, drag this expectation to minus. Drag this expectation to minus infinity. So, just to be sure, just to be on the safe side, I put this maximum of one and z here. And another thing is how this is actually related to the original partition function, to the log of z. Well, it's the following trick, and this is actually what I would probably call the Eisenman-Sim star trick or Eisenman-Sim star scheme, if you like. Sim star scheme, if you like. So, as an immediate implication of this proposition here, we can derive a corollary that says that as n goes to infinity, one on n expectation log z, I mean, never mind this max between one and z here, that's just a technicality. So the limit as n goes to infinity, one on n expectation log z is equal to beta free entropy. How do we see that? Well, what we simply do. Well, what we simply do is we write this out here, this expectation we write out as a telescoping sum. So we just write this expectation here as a difference of changes as we go from a system of size n to a system of size n plus one. Now these individual summons here, according to our proposition, all converge to B D, all converge to the beta free entropy. All converge to the beta-free entropy. So if I average them, the result is going to converge to the beta-free entropy as well. So that's how I'm getting the corollary out of this proposition. So once we understand the change of our free energy upon increasing the system size by one, we can actually calculate expectation log z. Okay, so how does this Eisenman-Simstar scheme? Okay, so how does this Eisenman Sim star scheme work? How do we pull off this? How do we prove the proposition up here? Well, we use a coupling argument. And so more precisely, we are going to use this two-way coupling. Starting from a formula phi n prime, a random two-side formula phi n prime, that contains almost as many clauses as the random formula of size n. It contains m prime clauses. It contains m prime clauses, so that's by a bounded amount less, by something like d over 2 less than the real formula on n variables. So starting from this formula phi n prime here in the middle, we are going to construct phi n on the one side and phi n plus 1, the random formula with n plus 1 variables, on the other side. And to be precise, we are going to obtain phi m. We are going to obtain phi n by simply adding in the missing Poisson d over 2 random clauses. So we will supplement Poisson D over 2 clauses to turn phi n prime into phi n. And on the other side, what we are going to do is we are going to add one variable, x n plus 1, and Poisson d adjacent clauses that Clauses that link this new variable with the bulk of the random formula with phi n prime. So that's going to be our coupling. On the one side, we add a bounded number of random clauses, and on the other side, we add one variable, and again, a small number of random clauses. And so we are going to meticulously calculate the change. The change in partition function upon going along this arrow and upon going along that arrow. And then we will take the difference. And if you take the difference between these two quantities, of course, the partition function of the guy in the middle is going to cancel. So we end up with the quantity that we aim to compute. So how do we do this calculation? How does this computation work? This computation works? Well, the beautiful thing now is that all we have to do is we only have to control local changes to our random formula. We don't have to look at a very big change. We only add, for example, following this error, we only add a small number of clauses, not a lot of clauses. So all we have to do is we have to check out what happens if we add Poisson D over two clauses, how the partition function changes. How the partition function changes. And if you remember the definition of these partition functions, this actually means that we have to compute the Boltzmann probability, so that's what these brackets are supposed to indicate, the Boltzmann probability that a random satisfying assignment of phi n prime, of this guy up here, also happens to satisfy the two new clauses. The two new clauses or the Poisson d over two new clauses. So, this is going to tell you how much the partition function changes because all of the satisfying assignments are going to survive that happen to satisfy the additional clauses. So, how do we do this calculation here? Well, now we use our spatial mixing property because Because these clauses that we are adding in here are random, it is very unlikely that the variables where they connect are close together. With overwhelming probability, these two variables where this clause here connects to are going to be far apart. So we can apply our spatial mixing result and we can say, well, presumably the true values assigned. Presumably, the true values assigned to these two variables are actually stochastically independent or nearly stochastically independent, because we already proved, thanks to Gibbs' uniqueness, that there's no long-range correlations in our model. So we can write the probability that this particular clause here will be satisfied as, well, one minus the probability that both these variables fail to satisfy it. And because of independence, And because of independence, we can write this probability here as a product of probabilities, namely the probability that the first variable fails to satisfy it times the probability that the second variable fails to satisfy this clause. And because the number of clauses that we are adding is so small, the clauses that we are adding are not going to interfere with each other. So this product. Each other. So this product here simply comes out of the logarithm and turns into a d over 2. So we end up with this expression d over 2 expectation log 1 minus mu 1 times mu 2. If you remember the formula for the beta free entropy from a few slides back, somewhat not so easy to go back with my mouse. So if you so if you so this is precisely this expression back here. A similar combinatorial calculation explains this expression here in front. And therefore, using this Eiseman-Sim star scheme coupling argument together with the absence of long-range Absence of long-range correlations, we actually get not just this expression, but also a combinatorial understanding of where this expression comes from. Now, we are almost done with the proof of this two-sub theorem. There's one ingredient missing. Namely, the missing ingredient is that up to this point I only computed the expectation of log z of phi. What I still have to show to you. What I still have to show to you is that this convergence here also holds in probability, that in fact log z concentrates about its expectation. And that I'm going to do tomorrow using the interpolation method. It's not going to take long. It's going to take probably less than 10 minutes to finish this. And after that, I'm going to show you an application of a belief propagation and this. A belief propagation and this cavity method technology to a statistical inference problem, namely to the group testing problem of finding a small number of infected individuals in a large population. So thanks for today. Thanks for your attention. And I'm going to stick around to answer any questions. So we'll give everyone the possibility to unmute themselves. possibility to unmute themselves and yeah now we should have the you can unmute yourself and we'll give a round of applause for Aming so now we'll do the the usual turning off all the recordings and this way you can so it's done and now you can feel free to You can feel free to unmute yourself and