Thanks very much to the organizers for inviting me to this interesting workshop. So, if the idea is to somehow merge people from different backgrounds, this certainly applies to me because I actually haven't done any work on the statistical side. So, I've been working more on the non-linear inverse problems. But what I'm going to talk about is basically instability or It's basically instability or ill-posonness of inverse bronze, which is a kind of central feature of these bronze. It has been around since the birth of this whole field. And it's somehow related to the idea that small errors in measurements can lead to large errors in reconstructions, unless you do things carefully. And also, the idea that some inverse problems are mildly ill-posed, and other ones are severely ill-posed. And somehow the talk will be about somehow quantifying the degree of instability in a given inverse prone. This is kind of well understood for linear inverse prones, but we found that for non-linear inverse problems, there was actually not so much like work done outside of some special examples, which I'll discuss in a moment. Which I'll discuss in a moment. And some of the upshot for the statistical side is that, as we saw in the talk by Richard Nicoll, the stability properties of the inverse form affect the performance of statistical algorithms and can lead to convergence guarantees for some of these methods. So, this is joint work with Herbert Koch and Ankana Riland. So, the talk is in three parts. First, I'll discuss Three parts. First, I'll describe some of the background for this instability, trying to give you kind of a general introduction on why some inverse bonds are unstable. And then the second part will be about the so-called entropy and capacity estimates. I'll describe what these things mean. So these seem to be the way to somehow generally understand the degree of ill-positions of a given inverse problem if it's non-linear. And then the third part. Here. And then the third part will be: I'll give a number of examples of in-response where this applies. Okay, so we're going to start with the textbook example of an in-response, which is image de-blurring. So here you see three images. So there's the original image and then there are blurred versions. So you think of the image as a function f in the plane, and this blurred Plane and these blurred versions are obtained by taking convolution of f against some function chi one or chi two as the point spread function and the inverse problem is to remove the blur go to from this blurred image back to the sharp image so so how is this done um so the blurred version as i said this fp is the convolution of the original image f Of the original image f with some coins function chi, and then if you take the Fourier transform, the Fourier transform, the blurred version is just the product of chi-hat and f hat. And now you have different scenarios. If your function chi-hat is compactly supported, then you are going to completely lose the high-frequencies in the image. There's no way you can reconstruct them anymore. If your chi-hat happens to be exponentially Chi hat happens to be exponentially decaying, then the high frequency would be exponentially damped. And if it's polynomially decaying, then the high frequency will be polynomially damped. And these somehow reflect the different degrees of ill-positions in the inverse problem. And now, if what you measure is the blurred image plus noise, noise is given by this epsilon. And then, if you just try to perform a naive reconstruction, which is obtained by multiple. Which is obtained by multiplying by one over chi-hat on the Fourier side, what you get is the original function f plus the inverse Fourier transform of one over chi hat epsilon hat. And this formula is saying that if you have some high frequency noise, so if this epsilon hat has some components for psi large, then if this chi hat decays very fast, then one over chi hat is going to grow very fast. So this is going to amplify. So, this is going to amplify the noise to a very large degree. And in this way, you can get huge errors in the reconstruction. So, you have these heuristics, which are very well known in the field, that some kind of smoothing or blurring effects imply instability in the inverse problem. And if you have strong smoothing, which is often understood as some kind of fast decay of similar values, this implies strong instability. Strong instability. And in this textbook example of deblurring, we could analyze this because the forward operator was very simple. Linear operator, which is just multiplication by chi-hat on the Fourier side. And the question here, what happens for more complicated im response, especially if the image is non-linear, for instance, like the Calderon problem? How do you quantify the degree of instability? So we give a framework for understanding this question in various, well, okay, also linear but non-linear inverse points. So the examples that we treated include the X-ray or Radon transforms, also with some limited data situations. There's analytic and unique continuation, which are also very like common things that have been used in universal programs. Inverse problems and some control and inverse problems for heat and wave equations, and also the Calderon problem. And somehow, so in this work, we identified three mechanisms which somehow give rise to this instability. So, there's a strong global smoothing property of the forward operator, and micro-local smoothing, and finally, only weak global smoothing. So, I'll try to give you an idea what this. Try to give you an idea what these three different mechanisms mean a little bit later. So, somehow to understand where this comes from, you can just start with an abstract inverse problem. So, you consider a map capital F, which is the forward operator in our inverse, which is a map between two metric spaces X and Y. And then the abstract inverse problem is given the small y in y. The small y in y, you want to find x, which solves the equation f of x equal to y. And the classical conditions for well-posedness of a problem, which was given by Adam Mard in 1902, were the existence of a solution, the uniqueness of a solution, and stability, which means that the solution H should depend continuously on. Depend continuously on y. And in in response, this third property, stability, typically fails in this form. So this is somehow what is understood by ill-position. This X does not depend continuously on Y, but very often you have conditional stability, which is the property that you can use for, for instance, for these convergence guarantees for statistical algorithms, which has been. Which has been discussed in these works by Richard Nicol and several collaborators. So, what does this conditional stability mean? So, it boils down to this fact from basic topology that if capital F is an injective continuous map between two metric spaces X and Y, then somehow if you restrict this F the forward operator to a compact subset of X. A compact subset of x. So if k is a compact subset and then you restrict f to k, then this will be a homeomorphism. In particular, the inverse of f restricted to k will be continuous and then you restore some stability. And typically, this restriction to a compact set K in X means that you are putting some a priori bounds on the unknowns. For instance, if X is the L2 space, then the set K could be the space H1. K could be the space H1, the Sovolev1 space, and you would be requiring that the H1 norm of the unknown is bounded by some fixed constant. So this would be an a priori bound. And if you have that, then this simple topological fact shows that there is conditional stability. So the inverse operator of F restricted to K is continuous. And then because you are working in metric spaces and things are compact. And things are compact. There is also modulus of continuity omega, so that the distance between two points x1 and x2 in this compact set k is bounded by omega of the distance between f and f. So if somehow if the measurements are close, then also these reconstructions would be close. And how close depends on these models of continuity. The basic examples would be. The basic examples would be if omega of t is equal to t, then the inverse map is Lipschitz. So, this is really the best stability you could hope for. So, typically, you could have omega t equal t to the alpha for alpha less than one. So, this is Hildrestability. And in these severely ill-postporons, you often have that omega is not even Hilda, but this kind of logarithmic thing leading to logarithmic stability. So, this is kind of So, this is a kind of basic setting for conditional stability of an abstract inverse problem. So, let's consider an example: this Calderon problem in electrical impedance tomography. So, you have this kind of same divergence form elliptic equation, which we already saw. So, divergence of gamma and W equal to zero in some domain in Rn. So, in this case, the unknowns is basically the space of quantum. The space of conductivity functions would be that all the L infinity functions which are positive, and you put the L infinity norm on this set. And then the measurement measurements are typically given by what is called the Diriel-ton map, lambda gamma, which takes Diri-Le data of solutions of this PD into Neumann data. So the forward operator takes the conductive function gamma to the Dirty Neumann Klambda gamma. And then the space of measurement would be the space of all bounded linear operators from sobola space h one half into h minus one half, where you use the operator norm, which I denote by this star. Okay, so this would be an example of an abstract inverse problem. So already uniqueness for this problem is highly non-trivial. So there are some important works by Silvester Urman, Asterla-Peiberant, and others. And others stating that you actually have uniqueness, but stability fails in the sense that the inverse operated f inverse is not continuous from fx to x. But what you can then hope for is this conditional stability, which is logarithmic in this case. So this was proved by Alessandrini. So the idea is you choose a compact set K, compact subset of L infinity, so that you have conductivities which have a positive. You have conductivities which have a positive lower bound, and then some high Sobolev norm of the coefficient is bounded by some constant, then you actually will have this stability inequality that the distance between gamma one and gamma two is controlled by omega of the distance between the measurements, where omega is logarithmic. And there are many variations and improvements of this basic instability, basic stability estimate, but also there's important. But also, there's an important result by Mandak stating this kind of result is optimal. So, if this kind of estimate holds in the Calderon problem for some models of continuity omega, then this omega necessarily has to be logarithmic. So, Holder stability is impossible unless you impose some stronger restrictions on the unknowns. Okay, so the argument in this So, the argument in this somehow important contribution was based on so-called capacity estimates for the compacts at K sitting in X and also entropy estimates for the image at F K sitting inside Y. But somehow these estimates were proved in a very ad hoc way by using spherical harmonics and separation variables in the ball, and some are constructions and special orthonormal basis. Autonomal basis, and maybe that's why this very nice somehow result has not been used so much later. But so, what we do in this work is kind of try to replace this ad hoc part of the argument by something general and structural properties of the forward operator. And the idea is that this thing is kind of very much independent of the inverse property. Very much independent of the inverse bound that you are looking at, so it applies to many things, not just electrical impedance tomography, also general geometries and coefficients. And some of the one of the messages of this talk is that the stability result for an investment is typically hard. So you need a quantitative version of the unique result. But the corresponding instability result, which quantifies somehow the inherent degree of ill-poseness of the problem, is somehow soft. Of the problem is somehow soft and it follows in many cases from some structural properties of the forward operator. Okay, so then let's go to the entropy and capacity estimates, which are the way of somehow making this work. So I'll try to explain this in pictures. So we are still in this abstract framework. We consider the forward operator capital A. The forward operator capital F going from one matrix space to another, and the inverse bound is given y, we want to find x solving fx equal to y, and now we are restricting to this compact set k and looking for conditional stability. And somehow naively, one way of understanding when you have instability is somehow if this forward operator strongly compresses distances. Strongly compresses distances. So if you have on the left, you have these four points which are relatively far apart. And then when you map them by f, these points will become very close. So now if you look at these points here, if instead of this one, you measure another one, then your reconstruction jumps from here to here. So you make a big error in the reconstruction, even though somehow the measurements are relatively close. So if you have this compression property, then you will have instability. You will have instability in the inverse problem. And how to do this more precisely? So, somehow you want to have sets in this K, sets of points which are relatively far apart from each other. So, you expect instability if this set K is extended. Formally, there exists relatively large epsilon-discrete sets. So, this means that a set is epsilon-discrete if the distance. Discrete if the distance between any two of its points is at least epsilon. So these points here are far apart, but this set F of K should be somehow compressed. So and rigorously, we mean that it has relatively small delta nets. The set is a delta net if the delta balls centered at the points of the set cover the whole F of K. So if you can cover by relatively small, Can cover by relatively small number of balls, then the f of k would be compressed. And if you have this kind of setting, and then if you are in a situation where you have more points on the left, and more, let's say, a bigger epsilon discrete set on the left than a delta net on the right, then necessarily, some by the pigeonhole principle, some two points on the left have to map into the same delta ball, and this is going to imply some high instability. Somehow instability that there is a question. Um, okay, uh, Christopher, have you raised your hand? Uh, yeah, I did. Uh, so just a question or a comment to this word compressed. Um, it's clear what you want to say, but what you want to say is closer, more closely related to compactness in some sense, because you're dealing with these nets. And compressed usually in a certain community has the idea of a significant. Has the idea of signal compressing or sparsification of expansions and so on. So, of wavelet expansion, say, then you compress by sparsifying. And what you mean or wish to say is maybe closer to, I'm not sure, maybe to squeezing or something that you is not compression in the sense of signal compression, but yeah. Yeah, yeah, yeah, that's uh, that's correct. So, so one could also say that f of k is somehow very compact, so that would be another way of stating these delta nets are usually quantifying in a finer way the degree of compactness that sets have in a larger ambient space. So that's okay, maybe that's just a nitpicking. Sorry, for That's just a nitpicking. Sorry for not continuing the talk and asking later. Yeah, that's fine. So, but somehow, just looking at these pictures, somehow this compression seems like a reasonable word to use for what is happening in this setting. Anyway, thanks. So we want to understand when the forward operator. A forward operator has this property that it somehow squeezes the distances in this way. So, one typical situation in inner response is when these spaces X and Y actually banner spaces, and this K will be a bounded set in some subspace X1 of X. You could, for instance, think of X as L2 and X1 as the suball space H1. And it also often happens that this image at F of K. That this image set f of k is contained in y1, where y1 is a subspace of y, which is in some sense compressed or somehow very compact in a certain sense inside y. So, and we are going to look at this situation. So, if this happens, if this f of k is contained in a compressed subspace y1, then you can study the somehow the extendedness of this left. The extendedness of this left-hand side just by looking at the embedding i, which embeds the function space x1 into x, and also the delta nets of f k are somehow related to properties of the embedding j, which embeds y1 into y. So now, if you are in this kind of situation, then somehow it's enough to study embeddings between function spaces, and then the forward operator somehow disappears from the picture. Appears from the picture. So, this is the reason why this is quite general and why it applies to many kinds of inverse. It doesn't matter what the forward operator is, if you can somehow realize this kind of compression property. And now it turns out there's an ideal tool for studying the compactness properties of embeddings in function spaces. This is well documented in the literature, and it's this idea of capacity and entropy numbers. Numbers. So these are somehow the notions of capacity and entropy metric spaces were introduced by Kolmogorov in the 1950s. He was somehow extending these information theoretic notions by Shannon to the metric space setting. And there's a lot of literature on these things on function spaces. There's a monograph by Edmunds and Tribel on these numbers. So let me try to explain. Explain what they are. So, if A is a bounded upright between two Banach spaces, and so A will be the embedding eventually, and UX will be the unit ball in the Banach space X. So, we define two sequences of numbers. Ek of A would be the entropy numbers, which somehow measure the sizes of delta nets in the image set A of U X. So, this power. So, this power 2 to the power of k minus 1, because we are dealing with entropy, so there's always a logarithm. We also have the capacity numbers, which are somehow measuring the sizes of epsilon discrete sets in A of UX. And the definition is maybe not so important, so I'll give you some examples later. It turns out it's enough to study these entropy numbers because it follows from this definition that these two numbers are actually. That these two numbers are actually very much comparable. And in our case, it's enough to study the entropy number of the embedding I going from x1 to x and the entropy number ek of j going from y1 to y. And these numbers somehow measure the compactness of the bounded operator A. For instance, A is compact if and only if these entropy numbers E k of A go to zero, and A is extremely compact, meaning finite rank. Extremely compact, meaning finite rank, if and only if these numbers, entropy numbers decay exponentially. And in Hilbert spaces, these entropy numbers are closely related to the singular values of the operator A. They are not exactly the same, but they are related. Okay, so a basic theorem in function space literature, which is stating that smooth spaces are compressed. So if omega is a domain, then the entropy number of the embedding Then the entropy number of the embedding I going from Hs plus delta into Hs, they decay like K to power minus delta over N. So this means that if this delta is very large, if you are embedding a high subolar space into a low one, then the entropy numbers are decaying fast. So this embedding is somehow very compact or compressing. And you have similar bounds on social spaces, on boundaries, and so on. And ultimately, these. On and ultimately, these kinds of arguments are based on a while asymptotics for eigenvalues of the Laplacian because you characterize all spaces by the eigenvalues of the Laplacian. Another kind of space that you could look at is the space of some real analytic functions on some domain, and you want some uniformity in the analyticity in some sense. They have uniform Cauchy bounds. So then the entropy number of the embedding of an analytics. Of the embedding of an analytic space into a standard Sovoli space will have exponential type dk. So analytic functions are extremely compressed in the spaces of sobola functions. Okay. And if you are dealing with the inverse problem like electrical impedance tomography or Calderon problem, your measurement is actually not a function, it's an operator. You can think of it as an infinite matrix. And one can prove a similar theorem for operators. The spaces of smoothing operators are compressed in the space of all bounded operators. So, for instance, Zm will be the set of all Biriel-Ten-Neumann type operators, which are self-adjoint and which are smoothing of order M in the sense that they map the domain H one half, not just into H minus one half, but into H minus one half plus M. So they are smoothing of degree. So they are smoothing of degree m and then this space W R, which is the space of operation which mapped H12 into analytic functions. So these are analytic smoothing. So then the entropy number of the embedding of the smoothing operators of order M into the bounded operators will have polynomial decay, whereas the analytic smoothing operators are extremely compressed in the spaces of bounded operations. Rest in the spaces bounded uprest, the entropy numbers are exponential decaying. And there are some open questions related to the optimality of these exponents. Okay, but maybe that's enough for the somehow the framework. And then finally, let's go to some examples, how this manifests in some particular inverse problems. So we have these three instability mechanisms that I'll try. Three instability mechanisms that I'll try to explain: a strong global smoothing of the forward operator, only micro-local smoothing of the forward operator, and also weak global smoothing. Okay, so let's start with the Calderon problem. And I have an elliptic equation. So this is also contains a potential. So basically, you could have any second-order elliptic PDE here. And the measurement is the Diral-Teneuman map for this elliptic PDE. The map which takes. This elliptic PD, the map which takes Girilet data into Neumann data. You can have this on a compact manifold with boundary or just a domain in Rn if you want. And the first theorem is stating that if all the structures, the manifold, the Riemannian metric, the boundary, are real analytic, and if you have a stability inequality with some modules of continuity omega, then necessarily this model is. This model is logarithmic. You cannot do Hilder or you cannot do better than logarithmic. And so, why is this? Why is this Calderon problem so severely ill-posed? So, follows from this abstract framework. You take the space X1 to be HS plus delta embedded into Hs, and the space of measurements will be the bounded operation from H12 to H minus one half. And then you have the forward operator. So, here I'm for a operator so here i'm for simplicity i'm looking at potentials q so this takes q into the direct non lambda q but actually it's necessary to look at the so-called difference forward operator so you subtract lambda of the zero coefficient from lambda q so this capital f will be the difference forward operator but if if you choose potentials q which vanish near the boundary Boundary, then somehow this difference forward up radius is giving you an elliptic equation with right-hand side zero near the boundary. And because of elliptic, analytic elliptic regularity, then the solution would be analytic near the boundary, which means that this difference forward operators in the strongly compressed space of analytic smoothing operators. And now we saw that the space of analytic And now we saw that the space of analytic smoothing operators is extremely compressed in the space of all bounded operators. So we get this strong instability. So this is the reason why the Calderon problem is so unstable. And some of the so this was done earlier for the Euclidean ball and for the Euclidean metric by Mandake, but we do it in this kind of structural way, so we get it for an arbitrary domain. So, we get it for an arbitrary domain, arbitrary variable coefficients, as long as everything is real analytic. Okay, so do you need S large enough here? Here, S can be any real number, I think. So, so it's the delta here is okay. Delta can be any positive number somehow. Okay, so you need some, yeah, so you need some minimal regularity of Q in. minimal regularity of Q in order to have this thing even well defined. I think Q in H n over 2 plus epsilon is settled enough because then it will be L infinity. But I'm going to show you a better result where this Q can be basically optimal. So I'm trying to understand what does the theorem say if the stability estimate holds for all Q1, Q2, that's what you're saying that satisfy this constraint? Yes, yeah. Constraint. Yeah, so yeah, you satisfy this a priori bound. So, but you're quantifying over all, so it has to hold for all q1, q2, and then the modulus has to be logarithmic. That's what you're saying. Yes. So basically, I'm saying that in the set of all h plus delta functions with norm less than or equal to one, there exists some sequences of q1 and q2, which are showing that this omega of t has to have this behavior. Okay, thank you. Okay, thank you. So, yeah. I think Francois has a question as well. Sure. Is life really much better if Q was supported all the way to the boundary? If Q was supported all the way to the boundary. So what we need for this kind of instability result is that we need these potentials to be equal to some fixed background function Q0 in a neighborhood of the boundary, because that's how we get. Boundary because that's how we get this somehow elliptic regularity near the boundary to kick in. So that is actually somehow needed in these instability results. So if you drop this condition that things are zero near the boundary, then okay, of course, this is a special case of the general thing, so you still get the instability. But yeah, but that's how this thing works. But that's how this this thing works. Thank you. Other questions? Okay, I guess I still have a few minutes. How many? I mean, I started a bit late. Quite a few. Well, I mean, let's say five more. I mean. Okay, yeah, that's fine. I mean, okay, yeah, that's fine. Right, so this is an example of this kind of strong global smoothing mechanism. So, the forward operator maps into a space of operators which are globally smoothing in a strong way. So, the next example is related to this other mechanism, which is my global smoothing. Okay, just before you begin, there's another question. Sure. Chris, go ahead. Yeah, so can we? Uh yeah, so can we just go back to this um previous? So, um, I understand the argument, but I do not see where the analyticity of the boundary comes in because if q is zero, you say near partial m. So let us say it's zero in some strip, in like a small, in a tubular neighborhood of partial m. And let's say is not smooth, m is lips. Not smooth, MS lip sheets, for example. So then still you have this analytic smoothing locally in the interior and you still have this key of your argument is still intact. So I do not see where the analyticity up to the boundary is really coming. Yeah, so it's local because this analyticity is a local result. Because this analyticity is a local result that you do on small balls in the interior. So the boundary doesn't this argument is completely unaffected if the boundary is not analytic. Yeah, so it's correct that in the interior you would have analytic regularity, but you would not have it up to the boundary if the boundary would only be indeed. These measurements actually live on the boundary. Yeah, I know. I mean, but you are the measurement to be analytic on the boundary in order to conclude that the forward operator lies in the set of analytic smoothing operators. But actually, I'm going to give a stronger result where the boundary can be leaps. So you're still. Yeah. Yeah. Yeah. Okay. So the next example would be the Radon transform. So in the plane, this is this R is the Radon transform just integrates the function f over straight lines. And the inverse point is from the integrals over straight lines, you want to determine f. And the theorem is saying that now you have limited data. You are integrating over lines in the set L, which is a strict subset of all the lines in R2. Of all the lines in R2, and it's closed. So you are missing some line in the data, and then you are missing a neighborhood of this line. So then, if you have this kind of stability, so you have uniqueness in the inner spawn. If you have stability for some modulus omega in any sub-bolling norms, then this modulus has to be logarithmic. And somehow the idea is that now this your measurement is just basically this redundant. Basically, this horizontal R multiplied by a cutoff function chi L. So you cut off to the set of lines where you actually have measurements. And this is not globally smoothing, it's only micro-locally smoothing. So this is where micro-local analysis comes in. So it smooths out any singularity near some x-naught, psi-naught, which is depending on the line that you are missing. So this is kind of a wavefront argument. And so this is a linear inverse paranoia. Is a linear inverse parameter, so there's an argument of Stefano van Ullmann, which is somehow based on testing this stability inequality with wave packets with singularity at x naught, sine naught. So this is showing that omega cannot be held there. And we give a version of this instability, micro-local instability result which also works for non-linear inverse pronunci. So we again we use some kind of analytic micro-local smoothing properties of this. smoothing properties of this cadaver forward operator. But now you are not looking at functions in H s plus delta, you are looking at p of those functions where p is a microlocal cadaf. So it somehow localizes in phase space to a certain region. And for this instability we need the while law for pseudo-differential operators which are only micro-locally elliptic at some points. So this is somehow the ultimate reason why you get this instability. This instability. Right. And we also have an example for the Geodesic X-ray transform in the presence of conjugate points, building on some earlier work by Monal, Stefano, and Ulmann. But maybe let me go to the final thing. So far, we have seen this smoothing implies instability phenomenon that if F maps into the space of smooth functions or micro-local smoothing operators. Globally smoothing operators, which is understood as compression of distances, then the inverse one would be strongly unstable. And we can prove these kinds of results when all of the coefficients are either C infinity and the boundary as well, C infinity or real analytic. And this works for general geometries and variable coefficients, whereas there have been various earlier results which somehow are based on some special computations on balls or half spaces and On balls or half spaces and constant coefficients. And the final thing is: if you drop this C infinity or real analytic condition, what happens then for if you only have very rough coefficients? And the answer is that you have the same kind of instability also in low regularity. There's a different compression mechanism, which we call iterated small regularity gain. And you get instability for Calderon problem even if the coefficients are L infinity, if the boundary slip. are L infinity if the boundary slips and so on. So I guess I'm already out of time, but just kind of a one slide idea. So you look at again, you look at the difference forward operator, but now instead of having a strong smoothing effects, you use kind of a tiny smoothing effect many times. So you put some artificial interfaces inside the domain, and then the idea is you jump from the original boundary to Jump from the original boundary to the boundary M0, and then somehow you go from the boundary of M0 to the boundary of M1 and always solve an elliptic PDE. The coefficients are irregular. You only have a tiny gain in regularity, which is given by the Mayer's estimate. And some norm is large depending on the distance between these two interfaces. So you factorize your difference forward, upward as a composition of many things. You estimate the entropy numbers of the composition. The entropy numbers of the composition, and then you optimize with respect to the number of artificial interfaces, and you get the same result as in the smooth or analytic case. Okay, so I'll conclude with summary. So we have seen that different smoothing effects, strong, weak, or micro-local imply instability properties of the inverse bonds. And somehow, this is due to kind of compression or squeezing properties of the forward operator, which can be precisely characterized. Operator, which can be precisely characterized by these entropy numbers, and this also applies to non-linear inverse bonds with general geometries and coefficients. So, there are some open questions. There's a big literature on regularization theory, and we don't yet know all the connections between this work and that literature. We know some, but there could be more. We did not get very good answers for in-response for wave equations. Inverse points for wave equations, and also this does not really apply to non-inverse for non-linear PDEs, it only applies to linear ones. So, that's another interesting stability question. Okay, I'll end here. Thanks very much.