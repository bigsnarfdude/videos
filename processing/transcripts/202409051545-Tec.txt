By problems with social impact. And I'm going to talk to you about. Am I speaking loud enough? Maybe not. Okay. And I'm going to speak you about a particular work, which is optimizing heat alerts, optimizing when to issue a heat alert to protect health. And we're going to use reinforcement learning for that. But in the pro like when we did that, we ended up using a lot of Bayesian techniques, which I think might be of your interest. Be of your interest. And then that's going to be like the first part of the talk, and then, or more like the first two-thirds of the talk. And then, by the last part of the talk, I'm going to talk a little bit about a first attempt to try to leverage like new techniques on LLMs for reinforcement learning and how we apply it to this problem. Okay. So, a lot of what I'm going to talk today is on published. It's ongoing work. Some results are already published, or others are not. So, I encourage you to ask a lot of questions, particularly you don't understand. Lot of questions, particularly, particularly you don't understand something, and also to while during the talk or at the end, it's totally fine. All right, it's a very long introduction. Okay, so heat is a public health problem. I don't know if it's obvious for everyone, maybe now with climate change, it is. And this is an example of like the resources in the city of Boston when there's like a heat wave, like they deploy cooling centers. However, like it's it's believed that this is not uh That this is not optimized in the right way. And just to give you a little bit of background, in the U.S., heat alerts are issued by the National Weather Service one to two days in advance of an extreme heat wave. And there's a lot of different criteria that is used on when to issue it, issue an alert, mainly when it's above or beyond a certain temperature threshold. However, there's a lot of like local variation due to discretionality. Variation due to discretionality by the local officers or also errors in the forecast, et cetera, which from a statistical perspective would give us the opportunity to learn from it. And some previous works have pointed out in the particular epidemiological literature that the current way the alerts are issued is not well optimized or aligned with health risks. That's the premise of the work. And just to give you a little And just to give you a little bit of background, this is how a heat alert looks like. And I'm not going to work with any of the NLP texts. All of the data we actually have is already processed. We know when, what part of the, in the US was, at what time it was issued. And so, right, so this is like what we want to do first. We want to optimize here issuance, or rather than optimize it, what we're going to talk about today is what is the tool that we can create that we'd allow to do this. Can create a would allow to do this. So, create a tool that would allow people or algorithms to optimize when to issue heat alert based on history, based on previous alerts, the cost that each alert has associated to it, alert fatigue, which is like if you're constantly alerts, people just don't react to them. And also, very importantly, confounders, demographic conditions, climate adaptation, climate types, depending on where you live. Adaptation, climate types, depending on where you live, income, et cetera. And so I'm going to give a little bit of background, and I'm going to go fast through this, but if I'm going too fast, I will just look at your face and know if it's going too fast. But first of all, variational inference is the thing that I would assume most people here are familiar with, at least the purpose. So I'm going to just skip to the main takeaway: we're going to optimize. Take main takeaway is like we're going to optimize and surrogate outcome that balances between the likelihood and the divergence between your prior and an approximation of your procedure. And that was very loose, but that's the idea. It's almost Bayesian, right? And we believe it. Then this is probably a little bit new to more people here, markup decision processes. So again, in the interest of time, I will just give you a little bit of like the big picture. The big picture. So we have an agent. We're going to operate under the premise that there is some algorithmic agent interactive with some environment which represents the structure of the decision-making problem. And at every time step, the agent takes, it's situated at a state S of the environment. The conditions, for example, like this is very hot, it's been really hot the last three days at the state, and takes an action, which of course. takes an action, which of course I'm going to, it's going to be issue an alert, and then observes a reward, and then the state advances to a new state. And you know, this can be put in formal mathematical terms that is not really worth delving to that, but the point of RL is going to be to learn a decision-making policy, which is simply going to be a series of rules that map. Map state to an action. Now, usually in the super formalism of reinforcement learning, we say to a distribution of our actions, even though we know that in the vast majority of decision-making problems, the optimal action, there's only one optimal action. But allowing for having distribution over action is what allows to have algorithms that explore in an interactive setting. And that's one thing. And then a little bit of the terminology: the return or cumulative reward, it's the formal objective or more precisely the expectation of that is what creates the objective of optimization is maximizing, given some initial unknown distribution of the states, which is not the important part. The goal is maximizing your expected return or your value function. Value function. It's a complicated expectation over all possible future states and pathways that the environment can go depending on the actions you take. And additionally, even though the simple expression, which is the usual RL objective, doesn't have it, there might be constraints on when you can or not take certain actions. And for the heat alerts. You know, for the heat alerts problem, this is like a table mapping this abstract terminology to the actual problem. And so, for example, like an episode is going to be, if you hear me say episode, this is going to mean like an entire summer from the beginning to the end, because we don't care about heat alert during the winter. And specifically, it's going to be, and when I talk about an episode, I'm going to mean in a given location. So it's a local problem. State is going to consist of two basic components. just as two basic components like the weather, the past weather, maybe past hospitalizations, everything that we can gather in the data about a decision problem. The exogenous part is going to be things that the agent cannot control with any action, such as the weather. And then there's going to be more endogenous parts, which is what the action, the agents or the policymakers' decision is going to change the state space. For example, the history of previous actions. Of previous actions that goes into the state, then that obviously is endogenous. That makes sense. And yeah, and then we have constraints. So the constraints part is a little bit tricky. For today, we're going to talk about simplified constraints, assuming that, for example, at the beginning of a summer, the decision maker knows beforehand. Maker knows beforehand that it can only issue a certain number of maximum alerts. For example, there's a maximum cost of resources that can be deployed. Of course, in a more realistic setting, this could be a little bit more elastic. So what I'm going to show you here first is a Bayesian reinforcement learning environment or how we constructed a tool, an environment, simulation of the truth that allows to use solver RLC. To use RL solvers to optimize this task. And this content is going to be in one of, I'm going to put a link at the end. This is already an archive and it's under review, optimizing heat alerts with reinforcement learning. And so the idea is just, it's very simple. There's two parts. It's really very simple. One is because the weather is exogenous, like we don't need to make much about it. We can just go and We can just go and use the actual climate data to as a basis of a simulator. Normally, just to give you a background in usual reinforcement learning, if you want to create a world model, you have to also model transition function of the state, but that's really hard usually. In this case, we don't need to do that. And then the second part is a hierarchical model hospitalization, basically creating our cost function or reward. I'm calling reward, but it's a cost. In reward, but it's a cost. But if I multiply by minus one, it's a reward. So I'm going to use reward for hospitalizations, even though it sounds weird. But that second part. And then it's, and you know, we take data from tons of sources. I'm not going to stop on that, but the main takeaway is we use real hospitalization data in all the US for the last 20 years. So it's the environments calibrated or learned from real data. Real data. And so, on the, you know, because this is a hierarchical model, it has multiple layers, multiple layers. So, starting from the top, we're going to use classic Poisson model because we have count data. So, the likelihood is going to be a Poisson likelihood where we have, oh, this is what nobody's using. We have a, this is the eligible population, and we have the per capita rate, how many people we expect to be hospitalized. And then here's the tricky part. tricky part. Because the real data, the effect of a heat alert is really small, usually between 1%, 2% of the observation data, we needed a model with a very strong inductive bias, something that if we were just doing like a free-form regression, sometimes we even observed that the effect of a hospitalization was learning to be, alpha heat alert was learning to be negative, which doesn't make sense here. Or even the effect of heat was on the other direction. Heat what's on the other direction. So, because of that, we have this like nice form where we basically have two functions that drive everything. Like, the baseline function is how many people are expected to be hospitalized, where there no, would there not be any alerts? And then the other part is the effectiveness of an alert. So, the magic here is like this not this function goes between zero and one, depending on the state. One means like no, you save everyone, zero means you save no one, and then that also changes. Note one, and then that also changes depending on the state, the time of the summer, etc. And then it starts getting interesting here. We're going to specify these, again, this is like a small data problem because even though we have a lot of data in the whole US, we only have a few years of observations per location. And if we don't allow to have coefficients in each location, we fall. Efficiency in each location, we're going to be at risk of what sometimes people call spatial confounding or confounding. And in practice, it works really bad if we try to do like a single regression for everyone. On the other hand, if we do use different regression, we learn the simulator differently for each location, then we have a problem of a small sample size. And also, this might be more technical, but you might be tempted at using like random effects to. Using like random effects to gain a little bit of efficiency in your learning. But the problem is, like, this is what happens when you do fixed effects. Like, you just do regression everywhere. And then this is like the learn parameters that govern those effectiveness and baselines. They should look very wild across locations. So, like, the random effects model alone is not really like a good solution here. And so, I hope this makes sense because if it does, then nothing's gonna make sense. But so, the idea here is. So, the idea here is we're going to design a data-driven random effects model where, like, this is where the true Bayesians are going to start, like, forgot, right? So, we're going to learn the prior. And we're going to, so the idea is we're going to use a neural network that's going to map the confounders, things that are specific to every location. We're going to map them to a prior, and then we're going to optimize this like. gonna optimize this like monster uh monster elbow monster objective that has uh like every variational it has a likelihood and prior part and then we have the variational posterior parameters that's that's just as usual but then we have also the parameters of the data driven prior so we're gonna learn everything is learned the beauty of this is like from optimization perspective there's nothing special about it this is just reading the sense nothing special about it. This is just gradient descent. What's weird about it is that part of the parameters are going to like fully Bayesian or well Bayesian, not fully, but like to be to learn a posterior and the other part of the parameters are going to learn the prior. Okay so in the interest of a Bayesian audience like the elbow converges and then you know And then, you know, every time you do variational things, you got to ask yourself, is there any point of being Bayesian if I cannot trust my uncertainty, right? And then you don't really know, but at least what we try to do here is using simulations, but simulations from an actual learned model so that it reflects the truth. We draw like over a thousand simulations and see if what the variation I would learn from that simulation would have good coverage. And then basically, Would have good coverage. And then basically, like, we get really good confidence intervals when it comes to estimating those parameters. But this is just saying that, okay, kind of variational seems to be working fine here. It's not mean field variational. It's a slightly more sophisticated thing, like low-rank multivariate approximation. Anyways. And then the other question you might ask yourself is: do we need something as complicated as this neural network mapping the confounders to The confounders to the means? Like, could it be something simpler? And it's a really hard question to evaluate if it even was worth it. My first instance is like, let's just do it because it's easy once you have this variational thing. We're using something called Pyro, which is made for neural networks and variational inference. But then we try to figure, this is like more of the ongoing thing. We're trying to figure out, you know, what's the gain of having done this. And it seems that at least. And it seems that at least if we're naive enough of trying to think of prediction as how it will predict on new data, as a measure of fit of using this kind of better priors, then it seems that using a neural network is better than just using like something centered at a linear function of the confounders versus something like just using random effects, random slopes to be more precise. Slopes to be more precise. Hope that makes sense. Now, I say it's hard to measure because it's real data. So, how you split between training and test data is kind of hard. We have two options. One is like evaluate in the future, but then it's like out of distribution data. The other is like split randomly, but then it's not really independent. So, I kind of figure out what's the best block structure to evaluate this, but it seems to work. And then the other. Seems to work. And then the other thing we're doing right now, and this is interesting, is fortunately Yunshin's not here, but like the self-supervised embeddings, I think, would simplify a lot of the complication here. We can avoid entirely learning a neural network and just start with something that has a better representation of those confounders or their effects on health. Okay. So to summarize this, this is how we would use this. This is how we would use this. First, we select an episode, meaning the location and year. Then we get the posterior or this data-driven posterior, and then we get our simulator, and then we train an RL agent there. And then, so in our paper, we show that the RL solver performs poorly in this environment, like the default PPO, QRDQ, and all those things don't work really well because this is a problem with a very long horizon. Is a problem with a very long horizon, 150 steps every day of the summer. It's really hard for RL. Then doing a lot of tricks like not allowing the model to issue, not allowing the RL agent to issue alerts on cold days or when there's low positivity, we estimate kind of like a propensity score with the observed data, things like that. We can show that we can do a little bit better on average than the national weather service. National Weather Service Policy. And this is, you know, it's quotation marks because we're doing everything of this like in a data-driven way, and the true effects that we observe in the data from a heat alert are like 1%, 2%. So, you know, so of the explained variants in hospitalization. So it's really hard to have definite measures of better. But this is where the next part of the talk comes in. Like, can we improve a little bit how dumb this, how bad the poor performance of this? How bad the poor performance of this default reinformation learning solvers. Because examining how this reinformance learning problems are learning is a little bit, this is what happens. So say that this is all an intuitive narrative of what I think is a problem. But say that we can only issue 10 alerts in the whole summer. Then the exploration policy, if it's too aggressive, is going to issue an alert very quickly at the beginning of the summer, and it's never going to see. And it's never going to see, have the opportunity to issue alerts during training late in the summer, or the other thing happens. So, really, reinforcement learning doesn't seem to be, oh, the default solver is like the right tool for this optimization problem. Maybe something more like tree search or planning would be better. But sometimes, like intuition says, look, the forecast says it's going to be really hot by the end of the summer. And you already know that this is when, I don't know, this is more or less. When I don't know, this is more or less the effect of heat is more or less dangerous. Like, can't you just be a little bit intuitive about it, right? Also, please don't try to explore sending alerts in cold days. Like, we don't, without a human having to manually restricting, which is what we did for the paper, when how to explore. And so, before just getting to that, the environment with it is in GitHub, and you can install it. It's in GitHub and you can install it and you can train your RL things. It's a very neat thing. It's the standard RL stuff, the gymnasium package. All right. So going into this part, it's very early, all of this. And so just out of the box, how did the LLM perform and this task? What I mean, LLM performing this task is without any training. What if we just ask an LLM, what would you do? If you were the policy. What would you do if you were the policymaker? Would you issue an alert or not? So, how well can these things reason about this? So, this is like the non-scientific thing is a dual prompt. This is what prompt engineerings do. It's not scientific at all. It's trial and error. So, here, for example, we create instructions. Hey, you are assisting officials from the National Weather Service in making optimized decisions about when to issue a head alert. You should receive. You should respond a simple yes or no, whether you would issue an alert in these conditions. And then we provide context: you know, this is the location where you're making the decision. This is the history of alerts in the last three days, streak, the heat index forecast for the next 14 days, and the max forecasted per week in the next summer, things like this. So, do you think it's going to perform well or bad? Bad. You don't. Bad. You don't throw why because we don't it's really hard, right? Uh, it does better than random, but it doesn't do better than a train RL policy. So everything does bad, but it does, it does worse actually than I thought. I thought it would be a simpler problem. It's a small difference, and that's part of the issue. And that's part of the issue. It's like, so I made this plot relative to the best one. So let's say, because it's really hard to, the real numbers don't make any sense. So this would be like, this is 5% worse than this one. And then, and then there's a lot of noise and variation across, like, we need many draws from this posterior because the posterior learn like big intervals because of, again, Intervals because of again, like the lack of strong evidence in the real data. So, the next thing we can try to do is: can we do a little bit better than just asking the LLM? Can we do, if you want, a bit more Bayesian about it? So this is like the part of using them as prior. So there's two relevant works about this. One is instruct ORL, which is like, and those two are parallel works. Uh, parallel works. One is regularizing Q-learning, where basically Q-learning is a very famous reinforcement learning algorithm, but they add regularization that looks like likelihood loss for the where you treat the LLM answers as sort of like as labels, as real data, and then you're optimizing for your policy takes the form of a learning density function because it's distribution over actions. So your likelihood. So, your likelihood is like log density, and then you evaluate that on the real labels. And then the other work is a little bit much more statistical. They describe the whole theory of KL regularized RL. And it ends up looking very similar to the elbow, if you want. There's a part that is the RL objective, and then there is this part that is penalizing diverging from the LLM estimated correct answer. And then Correct answer. And then I don't know how people are familiar here with the term distillation. So the idea of distillation is like some of these things, KL is really hard to learn, but then you approximate it by, again, like a supervised loss or like likelihood loss. And then, okay, so what happens when we do that? And then it's like, so they, oh, so maybe I went too fast. But the idea here is like, okay, take the actual RL agent, but just do. Okay, take the actual RL agent, but just do a little bit of regularizing the RL agent, and then you can help the RL agent a little bit again with a lot of noise. And I have to say, this evaluation is only for like one environment. So if you, you know, when we put this in archive, if you see a different number, please don't kill me. But it seems to be helping, or at least it's better than the LLM alone. So you might wonder yourself, like, you know, if it's just better by the LLM alone, what's the point of using LLMs at all? Of using LLMs at all, even if we were not to observe any difference here, which could be totally possible that this is random noise, there's still benefits of doing that. And this Hua Sadi paper, what they argue is that sometimes many decision problems have multiple solutions, but then by using the LLM prior, you get sort of aligned with what a human would choose. Like if you think of like a multimodal, multiple solution problems. Well, multiple solution problems, multiple modes, you sort of like it orient the solution towards the one that a human would find more interpretable. And then they conduct like a series of experiments of like, in fact, their paper is called Language Instructor RL for Human AI Coordination. So there's a little bit, if you're interested in that, I think there's a lot of exploration opportunities for that, like how to use LM to make the policy more interpretable or understandable by a human. Or understandable by a human, which at the end is what's going to be needed if we ever want to deploy this heat alert solution in a real setting. A human is to trust it. And then just like some final thoughts, it's like LLMs alone don't perform really well on decision-making tasks. This is known. There's a couple papers about that. There's ways to improve that with prompting and things like that. But other works have looked at LM's tools to design reward functions. It's something I didn't talk about this year at all, but it's worth looking at. talk about this here at all but it's worth looking at then there's uh beyond llms or different ways that foundation models as a emerging uh emerging theme research theme can help rl uh there's work on video language model generative environment etc like um and then lastly like here i only use lms like i didn't uh make the lms better but uh something i've been also doing on a different project is like On different projects, it's like find ways to fine-tune the LLMs to solve these problems better. And it's totally possible, but the problem is it is computationally really hard. These LLMs are optimized for chat things using reportable learning from human feedback. But if you want to optimize them for decision making, it ends up being like a really similar process. And that's everything. Thank you. Any questions? Any questions? A stupid question. A data-driven prior? Is it simply a Rahical model? No, because I am not learning the parameters of that part of the prior in a Bayesian way. I don't have a posterior over. You're learning something, otherwise, you wouldn't call it data-driven, right? I feel it's a tricky question because you, you know. I feel it's a tricky question because you know the answer. I was just wondering why you why you're belittling it so much. I think there's two reasons why. One is like in the history of like more empirical, I don't know what I'm going back. I'm not going to get all the way. Oh, maybe. In the history of, and again, this is a little bit outside my expertise, but the history of this like empirical based methods, like this can be related sometimes with a like marginal likelihood. Like marginal likelihood. So there's a series of works on how to learn the best prior by optimizing the marginal likelihood rather than the likelihood. So it's a little bit related to that. Thanks for the interesting thought. So at some point you started using LLMs as sort of a prior regularization. Right. The output of LLMs is random. And it's sort of like, in some sense, it's preformed text, right? So I'm wondering, you know, the output space of the LLM, so of course that's asserting some kind of prior distribution of sequences of characters, basically. I'm wondering, how do you ensure that the things that the LLM are spitting out are, I guess, interpretable or comparable to the things that you're actually playing with? So, for example, yeah, it's a really good question, and there's a trick. Yeah, I was going to say, I think all. And there's a trick. Yeah, I was going to say, I think all of us probably play with LLS at this point. And, like, even if you give it very structured queries, it'll the form of what it looks like. Am I not responding what you want? Right. So, if you, there's two answers to that. One is like, if you have a little bit of existing data, like good or right choices, usually you do supervised fine-tuning. And then, usually, with a little bit of examples, they will learn to only speed things that make sense. But the other is, this took me a way to figure it, took me time to figure this out. Took me time to figure this out. Is you know, the way the LLMs work is like they use this transformer, it's out of time. Uh, and the last layer of the neural network actually is used to predict the logits of what is the word. So, what you can do is grab those logics and directly compare those logits with the potential answers you have. So, you can directly compute only for the potential solutions you have, what is the likelihood of each one of the answers. Just look at the basic conditional distribution of. At the basic conditional distribution of the output of the LLM only on the tokens that you actually exactly, yeah, yeah. So then you know one is more likely than the other, even if the LLM would have chosen a different action. Are those the high mass tokens always, or do you often condition on low probability sets? Yeah, so I think if the LLM is good, they're going to have decently high mass, but then it's super highly dimensional. So mass is weird. Yeah. Thank you. 