Okay, so Shir Shindu will speak to us about watermelons, which sounds quite good right now, and the joint works. Just wanted to check, is the whole slide visible? I can see to the bottom of two boxes. Can you see this? I can see that just at the bottom edge. Yes. Okay. So take it away, Shushande. Yep. Yep. All right, good. So, yeah, thanks again for the organizers, to the organizers for this invitation and really nice series of talks. So, what I'll talk about today is some recent results about objects known as geodesic watermelons. I'll define what these are, and everything in this talk is going to be based on the joint work with Riti Basu, Alan Hammond, and Milind. I guess all of them are here. I guess all of them are here. And I think Melan will sort of navigate the chat. So feel free to ask questions. And also, you can, I'm not sure if this is allowed, but you can also sort of unmute yourself during the chat and ask questions. Okay. So just to sort of get things started, so I'll work with the lattice last passage percolation model. So this is sort of an instance of a sort of box in Z tu. So every vortex of the picture is really Vortex. So the picture is really about the dual lattice, but every vertex in Z2 actually has an ideal random variable. Xv will be the vortex, the random variable attached to the vortex V. And just for concreteness, I'll sort of start with sort of assuming all of them are ideal exponential with rate one. Although the arguments are pretty robust, but let's for concreteness take this to be the starting point. And then we have heard the word geodesic a lot of time, just to sort of again set the stage. So the geodesic for us would be. So, the geodesic for us would be: so you'll start from let's say 1, 1, and then go up to Nn, and it's going to be an oriented upright path, and such that the weight it accumulates, which is the sum of the random variables along that path, is maximized. And like I said, so this is the starting point, but the arguments really will not really use any precise details of this distribution. Okay, so again, some notation: so xn is. Again, some notation. So xn is going to be the GODC weight from between 1, 1, and n. And gamma n is going to be the unique GDC. So this is sort of a, the random variables are continuous, so no two paths are the same weight. So there's going to be a unique path which maximizes the weight. So that's going to be called gamma n. And again, so a lot of stuff is actually known for this exponential model. So it's known that the law of large numbers for this random variable xn is 4. So it's typically like 4. So it's typically like 4n, and then there are some fluctuations. And the fluctuations are given by this Kp's exponent one-third. So, more precisely, the scaling limit of this random variable is something which is an order one distribution. So, xn minus 4n, and then you have to scale by n to the 1 third. There's a constant floating around, which is not too important. And the limiting distribution is what is known as the Tracy-Wittem GUE distribution. So, not again talk too much about it, but one. Not going to talk too much about it, but one sort of important fact for us would be that the mean of this distribution is negative, which means actually xn for any finite n, the mean of xn is actually strictly smaller than 4n. And it's actually going to be 4n minus some constant times n to the one-third. And actually, much more is known. You actually do know a lot of strong tailbounds. Strong tail bounds for this random verbal XN and also large deviation bounds. And so these go back to work of Johansson and then subsequently via the random matrix connections, work of Ledoux and Ryder. So a lot of stuff is known about the tail properties of this random variable XM. Okay, but then there's also another geometric object lurking behind, which is the geodesic itself. And so you can also talk about fluctuation properties for the Geodesic. And so one particular observable of interest is: so you start from one. Is so you start from 1,1 and you go up to nn, and then one natural question is how much does it sort of move off the diagonal by? So we know if this was a Brownian bridge and this box is size n, then this would be typically root n, but it turns out here it's the other K-P's exponent shows up, and the maximum deviation you expect from the diagonal of this random path is roughly of the order of n to the two-thirds. And again, you have sort of quantified versions of this. So, Tf is going to be my So, Tf is going to be my notation for the transversal fluctuation, which is, I won't define it precisely, but it's going to be the maximum that you deviate from the straight line. And so a tailbone like this is true. So transversal fluctuation being bigger than x times n to the two-thirds has a straight exponential e power minus x cube type tail. And so this was some sort of not optimal, but tail conditions were. Not optimal, but clear conditions were proven already in this paper of Redi Basu, Alansla, and Brother Siddhartha on the slow bond problem. And then subsequently, sort of adapting this argument, one can actually get the optimal exponents. Okay, so we're talking about the geodesic from 1,1 to nn, but you could also sort of vary the endpoints. Instead of taking nn, you can vary them along the straight line x plus y equal to 2n. So this is, let's see, I can probably highlight. I can probably highlight it. So, is this marker visible? So, yeah. Okay. So, this is sort of the straight line x plus y equals 2n. And so nn is the center point here. And so, you're talking about geodesics going from 0, 0 to nn, but you can also sort of talk about geodesics going wherever. And so, it turns out that there is a process version of this, which again has been alluded to in many of the previous talks. So, just to set up again, set up the notation. So, just to sort of again set up the notation, so suppose you take any point, this red point on this line, and let's say it's sort of moved away from the diagonal point nn by some n to the two-thirds type scale. So, this is let's say n minus n to the two-thirds x, n plus n to the two-thirds x. So, it turns out that if you look at this entire process and again have the same centering and scaling, you get a process convergence. And the limiting object is this. So, there is some concavity going on by super additivity. So, this is this minus. On by super additivity, so this is this minus x squared term, and then of course you have the stationary redu process that gives you the fluctuations. So, as a result of this parabolic loss that you get if you go away from the central point, it turns out you can prove that if you take any path from 1,1 to nn with transversal fluctuation actually bigger than x times n to the 2/3, typically the weight loss that you see from the typical value of the geodesic is going to be x squared times n to the 1/3. So, n to the 1 third is the scale. squared times into the 1 third. So n to the 1 3rd is the fluctuation exponent. So that's always going to stay there. And so the dependence on x is parabolic. Okay, so typically things that are far away from the diagonal by x times into the two-thirds suffer some quadratic loss in x. Okay, and again, so we have already seen this line ensemble before. So this is sort of the top curve. If you plot this, this area 2 minus this x square object, this is what it looks like. Square object, this is what it roughly looks like, and this is the limiting scale weight of the geodesic. But again, as we have seen, you can actually embed it in a line ensemble, and I guess I should have said parabolically shifted, but it is actually a part of a bigger line ensemble. And it turns out that the other curves in the line ensemble also have some really nice sort of geometric interpretation. And it turns out the following thing is true. And this is really going to be the sort of starting point leading into the definition of a geodesic watermelon. Leading into the definition of what geodesic watermelons are. So the top curve gave you the weight of the geodesic. If you look at the top K curves, and if you sum them up, that's going to be in some limiting sense the weight of the maximum weight of K disjoint paths that you can get, let's say from 1, 1 to NN, so that the total weight is maximized. So you want disjoint paths. They can't all start at 1, 1 and NN because then they won't be disjoint. But you can sort of imagine in a continuum setting, maybe they do. In a continuum setting, maybe they do. In the discrete setting, you have to do something at the end. But roughly, you have k-paths going from the bottom corner to the top corner of this box so that cumulatively they pick up the maximum amount of weight. So, is this statement clear what the basic object I'm referring to is? Okay, and so just some more motivation. So, the line ensemble that we just saw enjoys a resampling property, which was set up the Property, which was sort of the key in many of the previous talks. And as we have already witnessed, it has been crucial in many of the recent advances. Even in the pre-limit, this maximal weight of K discharge parts actually sort of exhibit very remarkable connections. One particular would be in some of the integral models, it does actually admit a distributional equality to, let's say, length of top K rows in a random Yan diagram via the RSK correspondence. So it turns out that not only do you see this limiting That not only do you see this limiting k-path interpretation coming up in the limit, but also in the pre-limit, there are concrete objects that it sort of is connected to. Okay, so that's one interpretation. It also turns out that this notion of understanding disjoint near optimal or optimal paths, so which also Eric pointed to, like we were looking at the household dimension of points admitting disjoint geodesics, but if you also, if you sort of relax it to be If you sort of relax it to be near geodesic or almost a geodesic, it turns out that that's also a very useful thing to understand for many applications. Certainly, naturally, it gives you some understanding into the multiple valleys or the geometry of ground states in the LPP energy landscape. So the geodesic itself is a ground state. But typically, such landscape actually have multiple valleys, and near ground states or near geodesics are basically these valleys. Valleys. So understanding these paths actually gives you some insight into this rich landscape. And such an understanding actually has been crucial in many of the recent studies. So like in Eric's talk, so we really sort of are trying to really pin down the Hauser dimension of such things. In the work of Basu, Hoffman, and Sly, this notion was actually used to understand the existence of non-existence of 5GD6. And also in a more recent work of And also, in a more recent work of Alan Sly, Ling Fujiang, and Sarav Sarkar, you can actually sort of transfer understanding of such objects to understanding the kind of effect that a slow bond has in TASEF and more quantitative versions of that. And also, it turns out there are some nice cometal connections, and there's a lot of significant amount of literature in the algorithmic world trying to sort of come up with K-best paths, how to compute them efficiently. Best paths, how to compute them efficiently and quickly, sort of what is sort of the running time of such algorithms. So, in short, there are many, many different sort of places where sort of understanding of such disjoint neo-geodic or geodesics actually is quite useful. Okay, so the formal definition. So, this is my lattice box. So, like I said, so you cannot sort of all start and end at one, one, and then because then you won't be disjoint. Because then you won't be disjoint, but you do the next best thing. So, basically, let's say case two for the moment. So, you start around, you look at the two best paths, one of which starts from 1, 1 and end here. And the other one starts at, I guess, 1, 2 and ends at Nn. So, you look at the two disjoint paths, which cumulatively accumulates the maximum weight. And of course, you can generalize this definition to K, you look at the K points along this line and K points going along this line and tie their ends together. This line and tie their ends together. And again, if things are continuous, like the exponential variable is, then there's going to be a unique such collection. Of course, for discrete variables, you can have non-uniqueness, and I'll sort of mention some aspects of that maybe towards the end. And the notation would be, so this is a planar picture, so there is a natural planar ordering among the paths. So gamma and k1, gamma and k2 would be the paths in this k ensemble ordered from left to right. And x and k will be the same. And x and k will be the cumulative weight of this ensemble. Any questions about the definition? Actually, can see the chat window first. Okay, maybe I can. Okay. Actually, can see it. Okay. Yeah, so what kind of questions can we ask about this object? So the first question could be: so you have this weight x and k. So we know the weight of a geodesic has all this nice fluctuation behavior. It's all been done mostly. What is the fluctuation behavior for x and k? Fuctuation behavior for x and k, and then of course, the second obvious question is the geometric one. So, we have studied the geodesic, where the k equals one case. So, can you sort of build a similar understanding for the k geodesic watermelon for k bigger than one? In particular, remember gamma and k1 was the leftmost path. So, how much does that fluctuate? So, we know that the geodesic itself fluctuates by roughly order n to the two-thirds. What is sort of the fluctuation behavior for? Sort of the fluctuation behavior for the watermelon. Okay. So before going to all that, so I just want to sort of throw out you like a deterministic, nice property that sort of which holds irrespective of any randomness. So of course you can make sense of geodesic watermelons irrespective of what the random variables are. You can just put in arbitrary weights on the vortices and sort of talk about watermelons. So it turns out that this Watermelons. So it turns out that this remarkable fact is true, although the statement itself is almost surely it has actually nothing to do with the randomness. So you look at the k melon and you look at the k minus one melon. So you look at the ensemble of k paths that maximize the weights cumulatively and you can sort of talk about the same object for k equal to k minus one. So you have two ensemble of paths and it turns out that they interlace. So you have k paths ordered from the So you have k paths ordered from left to right on the plane, and k minus 1 paths ordered from left to right on the plane, and it turns out that the paths are sandwiched between each other. This picture is not completely correct because you will have some overlap, but it is at least set of the k minus one set of paths is in the region governed by the k paths. And turns out this is a deterministic fact. There is no randomness in the sort of the state, the proof does not require any randomness. And of course, but if things are discrete, then you might have a non-unic set of melons. You might have a non-unic set of melons, and then you have to sort of make sense of what it means for interlacing. But the version of this is true irrespective of what the weights are. So, is the statement clear about interlacing? Okay. Yeah, actually, this was also happening when I was teaching. I would ask questions, but then would have no idea of what the response was. Yes, it is clear to me. Okay, so. Okay, so just to sort of quickly give you an idea of how to prove things, this is going to be a proof by picture very quick, but just to sort of see what kind of arguments go into this. So let's sort of maybe start with the situation where maybe you don't have interlacing. So here, let's say the red paths are, let's say, the ensemble, the four ensemble. So this red path cumulatively maximize the weight of four disjunct paths. And let's say the blue ones are. Are the three melon. Okay, and so clearly this does not interlace because this red curve should be actually on the other side if interlacing had if interlacing was actually holding this picture. So you can sort of mark such excursion. So this is sort of a witness that you have don't have interlacing. So this red path actually goes on the other side of this green path. And similarly, you can go along from left to right. And similarly, you can go along from left to right, and you can actually map such excursions. And it turns out that if you actually swap them, so instead of the red path, you can actually sort of swap the red part, which actually violates interlacing by the green path. And you can sort of come up with two new sets of paths. And it turns out that because each of them individually were weight maximizing, you will reach a contradiction. So if you do not have interlacing, you can actually identify places where the interlacing is violated and make some swappings of paths. Make some swappings of paths, but it has to be a global move, you cannot just do things locally. And you can end up with two new sets of paths of size k and k minus one, and they will actually have at least as much weight. But then by uniqueness, if you are in a continuous setting, that sort of gives you a contradiction because you know that there is a unique set of k imbalance for every k. So I won't go too much into this, but it turns out that this sort of swapping trick would actually help you show that interlacing cannot be violated. Okay. Okay, so now, so it's okay. So, there is this deterministic property interlacing, and then we let's go back to the original question that we asked: the fluctuation properties of the watermelon. First, talk about the weight and maybe then the transversal fluctuation. So, here is so, so I want to understand quantities like this, right? So, here is a formula, and I want to understand how much does it deviate from the straight line by. So let's start with the wild guess. So let's pretend that, like, it's sort of not a super wild guess, as it will turn out. So, X and K, which is the thing that we are interested in. So, remember, each of the paths are the geodesic itself, the law of large numbers was 4n. So, typically, you would expect if you take k decision paths, you would at least be able to match the law of large numbers part. So, the law of large numbers should be 4nk, because each of them should roughly be 4n. them should roughly be 4n and then the interesting thing is actually the fluctuation and how it depends on k so so okay so n to the one-third was uh the fluctuation for k equal to one case so let's pretend that you have an exponent uh this you have a polynomial pre-factor depending on k which governs the fluctuation in this case so let's say it's k to the alpha n to the one-third and similarly the transversal fluctuation which was n to the two-thirds in the k equal to one case let's say it's k to the beta it has a polynomial prefactor k to the beta for some beta. The beta for some beta. Okay, now you want to predict what alpha and beta are. So, okay, so this is supposed to, let's say, hold for all k. And so in particular, it will, let's say, this sort of behavior, k to the alpha and k to the beta, holds for k equal to n. But if you have k equal to n, then there is only one choice what the watermelon looks like, right? Because if you want n disjoint paths to go from the left side to the right side, they all have to be this. Side to the right side, they all have to be this sort of horizontal lines, so it has to basically fill up the whole box. So, which means, but now let's say, but the random variables are exponential, so they have mean one. So, if you pick up the entire box, the total weight of the geodesic watermelon for k equal to n would be n square, not 4n square. So, this law of large numbers term, which is giving you 4n squared, must be corrected by a fluctuation term, which is also of order n. By a fluctuation term, which is also of order n squared, to get the right thing. Now, remember, k is n here. So the alpha which makes this thing n squared when k is n is 5 thirds. And similarly, if you look at the transversal fluctuation, because you know that there are n paths, so the farthest one should be linear in distance, linear in n away from the straight line, this thing should be n, which means if you plug in k equal to n. Which means if you plug in k equal to n, beta has to be one-third. Okay, so this sort of naive guess coming from just plugging in k equal to n gives you this prediction. Alpha is five-thirds and beta is one-third. Okay, any questions? Okay. Okay, so maybe a slightly more informed heuristic rather than just. from heuristic rather than just plugging in k equal to n. So we predicted that the fluctuation in space or transversal direction is k to the beta into the two-thirds. So suppose you have k paths whose total transversal fluctuation is k to the beta into the two-thirds. Then the average spacing between each of them is k to the beta minus one into the two-thirds, right? So the total space is k to the beta. You have k paths lining up, disjoint. So the average spacing allotted to each of them should be on average k to the beta minus one. Should be on average k to the beta minus one text into the two-thirds. And also, the farthest path, which is k to the beta away, recall that I said that any path that has a lot of transversal fluctuation sort of suffers a quadratic loss in weight. So, the path that is farthest away should be suffering a loss of k to the two beta n to the one-third by the quadratic loss that you suffer if you are farther away from the diagonal. Okay, so you have this parallel paths in channels. Parallel paths in channels, the farthest one should suffer a loss of size k to the two beta times n to the one-third. And so actually, okay, so now you can also do a separate computation. So these paths are assigned some small strip for them to lie in. So this is a situation where you look at geodesics, some are roughly geodesics, but they're not sort of given the whole space. They're constrained to stay within a parallelogram of some particular width. So you can actually use. So, you can actually use tail bounds to prove things like if you give me a parallelogram of size n and the fluctuation on the transversal direction is n to the two-thirds, which is the right scaling, but with the prefector theta, which you should think of as small, then the best path that sort of stays inside the parallel burn suffers a loss, which is theta minus theta to the minus one. You see that this sort of blows up as theta becomes smaller and smaller. Okay, so you can sort of put these two things together. So, you have this thing, so you're going to. Two things together, so you have this thing, so you're going to put in theta equal to k to the beta minus one, which is the average spacing between the paths in a watermelon. And if you do that and you match the two quadratic loss with the loss suffered because of it being constrained to a parallelogram, you again get the same exponent beta equal to one-third. Okay, so there is another sort of way to verify the exponent. Okay, you don't have to use a parses too much. The whole point is, there are k-packs, I give you some total space, so this tells. Give you some total space, so this tells me what the average spacing is. That tells me that individual path roughly is constrained to be in a parallelogram, which means it suffers some weight loss. And then I sort of match that against the quadratic loss that you get because you're moving away from the diagonal. Okay, so here, so given this heuristic, so here is sort of the main result. So the first thing is about the weight. So xnk is the weight, minus 4nk is the law of large numbers part. 4nk is the law of large numbers part. So the result is xnk minus 4nk for all k going up to linear in n is typically k to the 5 thirds 10 to the 1 third. So remember I told you that the quasi-vitum mean was negative. So the expectation of the length of a geodesic is actually 4n minus some constant 10 to the 1 third. So it turns out that if you amp up k and make k larger, the typical value is actually going to be much smaller than 4nk. It's actually going to be typically Smaller than 4nk is actually going to be typically 4nk minus some constant tens k to the 5 thirds tens into the 1/3. So the probability that the weight is actually not like this, so it the probability that it does not, the weight does not land in some order one window at scale k to the 5 thirds n to the 1 3rd decays exponentially in k square. Okay, so that's sort of the concentration or rigidity result, concentration result in this case, I guess, for the weight of the watermelon, geodesic watermelon. And then you can talk about the more geometric aspects of it. It's about the transversal fluctuation. Can I ask a question? Yeah. So the range that you're dealing with, it gets larger with K, right? Yeah, yeah, yeah. In principle, you would expect that it should get smaller, more and more localized with K, right? Because you're kind of compressing. Or do you really believe that? Or do you really believe that anything smaller would go to zero? Actually, I mean, like, okay, so we can go into, so I'll talk about some aspects of this later. So it turns out that you actually, particularly for the exponential case, because you have some connections to other point processes, you actually expect this to be very concentrated. This xn k should be really, really concentrated much more than, of course, this. More than, of course, this bond is sharp in however it's quantified, but you actually expect to have a lot more control on the weight of the GDC portal and because of some other connections here. And in particular, when K becomes of order C, this window becomes of order of the limit shape, right? So it kind of doesn't pick up the curvature of the limit shape. Yeah, yeah, yeah. But but the but the But the thing that I want to point out is that the probability bound is also sort of improving with K though. I see. Okay. But anyway, so I'll say something at the end which actually will sort of allude to this conservation behavior. Okay, so now the geometric aspects of this. So TFNK, remember, was sort of the maximum that any path in the Sort of the maximum that any path in this ensemble deviated from the straight line. So in k equal to one, we set up so it was n to the two-thirds, and here we predicted that the polynomial prefactor was k to the one-third. So the transversal fluctuation being bigger than c times k to the one-third, n to the two-thirds is actually digging exponentially in k. Now, this is sharp just because the geodesic itself has the same probability actually. Has the same probability actually doing the same deviation. So the geodesic typically is n to the two-thirds away from the diagonal, but on some rare event, it can actually fluctuate a lot. And then the fluctuation, the probability that it fluctuates by k to the one-third times n to the two-thirds is actually the same. And so it actually turns out that this is a sharp behavior for this object. But the lower tail, so the probability that the transistor functions is much smaller, that everything is super localized near the diagonal. So the problem, so you should think of delta as a small number. Think of delta as a small number. So the probability that the transverse of fluctuation is smaller than delta times k to the one-third, which means everything is packed in a sort of thin strip along the diagonal, is actually much more unlikely. So it's actually exponential in k squared unlikely. So any questions about the statement? Okay, so I'm actually running out of time. So let me just quickly sort of give you some insight into some of the geometric arguments that go into the proofs. So the key geometric ingredients that we use basically include a specific multi-scale construction of decision paths. So we want to understand the best possible key decision paths. So to say that it actually picks up enough weight, it actually suffices to sort of buy bare hand. Suffices to sort of by bare hands construct a set of k disjoint paths that actually has reasonable weight. It does turn out that our arguments actually do involve a multiscale construction of disjoint paths with a weight lower bound. So that actually tells you that the geodesic watermelon actually has weight lower bounded by the weight of this ensemble that you construct. Okay, the second thing is the interlacing property that I mentioned. So it turns out that the interlacing property Dimension. So it turns out that the intensity property is super useful. If you want to prove something for some k, it actually suffices to prove something for, let's say, some nearby j. If you can prove something for some nearby, maybe even random j, then by interlacing, you can actually transfer that knowledge to the to some statement about a particular k that you want to sort of make a statement about. So interlacing allows you to go between different indices. And the third is a correlation inequality, which is so this is the Vandenberg test inequality, which is Custom inequality, which is which has been super useful in percolation, and also recently in this LPP world, it was used in this paper of Basu, Hoffman, and Slide to sort of rule out by geodesics in LPP. So, these are sort of the three sort of key geometric things that we will sort of keep using. So, I'll probably skip this. So, just to sort of mention what the multiscale construction is. So, the picture sort of at least hints that it's a multi-scale thing and the It's a multi-scale thing, and the statement is you can produce k-digit paths with total weight, which is at least 4nk minus some constant times k to the 5/3 n to the one-third, which is the right scaling as we saw in the statement of the theorem, with high probability. So, if you can produce such a path, it already sort of tells you that the Jurisic watermelon has weight which is at least this. Okay, so that's sort of the construction part. Um, so transversal fluctuation upper bound, um, let me sort of quickly give you some. Let me sort of quickly give you some insight into the geometric things that go into the proof. So, again, recall that any path that moves away from the we want to prove that the geodesic quadrillion does not deviate too much from the diagonal. This is an upper bound statement for the transversal fluctuation. So, recall that any path that moves very far from the diagonal suffers a weight loss, quadratic in how much it moves away. Okay, so thus an approach to show that the transversal fluctuation is not huge is to show that typically in Typically, in the watermelon, every path has reasonable weight. So, the construction tells you that the watermelon itself cumulatively has reasonable weight. If I want to say that no path actually ventures very far away from the diagonal, it suffices to show that every path in the watermelon actually has reasonable weight with high probability. Because then that would mean that no path actually can have a lot of transversal fluctuation, because that would mean that it would suffer a weight loss which would contradict a reasonable weight assumption. Reasonable weight assumption. So, this is sort of what I say here. So, it's gamma n ki was the ith path in the key ensemble. So, if you can show that typically all the paths in the key ensemble actually has this weight load bound, then you can show that you don't, no path actually ventures too much into the away from the diagonal. Okay, so we want to lower bound the weight of every path. So, it turns out that there is a key inequality which is super simple, but very. A key inequality which is super simple but very useful: is that the weight of any path in the K ensemble is lower bounded by the weight of the K ensemble minus the weight of the K minus one ensemble. Why is that true? You see that suppose you have the K ensemble, right? And let's say I want to lower bound the weight of this path, the green one. If I remove this green one, then I have three remaining paths which are disjoint. So the weight of these three remaining paths is no more than the weight of the path. Three remaining paths is no more than the weight of the three ensemble, right? Because the three ensemble is the best possible collection of three disjunct paths. So you can use this observation to get this lower bound, that the weight of any path in the key ensemble is lower bounded by this thing, which is just a statement about weights of the full ensemble. And again, by the weight lower bound coming from the construction, you can apologically show, assume, let's say, on a high property event that the weight of every ensemble is reasonably large between, let's say, key and 2k. Reasonably large between let's say k and 2k. Okay, so this gives you the growth rate for the weight. And so then actually an averaging argument tells you at least that there is some random gene K between K and 2K such that the weight is large for every path in the G ensemble. So I want to use this inequality, which lower bounds the weight of every path by the difference of the weights of the ensembles itself. The weight lower bound tells me that the ensembles have reasonable weight for every J and K and 2K. For every J in K and 2K, and this is sort of the growth rate, then an averaging argument tells you that there is some J which satisfies this nice property, that every path in the ensemble actually has reasonable weight. So which means that I can, because of this transitional fluctuation quadratic loss argument, I can say that this G ensemble is constrained within a strip. It does not fluctuate too much. But remember, I wanted to conclude something about a fixed K, not about a random J. But this random J was between K and 2K. Was between k and 2k. So, by interlacing, because of the interlacing that we have already established, if you can prove that any j bigger than k is constrained within the strip, the k melon which is smaller than j will also be constrained within the same strip by interlacing. Although the picture here is not interlacing itself, it's just because the interlacing statement is about two consecutive millens. And so it can turn out that you can have non-trivial sort of intersections if you defer the. Intersections if you defer the index by more than two. But anyway, is the rough argument clear? So, I want to prove a weight, a transversal fluctuation upper bound for a particular index. I know weight lower bound for most indices. I want to argue that all the paths are nice. I can do that by an averaging argument. That gives me only the statement for random j and then I interlace and then compute for the fixed k that I was interested in. Okay. Transversal fluctuation lower bound. Okay, so here I want to say that actually it's unlikely that all my paths in the key ensemble is actually constrained in a very thin strip. That's the thing that I want to rule out. So I want to rule out a picture like this. Now recall that paths constrained in thin strips. Now, recall that paths constrained in thin strips suffer weight loss, which is something that I sort of mentioned earlier, where you look at a path constraint to be in a small parallelogram, thin parallelogram. So, actually at this point, and these paths are disjoint. So, Vanenbrook testing inequality, for people who are not familiar with this, this sort of says that if you look at disjoint occurrences of events, if you sort of want to sort of say that two events have occurred, but they have occurred using disjoint parts of the space, then the Occurred using disjunct parts of the space, then the probability of that happening is actually smaller than the product of the probability. So there, sort of, it's sort of even more unlikely for that to happen. So using such correlation inequalities, you can actually argue that it's sort of very hard to find key descent paths all packed within this thin strip, which all have reasonable weight. Because any given path already, it was unlikely for it to have reasonable weight because it's sort of constrained in a thin parallelogram. And now, if you actually sort of want k this giant copies of that, Sort of 1k decision copies of that, then it's even more unlikely. Sure, should you could you say something more about what the this Vandberg Kestian inequality is? Yeah, so shall we do that at the end of the talk maybe? Okay, I imagine you're getting close to it. Yeah, yeah, so I want to finish something and then maybe we can resume this. Yeah, so okay, yeah, so the correlation equal to that's unlikely to find KDE paths. You that it's unlikely to find k-decision paths, which are all have reasonable weight, and then we already know by the averaging argument that there is some j which is now between k by 2 and k, which all have reasonable weight. So the j melon exits this strip. And then again, interlacing tells you that the k melon must also exit the strip. So the moral of the story is you're going to prove something for k, you prove something for a random j by some averaging and then use interlacing. Okay, and so sort of really towards the end. So, sort of really towards the end, so this is sort of more related to what Ivan was mentioning before about concentration. So, it turns out that this watermelon, geodesic watermelon weights are actually a natural point process. So, let's say ynk is a difference of the two melons, so xnk minus xnk minus 1. And so then, of course, you can look at yn1, yn2, ynn. So, that's an end-point process if the system size is n. So, it turns out that for integral models of LPP, in particular for exponential LPP, these have connections. For exponential LPP, these have connections to various nice objects. In particular, in this particular case, it's distributionally equal to the eigenvalues of a Vichart or matrix or an LUA matrix. So the Lagrange unitary ensemble, the eigenvalues are distributionally the same as the point process given by the increments of the watermelon weight. And it's also known that the edge of the point process, if you look at, let's say, the largest values, then it converges to what is known as the Airy point. What is known as the Airy point process. And actually, you could have predicted the weight fluctuations, this k power, whatever, five-thirds, by understanding the asymptotics of the Airy point process. But of course, the arguments are geometric, so you hope to actually go beyond this integral models. But so I'll get there very soon, but just to sort of mention a result about this concentration. So this is the thing that we have for the process Y and K. So we have understanding of X and K. Y and k. So we have understanding of x and k, and you can actually translate that to an understanding of y and k. So y and k are typically like 4n, the law of large number, and the increments are now k to the two-thirds. So the probability that the kth point in this point process is not in this interval is actually, again, exponentially unlikely in k squared. And this is actually very similar to a rigidity statement that you might imagine about eigenvalues: that eigenvalues are really sort of very concentrated around where they're supposed to be. Around where they're supposed to be located. So, in this particular case of exponential LPP, because of this eigenvector connection, you can also use that machinery. But turns out that if you straightforwardly apply determinantal point process techniques, you only recover an exponential in K bound instead of this exponential in K squared. But, like I said, so the arguments that I presented sort of hopefully were geometric, same geometric enough, and it turns out that you can actually. Geometric enough, and it turns out that you can actually push these arguments to basically the verbatim for the let's say the point-to-line setting. So, here we were looking at disjoint paths going between two different sets of points. But you can instead actually sort of make the other endpoint completely free and then allow them to vary on a single line. So, it turns out that the same arguments actually give you similar results in this new setting, point to line, where I don't think there are determinantal connections to this point process. Connections to this point process. So the top, the y in one, the weight of the geodesic has a connection to an eigenvalue of a GOE, of an LOE matrix, but not for the full ensemble. So just to sort of wrap up, so although we started with exponential LPP, the geometric arguments are robust, and actually it turns out that the main theorem just works under some strong local convexity of the limit shape and some non-trivial bounds on the non-random function. Trivial bounds on the non-random fluctuations and some TL estimates on the fluctuation behavior for the geodesic. So, if we just assume something about the geodesic weight and the geodesic weight profile having some strong curvature assumptions locally, it turns out that just having these two set of assumptions make all the arguments go through. So, in particular, they should be verifiable for all known integral models of LPP, but these things are known to be true. So, I think I'm already out of time, so I'll just stop here. Yeah. Stop here. Yeah. All right. Well, thank you very much, Trishandu. So I'll thank him.