research member at IBM Research in Paris and he's working on integrable machine learning with a background in applied physics. He holds a PhD degree in theoretical biophysics from the Einhover University of Technology. His work was actually awarded with CUM Laude, so top six of awarded PhDs, yes, and he won the best physics thesis for the year twenty sixteen, integrating interpretability in neural network-based models. In neural network-based models. He is now focused on designing F2N differentiable through learning algorithms for business automation. And I think that's what his talk is going to be about. So welcome, Remy. I think the floor is yours now. Well, thank you very much for this very nice introduction. Can you all hear me well? Yes, we can. Okay. So the topic of my talk today will be indeed about differential rules. Will be indeed about differential rule induction for business automation. Before I start, let me give you. Wait, let me give you some context. So the work that we'll be presenting today are actually the first preliminary results from a newly founded research team at IBM Research in Paris. So the team currently consists of four permanent research. Four permanent researchers, two PhD students, and one research manager from the IBM France Lab. So, all the results I will be showing today is actually a team effort, and it's mainly preliminary results because the team has started roughly one year ago. So, the focus of the team is actually to build hybrid machine learning models for business automation. And more specifically, what I'll be focusing on today is actually to design prescriptive models that are Prescriptive models that are interpretable by design. So, what I exactly mean with interpretable by design will hopefully be clear by the end of this talk. But to give you some context, these days there is an increasing push, as you all heard also in some of the previous talks, towards automatizing decision models, even in industries where the stakes are pretty high and where the data itself can be intrinsically biased. Itself can be intrinsically biased. So, to give you some examples of this, there is AI-enabled hiring. There is applications of machine learning in medical applications, in the financial sector, in loan allocations, etc. So there's a very wide variety of application fields. And in all of these examples, it is very important that the machine learning models that are deployed are fully interpretable. So, what is key from both an ethical as well as a regulatory stand is that these decision models that are obtained with these machine learning models are, first of all, transparent. So, with transparent, I mean that at every stage of the decision process, all of the stakeholders can see why a certain decision is taken. That the machine learning model is auditable. So, that means that essentially, if a regulator asks what if a regulator asks what the exact criteria were for a certain why a certain decision was made that the practitioner can always explain that and finally that the model is human interpretable and that it's that it's expressed in the appropriate vocabulary and this is a point that i will come back to in the next couple of slides what i exactly mean with appropriate vocabulary but one of the premises of of the research that we do is that if you if you want to consider a model to be If you want to consider a model to be interpretable, it also has to be interpretable for humans. So the vocabulary that is used to describe the decision model should be to some extent interpretable. And this is subjective, but still I think that this is a very important point of the model itself. So to guide you through some of the results that I'll show today, I will actually use a very simple toy model, so toy data set. So, toy data set. So, we generated a very simple toy data for a loan allocation problem. So, essentially, we have some data from the applicant. For example, the revenue, the credit score, whether or not that person has defaulted on a loan in the past or not. We have some data on the loan itself, which can be the amount of the loan, the duration of the loan, and the rate. And the goal of our decision model is to have a model or whether or not. A model or whether or not we should allocate that loan given this input data. So, the first approach that we follow is just using a black box model where essentially we feed in all the features that are shown at the left, we normalize them and we provide them to, for example, a neural network. And then we obtain a simple prediction on whether or not the loan should be approved or not. So these black box models, which could be neural networks, but there are many other Which could be neural networks, but there are many other examples, typically are considered to be very highly performance. But one of the problems with these methods is that they're most of the times not intrinsically interpretable. That means that the decision model that you obtain is typically rather opaque. And of course, there are methods to do post hoc analysis on your model to actually render it to some extent interpretable, but that's not always possible. That's not always possible. So, this is one approach. So, this is the black box model. An alternative approach would be to look at rule-based decision models. So, this is more the good old-fashioned AI models. For example, decision trees like CART or separating conqueror algorithms like RIPR, which I've seen also posed by in one of the previous talks. So, the decision model that you obtain with these types of approaches is you obtain a rule set. Is you obtain a rule set in disjunctive normal form. So, for example, you get declined a loan if, for example, the credit score is smaller than a certain value, or the revenue is smaller than a certain value, or you get a conjunction of these predicates. And the advantage of these type of models is that they're intrinsically transparent, as in your decision model is essentially a set of Of conditions that you can execute independent of your decision model, and that will always render the same results. The problem with these approaches in general is that they don't generalize very well. And one of the arguments that I'll be trying to make in the next couple of slides is that if the rule language in which you try to express these models is not adapted to the problem that you're tackling, that you That you can get a very, very long rule set, and that therefore it can be auditable, but it cannot necessarily be interpretable. So let's look at this example specifically, and let's zoom in on, so this example of loan allocation, and let's zoom in on two specific features, just to give you an example. So in this image at your left, I show the decision boundary of the revenue and the loan amount. Loan amount. And what you can see is here that we visualize a rule set that fits the decision model best. So that's to say that essentially every rule in this rule set corresponds to a block that eliminates the decision boundary. So here the decision boundary as a function of these two variables is very simple. It's two lines. So one single block in this graph corresponds to, for example, a conjunction of two conditions. So for example, revenue is small. Conditions. So, for example, revenue is smaller than a certain value, and loan amount is larger than a certain value. But this shows that what essentially what you're trying to do with this vocabulary of univariate value comparisons is that you're trying to fit two lines or actually a non-horizontal or vertical line with a set of blocks. So that means that very naturally here, this will not generalize very well, this approach. And you will have a trade-off, an intrinsic trade-off. Will you have a trade-off, an intrinsic trade-off between the number of trading instances and the number of rules that you'll obtain? So you have to make a choice between accuracy and number of rules if you want to optimize performance. And this is essentially to model this boundary because you're modeling this boundary with boxes. So this is not ideal. Alternatively, we can go to the We can go to the other example that I mentioned, a purely statistical-based approach. We can use a neural network to model this decision boundaries, but then again, without any post-processing, we will not have an interpretable decision model itself. So this is to sketch some of the issues. So the approach within the research team that we decided to take is actually to integrate statistical-based machine learning models with rule-based machine learning. Models with rule-based machine learning. So, if we focus on the workflow of state-of-the-art rule-based machine learning models, essentially the two processes that happen is first of all, before you create your decision model, you first predetermine or handcraft the features. That means that either you have a pre-processing step that does feature engineering, or you have expert knowledge that, for example, handcrafts features. That, for example, handcrafts features based on expert knowledge. And then you use these handcrafted and predetermined features within your decision model. So these are historically two separate steps. So the approach that we follow is actually to combine these and to learn the decision-relevant features alongside the decision model itself. So we will try to learn the features that are relevant within your decision model. So the vocabulary. Decision model, so the vocabulary that is relevant or that is used to express your decision model in an end-to-end fashion together with the decision model itself. So our research will actually be based on two pillars. The first one is that we will create a neural network architecture that, by design, encodes a rule-based decision model. And once that we've obtained that, the next step would be actually to improve. The next step would be actually to improve the rule language to enhance interpretability of the rules. So let's start with this first point. So the neural network architecture that by design encodes a decision level. So let's first of all make sure that we agree on the terminology here. So if you look at a rule in disjunctive normal form, similarly as what I showed before, you have if credit score is small. If credit score is smaller than a certain value. So, what I call feature is in this case credit score. Once you evaluate this condition, so for example, revenue is smaller than 20K, this Boolean evaluation is called a predicate. And then a disjunction of predicates is called a clause. So our goal is actually to have a neural network that will one-to-one map to a rule set in disjunctive normal form and then try to train what One, and then try to train what the most optimal rule set would be. But there's one obvious problem here, which is that these operations, this generating a predicate, is an intrinsically Boolean operation, so it's non-differentiable. So one of the big challenges will be to render these operations differentiable. So the approach that we chose is actually to propose a two-layer neural network for the To mimic this rule set, where the first layer consists of an AND operation, where we go from predicates to clauses, and then the second layer will correspond to an or operation. So let me explain this in a bit more detail. So the first layer actually takes as an input a set of binary predicates, phi, that for this example are predefined, but have This example are predefined, but as I'll show later, that's not necessarily the case. And we define a forward operation that learns the conjunction of the truth values of these predicates through the weight matrix W. So this weight matrix W is something that we will learn. So this weight matrix W, essentially, the elements within this weight matrix that are equal to one will be related to the predicates that will be included in a certain rule. That will be included in a certain rule, a certain clause, J. So, essentially, what we want to obtain is you want to obtain a matrix W that is fairly sparse, since most of the clauses that we will use in our final rule set should only contain a few predefined predicates to remain interpretable. So, how do we train? These, how do we train these? Uh, this vector w? Because what I mentioned before is that it's intrinsically boolean, it's an intrinsic Boolean operation. So, to circumvent this, actually what we do is during training, we define, we have a learnable parameter u and we define a cooled sigmoid function that depends on u itself and the temperature tau. And during the training, we actually lower the temperature of our sigmoid. Lower the temperature of our sigmoid function so that at the end of the training we have a perfectly binary operation. So at the end of the training, we actually have a heavy-side function. That means that during training, we keep our rural network soft in a sense. And at the end of training, everything will be binary. So it has a flexibility to train, but at the end, you also have the guarantee that the network that you learn or that the operations that you learn are intrinsically binary. There's a similar There's a similar operation for the OR, where essentially you encode a conjunction that has to be included in the final rule set. So at the end, what you obtain is you have a two-layer neural network that one-to-one maps to a rule set in disjunctive normal form. So this illustrates what we mean with by design interpretable. So we have a neural network that one to one Have a neural network that one-to-one nets after training with a rule set in disjunctive normal form. So we can assure that the prediction of the neural network on new instances will always correspond to a set of rules in disjunctive normal form. Okay, so why go to this extent actually? Why use a neural network in this case? The rule sets that you obtain here are very similar to what you could obtain with, for example, RIPR or what you could obtain. Ripper, or what you could obtain with a decision tree. So, why go to this extent to do this? Well, one thing that I haven't mentioned and one thing that I fix in this is that I predefined the predicates. But since we have an end-to-end trainable architecture here, what we can do, what this approach permits us, is to actually, instead of predefining these predicates, to learn these predicates together with the decision model itself. So, let's take a very simple example. So, instead of taking the univariate So instead of taking the univariate value comparisons that I showed before, let's add a single linear layer in front of this network where now the predicates themselves will be learned. And what do I mean with these predicates will be learned? Well, sorry. Essentially, we'll go from univariate value comparisons to a linear combinations of input features. So all these predicates will be linear. Be learned as a linear combination of these input features. So instead of be limited to these simple univariate value comparisons, we'll have a much richer space. So if we apply this on the example that I showed you before, what we will actually find is that if we look at revenue versus loan amount, the type of rules that we will learn is if the revenue is smaller than a certain value or the loan amount divided by the revenue is smaller than a certain value, then label. Is smaller than a certain value, then label equals zero. So, what this shows is that now what we've actually learned with this approach is that the relevant and the general rule in this model is actually that the loan amount divided by the revenue is smaller than a certain value. That is relevant for our decision model. And this can be rewritten, of course, as a linear combination. And this is how we discovered it. So, if the decision boundaries in your The decision boundaries in your decision model are linear, this will work perfectly well. But I will show you in the next slide that, of course, in most decision boundaries, this is not the case. It's much more complex than this. But even by only increasing the vocabulary from univariate value comparisons to these linear combinations of input features, you already see for this specific example a big improvement in terms of representation. Because if you look at this expression, this is what... Because if you look at this expression, this is what I call human interpretable. If you get a set of decision rules that is very long and it has a lot of different rules, the way that the rules were expressed, so the language in which the rules were expressed, was not necessarily very interpretable. Okay, so let's look at some results. So we tested this approach. So the results of this approach are shown in yellow here. Are shown in yellow here on a set of data sets for which we knew the ground truth. So we have five input features, x0 till x4. We randomly sample them between 0 and 1, and we define a set of rules, and then we try to recover the underlying rules. And we compare our approach with the RNet, which is also a neural network-based approach that uses univariate value comparisons, with two more classical rule-based approaches, RIPR and BRCG. Based approaches, RIPR and BRCG, and finally with just a deep neural network. And what we find, so we assess our methods on the accuracy. And essentially, if we compare the accuracies for all of these approaches, the accuracy is near perfect. But the big difference is in the number of rules that we obtain and the average ruling. The approach that we propose has much, much shorter rules and much fewer rules than, for example, the RNet, RIPR or BRCG. RNA, RIPR, or BRCG, because the language in which these rules can be expressed corresponds to the vocabulary that we provided to it. So of course, in all of these examples, we've chosen the problems so that all the decision boundaries were linear combinations of input features. So in a way, this is not the most fair comparison. So let's look at some UCI data sets. So these are more classical data sets and benchmarks. Data sets in benchmarking. So Magic, HELOC, Adult, House. And there you can see that if we compare our methods compared to the others, see that again, the number of rules and the average rule lengths is considerably shorter than with the other approaches. And also that the accuracy that we obtain is close to that of deep neural networks and significantly better than the more classical rule-based approaches. Classical rule-based approaches. So, so far, what I presented is that we have a neural network architecture that can represent a rule set in disjunctive normal form. We can extend this with a predicate learning layer where we learn predicates that are linear combinations of input features. But as I mentioned before, the interesting question or the interesting problems actually come when the data becomes more complex. And for example, when we try to. Complex, and for example, when we try to learn rules from aggregates of data, from sequential data, or from time-dependent data. And the nice thing about this architecture is that we can very easily extend it by including other predicate generating models and just plug it into these two rule layer models. So, the idea is really that we can adapt the rule language to the problem at hand. So, just before finishing up, I will present one very, very I will present one very, very simple, one very concrete example that we're currently working on. Assume that you have a classification problem for sequences. So you have a sequence which is like this, for example, A, B, D, A, C, B. And you have at every sequence, you have a label attached, and you try to discover a certain pattern. So you try to discover what the pattern is that gave rise to a certain label on the sequence. To a certain label on its sequence. So, for example, you try to discover a global pattern, which can tell you if AB is present in the sequence anywhere, then the label equals one. Or if you have a local pattern, which tells you if A at position x minus five and B at position x minus four, then label equals one. So, with this approach, we can easily modify it and actually learn rules that apply for sequences. So, in this case, what we did here is really we What we did here is really we adapted the rule language to be able to handle sequential data. So, to summarize what I discussed today is that with this approach, we took the first step towards by design interpretable neural networks for representing rule sets in disjunctive normal form. And what is essential in this approach, and this is something I really want to stress, is that the way to do this The way to do this is to learn the rule language in which you want to express your decision model alongside your decision model itself. And this really helps to provide a decision model that is adapted and that is interpretable. So with this, I would like to end the presentation and I'll take any questions. So thank you very much. Any questions from the audience? Hi, Erin. Hey, yes. So like I had a very practical question, Jordan, when you said like you're scheduling the temperature for learning the final step of the work-based neural network, how easy it is actually? How easy it is actually because I also assume that, like, if the temperature is too small towards the end, then like it kinda increases the logo gradient, right? That's a very good question. And so that remains a hyperparameter in the model? But it doesn't seem to be very, very sensitive to it. That means that you can, the decay rate of that value, you have to choose it so that. You have to choose it so that at the end of your model, so you have to more or less make sure that it's small enough that at the end of training, that everything is perfectly binary, that you don't have values that are between zero and one, because then your interpretability of your model is lost. And you have to make sure that during training, your gradients don't become too high. So there are some practical tips and tricks that we use to ensure, for example, we perform gradients. Used to ensure, for example, we perform gradient clipping to make sure that the gradients don't explode too much. And when you do that, actually, you can go to temperatures of 10 minus 5, 10 minus 6 without any issue. And then you're sure that all the values are 0 and 1, so that your network is perfectly binary. So I don't have a very clear answer to that. It remains a hyperparameter, but it's. Hyperparameter, but it's not extremely sensitive to it. So, in your test, like in your experiments, if you go from the temperature to a perfectly amic function, then like you don't lose anything. Also, do like some synthetic experiments where you have a more dense data set, where there are some samples that are misclassified when doing the complete mineralization. Complete iteration? Yes, actually, what we did, and I think that we also showed this in the paper. We added noise to the data to see how robust it is with respect to noise. And as you would expect from neural network-based models, it's fairly robust to noise. So there are different ways, of course, to add noise. You can just randomly misclassify, or you can misclassify close to your decision boundary, or misclassifying close to your decision boundary. Misclassifying close to your decision boundary is most of the times more realistic. But in the paper, we show some graphs, if I remember correctly, as function of the noise level in your data, at least for the synthetic data sets that I presented. There's another question. You talked about the weighted combinations of features, and then Of features, and then how do you really estimate those weights? So, these weights are trained alongside the rest of the weights in your neural network. So, essentially, here you have a weight matrix WP, which corresponds to, so for every all of these predicates, which are learned, they have a set of weights and a bias. So, the bias here is zero, but this bias is also something that is learned. Something that is learned. And essentially, you train these end-to-ends together with the weights in your end layer and in your OR layer. So through gradient descent, essentially. So these weights are known to you? No, these weights are learned. Weights are learned. Okay. Sorry? No, that's fine. Thanks. So I So how does this scale with the size of a with the number of features or the length of the sequence? So for sequences, we haven't looked too much at how it scales. So we haven't done a very deep scaling analysis. If I compare it in terms of runtime, so there are a couple of parameters here that you can vary. You have the number of predicts. Predic the number of predicates. Essentially, you have a set of input features. So, the question is: how does it scale with the number of input features, with the number of allowed predicates that you have in your decision model, and with the number of allowed clauses in your decision model? So, it scales very similarly as a normal neural network scales with the number of input features, because essentially these are number of features are your number of input features. Are your number of input features, the number of predicates are the hidden layer dimensions of your neural network, and same for the number of clauses. But since we're always, at least the type of data that we considered so far was mostly tabular data, so we never really had any issues with scalability at this stage because you try to express your model with not. With not too many predicates and not too many clauses. So that means that you're always in the ballpark of I'm looking for rules somewhere between one and I don't know, 50. 50 is already a lot. So we haven't really did a very, very thorough analysis of scaling, but it scales similarly as a neural network. I don't know if that really answers the question, but. It was mostly referring to the actual performance or the compression, the number of rules that you would. compression of the number of rules that you would generate match with the number of features so one thing that i that i did not mention is that uh this may be important for the for the for the predicate learning part we have an an l1 penalty on the weight matrix w to make sure that that in the end the the predicates that you learn are relatively sparse and also we have some form of sparsity penalty within the end layer Penalty within the AND layer and within the OR layer. So having this sparsity penalties in these layers makes it to some extent less dependent on the number of neurons that you have in your class and the number of neurons that you have in your predicates. That means that the output or the results that you get here, so the number, sorry, the number of rules that you will obtain if you induce sparsity in some form or another. Sparsity in some form or another, you will limit the impact on the number, as in it, there's an upper limit, as in you need to have at least as much neurons in your hidden layers as the expressivity that you need to describe your results. Does that answer your question? Yes, thank you. Okay. I have a question. Can you hear me? Can you hear me? Yes. So, very nice work. And so, I was thinking about, I read some papers about neurosymbolic computing. And so I was wondering, just, I mean, it was just thoughts. Can you return to the next, the slide before? Like, when you learn the predicates, they are probabilities or they are real values? They are real values. They're real values. Okay, okay, I see. Okay, because I was wondering if how this compares to when you learn like probability predicates. You define a rule and you say, yeah, you say you get a probability rather than a real value. So, this is this is a different question. This is a very interesting question. This is not the approach that. This is not the approach that we're presenting here. In this case, you're learning a fully deterministic set of rules. The type of problem setting is different than within neurosymbolic, so for example, logical neural networks, which typically deal with more complex reasoning tasks. Essentially, the only thing we do here is we try to learn. So we impose a lot of inductive biases. We impose a lot of inductive biases on our network here. So, for example, we're forcing neural network to learn a rule set in disjunctive normal form. And then we set certain constraints on what the predicates are that can be learned. So, in that sense, it's different. There are definitely parallels, but it's thank Remy. Thank you very much. If we can large