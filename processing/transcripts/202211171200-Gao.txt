First of all, I would like to thank the organizers for the very kind invitation. And today I'm going to talk about this ranking problem, which is actually quite useful in the context of sports statistic web search and recommendation system. And this is a joint work with my student Ping Han Chen and also my co-author, Anderson Chang from Morton. So statistically, the problem of ranking can be formulated as the famous Formulated as the famous Bradley-Terry-Luce model, the BTO model. At least this is one of the simplest and most popular formulations. And imagine that we have M players and we want to rank according to their abilities. And in this framework, we are going to model the outcome of a game played between I and J by a Bernoulli distribution with probably the proportional to the exponential scale parameters of the players. And to be specific, the probability that I win the game against J is a sigmoid transformation of the difference between Of the difference between the two scale parameters. And in our sampling scheme, we are going to consider an illustrating graph with connection probability p to be at least of order log and over and just to guarantee that the graph is connected. Then as long as Aij is one, there are capital L independent games played between I and J. The outcomes are distributed by the BTL model. And here I would like to decouple the rank vector from the scale parameters. From the scale parameters. Okay, so here we have two components in the parameter. The first component is a decreasing vector theta. They are the scale parameters of a player. In particular, theta one is the scale of the best player, theta two is the second best, so on and so forth. And we also have a permutation vector. This is the rank vector that we want to estimate. And in the end, the scale parameter of the ice player is characterized by theta sub Ri, because Ri is understood as the rank of. Is understood as the rank of the ice player. And we are going to estimate the rank vector r and treating the scale parameters theta as the nuisance. In the literature, there are two ranking problems that people have already studied. The first one is called full ranking. The goal is to estimate the entire rank vector. The second one is called top K ranking. The goal is to recover the best K, the set of the best K players. And we are going to talk about both problems in today's talk. And we are going to talk about both problems in today's talk. And first, I'm going to discuss full ranking. And let me introduce two loss functions that we characterize the error of ranking. The first loss is called Candelstaule. It measures the difference of the two ranking vectors by counting the number of inversions. And then Candel's style is defined to be the number of inversions divided by n. The second loss is called Spearman's foot rule, which is defined just as the normalized L1 norm of difference for the two rank vectors. Two rank vectors. It turns out these two loss functions are equivalent by this inequality by a factor of two. So in today's talk, I'm just going to focus on the result in Candel's star loss. And I would also like to mention that there is a previous work by Chen Mao, Johnson Weed, and Philip Reglet in 2018 that also discussed minimax optimal ranking and condos tau, but they are considered very different settings from the BTL model in today's talk. Okay, so in addition to the Okay, so in addition to the loss function, we also need some regularity condition for the nuisance parameter theta. We are going to introduce a signal parameter beta that characterizes the signal of the data. In particular, we assume that theta belongs to the following set of parameter space, in which theta is the smallest gap between any pair of neighboring players. And moreover, there is some constant C0 such that C0 times beta is the largest gap between. Largest gap between the skills of two neighboring players. In other words, the difference of theta i and theta i plus one is of order beta and it's lower bounded by beta. I understand that this assumption is a little bit restrictive when it comes to reapplications, but it turns out even that this assumption already captures the main essence of the ranking problems. And I'm not going to discuss any extension into today's talk, even though it is. Into today's talk, even though it is impossible, I'm going to focus on this parent space in today's talk. Okay, so before talking about the ranking, the minimax rate under the BTL model, I will first present the result under a much simpler Gaussian pairwise comparison model. And turns out, ranking under Gaussian model is a much simpler problem than ranking in the BTL model. And we can already see some of the unique and interesting phenomena well illustrated by the Gaussian model. So the plan is to first understand plan is to first understand what's going on under the Gaussian model and then we can talk about the more complicated results in the BTF model. I will then highlight the difference between ranking under two different settings. So for the Gaussian model, we still have the Elder Francis sampling graph, right? And we assume that P is greater than log n over n to make sure the graph is connected. And given the connectivity of an edge Aij equals one, the comparison result is now given by a Gaussian random variable yij. The mean is given by the difference of the scale parameters. Difference of the scale parameters of the two players, the variance is just sigma squared, and then we have the nuisance parameter as theta, and the goal is to estimate the rank vector r. Okay, so the first result is for the minimax rate that we assume that theta belongs to this parameter space of regularity. Remember that this means the difference between theta i and theta i plus one is of order beta. Okay, and we assume that p is at least of order log n over n. And then the minimax rate of full ranking under the Under the Gaussian model with respect to the Klendel-Taus loss, it is given by this formula. So the formula looks slightly complicated, but basically, you have two regimes. The signal to the noise ratio parameter is given by Mp times beta squared over sigma squared. And when the signal to noise ratio is above one, then we get an exponential function. Otherwise, it's a polynomial function. So there is a transition between the exponential rate and the polynomial rate. And in the exponential regime, it is an average of... It is an average of n minus one exponential function, and each one, each exponential function that you see an exponent characterized by the difference between theta i and theta i minus one. So you're already trying to tell the difference between ranking at the i's position and ranking at the i plus one's position, okay, which reflects the local difficulty of the problem. I actually want to emphasize that the result holds for each instance of theta, because here, when we define the minimax rate, we only take supreme over the rank vector. We do not take supreme. Supreme over the rank vector, we do not take supreme over the parameter theta, even though we assume that the theta belongs to the regularity parameter space. Okay, so you can see that there is a very interesting phase transition phenomenon of ranking problem between exponential rate of convergence and a polynomial rate of convergence. I want to emphasize that we can actually simplify the expression by taking the special case where theta i minus theta i plus one is exactly. And then the formula of the minimax rate can be greatly simplified. And when the signal-to-noise ratio is above one, it's an exponential function of the signal-to-noise ratio. Otherwise, it's a polynomial function. Okay, so we can see a picture there. There is a transition between several regimes. So the reason that you have this transition between the polynomial rate is and the expansion rate is because of the property of. And the expression rate is because of the property of the rank vector. So, what is a rank vector? The rank vector is just an n-dimensional parameter that we want to estimate, but it is not a usual n-dimensional parameter. It is a discrete vector. So, when the signal-to-noise ratio is very large, then in order to rank the players, the most essential problem is to localize the knowledge between neighboring positions. For example, for the ice player, you really try to tell the difference between Really, try to tell the difference between whether he or she is ranked at the fourth position or maybe at the fifth position. Okay, so in this sense, when the signal-to-noise ratio is large, it is much more like a hypothesis testing problem. And therefore, we will have an explanatory rate of convergence, which is a typical error for hypothesis testing problem. However, when the signal-to-noise ratio is below one, then you have a very large noise level, and then the discrete property of the rank vector will be blurred by the high noise. Blurred by the high noise level. In other words, you cannot see the discreteness of the rank vector. It is just as if you are estimating an n-dimensional continuous parameter, and then by a classical entropy calculation, you will get the polynomial rate of convergence. And from the picture, you can see there are two additional regimes. For example, when the signal-to-noise ratio is below the level of 1 over n squared, then you get the random guess phase. Here, the rate of convergence is always of order n. Is always of order n because this is the largest number that the loss function can take by definition. So, in this phase, that you can achieve the optimality by just random guess. And another phase is called the exact recovery phase when the signal from this ratio is above the threshold of four log n. This is where you get an error that is smaller than one over n. And because by the definition of the loss function that you get that the Kendall's tauff loss, That the Kandel's task loss can only take value of 0, 1 over n, 2 over, and 3 over, and so on, so forth. Whenever the error is smaller, it must be exactly 0. Okay, that's why you can actually achieve exact recovery without any error with high probability. Can I ask a question? Yes, please. Yeah, so your result is within multiplicative constants, right? Yes, it's within multiplicative constants. So you cannot get random better than chance because the loss of factor of two or three. Or three, if you know what I mean. I don't understand what you mean because you elaborate, right? So, random guess gets one fixed point, I guess, right? I mean, I don't know how it translates to Kendall Tau. Random guess, okay, so think of the equivalent loss function of Kendall Star, which is the normalized L1 loss. If you do random guess on each coordinate, then the difference of the random error you make. Of the rank error you make is of order n. So, average, you will get an order n error. Oh, I see. So, this is still only the order of random guess, but maybe you can beat random guess by a factor of 10. Right. Yeah. Okay. So, that is not captured because you have constant, right? Okay, yes, you're correct. Thank you for the clarification. Oh, by the way, here we have this physical energy between the two types of rates. Two types of rates, and it seems to be a unique phenomenon for the ranking problem. But if you see such similar problems arise in other single problems, then do please let me know. So the output procedure to achieve the minimax rate under the Gaussian model is actually extremely simple and straightforward. Just to compute the maximum like equal estimator, in this case, the least square estimator, and then rank theta, then you will get the optimality. So this procedure is very easy to compute, and it's also very easy to analyze. Compute and it's also very easy to analyze because it's just a least square space. You can exactly characterize the distribution. Given the realization of the random graph, theta hat is exactly Gaussian with the correct mean. And the covariance is given by sigma squared times the inverse graph Laplacian of the random graph. And then you can just do a simple risk analysis by the definition of the Kandel style. What you need to do is to compute the probability such that R i hat greater than. Such that Ri hat greater than Rj hat for all those pairs whose relations are reversed. Okay? And since R hat is the rank of the vector theta hat, so the event that R i hat greater than Rj hat is equivalent to the event that theta r i hat is smaller than theta j hat. Okay, and then to bound the probability of theta i hat smaller than theta j hat, you just apply the Gaussian result, Gaussian distribution result, and then apply a standard Gaussian tail probability. Okay, so you need Gaussian tail probability. So, in the end, you just add up all these Gaussian tail probabilities, you will end up with the correct mean max rate with the phase transfer phenomenon. Okay, so we just understand the property of ramping under the Gaussian model. Now, I want to talk about the BTO model. First of all, let's talk about some intuition. So, let's consider the following toy problem, which is extremely simple, that we want to estimate parameters either one. Parameter theta one, and we are going to assume the knowledge of all the remaining parameters theta two up to theta n. Okay, so this is an extremely simple parametric estimation problem of dimension one. You can just apply the mathematical estimator, and then you can show it is asymptotic normal. And in fact, asymptotic variance is exactly one of the facial information of the problem. You can compute the facial information, which we in this case we call the oracle information because it depends on the knowledge of theta 2 to theta. Of theta 2 to theta n. The oracle information, facial information, has this formula. It is a summation of n minus one terms. And each term in the summation has the meaning of characterizing the information between the of the game played between player one and player j. So in other words, you have n minus one opponents, and then you play games against each different opponent, right? So the information from the games are very different. And we know that. And we know that the derivative of the sigmoid function is an exponentially small function. Therefore, if we have two players whose rank difference is very large, then the contribution of the game in terms of the information will be exponentially small, because we assume theta is regular. So the difference of theta one and theta j is at least bounded by below by beta times the difference of the rank. Okay, in other words, Okay, in other words, you do not really care about opponents who are too weak or too strong because the contribution of the information will be expressionally small. You only care about those opponents whose skills parameters that are close to you. I want to make this intuition precise by doing a truncation of the fiction ratio. You can actually truncate this fiction ratio by only summing up over the games within a neighborhood of player one. Of plane one. For example, if we can consider a neighborhood of radius m over beta, where here capital M is chosen to be a very large constant, then the approximation error can be shown to be exponentially small. In other words, here the radius, which is of order one over beta, is understood of the is understood as the number of plates that really matter. It is also understood as the effective sample size. So remember, the small n is the actual sample size. The small n is the actual sample size, but n is not the effective sample size. The effective sample size turns out to be the number one over one over beta. Okay, and this is actually the most important slide of today's talk. And later, the conclusion will all be derived from the intuition from this slide. Actually, I want to illustrate this point by this very simple example. Just to consider the example of comparing two teams here, I want to compare Manchester United and Arsenal to see. Manchester United and Arsenal to see which team is better. But here, from the realization of the graph, we do not have a direct edge between then United and Arsenal, so that the comparison has to be made indirectly through the games between other teams. No data is needed for this problem. Sample size is zero. Exactly. So it's an imaginary problem. So it's a thought experiment. So, okay, imagine that you play games between Manchester Zen. Between Manchester United and Oxford United. For those of you who do not know Oxford United, it is a team that is in the third league of the English football system. It's a league one team. So Oxford United is much weaker than both Man United and Arsenal. For example, Manchester United were likely to win all the games against Auction United, and so does Arsenal. So comparing against Oxford United will give you no information. But if you compare against Everton, that's a different story. Everton is a Premier League team. So probably major. So probably Meghanadi will win about 70% of the game. Arsenal, we're likely to win 60%. Then you can conclude that, okay, perhaps Magneti is a stronger team. Okay, so I also want to here conclude there is actually really a double role played between the parameter beta. Remember, beta is introduced as a signal strength of the model. It is the minimal gap between theta i and theta i plus one. So if beta is zero, all the players have the same skill. All the players have the same skills, it is impossible to do ranking. So, your whole beta is large so that you have a strong signal. On the other hand, from the facial information approximation formula, one over beta serves as the effective sample size. So, from this perspective, you want beta to be small, so you have more players whose skills, abilities are close to you, that will increase the sample size. So, in the end, whether you want a smaller or larger beta is not clear. So there will be trade-off between the two roles, which we will illustrate in the mini-max rate formula. Will illustrate in the minimax rate formula. Okay, so here is the main result of minimax rate. We assume theta again is in the prime space of regularity, and then the connectivity assumption of the random graph is different. It's replaced by a p over beta greater than log n. I will come back to the meaning of this assumption data, but let's first take a look at the mean max rate. It is given by this formula. Again, we see a transition between a polynomial rate and exponential rate. Polynomial rate and exponential rate, depending on the size of the signal-to-noise ratio, here the signal-to-ratio is the ratio of Lp beta squared over the maximum of beta and one over n. And in the exponential regime, again, it is an average of n minus one different functions. It is more complicated because in each expand function, we have an exponent depending on a Exponent depending on a local variance function vi of theta, whose formula is given here. And if you remember the visual formation formula, this is exactly proportional to the inverse visualization, which also explains why it is the correct exponent. And again, this rate is also derived for each individual feed. Here, the supreme is only taken over the rank vector, not over feet. Now I want to explain the condition that we have. The condition that we have p over beta greater than log n, and what does that mean? So, previously, for the Gaussian result, what we assume is p times n greater than log n, which is the standard result for the connectivity of our adoption graph, right? But remember, for the BTL model, we have explained that the information really concentrates on the local neighborhood of each player. So what you really care is not the global graph, but the local graph. And since effective science simple size is of order one. sine symbol size is of order one over beta therefore one over beta is actually the size of the local graph so this condition p over beta greater than log n actually makes sure that we have connectivity of each individual local graph which is a subgraph of the overall graph so it's just actually stronger condition that we need okay now i want to actually simplify uh the minimax rate to uh to have some further discussion uh the most complicated Discussion. The most complicated formula is the exponent, but we can show that the exponent is of the same order as this signal-to-noise ratio parameter. And the signal-to-noise-noise ratio parameter can further be simplified into LP beta, as long as we assume that beta to be greater than one over n. And then the minimax rate can be simplified into the following formula, which is much simpler. Here, the signal-to-noise ratio is just the L P beta, and depending on whether it is greater than one or not. Depending on whether it is greater than one or not, we get exponential rate or polynomial rate. All right, then let's try to compare the two results. We have the minimax rate of BTO model and the minimax rate of the Gaussian model. The main difference is in terms of beta. For example, if you look at the Gaussian model, the signal-to-noise ratio, MP beta squared over sigma squared, depending on beta in a quadratic way. However, for BTM model, the dependence is actually. Uh, uh, is actually linear, right? So, why do we have this difference? Well, it's actually helpful to rewrite the signal-to-noise ratio LP beta in the BTO model as a product of L times one over beta times P times beta squared. And then you can find exact correspondence between the parameters that you have in the BTO model and that in the Gaussian model. For example, you can correspond one over beta to n in the Gaussian model because and in the Gaussian model because both serves as the role of effective sample size, right? And then for beta squared, then here for both the BTO model and the Gaussian model, they are the signal strengths. Okay. And this is exactly because of the double role of the beta. Beta plays as both a trajectory sample size of signal strengths in the BTO model. That's why you have a cancellation and results in a linear dependence of beta. Okay, so it is actually helpful to study the Gaussian model first. Helpful to study the Gaussian model first so that we can have a good understanding of the BTO model from the difference. Okay, so having seen the minimax rate, now I'm going to tell you the algorithm that we have designed to achieve the minimax rate in terms of both the polynomial rate and expander rate. So the main idea of the algorithm is also inspired from the approximation of the fishermen, which means that the information actually concentrates on the local neighborhood. Actually, it concentrates on the logo neighborhood of each player. So, what we want to do is to first divide the players into several leagues. And we hope that in each league, the player's skills are actually similar. And then we can actually run local maximum likelihood for those players whose skills are similar and then aggregate the results into the overall rank. Okay. All right. Let's start with the step one, which is called league partition. Called league petition. So, what is the league petition? So, here the goal is to petition the end players into different leagues. And we do want the petition to satisfy some good properties. And what we mean by a good league petition, let me first give you an illustration using again the example of the English football system. So, what we see here are the tables that we take from the English football system. Here, I give you the results. System. Here, I give you the results of the top five leagues from the season of 2019 to 2020. The top league is Premier League, the bottom league is National League, and between the two, you have Championship League 1 and League 2. And this is actually a very good league petition. For example, Premier League has the best 20 teams, and then Championships has the second best batch of the team, and so on, so forth. Batch of the team, and so on, so forth. Of course, you do have those acidity between the two neighboring leagues. For example, if you look at teams such as Aston Villa, Bourmer, Watford, Norwich, those are bottom teams of Premier League. They are not necessarily stronger than these United West Brom and Fulham, who are top teams of their championship. That's why you have this relegation promotion rules in the system to make those local adjustments every season. Local adjustment every season. But anyway, despite this local ambiguity between neighboring leagues, you don't have any ambiguity between leagues who are not neighbors. For example, if you compare Premier League and League One, you can make a very safe conclusion that every team in Premier League is stronger than every team in League One, right? The same conclusion can be made between Championship and League Two. Every team in Championship is stronger than every team in League Two. Stronger than every team in league two. Okay, so this is exactly what we want as a good league partition. So let's try to characterize those good properties that we need. So, first of all, we want players in either the same league or the neighboring league to have close skills. Okay. And the second requirement is the reverse statement. We want players who have close skills are either partitioned in the same league or in neighboring leagues. In the same league or in neighboring leagues. Okay. And the third requirement is that players should have a very clear advantage against who are at least two leagues below. Okay. And with these three requirements in mind, let's try to look at the algorithm. Here, I want to do a sample splitting trick just to make sure in the proof we have approximate independence between step one and step two. This is just a technical that is not important. And remember, we have capital L games. Remember, we have capital L games. We are going to partition these games into two batches, and we are going to use the first batch of the game in the step one of the partition. Okay, so the algorithms proceed as follows. In the first step, we are going to compute the statistic Wi with a superscript one for each player. Here, the superscript one, because this is the first step. And the meaning of Wi is the number of players that clearly dominate. Players that clearly dominate the player I. And here the definition is given by the summation over Aij times the indicator such that yij bar smaller than pasi of negative 2m. So here, the capital M is chosen to be a very large constant. So pasi, which is the sigma transformation of negative 2m, is a number that is very close to zero. So the event that the yij bar smaller than The Yij bar smaller than pi of negative 2n means that player I has almost lost all the games against player J. In other words, player J is a player that dominates player I, right? And this summation simply counts how many players that you have that are dominant player I. Okay, so that's the meaning of WI. And then I'm going to define the top league, which is the set of S1, to be the set of S1 to be the set of players who do not have too many players that dominate them. And these are defined by the players whose Wi is smaller than some parameter H, which we will set later. So this is the top league that we have. And once we have the top league, we can remove the players in the top league from all the players and just repeat the same process among all the remaining players. Okay, so we'll repeat the same process. Okay, so we'll repeat the same process, then we will find the second league and repeat it again. We will have the third league, the fourth league, so on and so forth. In the end, it is guaranteed that the algorithm will terminate and every player will be partitioned into some league. Okay, so here is a result for the property of the league partition. It basically says that the three properties that we want are satisfied as long as we set the threshold H. We set the threshold H as capital M times P over beta. This is actually a parameter that is set to satisfy theoretical property. But of course, you can actually replace by a data-driven estimator. And we can also show by a very simple estimator, the theorem will also be true. Okay. So the three properties are satisfied. The first one is if player I and J are either in the same league or in the neighboring league. Or in the neighboring leagues, then the difference of the ranks are bounded by one over beta. Remember, one over beta is the effective sample size that we want. And the second probably is the reverse argument that if the ranks, the difference of the ranks are bounded by the order of one over beta, then the two players must be either partitioning to the same league or in neighboring leagues. Okay? And the third property says there is no ambiguity. There is no ambiguity between leagues that are not neighbors. For example, if player I is from the K's league, player J is from the L's league, if K minus L is at least two, then we can conclude very confidently that player I must be stronger than player J. In other words, player I is ranked higher than player J. Okay, and all these properties hold with high probability. Okay, so this is the property of player one. Sorry, of The of player one, sorry, of step one. Now I'm going to talk about step two of the algorithm. But before talking about step two, I'm going to first spend a few minutes talking about a very interesting property of Kandel's taut loss. It is actually this property of Kandel's taut that motivates the design of step two. Okay, so the Kandel's taut is defined to be one over n times the number of inversions of the estimated rank. Okay, it can also be written in this way. Can also be written in this way, which is 1 over n times the Hamming distance between the estimator of the relation matrix. Here, the relation matrix capital Rij is defined as the indicator such that a smaller, a small Ri is smaller than a small Rj. Okay? So by this equivalent definition, the task of ranking is equivalent to the task of estimating the relation matrix. So if you give me a relation matrix, Relation matrix. So if you give me a relation matrix estimator, then it is as if you have done a rank. Okay, but this is actually not exactly true because if I have a rank vector, it is very easy to turn it into a relation matrix by the definition such that the indicator of R i is smaller than Rj. But if you give me an arbitrary relation matrix, it is not so obvious how I can turn it into an estimator of the rank vector. Of the rank vector, okay. But there is a way to do that, it's a very simple way, which is characterized by this lemma. If you give me any relation matrix estimator r hat, I can turn it into a rank vector, small r hat, by rank by simply sorting the row sum of capital R hat. Okay, if you rank the player in this way, then we can show that in terms of Candel's tar, it is actually bounded by four times the normalized hemming distance. Times the normalized timing distance of the relation matrix. Okay, so by this lemma, as long as you can have an algorithm that fills in the relation matrix, then we can turn it into an estimate of the rank vector. Okay, so this is exactly what to do. And in fact, the step one can already fill in some of the blocks of the relation matrix. So here, for example, let's imagine that step one divide the m players. Step one, divide the M players into eight leagues. So the whole block is the whole matrix is divided into eight by eight, intogether 64 blocks. Let's, for example, take a look at this block. This block characterizes the relation between the top league S1 and the third league. And since they are not neighboring leagues, so by the property of the league partition, we know that every team in the top league is stronger than every team in the third league. Therefore, we can, of course, filter this matrix, filtering this block. For filtering this block. And by the same reason, we can also filter these blocks in the first row, right? We do not fill in the block of the first two blocks of the first row because these are the same block, these are either characterizing relation between the same league or neighboring league. And by the same reason, as long as the block does not characterize the relation between the same league or neighboring league, then we can already fill in the relations. Uh, the relations in these blocks just by the algorithm of step one. Okay, so step two: in step two, we are going to take care of the remaining blocks. So, let's, for example, take a look at the specific problem of the local problem of how do we rank players from the K-s league and K-plus wins league. So, here the idea is going to be very simple. We are going to use the second batch of the games. The games. First, write down the likelihood function and then compute the maximum likelihood estimator. And then we are going to estimate the relation of these players by comparing which estimator of the scale parameters is larger. Okay, so the idea is very simple. The only non-obvious construction is when we define the likelihood function, not only do we need to include players that are in Players that are in the K Steague and K plus one league, we actually also need to include players from the previous league, that is S K minus one, and the league after, that is S K plus two. The reason we want to do that, I want to illustrate again using this picture. Now, let's imagine what we want to compare is two teams, Fordham and Sutherland. So, if we compute the maximum estimator, of course, we need to involve Course, we need to involve players that are in championship and league one. But remember, in order that the approximation of the official formation is accurate, we actually need to include players in all the players in the neighborhood of the two teams, Fulham and Sonoman. So for example, for Fulham, not only do we need to include teams in championship, we also need to include teams that are appearing in the bottom of the top of the Premier League. For example, Austin Villa. Premier League, for example, Aston Vela, Balmer, Waterford, and Knowledge. These are teams whose skills are also close to foreign. That's why we actually need to include teams in Premier League as well. And for the same reason, the teams in League 2 should also be included. That's why in the definition of the likelihood function, we need actually involve four leagues. Okay. And this makes sure that the MLE will give you an efficient estimator in terms of statistical property. Okay. And this divided conquer. Okay, and this dividing conquer strategy also gives you computational efficiency because the smaller local MLE is much simpler to compute than the global MLE. It can be shown that the condition number of the Hessing matrix grows as the difference between the strongest player and the weakest player. So, in the worst case, if you look at the global MLE, the condition number grows as an exponential function of n times beta. Okay, so. Times beta. Okay, so even though the MLE is a convex optimization, if you just include all the players, the computation will be actually slow. But for the local MLE, the condition number is actually can be shown to be bounded. So optimization is extremely fast. Okay, so this is the main idea of the dividing concrete algorithm, and then we can show it to achieve the mean max rate. All right, so let's finish the algorithm by Algorithm by filling the remaining blocks. For example, let's try to fill the blocks for the first two rows. Here, we want to rank players in the top league and the second level league. And since the neighborhoods are S0 and S3, we actually need to compute MOE involve S0, S1, S2, and S3. There is no S0, so just to compute the local MOE using the top three leagues, and we can field in these blocks. And for the same reason, we can fill these blocks as well. Fill these blocks as well. And then I just finish the process. Now we can feel all the blocks and then just apply the previous level. We can turn an estimator of the relation matrix into a rank vector that will finish the algorithm. Okay, so this is the first part where I discussed the mean max rate of full ranking and the alpha algorithm. And now I still have maybe 10 minutes left. I'm going to talk about the second problem, which is the top K ranking. The goal is just to select. Ranking. The goal is just to select the top kick players from all the players. Now, this problem has also been studied in literature. There are basically two popular algorithms. Let me briefly give you a review. The first one is MLE. So it's extremely simple just to compute the likelihood function and then find the MLE and then rank the scale vector. Okay, so it's straightforward. The second algorithm is called special method. It is also called rank certain. It is also called rank centrality, proposed by Negeban, O and Shah back in 2017. So it also has three steps. In the first step, we are going to compute a Markov transition matrix, P for all the off-diagonal entry. Pij is defined as one over D here. D is some parameter times Aij times Yji bar. Okay, so here the diagonal entry is chosen. The diagonal entry is chosen so that the row sum of the matrix is one, and then the number d is chosen to be a sufficiently large constant so that for each entry it is a number between zero and one, which can be understood as a probability. So the entire matrix has an interpretation of a transition matrix of Markov chain. In the second step, you can find the station distribution of pi hat, and then you can rank pi hat. Okay, the reason why the special method works can be easily seen from the population version that you can take. Population version that you can take a conditional expectation of the matrix P, then which has this formula. Okay, and then it turns out the stationary distribution pi of the population version of the Markov chain can be shown to be exactly this vector. Okay, and pi defined in this way is the station distribution of the population version of the Markov chain. Population version of the Markov chain. And since pi, and since each entry of pi is an increasing function of theta, so ranking using pi is equivalent to ranking using theta. This explains why the special method will also work in terms of ranking. And in terms of theory, the state of the art of the theory is given by this paper, which is by Yuxin Chen, Jeju Fan, Tongma, and Kaijun Wang, published in 2019. What they show in this paper is that What they show in this paper is that both the mathematical estimates and special algorithm are both optical for the task of top peer ranking. So, in order to review the results, I'm going to talk about the assumption that they take. So, here, again, we can sort the scale parameter from the largest to the smallest. And then, since the goal is to recover the top case set, it is necessary to assume there is a gap between the skill of the case player and the skill. The case player and the scale of the k plus one player. So delta serves as the gap parameter, it is separation of the signal. Okay. And then we also define a second parameter called kappa, which is which upper bounds the difference between the strongest player and weakest player. It is called the dynamic range. It characterizes the diversity of the n players. And kappa is assumed to be constant. And the first result of And the first result of Chain et al. says that if the separation parameter, the delta squared, is above the threshold of log n over MPL up to some constant, then both the MLE and the spectrum method are going to find the top case set with high probability exactly. Otherwise, if this result is not true, delta squared is smaller this way up to another constant, then no algorithm can actually find the top case. In other words, log n over mpl is the optimum rate. Over MPL is the outcome rate, and both the MOE and the special algorithm are able to achieve the output rate. But our result says something actually a bit different that we can show indeed MOE as optimal, but optimal in a stronger sense. However, we should actually show that the special method is in general not optimal. Okay, and I'll illustrate by what do I mean by not optimal. But in order to present my result, our result, I'm going to introduce two definitions of the Use two definitions of the variance functions. The first definition of the variance function is this function, v of kappa. Remember, kappa is the dynamic range. It is the bound of the difference between the strongest player and the weakest player. The definition of V of copper is through this optimization problem, which is actually not important. You don't need to care about the definition. Later, I will show you the plot of this function so you will get a sense of how the function looks like. How the function looks like. So, this is one of the two variance functions. The other variance function is defined in this way. Okay, so we have two variance functions: v of kappa and v bar of kappa. Okay, so our first result is for the performance of the metamico estimator. We show that if delta squared is above the threshold of two times the variance function v of kappa times this rate of log n over mpl, and then it will recover the top case set with high probability. And otherwise, if the High probability, and otherwise, if the conditions do not hold, the MOE will fail. Okay, so this result is actually sharper than the result of Trinidad because not only do we characterize rate, we also characterize that the exact constant is two times the variance function. Okay. And the rate function is more precise as well in terms of the numerator. The denominator is the same MPO, but the numerator comes in the form of the square of root log gamma plus root log N, sorry. root log k plus root log n minus k. There is a symmetry between k and k n minus k. This is actually intuitively to understand because the top k ranking is mathematically equivalent to the bottom k ranking. And this is the result for MLE for special algorithm that we show that if delta squared is above this threshold, this is the same threshold of MLE, except that we replace the variance function. Except that we replace the variance function from v by v bar. Then the special algorithm will find the top case set. Otherwise, it cannot find the top case set. So the special algorithm will have the same rate, but the constant is 2 times v bar, which is actually very different from the MOE. Okay, so the only difference between MLE and the spectral method is not in terms of rate, but in terms of constant. So let's try to compare the two variance functions. We show that both. Functions. We show that both functions are increasing. In particular, v bar is always greater than or equal to v. So the variance of spectral algorithm is always greater than or equal to the variance of MOE. In particular, the equality only holds when kappa is zero. Okay, that's when all the players have the same values. Now, here I'm going to show you the plot of these variance functions for different values of k over n. Values of k over n. So the orange curve is the v-bar, which is the variance function of the spectral method. The blue curve is the variance function of the mathematical estimator. So the orange curve is always above the blue curve. I also want you to notice that here, I actually grouped the plot into the first row and the second row. The first row and the second rows are actually exactly the same. The difference is the way I plotted. Is the way I plotted. The functions in the first row are plotted with the standard y-axis, but for the second row, it's placed again, it's plotted against the logarithmic scale. So as you can see from the second row, it's very clear that both functions are not only increasing, but are exponentially increasing. And not just that, you can show actually the difference between the two variance functions are exponentially increasing function of the kappa. So if you increase kappa, Of the kappa. So if you increase kappa, the difference of the two constants are going to be extremely large if kappa is large. Okay. And this is actually verified by a simulation that, for example, here we consider 200 players and what we are going to do is to do top 50 rank. So for the first group of top 50 players, we are going to divide them into further two subgroups with scale prime 10 and 10 minus tau. And for the for the for the And for the bottom 150 players, I'm also going to divide them into two groups, two further groups. So, in together, we have four groups. And then you can see as I vary as a fixed tau equals four, a very delta, and there is a significant gap between the probability of success in terms of the exact recovery of the two algorithms. And if we do a uniform sampling of the scale parameter and then Parameter, and then we can also see a difference. Okay, so in practice, there are lots of differences, and the only scenario that you don't see such a difference is actually the two-group model, by which I mean that the top K players have exactly the same skills, and the bottom M minus K players have the exactly the same skills. This is the only scenario that you can provably show that the MOE and the special algorithm have the same performance. Spectral algorithm have the same performance. But other than that, the performance gap is going to be large. Okay, so in conclusion, both MOE and spectral algorithms are optimal, but only rate optimal. The constant before the two algorithms are different. And MOE has exactly the sharp constant, the special method does not. Okay? All right. I'm going to stop here. And thank you for listening to the talk. And perhaps I can take some questions. Thank you. Thanks, Xiao. Any questions? Thanks, Xiao, for the great talk. So I just want to ask a question. I'm curious that instead of partition into local leagues, then run local MLE each. If I run local MLE, sorry, if I run global MLE for the full ranking problem, is it probably bad? It's probably bad? No, it's not probably bad. Actually, we think it should also work. But we just spent so much time. We don't know how to prove it. I see, I see. So it might achieve the optimal rate as well. It will achieve. I see. Because we have done simulation. It should work. So the difficulty is also coming from the fact that the condition. The condition number of the Hessing of the likelihood function of the global MOE is an exponential function of n times beta. And the state of the Dao proof depends on the Li-Wu-Nau trick by the Chong-e-Lao paper. So if you apply this trick, then the boundaries of the eigenvalue of the Hessian is very important, without which I don't know how to prove the MLE. So global MLE, well, computationally, you can say it's much worse. Can you can say it's much worse, but theoretically, I think it should work. Just don't know how to prove it. So, the second question is: how much does it rely on the logistic form of the psi function, which has this exponential decay and then allows you to conclude super strong and super weak opponents are not useful. So, in the BTL model, suppose there is some other utility function. It is a very good question. So, I think if you replace the Replace the lean function by another function because here the probability has to be bounded between zero and one. So in some range, it has to give you the conclusion that the players with extremely different rank positions will be negligible in terms of information. The shape has to come in this way. But exact threshold for a different But exact threshold for a different link function would be different. Thank you. Hi, Zhao. I have a kind of general question, which is, suppose you don't just have these theta i's, but you also have like an external field where, you know, which can corresponds to like the Like the, you know, just the conditions, like, you know, is it hot, cold, et cetera? And, like, you know, you could imagine that different players reacted different external conditions differently. Yeah. So, you know, in particular, there could be different rankings under different conditions, but, you know, maybe there's like an integrated ranking that you want to discover. Have people thought of this? Or is it a specific knowledge term? I'm not sure people have thought about the ranking problem, but there are definitely. Problem, but there are definitely such a model that characterizes what you said. But not, you don't just have the skill parameter, or you may have multi-dimensional skill parameter, or you may have one-dimensional skill parameter plus some external variables. But the difficulty of to-do ranking is here, in the current setting, the rank is defined as the rank of theta. So, when you have multiple features, how do you define the rank? Define the rank, yeah. Yeah, so I guess, yeah, I guess you know what I was thinking: you know, if you have an external uh field, you know, you could kind of integrate, you know, you have different rankings under different conditions, but you could kind of integrate over the external field and that maybe that's like an average ranking that you want to reconstruct. Right, right, right. So, so I think the difficulty is instead of mathematical, is first of all, to find a reasonable formulation of the ranking problem. Reasonable formulation of the ranking problem so that people will actually use that in practice. And then, in terms of mathematical ingredients, I would imagine many of the intuition that we have in the current setting can be extended to the marginal setting as well. But this is indeed a very good direction to explore. Thank you. So, I think there are a couple more questions. Yeah, okay. Yeah, can I answer my question? Sorry, can you hear me? Can I answer a question? Yes, yes, please. Question, yes, yes, please. Um, yeah, so I think something is kind of intriguing is that you set up the problem as a like estimating the permutation, but then you only need to kind of estimate the parameter theta first and then just rank them. So, I was wondering, for example, in your Gaussian model, what you can possibly do is that you can directly apply MLE for the original, like the permutation, right? Yeah, but then I know that you will, that one is the NPR in the worst case problem. Hard in the worst case problem. But I was curious: like, do you, yeah, something I want to ask is that do you always see like by ranking this parameter first, it's always optimal comparing to like really you want to estimate this permutation directly? I have a previous paper which I, you know, we have a calculation that assumes theta is known, the set of theta is known in a decreasing order. see that is in the decreasing order okay and then if you only optimize over pi sorry or over the rank vector it will also be optimal but as you said the optimization is uh is mp hard but uh what is interesting is uh you know if you optimize over theta the optimization is extremely easy and then you can do the ranking is still optimal so uh what i want your comment is here the rank vector is not exactly permutation actually it's much more than permutation because Actually, it's much more than permutation because when we define the loss function, you can actually compute the difference between two ranks, right? You can compute Ri hat minus Rj hat. Whereas for permutation, you cannot, such operation does not make sense. So in a sense, a rank is very different from permutation. Maybe that's why in terms of computation, we don't find such a gap. Oh, but I thought that you showed that this camel distance. You show that this Kamley distance or like Kendall Tau distance is kind of equivalent to the Spearman's distance. So, in that sense, if I understand correctly, the Kendall Tau distance is for the permutation, right? The Kernel Tau, you cannot write down the Kernel Tau for the permutation. The Kernel Tau requires you to compare. Let me go to the slides of the definition of the loss function for the discussion. Definition of the loss function, so the discussion can be made more clear. So, oh, it's stuck. Sorry, okay, it's here. So, here from the definition, you can see from the definition of Kandel's tau, right, you actually need to compute ri hat minus rj hat, which will be meaningless if you have a permutation object, because for permutation, the magnitude of ri is just a label. It doesn't have a real meaning. It doesn't have a real meaning. There is no linear order of the permutation instead of a rank. I don't know if that makes sense. Okay, yeah, so I guess I can think a little bit more. Yeah, I think I'm good for now. Okay. Yeah, so I think there are some more questions. I mean, the only loss function that I can think of for permutation is the Hamming distance that you compute. You compute the sum indicator such that Ri hat not equal Ri. That's the last I can think of for permutation. Joe, hi. Can I ask you another question? So do people so here you always assumed some minimal separation, right? So I actually played a little bit with this models for hockey prediction, and my conclusion was that the data is so noisy that basically. Noisy that basically, like, all these probabilities are very close to one-half. So, then I was wondering: do people look at it in a machine learning way where, let's say, you don't care about reconstructing theta i's and theta j's or even ranking of them, right? But you just care about, let's say, reconstructing probability pij that you know, i team beats the j's team. And maybe there is somehow, you know, self-consolation that, you know, then the minimum separation doesn't enter the answer. The minimum separation doesn't enter the answer. Do people look at this? So, I have two answers for this question. First of all, I think our results can be extended without assuming the minimal gap beta, because the reason is here, the minimax rate is taking soup over the rank vector r only. It's not taking soup over theta. In a sense, Theta, in a sense, we have an instance mean max rate for each individual theta. So you can imagine, even if I don't have any assumption for theta, we can still derive some formula, look like this. Sorry, but what is beta? Beta is the minimal gap that we assume so that in the polynomial regime, we can simplify the function into a very simple function of all the root sigma squared over mp. The root sigma squared over mp beta squared, right? So, otherwise, without this assumption, then the expression will be complicated, it will not be that simple. That's the point I want to make. Now, the second answer is, yes, people in the literature do have lots of work on SVDPIJ, especially the group by Martin Wainwright, Niha Shah, and co-authors, that they have the estimation of the probability matrix Pij. Uh, probability matrix Pij with respect to the Frobenius norm, and this can be done for the BTO model, and this can also be done for a much more general non-parametric model, which does not assume a parametric link function, but as long as you assume some gap, not some gap, some non-parametric assumption, then they have a very good result that you can take a look. Okay. Oh, that's right, that's right, yeah, yeah. Okay, yeah, I remember this. Okay, so and then the second question is: so let's suppose status are drawn from IAD from some prior distribution, right? And then maybe, right? So maybe you studied this, you know, this ranking problems for players for many years, right? So you have like some idea about this pie, right? Does it help? It does it help much for like some? I don't know. Do people look at it in an empirical base way? Yes. There is a paper by Deborah Shah from MIT. Actually, a study estimating the density function of theta in the BTO model. Yeah, that's exactly in the study that you're talking about. Yeah, but no, no, no, hold on. So that's by my former student, Anuran Makur. Yes. I know. No, I know that. Okay, good. Good, good. No, I know that paper. But that paper was about estimating the density itself, right? What I'm saying is that suppose I tell you the density, right? Does it, I don't know, do people, so it's kind of Bayesian a little bit, right? And then the next step is to estimate the density and then plug in into the Bayesian optimal estimator. I mean, presumably this would give some boost to, you know, to the, in practice, at least. I'm not sure if this will give us boost for the minimax rate. Minimax rate, but it will definitely be very helpful in terms of designing the divide and conquer algorithm. So, currently, our divide and conquer algorithm, a key step is the first step, which is leak partition. So, in league partition, we roughly, so it's this algorithm, we roughly partition the M players into leagues of Of sizes of the same order. And this is exactly due to the assumption that the players have a similar gap, you know, the difference between theta i and theta i plus one are of the same order beta. But you can imagine that if we drop the assumption, then if you can tell me the density, it'll be very much, it'll be very, very helpful because then when I uh when I uh when I choose this uh bandwidth parameter, H. So here I use H because you can think of the H as the bandwidth. Because you can think of the H as the bandwidth parameter, I can choose it differently according to the smoothness of the density function. So if the density in the local area of the top league is very smooth, then I'm going to choose the h for the top leak to be very large. So I get a larger league. For the second league, maybe the density function wiggles a lot. Then I'm going to choose a smaller h. Okay, as long as you can tell me the density. Okay, as long as you can tell me the density function, I can choose the bandwidth H adaptively according to the knowledge of the density. And then I will have adaptive partition with different sizes. And together with the second step of local ME, it will still be proven, give me min max optimal ranking results. So, yes, it will be very, very helpful. We haven't explored that in detail. All right. Okay. Thanks, Cheryl. Great talk. Thank you. Great talk. Thank you. Okay, if there are no further questions, let's thank Charlotte again. Thank you so much. Going to eat now.