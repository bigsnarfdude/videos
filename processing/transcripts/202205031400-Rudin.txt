She directs the Interpretable Machine Learning Lab, and her goal is to design predictive models that people can actually understand. And her lab applies machine learning in many areas, healthcare, criminal justice, energy reliability or some of this. She holds degrees from the University at Buffalo and Princeton, and she's also a recipient of the 2022 Squirrel AI Award for Artificial Intelligence for the Benefit of Humanity from the SA. The benefit of humanity from the Association for the Advancement of Artificial Intelligence. So, this is kind of Nobel Prize in AI, and this is a very cool thing, obviously. So, she's also a fellow of the American Statistical Association at the Institute of Mathematical Statistics and at the Association for the Advancement of Artificial Intelligence. And her work obviously has been featured in many articles, such as the New York Times, Washington Post, and the Boston Globe. And today, she will be giving a talk. And today she will be giving a talk on the theme of interpretability in machine learning, sparse generalized additive models, and optimal sparse decision trees. The floor is yours, Cynthia. Thank you. So it's delightful to be here. It wasn't too long ago where you could never have a workshop like this. I remember a time when there just weren't people working on this. So if you had a workshop, there would be nobody there. So it's delightful to have you all here. Delightful to have you all here. Okay, so in this talk, I'm going to discuss interpretable models that are small enough to fit on a PowerPoint slide. Now, I hope you remember from Rich Carawana's talk what generalized additive models are. So, this is what a gamm looks like. It's a sum of nonlinear functions of the original variables. Okay, so it's nonlinear, right? These component functions can be very nonlinear and very flexible. And hopefully, you know. And hopefully, you know, Rich would have shown a lot of these to you earlier. And luckily for you, I'm not going to give the same talk as Rich, although, to be clear, I think Rich is totally awesome. But today, I'm going to talk about how to produce very sparse GAMs. Okay? So I'll. Hey, Cynthia, apologies for interrupting. Are you sharing your slides? Am I? That's bad. Apparently, I'm not. Let me go and fix that. Okay, great. Okay, cool. All right, so this is a GAM. So it's a nonlinear function of the original variables, okay? All right, so the way that we're going to construct these is probably similar to the way that Rich is constructing his, which is that each of these non- Each of these nonlinear functions is a sum of dummy variables of the original variables. So here you have a bunch of weighted step functions, and these are all step functions on variable one, and these are all step functions on variable two and three, and so on. And then when you add these up, you get these nonlinear functions of that original variable. Okay, and then for x2, you would add those up, and then x3, add those up, and you get something like that. And the different And the different alphas determine the different heights of these step functions when you combine them together. So you can produce something that's very, very flexible when you do this. They're flexible, they're elegant, they're easy to work with, and they have a long history. People understand them. All right, so the paper that I'm going to talk about is, it was recently accepted to AI stats. It was written with my forever collaborator, Marco Seltzer. And then you heard from Chudi earlier. And then Jia Chong is the And then Jia Chang is the student, the other student who's leading the project with 2D. Okay, so the goal of this project is to solve this optimization problem. So we're trying to solve, to do sparse logistic regression. Okay, so this is minimize logistic loss. This is a standard loss used in logistic regression with L0 regularization, where the L0 term is just the number of non-zero terms in the model. Terms in the model. L0 is better than L1 because it doesn't have the bias of L1 that shows up when you want a really sparse model. And the model is linear. And then the model translates into a conditional probability using the usual formula for logistic regression, which is right here. This is the standard formula for logistic regression. Now, to get a generalized additive model from this, the simplest way to do that is to transform the variables before you do anything like as a pre-process. Before you do anything, like as a pre-processing step. So, for instance, if you have age as one of your original variables, then you can transform age into lots of dummy variables by creating these thresholds sort of at every possible age. Okay, so now you have a lot more features, right? You blew up the feature space, but now you can create very, very flexible functions of age since logistic regression will give you a weighted sum of these dummy variables. Okay, so this is very flexible. Okay, so this is very flexible. And so sometimes we also add in a small L2 regularization term just to help us with convergence. And like the vast majority of machine learning algorithms, to minimize the subjective, we're going to use some variation of gradient descent. We're going to use coordinate descent, where you optimize one coefficient at a time. And since we want models that are sparse, we're going to do subset selection. We're going to do subset selection. We're going to try out lots of feature subsets that seem promising according to the objective. Okay, but basically, a lot of this computation is just sending things to zero. Okay, so let me show you how it works. So as I mentioned, we're going to minimize the objective along one coordinate at a time. And why is that? It's because most of the time we're trying to make decisions about whether to set that coefficient to zero. Now, one way that people usually Now, one way that people usually do this kind of minimization is by using auxiliary functions, like what they do in the EM algorithm. So, you start somewhere, you're trying to get here, but you don't know where this minimum point is. So, what you might do is start here, and then you would create your auxiliary function like that. Now, this is a quadratic, so it's easy to minimize. Everything here I'm doing is all in one dimension, so this is actually quadratic in one dimension, so you can, you know, very. Very, very easy to minimize analytical solution. So, you do that, and you go here, and then what you might do is create another auxiliary function, and then you might keep going like that until you get to the minimizer. Now, the problem though with this whole algorithm, with this whole idea, is that we realized that making all these auxiliary functions was slowing us down because we could never get to the other side of this minimum. Like, we proved it, we proved it. Like we proved it. We proved you could never get to the other side doing this. So we decided to use a more aggressive step size. So we decided to go sort of twice as far. And then this is often on the other side. And when it is, we can use cutting planes. Okay. So what we would do is create a cutting plane like this, like facing this way, and then we need one going the opposite direction. We need two opposite-facing cutting planes so we can get a bound on that minimum there. So we could. Minimum there. So we could put one here, but we decided to try to like narrow in a little bit first to get like a tighter bound on that minimum. So we went to the midpoint, and from there we would create a cutting plane. And so if it's aiming this way, we have two opposite-facing cutting planes, which is what we want. And then if it's aiming the other way, like if it's aiming this way, then we can just use these two points instead so that we would have still two opposite facing cutting planes and then we can still get a nice tape. Planes, and then we can still get a nice tight bound on our minimum there. Okay, so now this is this drawing got a little bit tight, right? I actually got a really good tight bound on my minimum there, and so that prevents me from showing you what's going on. So, what I'm going to do is zoom in on this little area there and blow it up so that you can see in a little bit more detail. So, here I made it a little rounder so that you could see it better. And now we have our two opposite facing cuts. And now we have our two opposite face and cutting planes, and this gives us a lower bound on the objective right here. Now, if it happens that earlier in the computation, like maybe way earlier, we had found a solution with a better loss than our lower bound, then what that means is that if we add in this feature j, if we have any coefficient value for this feature j, we are only going to be worse than what we had already, okay, even at the best possible value of this. Possible value of this coefficient, we are still not going to be as good as what we had so far. So, in that case, we should just set the coefficient to zero because it's not helping us. No matter what value it is, it's just hurting our loss. So, if we set the coefficient to zero, we don't lose the regularization that would, you know, the L0 term that comes with that coefficient. Okay, so we set that term equal to zero, and we go find a better coefficient. And we go find a better coefficient and repeat this procedure again. Okay, now you can see that we did a lot of work to get really fast, tight lower bounds here. Obviously, if the best objective so far is above this, then we can actually set this to the minimum here. We can actually go find that minimum and go set it equal to that. But like I said, we found, we did a lot of work to get very nice, tight lower bounds here. And it turns out that we can get even tighter lower bounds. And it turns out that we can get even tighter lower bounds if we use quadratic cuts instead of cutting planes. And this technique really helps us to prune the search space so that we know when it's not worthwhile to add in a feature. And this whole computation is very, very fast, so that we can do it over and over again, do many iterations very quickly and search over the whole space of combinations of coefficients so that we can find, so that we can do subset selection at the same time as we find a solution that has a low. A solution that has a low logistic loss. All right, so that's the idea of that technique. And we also found what we think is a very helpful way to reformulate the problem to make it even faster. Okay, so let me show you that. So you can see here we've been using the logistic loss, and here's its formula for conditional probabilities. And if we switch the logistic loss to the exponential loss, then you know, I know we're. You know, I know we want to do logistic regression here, like I get that, but to be honest, the exponential loss does pretty much everything that the logistic loss can do. You get conditional probabilities, just like in logistic regression, and the equation is almost the same. So I'll go back to the logistic loss, and here the only difference is a factor of two over here and here. Okay. So, and there's a major advantage in using the exponential loss because when you're using features that are dummy, When you're using features that are dummy variables, it turns out that you get an analytical solution for the line search. And it's kind of a crazy formula, but you can calculate it, and you can calculate it directly. So it's, you know, if d minus is within this interval, set the thing to zero. Otherwise, set it to this value. So it's a bit of logical stuff, but you actually get an analytical formula. And so what that means is you can easily calculate it. You don't need cutting planes. You don't need quadratic cuts. You don't need anything. Quadratic cuts, you don't need anything, you just go straight to that minimum right there using this formula. And this makes it really fast. So we actually improved speed by a factor of about five doing this over the quadratic cuts. And the models seem to be just as accurate. Okay, so it does everything that the logistic loss does and more. It's just, you know, it's just the magic of the exponential loss. I did my PhD thesis on boosting where I kind of learned the magic of this loss function. Where I kind of learned the magic of this loss function and it sort of just keeps surprising me in nice ways. Anyway, so by running this technique, which is like coordinate descent plus bounds, and optimizing the heck out of each iteration of the code, we can get very, very sparse additive models really, really quickly. All right, so we have this algorithm, produces fast, sparse, generalized additive models. So now I want to show you how it performs on a benchmark data set where interpretability is important. Data set where interpretability is important. So I chose the FICO XAI challenge data. We decided to try to just, you know, take the whole data set and run it with our code. So this data set, by the way, it's about 10,000 loan applicants. And your goal is to predict whether someone's going to default on their loan. And you get a whole bunch of factors about their credit history. And if you run black box models on the data set, you'll get that. Models on the data set, you'll get that the best black box accuracy is about 73%, and the best black box AUC is about 0.8%. Okay, so that just gives you a baseline for what accuracy we could try to hope to get with a spar scam. Okay, so if you take the data and you take all the features and you put a dummy variable between every possible value, every possible two values of every continuous feature, you expand this data set to sort. You expand this data set to sort of over 1900 binary features. Which it's a lot, but as it turns out, about no matter which regularization parameter you choose, the logistic regression algorithm with the quadratic cuts takes less than a minute, and the exponential loss algorithm takes less than 20 seconds. So, and the accuracy and AUC are excellent, they're very comparable with the best. Are excellent. They're very comparable with the best of the black boxes. And on the next slide, I'm going to show you one of the models that came out of this algorithm. And I can do that because it fits on a PowerPoint slide. And the model has only 21 binary features. Okay, so this is the whole model right here. And you just get a score for each of these features and you add them up, right? This is the whole calculation. You just look up the points for each feature and you add them up. Up the points for each feature, and you add them up. So, let's take a look at some of the factors here. So, months since oldest trade open. So, if your oldest trades were all recent, like within the last hundred months, then you get more risk points. And if your number of satisfactory trades is less than 10, then you get a bunch of risk points. And external risk estimate seems to be a very valuable feature, and it's particularly sensitive between values of around 60 to 80. But what's really To 80. But what's really shocking about this model, besides its simplicity in describing this difficult benchmark data set, is that we created it in under four seconds from a data set with over 1900 features. And that's how fast this algorithm runs on average. It usually takes about four seconds. Now, this competition, this XAI data set, the FICO competition data set, right, this arose from a competition where they told everyone to create a black box and explain it. And explain it. And they told us this because they didn't know it was possible to create a model like this at all. And we did it in under four seconds. Right, you just can't get to this level of interpretability if you're taking a black box and trying to explain it to figure out which variables are important. Here I know exactly how important each variable is for every prediction. It's right here on the screen. All right. All right, so what I've shown you is that even on these very challenging benchmark data sets, it's often possible that interpretable models accuracy is equal to black box accuracy. And we've found this on every benchmark data set we've tried. So it's really quite remarkable to me that these, even these difficult benchmark data sets admit sparse interpretable solutions. All right, so I was thinking, like, okay, what do I want to talk to you about? Well, okay, so I've already talked about, you've already heard about GAMS twice today, right? I talked about it, Rich talked about it. Okay, well, my other favorite approach for tabular data is decision trees. But you've already heard about those too. And so after I found out Chidi was giving a talk on decision trees, I had to rewrite my talk very quickly. So I thought, Rewrite my talk very quickly. So I thought, okay, I won't talk about sparse decision trees because she already talked about that. So, but before, you know, I was thinking that, you know, I just couldn't resist kind of leaving it totally out of the talk. So I couldn't resist kind of going back to the FICO data set and showing you the model that Trudy built from that data set. Okay, so the same data set. And again, this data set is one where they told us create a black box and explain it because they didn't know that you could create, you know, a model that fits. You know, a model that fits on a PowerPoint slide. So, I'm going to show you the model that she created with the code from the AAAI paper from this year. And this tree has 10 leaves. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. It's a 10-leaf tree. And again, the data set 10K data points over 1,900 binary features. She set a depth limit of 5 on this tree. Again, On this tree. Again, the accuracy is very comparable to the best of the black boxes, and she produced the tree in 8.1 seconds. So you can produce trees like this for these very challenging benchmark data sets, even in 8.1 seconds. Now you can look at this tree and see what it says. So for instance, external risk estimate, which I told you was a very valuable feature. We found that out from the GAM model that I just showed you. And it says that if external risk estimate is really That if external risk estimate is really high or really low, then you know the class. Okay, so really low or really high, then you've got the class. Okay, if the external risk estimate is kind of in the middle, then you've got to ask the more nuanced questions like, well, how many percent of trades do you have with a balance? And how many months has it been in file? Okay, so yeah, so again, we didn't know it was actually possible to construct a single decision tree with this level. Construct a single decision tree with this level of accuracy when we got the data set. And in fact, without this, without Chudi's algorithm, I really don't know of another way we could have done this because I didn't know we could get to this level of sparsity and maintain this level of performance. So this is the kind of result that we've been kind of aiming for. And this is a decade worth of work on decision trees at least. All right, so great. So you've already heard about GANs twice. You've heard about decision trees. Okay, what about interpretation? Trees. Okay, what about interpretable neural networks? Oh, wait, Joe just talked about those, so I can't talk about those either. And he did a great job, so I wouldn't want to try to top that. So I said, okay, let's keep trying. So I think you haven't heard about exploratory data analysis. So I'll talk about that next. Okay. So I want to talk about dimension reduction. So dimension reduction, these algorithms are basic tools of exploratory data analysis. They just want to be able to give you a They just want to be able to give you a sense of what's in your data set before you start messing with it. And the main dimension reduction algorithm that everyone uses is Tisney. Now, Tisney, its input is high-dimensional data, like all dimension reduction algorithms, and its output is low-dimensional data that preserves something about the high-dimensional data. It preserves who knows what, like the graph structure, local neighborhoods, global structure. Like, who knows what it actually preserves? Like, who knows what it actually preserves? Okay, now the problem with Tisny, as we figured out in our reading group, we did a reading group a couple years ago on dimension reduction, but the problem we immediately found out when we started using it is that it is nearsighted. It doesn't pay attention to global structure. So it creates fake clusters that we don't want. And the example that I have here of Tisney working on MNIST is just exemplifying exactly what I'm talking about, because here you have these two clusters. About because here you have these two clusters of the same handwritten digit that should be in one cluster because they're close to each other in high-dimensional space. And then, same with this, this should be one cluster. And so Tisney just put these anywhere. It created fake clusters. And since it doesn't look at global structure, it just, you would never even know. Okay, so I got a chance to listen. I was lucky enough to get the chance to listen to Martin Wattenberg give a talk on Disney. Talk on Tisney. And what he said was he pointed out that Tisney had a bunch of hyperparameters that were really important. And if you fiddle with those hyperparameters, then you can get completely different global structure when you run it. Like here, the data was actually two Gaussians. And when you run Tisney, sometimes it produces these stringy things and lots of different little clusters. So if you didn't actually already know the labels, it would be very hard to figure out that all these yellow ones were orange ones are close to each other. ones were orange ones or close to each other in the original space. And that's what I mean by it just doesn't pay attention to global structure. And then also he pointed out like the cluster sizes don't mean anything. Like here I produced a donut. And the thing is, you usually run these algorithms on unsupervised data. So you never know what the right parameters are. You're basically just kind of fiddling with it and you just never know what the truth is. So we and you know. We, and you know, it's really not good to have these cases where you have clusters that you don't want. And I want to contrast that with the result on the same MNIST data set from our method, which this is our latest method, which pays attention to both local and global structure. And because of that, it tends to be more reliable and more stable to parameter choices and stable to initialization and stable to like small changes in the data. It's just an Like small changes in the data, it's just an overall more reliable algorithm. So, I'm going to show you some results on the mammoth data set. The mammoth data set is a three-dimensional data set, and what we want to do is project it to two dimensions. So it's like taking the mammoth and like crushing it into the page like a leaf, like smoosh. But we want to preserve its global structure as well as its local structure. Well, as its local structure. All right, so if we run Tisney with different perplexity values, then it's just, it's like it dismembered the mammoth and pan it over with a steamroller. Like this is just not good. And if you up the perplexity a whole lot, then you can get it to preserve sort of global structure, but it looks like a chicken. It doesn't really look like a mammoth here. This is UMAP, which again had a lot of trouble preserving the global structure. Structure. It again just didn't really work. It still looks like a chicken, I guess. LargeBiz separated out its legs into separate clusters. Yeah, that's bad. And then this is our method, which kept everything, the global structure, and it kept the little toes, which is the local structure that we wanted to preserve as well. And then if we go back to MNIST, so UMAP was particularly bad on the mammoth thing. Was particularly bad on the mammoth data set, but here it did great. Like it just, it's it really did well because MNIST is really a data set where local structure very much matters. And Tisny, unfortunately, as I mentioned, has this issue with the breaking of clusters into multiple clusters, which could confuse you if you're a biologist and you found two clusters of cells. You might be like, oh, they're actually different, but they're not. And then TriMap unfortunately just mushed all the other. Just mushed all the other classes together and just expanded out this one. I don't know what's up with it. And then R1 did as well as UMAP did on this data set where local structure was important. So PacMap tends to preserve global structure when it's important, like on the MAMAT data set, and then it preserves local structure when it's important, like on this and this data set. So it preserves kind of both local and global structure. So we wrote this paper. We designed an algorithm called We designed an algorithm called PacMap. And as I showed you, it maintains both local and global structure. It actually is much, much simpler than Tisney or UMAP. Tisney and UMAP have these equations that are very difficult even to put up on a PowerPoint slide because they're so complicated. Pac-Map, it's possible to put it up on a PowerPoint slide, although I didn't do it in this talk because I wasn't, like I said, I scrambled to sort of write something that would be entertaining. Write something that would be entertaining. But it's actually one equation that you can put on a slide, and it's got three terms in it. It's very, very simple. One term handles kind of pulling neighbors in, another term handles pushing points that should be far away, pushes them away. And then there's a third term that tries to pull in some points that are sort of in our neighborhood, but not our nearest neighbors. And that's it. And they're simple fractions. Okay. So because it's simple, So, because it's simpler, it's much more computationally efficient because it doesn't have these complicated equations with exponentials and logs, and you don't have to solve anything. You just minimize the sum of three fractions, and so it's much more computationally efficient than TISNE or UMAP. And my students gave it this name. They called it Pairwise Controlled Manifold Approximation Projection, Pac-Map. And I thought about the name and I thought, okay, well, it sounds kind of like Pac-Man. That's kind of cool. That's kind of cool, but it doesn't really have any relation to Pac-Man. And I thought, well, whatever. So I thought, okay, let's give it a try on FICO. So we took the FICO data set and we removed the labels and then we ran Pac-Map on it just to see what it would do. Okay. And I swear, like, I've never seen it do this before. I don't know what happened, but. I don't know what happened, but like, I just, it, I've never seen it do this before. This is the only data set I swear that it's ever done this before. And I'm sure there's some technical explanation for this. Like, I mean, I was asking, like, what is it doing? Like, and so the students did explain to me, like, there is a reason for this. Like, there is this little patch here has some structural missingness in it. So these are the data points that are missing this particular factor that's important. Particular factor that's important. And like the bottom part is people who have a high external risk estimate. So these are people who are more likely to default on their loan. And like the people up here are like less likely to default on their loan, but they have higher other things that make them want to default on their loan. Anyway, so yeah, it's sort of mimicking what we saw in the other two models, like the GAM model and the decision tree. But I generally But I generally find this kind of exploratory data analysis useful because it tells you something about the clusters or the manifold structures within the data. But I have to say, it's really, really hard to take this result seriously. All right, so if you want to try it out, you're welcome to. We're starting to have kind of, you know, an audience of this method. We have about 160 stars on GitHub so far. And here's the paper. It's a JMLR. Here's the paper. It's a JMLR paper, so you could check that out if you want to. Okay, so the last topic I want to kind of touch on today is exploratory model analysis through variable importance. So we wrote this paper recently called Exploring the Cloud of Variable Importance for the Set of All Good Models. So I have a lot of issues with sort of regular variable importance analysis because Analysis because it usually just tells you how important the variable is for one model, but that doesn't tell you how important the variable is in general. Like it's possible for you to find two models, one of which heavily uses that variable and one of which doesn't use the variable at all. And so if you concluded from the first model that the variable was important, you've made a mistake, right? Because that variable is not always important. It was just important to that model. So we decided to explore the cloud of variable importance. Cloud of variable importance for the whole set of good models. Okay, so the set of good models, I usually call it the Rashiman set. So it's the set of all models that are just about equally good. And so what we did in this paper was we projected the set, the whole Raschiman set, the whole set of good models, we projected it onto variable importance space. So each dimension, each axis here represents the variable importance of one of the variables. Of the variables. So here, this way you can see whether the importance of one variable goes up as the other one goes down. Okay, so here, this is like the whole Rachaman set. So these are all of the good models for this data set. We're using two dimensions. And here you can see that if the model is good, then as the importance of variable one goes up, then the importance of variable two must go down in order to stay within the set of good models. Okay. Okay, so that's the idea. And we suggested a plot called a variable importance diagram. And the variable importance diagram projects every two variables, the importance of every two variables against each other. So each dot here is a model. And the dots represent the importance of two of the variables against each other. So here I'm showing you a result on the program. The ProPublica Florida recidivism data set. And what you can see is that there are some good models where age is really important and gender is not ever really that important. But if you wanted to pick a model where age was not important, you could. And in that case, you could also pick a model where gender was sort of mildly important. Prior crimes, you could pick a model where prior crimes is very important. And age is important, or you could pick a bottle where priors is important and age is not important. So the idea is that to work on tools like this where people can choose among models from the Rashman set based on the importance of variables. So if they have some outside, like the machine learning in the data can't tell which of these models to use. There's enough uncertainty in the data that all of these models are about equally viable. Models are about equally viable. So, how do you choose among them? And people often want to choose among them based on whether the importance of variables agrees with their domain knowledge. So, a tool like this will help a user kind of search through the set of good models and find a variable, find a model that agrees with the variable importances that's in their domain expertise, agrees with their domain expertise. All right, so that's essentially what I wanted to talk about. So that's essentially what I wanted to talk about today. I'm going to end by telling you just about the projects going on in my lab, and then we can have a little discussion. So, this is essentially the full set of topics that we're working on in my lab. You've heard about a bunch of these today. So, generalized additive models over here, which we're using right now for healthcare and criminal justice. Optimal sparse decision trees, which Chudi talked about, and she's using that. These models are being used in material science. Are being used in material science. And then our dimension reduction tools are mostly being used in biology. We have a new paper that was just accepted on how to evaluate dimension reduction methods for biology, for cell biology. And then I talked a little bit about understanding the set of good models and importance of variables, because I really do believe in some sense that users don't just want one model spit out to them by a machine learning method. out to them by a machine learning method, they often want to kind of select among those models. And so we're working on tools that kind of allow us to see more than one model. And then interpretable neural networks for computer vision. Jid talked about his work on neural disentanglement. And then I have another branch of work on interpretable neural networks that use case-based reasoning. And that is being used right now for computer-aided mammography, where we have radiologists who are Radiologists who are looking at these explanations from our method, which is inherently interpretable, right? The method's explaining exactly how it's computing its result. And we're trying to get feedback from them on how it's working at the moment. And then the last topic that my lab works on is almost exact matching for causal inference, which we're applying to criminal justice applications. Applications. So, this almost exact matching, the goal is to mimic a randomized control trial where you take a treatment unit and a control unit that are, they look like identical twins, and you match them. And then you can figure out what the treatment effect is from those twins, because you're mimicking what would have happened if you did a randomized controlled trial. But you're constructing these sort of twins from very large observational databases. And so the question is sort of The question is sort of: how do you do that matching? Who are identical twins? How do you figure out which people to match? And that is the whole, we have a whole lab at Duke dedicated to solving just this exact problem in scalable and interpretable ways. And we're applying that right now to a program in Durham, which is a pre-trial training program. And the goal is to figure out whether it reduces the risk of those. Reduces the risk of those people who are in the program being re-arrested or repeat arrested in the future. And we're figuring that out by matching people who attended the program with people who didn't attend the program to try to figure out whether the program works. Unfortunately, the program does not seem to work, but our techniques work beautifully, so that's at least good. Anyway, that's it for me, so thank you very much. So I think we have already a question. Yeah. Hi, Cynthia. This is Jusvita. I was wondering for your model, PacMap, does it not have any tuning parameter in it? It has plenty of tuning parameters. It's just that it doesn't have a tuning parameter that switches between local and global. So with Tisney and UMAC, Tisney and UMAP, they can only, there's just one parameter called perplexity, and it's basically like the number of nearest neighbors that you want to preserve as neighbors when you project. And if you preserve a few neighbors, then you can preserve local structure. But if you preserve a lot of neighbors, then you try to preserve global structure. But our method doesn't have that kind of parameter, it has different parameters. So because our method preserves both, it preserves like global structure first, and then once it's got the global structure, Structure first, and then once it's got the global structure, it sort of figures out the local structure. So it doesn't have like a single parameter that like trades off between them. So that's why we didn't try to fiddle around with it. I have one more question. When the covariates are dependent, does it make any difference? So the way that these dimension reduction methods work is that you start off with high-dimensional data and you have distances between the And you have distances between the data points. Okay? And your goal is to keep the whole kind of, you know, who are my neighbors and how far away are they from me? So if variables are correlated with each other, then what you might have is a whole bunch of points that are fairly close to each other. And then in that case, when you project down, they should still be close to each other. But have you tried? But have you tried this? I mean, uh out of the I mean, you know, maybe I mean we've we've definitely worked on a lot of different data sets for this, lots of biology data sets, lots of synthetic data sets. So, I mean, and all of the variables are sort of correlated to each other in interesting ways. So, for example, with the mammoth, you have, you know, if you think about the mammoth, you have all of these data points like in the mammoth's leg that are all heavily correlated. Mammoth's leg that are all heavily correlated with each other, right? Does that make sense? Like, but the mammoth, you've got these correlations everywhere. Yeah. Thank you. Wonderful work. I hope you try it out. Thanks for your talk. Yes, parse generalized additive models. So So, what happens with the sensitivity, consistent out of distributions? Or do you not have all the values for a third-time variable that they are going to be, for example? So, like, missing, are you asking about missing data? No, not exactly, because you are using an extreme minimization of the Green minimization of the problem, and every time that I have tried to use something like that, it will quantify, make contacts. Oh, you mean like confidence intervals around the usually expanding goes hand by hand with our meeting. So I am really impressed that it does converge. Okay. I don't know if the question is. If the question is so, do you have missing a lot of missing values? So you don't have the full distribution of the pyramids present in the training data. Oh my gosh, I wish I could hear you, but I can't. So, what I was saying is that if you are doing an extreme vinylization. An extreme binarization of continuous variables, just one pin for each one of the values present in the training. So, how sensitive it is when, for example, you don't have all the values that are going to be present in the future. So, new data have these values that were not present in the training set because when I perform something similar with. Because when I perform something similar with ballistic regression and using elastic net penalties, that is quite similar, but you are using a mix of L1 and chill penalties, usually overfits. So I'm quite impressed what you have achieved. So what happens there? Okay, so I think I heard it, but I'm not sure. So I apologize if I answer the wrong question. But I think the question was: what happens if you don't use all I think the question was: what happens if you don't use all the thresholds, right? That was the question, I assume. What happens if you don't use all the thresholds, or how can you avoid using all the thresholds? Because it's computationally less feasible to do that. There's a couple different ways to do it. The easiest way is to run a boosted decision tree on it and then only use the thresholds that the boosted tree used. And so, in that case, you may lose a little bit of accuracy because you. Lose a little bit of accuracy because you've cut out the possibility of using some of these thresholds, but you at least get an accuracy that's comparable to the boosted decision tree. So that's one very nice way to handle that. We haven't needed to do that because even on the FICO data, which is like, you know, 1,900 features and 10,000 loan applicants, we're still finishing in under four seconds. So we're pretty happy with that performance, and we haven't really needed to use a subset of. And we haven't really needed to use a subset of variables. But maybe if I was using a data set of the size of Rich Carijana's data set, which I'm not brave enough to do that, but because that data is really, really noisy. But if I was using a data set of that size, I might do something like that to try to boost that. Thank you. Sure. And then I think you asked me about sort of regularization. So it's possible. So, it's possible to tune the regularization parameter. You can do that if you need more regularization. So, I see that Rich has the hand raised. Oh, thanks. Thanks. Hey, Cynthia, fantastic talk. I really, really, really enjoyed this. It's a workshop, so let me try to inject a little controversy and discussion into the workshop. So, I guess I'd like to distinguish. So, I guess I'd like to distinguish between two different kinds of sparsity. You have sparsity, say, within a feature, which think of it as the smoothness or the regularization or how many jumps are occurring on the graphs, the GAM graphs for any one feature. And then there's sparsity among different features. And this is going to sound kind of weird, but we've intentionally designed our algorithm to be anti-sparse with respect. Sparse with respect to the features themselves, with respect to what features it uses and doesn't use. And that's because we found that there was a trade-off between intelligibility and sparsity that we weren't expecting. And what happens is the best way for me to describe it is just give you a simple example. We have risk of something as a function of age. And then we might have a separate feature, which is whether the patient has retired or not. Has retired or not. And it turns out there's very strong correlation between certain ages in the age graph and whether the patient has retired or not. And if you promote sparsity among the features, it'll take all of the retirement effect and it'll put it into the age graph. And then it won't use the retirement feature. And it makes the age graph more complex and harder to understand because the age graph is no longer just intrinsic risk associated with, say, biological age. It's now risk associated. Age. It's now risk associated with retirement. And we found that the model is actually easier to interpret, even if it has more graphs, but if each graph is simpler so that we've undone the confounding that's created by correlation. So we actually prefer to have the retirement effect show up on the retirement variable, and then the age graph is a simpler graph, which is more biologic and intrinsic risk. And we see just many, many examples of this. Many examples of this. And if we push the number of features to a smaller set of features, we can often get equal or even like epsilon better accuracy by using fewer features. But we find that we lose intelligibility. We end up with much more hard, difficult detective work to have to do to understand each graph. I was wondering, is there a way for you to do a separate norm within your graph? Within your graph, like I like your decomposition into all these step functions for each feature. We don't train that way, but I like the fact that you do that. Can you possibly break it into just a norm that's going for sparsity within the features and otherwise has no pressure for sparsity between the features? Because we really, we actually, our algorithm is slower. Our algorithm is slower than it should be because we force it to be round robin so that every feature always gets its opportunity to learn in order to avoid sparsity. Yeah. So I have a confession to make, which is that this model, so which is the confession that I have to make is that our algorithm simply does L0. So there's no preference as to a larger number. As to a larger number of features being used or a smaller number of original features being used, it's simply aiming at a smaller number of step functions. That is all it's aiming for. So it has no customization in it whatsoever at the moment. It's literally just trying to create a sparse model. What I would hope is that if the retirement feature itself was quite strong, that the retirement signal would have a nice Signal would have a nice jump in it, and the age thing would be sort of flat. That's what I would sort of hope for. If it didn't happen, you're absolutely right, that's bad. But we don't have a version yet that can do sort of some variables in the L2 regularization and then some variables just in L0. That is something that is very easy to build, though, based on the knowledge. Based on the knowledge of the code that I provided, the knowledge of how the algorithm works that I just provided everybody, it should be pretty clear that it's very easy to build that in because it's an iterative algorithm optimizing something. It should be pretty easy. The only issue is that I think you'd have to use like the quadratic cuts and you wouldn't necessarily be able to use the exponential loss. Because if you try to use that, then you wouldn't, you'd no longer necessarily have an analytical solution for the line search unless you took that feature. Unless you took that feature literally out of the. Well, no, maybe not. No, you could keep the feature in there. Never mind. I think it would still work with the exponential. I haven't thought about it that long, as you've figured out, but it's definitely doable. It's just not something we've done, is the short version of that answer. Hey, that's great. I really like the answer. So this would be fun to play with. I'll have to try playing with your code a bit to see what we can get it to do. Get it to do. Hey, a different question on the PacMap. And I was kind of happy when you showed the example on the FICO data, where in some sense, the visualization didn't respect the two classes. It respected, let's say, other properties of the data. And we see this a lot in clustering, which is that clustering is kind of underspecified, and there isn't in real complex. Isn't in real complex data sets, there isn't just one way to cluster the data. There are actually alternate clusterings that are good for different purposes. And an example I sometimes use is, you know, you've got a data set of people and you've got maybe 100 different characteristics that describe each person and you want to do a clustering. Well, if you're a car salesman, the clustering you want is very different from if you're a doctor and possibly very different from if you're looking for a mate, right? For a mate. The clusterings are remarkable. It's the same data, the same features, and yet the clusterings that you want, the visualization you would want, would be very, very different for these different purposes. So I was wondering, the parameters that you've got, if you play with them, can you, like, suppose you really wanted on the FICO data, you really wanted it to cluster with respect to the label as opposed to the other property? Can you? To the other property. Can you play with the parameters in such a way that it'll actually end up doing that? Because one of the things we found was good about clustering was like textbooks would have you believe that there is one correct clustering and the goal of your algorithm is to achieve it reliably with as little data as possible, as quickly as possible. And in practice, we found that's not true. There's actually usually a dozen alternate clusterings of the data set, each of which has different purposes or different value. Purposes or different value. And what we found was really valuable was if our clustering algorithm was easily like drivable, so that by adjusting the parameters, we could steer it towards the clustering that was useful for our purpose and away from the clusterings that were useful for somebody else. Do your parameters kind of give you that kind of control, do you think? No, they don't, but I know how to do it. Okay, so I want to point out that, okay, so the Out that, okay, so the dimension reduction results are pretty robust. If you fiddle with things, then they stay the same, okay? But the high-dimensional distance metric is something that the user inputs. Okay, so if you change the high-dimensional distance, if you change the distance between points in the high-dimensional space, you get a different result on the low-dimensional space. So, right now, the way these are constructed is with without regard to class. Without regard to class, and then we put the class back in as colors afterward. And you can see that they're pretty mixed up in all of these three clusters. Like they're right on top of each other, which basically means they're not going to be that easy to separate. But what you could do, if you really wanted to separate them, you could add the class variable into the high-dimensional space, and you could create a distance metric that would depend heavily on that label. So you could create even a little movie that goes from something like this. It goes from something like this, which totally doesn't use a label at all, to something that completely separates the points with respect to label just by changing the inputs to the algorithm. So there's a lot of things we figured out that you can do with the visualizations by changing what the input is to the algorithm. But in terms of whether or not you can fiddle with the parameters of the algorithm to separate these the way they are with the current distance metric, I would say no. I would say no, and that's a feature, not a bug. That's very cool. Could you? One of the things we used to do is we would do a clustering, and then we would do another clustering. In fact, we would do thousands of clusterings, but we would make later clusterings be biased to be different from previous clusterings so that we were sort of more likely to explore the space of alternate clusterings. Alternate clusterings. Do you think you could do that global distance metric so that it favored things that were different from like the first distance? Like if you learn a distance. So we're not learning distance metrics. Yeah, that's the problem. That's the thing. We're not learning it. It's just input. Right. Could we somehow create a distance metric that would take the result of your first run, and now it would just want the And now it would just want the second run to be different from the first run. Do you think we could somehow bias it to do that? I don't think it would be very good. I think you have to know how to do that biasing so that you don't just produce garbage. Yeah, and what we would do is we would do thousands of clusterings and then we would cluster the clusterings because there tended to be only, you know, half a dozen or a dozen different flavors. And then we would, from each type, we would find the one that. From each type, we would find the one that seemed to be most compact. But I'm there's got to be a way to do it with your Pac-Map because I really like the Pac-Map. So, well, I mean, you know, remember, this is just an alternative to Tisney and UMAP and so like that. So it doesn't learn distance metrics or anything like that. I mean, if you think about clustering on the projection, right, you could create lots of clusters. You know, there's lots of different ways to cluster this, right? So you can cluster after projecting. That's something that's viable. Oh, and that makes sense to me. Oh, and that makes sense to me. Thank you. Quickly following up on this discussion, can you actually maybe instead of doing successive clustering, like sequential clustering as a result, can you could you do something like an adaptive parameter for the local distance? Because I assume probably because you showed only the example on the 3D number. Example on the 3D Manual data set, right? But I assume if the data set is quite high-dimensional, you have like some kind of a curse of dimensionality, right? And probably if the data set becomes too sparse, then the local features would be very difficult to catch. Yeah, that's a really good point. So luckily, we've just written a paper on this cell data set, which has thousands upon thousands of features. Which has thousands upon thousands of features. So, our latest paper, which was just accepted and not yet posted, is called Comprehensive Evaluation for Dimension Reduction Methods. And so, we took all these giant biology data sets and tried to figure out how well all these different methods were performing on these biology data sets. And what people typically do when you have an enormous data set, people typically do PCA down to 100 dimensions. 100 dimensions. And I thought about this and I thought, this is kind of bad. Like, why are we doing this like PCA down to 100 dimensions? And it turns out there's actually a really good reason for doing it. It's because a lot of these dimensions are actually completely useless. And so if you get rid of them before you start and you just work with the thousand sort of principal, or sorry, 100 principal components, then you can actually do really good dimension, good high-quality dimension reduction and get rid of all that sparse. Reduction and get rid of all that sparse crap that's, you know, all these useless directions in the space. And that's good for, especially if you have a small amount of data in an enormous space. Like you really just want to get rid of all the components where there's just nothing. And so hopefully that answers your question. Thanks. But then you are assuming your data is linear. I mean, otherwise, PCA dimension reduction is. No, what you're doing is you're preserving all of the interestingness in the first hundred principal components. Okay, so after you do PCA, you have a hundred dimensions and the data lives in that hundred dimensional space and it can live anywhere in that hundred dimensional space. And then from there, you do dimension reduction. So all the principal component analysis does is it reduces the thousand, you know, 100,000 dimensional space to 100 dimensional. To 100-dimensional space, and it gets rid of all the principal components from over 100 up to 100,000, which there was nothing in those dimensions anyway. Yeah, I know, but overall, BCA, you don't use it if you don't assume the data is linear. That's the idea, right? No, you don't, you're not making a linearity assumption. It's, I mean, I know what you're saying. Like, PCA is a linear transformation, right? Because you're transforming the components. You're transforming the components, but you're still keeping everything within the first hundred principal components. So you're finding the directions along which the variance is maximized, right? You're finding the directions along which variance is maximized. And then you're not eliminating the information when you do that. Let's say you do principal components. Let's say you're in a two-dimensional space and you project with PCA to two dimensions. Then you're not losing any information. Then you're not losing any information because all you're doing is rotating the you're just rotating the space. Yes. So what this method does is it takes a hundred thousand dimensional space of which there are only a hundred dimensions in which there is variance and it just keeps those hundred dimensions. Yeah, I know, but that's but I think what she's saying is the way that you determine the x the variance that's linear, like you assume that it's varying along x direction. Going along a direction you're keeping all the variance within the first hundred principal components, and then beyond that, you don't have any variance in those dimensions. So those dimensions are literally just useless. The problem, Cynthia, is that like in biology, like 100 dimensions 100 dimensions is not really informative. Like we generally work with, you know, we reduce the number of genes we work with to 5,000, 2,000. So when you say like 100, it's really like a big reduction. And for most studies in biomedicine, this is really a mini thing. So, I mean, we were working with genetic cell biology data. So, I mean, I'm not an expert in biology. Luckily, I had a collaborator who. In biology. Luckily, I had a collaborator who is. Otherwise, forget it. There's no way I can do it. But we're not reducing to 5,000 genes, right? We're reducing to 100 principal components. And those principal components, each of those, is a linear combination of the original variables. So you're still using all the original variables. It's just that your new space is 500 linear combinations of the original variables. Original variables. Right. Yeah. Yeah, so sorry, I just see that there is a raised hand in the chat, sorry, in the Zoom meeting. So maybe go first to this and then. Hi, thank you. I'm Jake from Charlotte. Nice to meet you. It was an amazing talk today. And I'm passionate to use the Pac-Man. It's kind of just kidding, but. Kind of just kidding, but uh, did you name the tech man uh from your visualization or you just kind of oh no, I'm telling you, this is what happened. The students named it. Okay, the students named it. And then I emailed, I literally, I was writing a talk. Okay, I was writing a talk like a couple of months ago, and this paper is years old. They named it years ago. I was writing a talk like last month or the month before that, and I emailed one of these two students, Ying Fan and Hai Yang, and I said, Can you please? Students, Yingfan and Haiyang, and I said, Can you please produce me a nice plot on the FINCO data set so that my talk can kind of go together? And then she sent me that, and I was like, No way. Like, you've no way. This is not real. And then I said to them, I'm like, you know, can you explain that? Like, what's going on there? That's weird. And then they were like, oh, and then they showed me all of the stuff that, like, oh, this side is this. And, um, but it was, it was just, I couldn't, I couldn't believe. Just, I couldn't, I couldn't believe it. I was shocked. Awesome. Actually, my question is about this. When your data contains multiple relationships that can be related to multiple graphs, then would Pac-Man