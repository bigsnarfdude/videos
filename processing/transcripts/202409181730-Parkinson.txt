Learning via representation costs. So, looking for what the gaps are between the behaviors of neural networks are at different layers. And the key tool I'm going to be using is the representation costs, which Becca talked about on Monday. So, when we talk about depth separation, we're looking for these gaps in behavior. And just kind of as a baseline, we all know that if you have a shallow Rayleigh network, it can approximate any continuous function if you give it arbitrarily width. But there are these results that say that. But there are these results that say that if you consider finite widths, there are functions that you can approximate with many fewer units using a deep print number. And there are also results that say that you can, there are functions that you can represent with much smaller parameters than you could with a deeper network than you could with a shallow network. But at the end of the day, what we really care about is learning and generalization. And so it's not immediately obvious that these results about separation in width or About separation in width or representation cost matters when we care about generalization. Let's say that we're trying to train a very, very wide neural network and we want to approximate a target function instead of exactly represented. These results don't immediately imply that the deeper network is going to generalize better or with deeper samples. And so just to make sure we're all on the same page, I'm going to start by establishing some notation and defining what I mean by learning. So we're going to assume that there's some true underlying So we're going to assume that there's some true underlying distribution on the input samples x. And then just for simplicity, we'll assume that our labels are deterministic function f of x. And then we're going to receive m training samples put into a set S. And then we're going to use a learning rule A that's going to map our samples S into a model class based on the training set. So for example, our learning rule could be we're going to pick a function from some model. We're going to pick a function from some model class that minimizes our schedule loss. And our goal is to have good test error, so small generalization error, but we have some limitations. So we only get finitely many training samples, and we're also using a limited model class. So the best that we can hope to do is to be probably approximately correct. And what do I mean by that? I say that a learning rule with M samples is epsilon delta, probability, probably. One delta probable probably approximately correct if with high probability of the training samples, the generalization error is small. So this is like kind of the broad idea. The general PAC learning framework assumes you're taking a supremum over all input distributions. But this is kind of the general framework for how we talked about generalization error. Any other questions? What was that? What was that? Oh no, this is not new. This is very old. And so we say that if a learning rule gives us a probably approximately correct, yeah, a learning rule is probably approximately correct with M samples, and we say that we learn with sample complexity M. And so our goal is to be able to bound the generalization error. But again, we have finitely. But again, we have finitely many samples and a limited model class. So when we're bounding our generalization error, especially when we're focusing on learning rules based on minimizing the sample loss, we end up with bounds that look like this. So the first term is our approximation error. So this is the error that I incur by using the limited model class. So this is the error of the best model, not model class. And I also have the estimation error, which is the error that I incur by having only finitely many samples. So I need to make sure. So, I need to make sure that my sample loss and my population loss are close together. When you bound these terms, use different techniques. So, to bound approximation error, you just need one good approximator in your model class. To bound the estimation error, you need to think about the size of your model class. And so, again, depending on the setting, you can measure the size of the model class using VC dimension, Rodomach complexity, metric entropy, et cetera. I'm going to focus on Radomach. Okay, I'm going to focus on Rademacher complexity today. That's the relevant notion of size. And Rademacher complexity, just as a broad overview, essentially measures how well functions in your class can fit random noise. So if you can fit random noise really easily, then your model class is 2-bit. So let's talk about how this applies for neural networks. This is essentially the same slide that Becca had on Monday. We have some input X, we multiply it by a weight matrix W, we apply a nonlinearity. W, we apply a non-linearity, we do that a bunch of times, and that gives us a deep neural network. The only difference for Monday is that today all of our activations are going to be regular instead of linear layers. And so again, the learning rule we're going to be going to be thinking about is going to be minimizing the sample loss, except that we're also going to add this regularization term where we regularize the by adding the square of all the parameters to. Square of all the parameters to our plots. So until now, we've been thinking about everything in parameter space, but it's equivalent to think about this in function space. So in function space, our learning rule is going to be minimize the sample loss plus lambda times the representation cost, where the representation cost of a function is simply the smallest norm of parameters I would need to implement that function with a neural network with that architecture. Any questions there? So, yeah, that's when we are going to roll. And so, this representation cost is very sensitive to network architecture and especially to depth. And so, we want to see if understanding representation costs across different depths can help us understand these gaps in learning and generalization capabilities. So, we want to know if paper networks are better at learning. And I'm just going to focus on depth two versus depth three Rayleigh networks. Every regular network. So let's talk about ideas that are yeah. So for now we can think about width as being we're going to assume that width is very very very large approximately infinite. We do have some results about what happens if you have finite widths and what widths you need for a result to hold. Does that answer your question? Does that answer your question? So, kind of just the first past intuition here. We know that both of these are going to be universal approximators if we give ourselves arbitrary width. This one looks like it has fewer parameters. So, same approximation error, bigger estimation error with three layers. So, this one should generalize works, right? But that's not what we observe in practice. In practice, we have these ginormous, deep, deep neural networks that perform very well. Neural networks that perform very well. So, why is that happening? So, let's revise our first intuition. So, first of all, we have the step separation of the width of a fraction that I referenced earlier. Mathematically speaking, what this says is that there's a function or family of functions from Rd to R that require width 2 to the omega d to approximate within a fixed epsilon with depth 2. So you need exponential width. But with depth 3, you only need polynomial. But with step three, you only need polynomial width to approximate within any epsilon. But again, that's maybe not super informative for learning. As Peter Bartlett said, for valid generalization, the size of the weights is more important than the size of the note. So we're somehow modeling or measuring model size the wrong way when we look at width. So we want to think about the size of the model in terms of the norm of the parameters. In terms of the norm of the parameters instead of the number of parameters, which brings us to the representation cost. And so, is there depth separation in representation costs? And the answer is yes. There are functions for which the R2 representation costs, so the cost to represent it with a two-layer network, is much, much higher than the cost to represent it with a three-layer network. But again, when we're trying to learn functions, we don't care about exactly representing their target function. We only care about approximating it. So maybe these functions which exhibit separation, you know, Separation, you know, if you approximate them and call that good enough, then there's no longer a separation between the representation cost VG approximate them. Is that clear? So when we think about learning and trying to establish depth separation in terms of learning, we need to be a little bit careful here. So, mathematically, the question we're asking is, is there a function or family of functions from Rd to R and a family of distributions on the inputs such that Inputs such that the sample complexity to learn within a fixed epsilon with a depth Q learning rule based on minimizing the loss plus the representation cost, that you need exponential sample complexity in that case. Whereas if you try to use the same learning rule, so minimize sample loss plus a regularization term, then you only need polynomially many samples to learn within any epsilon and any delta. And the answer to this question. And the answer to this question is yes, and I will prove it to you, hopefully, if you have that. Okay, what about the other way around? So, what if we want to know if there are functions that you need exponentially many samples filling with F3, but only polynomially many samples filling with F2? And the answer to this question is no. So, again, the key here is representation. key here is representation costs, understanding how the representation costs depends on depth. So here's the high-level sketch of the proof that a function that is hard with step two, or hard means exponential sample complexity. Sorry, there's a function that's hard with step two, but easy depth with step three. And the ES2 is a function that has very large representation costs to approximate step two, but very small representation costs to represent with step three to approximate step three. Is that clear? Okay. What's my time? Okay, so I'm going to sketch this proof then in a little bit more detail. Yeah, kind of the same thing as the previous slide. That's the actual function we're going to use to separate. But here's a little bit more of the details. So here we're going to show that this is a hard function to learn with our two-layer printing tool. So we're going to start with this lemma that says that if I have a function That says that if I have a function that can be epsilon-approximated with the depth of two network with small representation costs, then I can also epsilon approximate it with a narrow two-layer network. Is that clear? So if you just look at the converse, or sorry, the contraposal. Yeah, the opposite of that. We see that if we have a function that requires very, very large width to epsilon approximate it, it's going to also require very. Approximate it, it's going to also require a very, very large representation cost. Separately, we show that with high probability, a depth to interpolant of the samples of this function exists, and the representation cost of this function, of this interpolant, depends only mildly on the number of samples. And then, just looking at our learning rule, we see that the output of our learning rule is going to have smaller representation costs than any interpolant of our samples. And so, And so the output of our learning role also has a small representation. And so, based on this converse or punch positive or whatever it is, we conclude that the output of our learning rule is going to be a bad approximation of our target function unless the number of samples is very, very large. Any questions there? So, what about showing that this is a function that is easy to learn with our three layers? This is a function that is easy to learn with our three-layer learning rule. So, first, we show that this is a function that's easy to approximate with the three-layer network. So, there's a depth three approximation of our target function that is a good approximation and also has small representation costs. And if you choose Lambda in a reasonable way, we can talk offline about what that means, but don't be stupid about how you pick Lambda, then your representation cost of the output of your learning rule is also going. Cost to the output of your learning rule is also going to be small. And so we can restrict ourselves to the model class of dead-free neural networks that have bounded representation costs. And if we do that, then we can bound our generalization error. So our approximation error is small because we have this approximation of epsilon that lives in this restricted model class. And then we want to look at this estimation error. And Nashibur et al. in 2015 had this paper where they did all the They did all the dirty details of broad-market complexity analysis for Rayleigh networks. And they came up with these bounds that imply that if your representation cost is bounded, then with high probability, your estimation error is going to be bounded as well. So therefore, we can conclude that with high probability, as long as we have polynomially many samples, then we're going to be able to learn some stuff. Any questions? Was there anything to say about the fact that the function was very standard or not? So the function that we're trying to learn is a composition of an inner product with something that looks like a bunch of ray loops. And so that's very natural to approximate with the three layer networks. Um, approximate with the three-layer network. Like the first layer is just going to be your inner product, the second layer is going to be your Rayleigh-looking thing. The fact that it is hard to approximate to depth of two network is a more intricate proof. We can talk offline about it. There is some connections to harmonic analysis there. Some of the proofs of this type of flavor of result look at like what the support of depth to neural networks looks like in Fourier's case. Two neural networks look like in Fourier's case, and it's not very big. And so, therefore, there must be functions that you can't cross very well with older networks. Yeah, so then we have the second theorem that says that there's no reverse step separation. So any function that's easy with step two will also be easy with step three, where again, easy means polynomial sample complexity. And the key here is to notice that any function that has small representation cost with step two also has small representation cost cost cost cost cost cost Of step two also has small representation possible step three, and this is pretty straightforward to see. You take this network, you add an identity layer, now you have the same function, and the representation cost or the parameters you need is not that much. So a little bit more details here. We show that if your two-layer learning rule learns with polynomial sample complexity, that implies that there's going to be a good approximation of your target function. A good approximation of your target function with small representation costs. Then the three-layer representation cost is also small. And again, if you choose lambda in a reasonable way, the output over your learning rule will be less than this good approximation. And so we can apply that same Rademacher complexity analysis, or very similar analysis, to say that you're going to learn with high probability as long as your number of samples grows polynomially with each. Any other questions? Any other questions? Okay, so in conclusion, I hope that I've convinced you that functions that are easy to learn with step two networks form a strict subset of functions that are easy to learn with step three networks. And I've presented the learning rules here as if you've like exactly minimized your regularized loss. We can loosen that a little bit. We can assume that you've only nearly minimized your regularized blocks. But I think it's a really important open question to think about how in practice. To think about how, in practice, we don't nearly minimize our loss. So, how does this loss landscape to different depths affect learning? And I have no idea. People have ideas that that would be awesome. There's also interesting open questions about what about separations between other depths. So two and three is a good start. Matus Polgarsky had width, a depth separation and width results about like depth L and depth L squared, but depth L and depth L plus one tends to be really hard. Tends to be really hard. Lots of interesting questions people can talk. So, yeah, just like to thank my collaborators, Greg Anchi, Market, First Becca Woolett, my advisor, Ohad Shamir at the Wiseman Institute, and Matthew Sugar at the UKC. So there's, again, we have some intuition from the separation and width results. And there are results that say that like depth four, I don't know if it's three versus four or four versus five, but it becomes equivalent to some really long-standing open problems in like circuitry. So like think of your neural network as circuit, you have gates instead of ReLUs. Have gates instead of ReLUs, what kinds of things can you do or not do with those circuits? And that's equivalent to these kinds of problems about depth separation. So it's part. So I think that there's a big gap between hard to prove and true, right? Prove and true, right? So I think it's really hard to prove that there's a separation between, like, say, three and four layers, but in practice, it may still exist. Anybody else? Yeah, I might be kind of naive, but um I I understand like the speakers even at are the pair partners that you're taking. It seems like in some of the chemistry applications I've worked with, we'll tend to use pre-shallow net. And then the weather forecasting applications are like somewhere in the middle. Is there any sense among the rest about what functions are better for architecture? I mean, your your thoughts with the example would I think lovely for the purposes of the proof, but maybe not as intuitive. So my intuition is that the type of work that Becca was talking about on Monday, which tells us what kinds of functions are easy to represent with, you know, linear, linear, linear, Rayloop, we need to do that for like other types of activations and other types of architectures. And that's going to inform the answer to your question, right? So if we can understand what kinds of functions have small representation costs, those should be functions. Representation costs, those should be functions that are easier to learn. And yeah, we don't understand that very well. Does that answer your question? I think you told me it's answered here. Yeah. Yeah. Again, lots of open questions. Yes, fair. So there's some work by Jacob that looks at what the representation cost is for gray. Representation cost is for Rayleigh, Rayleigh, Rayleigh, Rayleigh. And his some papers he calls it a conjecture, in some places he says he's proven it, and I'm not sure which it is. But he believes that it's like the like the representation cost as you add infinitely many layers like that becomes like the minimal latent dimension that you would need to do a composition of a piecewise layer function. Of piecewise linear functions. So, input piecewise linear function to a low-dimensional space. What is that dimension? And that becomes the representation cost for de free lunars. So, functions that are very natural to represent as compositions is kind of the conclusion that that leads to, which is a little bit obvious because neural networks are compositions, right?