Can everyone hear me? Yes. Sorry, Diana, do you agree to be recorded? Oh, sure, yes. Thank you. And you can see my slides here. Yes, we can read your slides and we can hear you. So please proceed. Thank you. Great. Thank you. Hi. Hi, so my name is Diana Kai. I'm a PhD student at Princeton and I'm very happy to join virtually here today. I'm currently here in New York where it's snowing, so I hope the rest of you in Mexico are having better weather there. So I'm going to talk about finite mixture models today. And in particular, what we're going to show is in this work. To show is in this work, under the slightest amount of model misspecification, a Bayesian posterior on the number of components will diverge to infinity. And this is joint work with my collaborators, Trevor Campbell at UBC and Tamara Broderick at MIT. So, scientists and engineers are often interested in learning the number of subpopulations from data. For example, From data. For example, computational biologists want to learn the number of latent cell types in gene expression data. And there are many more examples in the literature where practitioners are interested in kind of learning this number of components from data. So when we expect there to be a finite but unknown number of components, it might be natural to use a finite mixture model, but with a prior on the number of components. Components. And so then what you can do is compute this posterior marginal number of components. So now I'm going to focus in for a bit on this specific example of learning the number of cell types from gene expression data. So here's an example of mouse single cell RNA sequencing data, and we fit a Gaussian mixture model with a geometric prior on the number of components. So in this plot, the x-axis denotes the number of components. x-axis denotes the number of components k and the y-axis denotes p of k given data. So this is the posterior marginal number of components. And what I've outlined here is the so-called ground truth number of components. So this is some number that's been reported by domain experts. And we can see that for 100 data points, our posterior is relatively peaked around this value. However, like suppose we now get more data and Now, get more data, and we're going to repeat this exact same analysis. If the model is correctly specified, we should expect to see a more concentrated posterior at our ground truth number. But here, what we observe is that with 300 data points, so while this posterior does appear more peaked than before, it's shifted its mass to the right. And so it's no longer placing its mass on the true value. And so what's going on here? Well, I think many of you here probably. I think many of you here probably recognize this is due to model misspecification. And so the posterior is now putting weight on more and more numbers of components to make up for this misspecification. And so as we get more data, we'll see that this posterior is going to keep shifting on to the right. So here, the colors getting darker represent and growing in size. So now what one could do is spend a significant amount of time and effort building, say, a better fitting model. Say a better fitting model, and they might ask, well, does this then fix the issue with learning the number of components? So, this kind of question motivated the development of our theoretical contributions. And in practice, we don't really know how misspecified a model even is. So, what we prove is that under an arbitrary amount of component misspecification, the posterior number of components diverges to infinity. So, now what I'm going to do is I'm going to So, now what I'm going to do is, I'm going to go through some of the existing literature, then I'll cover our theory, and finally, we'll look at some experiments and some additional extensions. So, the asymptotic behavior of the number of components in finite mixture models has been studied a lot. So, existing theory for the finite mixture model with the prior on the number of components says that it's consistent under a perfectly specific Under a perfectly specified model. So, what this means is that as the number of data points n grows, this posterior concentrates at this true generating number of components. And there's also been a lot of work in related models, such as over-fitted fitted mixtures that establishes posterior consistency for the number of components when it's well specified. And additionally, in a number of other settings, there's work. Number of other settings, there's work establishing posterior consistency and convergence rates for the mixing measure. So that's the weighted collection of parameters. So now a practitioner that is interested in learning the number of components in their data set might be like, they might see these results and think, well, great, finite mixtures are consistent. And so I should expect to learn the correct number of components. However, models for real-life applications. Models for real life applications are not going to be perfectly specified. And in practice, we don't know exactly what the shape of the component families are. And just note that this above work, it doesn't necessarily generalize straightforwardly to the misspecified component case that we are interested in due to either assumptions on the number of components or they might require additional conditions that significantly restrict the component family shapes we're interested in. Family shapes we're interested in. So we've seen all this literature that's developing consistency when these mixture models are well specified. But it's also well known from data analysis folk wisdom that under component misspecification, finite mixtures will learn too many components. So below here, we have data that's generated from a Here we have data that's generated from a two-component Laplace mixture, but it's fit with this Gaussian mixture model with a prior on the number of components. And what we see is that the posterior not only isn't concentrating at the true generating number, but it's also shifting off to the right as we get more and more data. And so this type of behavior has, of course, been observed empirically when we have misspecification by those in the data analysis community. By those in the data analysis community. And so, what we'll show is posterior divergence. So, what this means is that as the number of data points goes to infinity, not only does the posterior not concentrate at this generating number, it's not even going to concentrate at any finite number of components. And so, what our work is doing is we're adding rigor to this existing data analysis bulk wisdom by proving posterior divergence for the number of components under any amount of misunderstanding. Under any amount of misspecification. All right, so now what I'm going to do is I'm going to explain the modeling setup and then our full theorem. Okay, so first I'm going to set up what this Bayesian mixture modeling setup is, and I'll describe what I mean by misspecification. So suppose we have some likelihood given by P sub G. Given by P sub G. So here G represents a mixing measure, which is this weighted sum of parameter values. And P sub G is then this weighted sum of distributions. So for example, here, this psi theta j might represent some Gaussian with parameters theta j. Okay, so now we're going to specify a prior distribution pi. So this is a distribution on the number of components. Number of components, the weights, and the parameters. And so, an example of this might be if you draw k from a geometric distribution, you put a Dirichlet distribution on the weights and a Gaussian inverse gamma prior on the parameters. And so what this does is it induces a prior distribution on all finite mixtures, P sub G. Now, kind of in the classical Bayesian modeling setup, we're going to assume you. We're going to assume you assume that data are generated from the model. But here, what we're actually assuming is that there is model misspecification. So, what we're going to assume that what we're going to assume is that the data are actually generated from some true distribution P0. And so, suppose this square here represents the set of finite mixtures of our component family, which we'll denote by this capital psi. So, important. So, importantly, when we say model misspecification, what we mean is that this true generating distribution P0 is not contained in our model class. So, for example, if our model class includes mixtures, finite mixtures of Gaussians, this p-not here might represent a finite mixture of Laplace distributions. So now I'll go over the main result of our paper. So, if you recall from the previous slide, we have a model that's a finite mixture of psi, for example, a finite mixture of Gaussians. And we're going to assume there's this component misspecification in that the generating p-naught cannot be represented by a finite mixture of psi. So then under my So, then under mild regularity conditions on our model, this posterior number of components diverges. So, what this means is that for any finite number of components k, the posterior probability of k given our data goes to zero as the number of data points goes to infinity. And so, I'll talk a little bit more about what these regularity conditions are in the following slides, but at a high level, High level, I'll note that the regularity conditions on our component family hold for a wide class of finite mixture models, such as mixture-identifiable location-scale families. So this includes, but of course, is not limited to Gaussian components. So here's just a sketch of the intuition behind the result. So again, suppose this big rectangle represents our class of finite mixtures. Of finite mixtures. So, this is the space of our models. And on the boundary, we have these infinite models, infinite mixtures. So, if our regularity conditions are satisfied, this is going to ensure that as we get more data, the posterior is going to concentrate on this infinite mixture P0. And then what we'll show is that for any fixed finite k, there exists some neighborhood of P0 that does not contain any mixtures with at most k. Mixtures with at most k components. So we have two main regularity conditions that need to be satisfied. So the first is a condition on the prior. So this is a KL support condition on the prior. And what this states is that the prior places positive bass on mixtures that are near this true mixture P0. So in practice, very rough. Very roughly, what this condition means is that you need to place a distribution on the number components with full support on the positive integers. For example, a Poisson or a geometric distribution. In particular, I'll note that we're not at the moment considering models where there's an upper bound on the number of components. So, this regularity condition allows us to establish that while P0 is not a P naught is not a finite mixture and it's not contained in our model class and actually lies on the boundary of our model class. So, this means that it can be represented as a limit of a sequence of finite mixtures. Okay, in addition, so this KL support condition is also going to imply that the posterior is concentrating on weak neighborhoods of an. On weak neighborhoods of an infinite mixture, P0. So at this point, we then need to go from this convergence of the mixture densities to the parameters, and in particular, to the number of components. And so it's possible to come up with counterexamples from highly irregular component families where things don't necessarily work out. And so we're actually going to end up needing some additional regularity conditions on the component families themselves. Families themselves. So the second condition is that the component family is continuous, mixture identifiable, and has degenerate limits. So if the component family psi is continuous, what this means is that similar parameter values should result in similar component distributions. So as an example, in Gaussians, if In Gaussians, if we have a sequence of converging means, then we'll also have likewise a sequence of kind of weakly converging Gaussian distributions. The second condition, mixture identifiability of the component family, guarantees that each mixture piece of G is associated with a unique mixing distribution or set of parameters encapsulated by G. So if we have any two of these mixing measures, such any two of these mixing measures such that p sub g1 and g p sub g2 are equal, this implies that g1 and g2 are equal. And so I'll note that this is one of the weakest conditions that's typically specified for identifiability in mixtures. And it's satisfied by a large class of families, including multivariate Gaussians with mean and covariance parameters. And in the literature on the convergence of mixtures, many people Mixtures, many people end up appealing to much stronger notions of identifiability that might involve additional conditions on, say, the second derivatives of the densities. The final condition is that the family psi has degenerate limits. And so, what this essentially guarantees is that if you have a poorly behaved sequence of parameters, you're also going to get You're also going to get a poorly behaved sequence of component distributions. And so I think this condition is most easily explained by an example. So suppose we have a sequence of Gaussian distributions going from these light to dark colors. So if we have a sequence of means that are diverging or variances that are diverging, then the corresponding sequence of Gaussians will not be tight. Of Gaussians will not be tight. And so, you know, this probability mass will escape off to infinity. Finally, if we have a sequence of variances that are converging to zero, then this corresponding sequence of Gaussians will converge to kind of a point mass. So this is also likewise not going, or it's likewise also going to be poorly behaved. So importantly, all three of these conditions that I specified here. That I specified here, they hold for the Gaussian component family, characterized by a median covariance, as well as kind of location scale families, provided that they're mixture identifiable. And so, as I mentioned earlier, a lot of the results in this literature on mixtures, they end up, they're often interested in proving much stronger implications such as convergence rates. So they end up needing stronger conditions as a result that don't. All that don't include these models. So, this might be stronger notions of identifiability, additional smoothness assumptions, or compact parameter spaces. All right, so now I want to go back to this gene expression example that I mentioned at the beginning of the talk. So, here we're going to consider an axis-aligned Gaussian mixture model. Aligned Gaussian mixture model with a geometric prior on the number of components k and conjugate priors on the parameters theta. So as I showed before, we compute these posterior number of components for increasing data set sizes. So on the left, we have this single cell RNA sequencing data example again. And on the right, we have this cancer gene expression example. Example. And we can see in both of these examples that for smaller numbers of data points, this posterior plausibly captures the so-called ground truth number of components. And so this hints at an implicit regularization effect that we get from using finite data. And in particular, we don't know the ground truth number of components in practice. And so there's no way of knowing how reliable our inference is or how many data points to stop at. Points to stop at. And so we see here that the posterior number of components is going to vary substantially within realistic data set sizes. And repeating this exact same analysis on different data set sizes is going to give you different answers. So often we'll have folks that we talk to and they'll look at some of these simulations and say, well, you only used axis-aligned Gaussians. What if you used a better model? And so, as I mentioned at the beginning of the talk, And so, as I mentioned at the beginning of the talk, that question kind of motivated our theory in that we can kind of show what happens at the limit under an arbitrary amount of misspecification. Of course, in real life, we have a finite data set, and perhaps you really think your model is really, really well specified. So, what we can do, even though in practice we don't know how misspecified our model is, we can look at these simulations for synthetic. For synthetic data and look at increasing number of data points when we just have a small amount of misspecification. So, here's an example where our true distribution is a two-component Gaussian mixture model. And then, what we've done is we've just added a tiny amount of epsilon contamination with a Laplace distribution. So, that just means that we're drawing this very small proportion of data from this Laplace. Portion of data from this Laplace. Now, suppose again that our model is going to be a finite mixture of Gaussians, and we have this geometric prior on the number of components and conjugate priors on the parameters. So, here are the results for these very small amounts of Laplace contamination. So, here we have epsilon equals 0.01 and epsilon equals 0.1. In epsilon equals 0.1. And so, what we can see is that even for these very small amounts of epsilon contamination, we still get these varying results for our estimate of the number of components as we vary the data points. So, now I'm going to mention a couple of extensions that we've looked at so far. Okay, so the first is that Okay, so the first is that in the earlier part of the talk, we assumed that the prior was fixed. And you can also extend this to priors that vary with the number of data points. So here, this might be useful, for instance, if you're interested in taking an empirical Bayesian approach. And so the theory of our kind of in the other case holds again here, but all we have to do is replace this KL support. Is replace this KL support condition on the prior with this new KL condition on the sequence of priors. And here are some simulations of what that looks like. So our data are coming from a two-component Laplace mixture, and we're again using a Gaussian mixture model. And what we can see here is that we're getting this posterior divergence for the number of components. A second A second extension that we looked at were priors that have support on a finite number of components. So here we have this upper bound k tilde. And what happens there is what you might kind of guess intuitively is that the posterior will now concentrate on this k tilde or the sex finite upper bound. So in this example, we have this prior k that is uniform on one to six. Uniform on one to six. And what we can see here is that this posterior mass just kind of cuts off at six and concentrates kind of at that value. So you can read our paper for more details on these. Okay, so another thing we looked at was So, another thing we looked at were kind of power posteriors applied to this problem and looking at the posterior number of components. So a power posterior is the distribution that's induced by raising the likelihoods to some fixed power alpha between zero and one. And it's been applied for Bayesian robustness to likelihood mispecification and kind of a number of other problems, such as regression. Problems, such as regression problems and perturbed mixtures. And the kind of rough interpretation is that the posterior is going to behave as though they're like alpha times n number of data points. And so here you might kind of think about using this to cut off some of this divergence. But what we show is still asymptotically that this power posterior number. That this power posterior number of components under the same kind of conditions as the usual posterior is still going to diverge. And so this statement here is sort of assuming that alpha is fixed between 0 and 1, but we're also able to sort of show for certain sequences alpha n that we're still going to get this divergence for the number of components. In particular, I think as long as you have. In particular, I think as long as you have like n times alpha n going off to infinity, this behavior will occur. So, you know, you can see some simulations here of power posteriors where here we have this misspecified Laplace mixture that we saw earlier. And you can see with an exponent of 0.5, alpha equals 0.5 that. Alpha equals 0.5, that we now still have this kind of posterior divergence. It's just happening a little slower. And again, on the bottom, we looked at this like galaxy data set. And we can again see that kind of even though we have these different powers or the power posterior, and it is slowing down the sort of rate of divergence, we're still kind of getting these wildly varying answers that are within these real. Answers that are within these realistic data set sizes. Okay, so the main takeaway from this work is that model mispecification is going to be unavoidable in practice. And so even though our results here are asymptotic, it does have a number of important practical implications about the reliability of using finite mixtures to learn. Using finite mixtures to learn the number of components from our data. So, our theory allows us to establish kind of also a baseline for studying and developing robustness methods. And we saw this example here where we looked at power posteriors. So, one thing I'd like to emphasize is that ultimately, in practice, we are working with a finite amount of data, and our results here are asymptotic. Are asymptotic. And so, one thing we'd like to do is we'd like to quantify for when, for a finite sample, the posterior gives useful inferences. So, we're currently thinking about two main future directions. The first is thinking about finite sample bounds that quantify how quickly our answer becomes unreliable through, for instance, rates of divergence. We're also thinking about exploring further connections with practical robust. Further connections with practical robust mixture modeling methods. One thing we're particularly interested in are these coarsent posteriors. And so that can be interpreted as a power posterior whose rate sequence follows a particular rate, such that you're essentially keeping the posterior concentrated at some constant number of data points. Yeah, so it looks like I finished a little early, but thanks for listening. Thanks for listening. I'd love to chat a little bit more if anyone has questions. Oh, you can also find our paper here online. It's called Finite Mixture Models Do Not Reliably Learn the Number of Components. May I ask a very basic question? Yes. I mean, if I suppose I am just in a univariate case and I want to look at the number of components. To look at the number of components, should I look at the number of modes of the density? Because of the problem of misspecification, so the components would just be kernels to build the flexible density, and people would look at mods as components. Yeah, so I think often. I think often in practice, what people care about are kind of some kind of measure of like well-separatedness of the components. And so in the univariate case, I think you could look at modes. But I think when you kind of go to this kind of multivariate high-dimensional case, things kind of get a little bit more complicated there. So Yes, sure. Thank you.   We cannot hear you guys. Oh, maybe I cannot hear, maybe the others. And like if we take here, can you hear us? Yes. Okay, okay, so it was just a question of microphone. So yeah, so thanks for the talk, Diana. I had so essentially your conditions that you had on the mixture models were to The mixture models where to prove that you cannot find a p-naught, so distributions that's not in the mixture and but that can that's in the boundary. So there is no distributions on the boundary. Is that correct? There is no proper density on the boundary. So the um not I'm not sure I understand the question. So you're asking because yeah, because so if I understand. Because, yeah, because so if I understand correctly, so your Qback level convergence implies weak convergence of the posterior distribution. Yes. And so the only thing that you need to prove is that you cannot be on the boundary of any finite k mixture, otherwise, and that sort of implies your result. Is that correct? So essentially? We're just, or we need to show that the So, the P0 can't be contained in it, but the conditions will kind of imply that it will lie on the boundary. Yeah, exactly. So, that's what I was saying. And so, what I was wondering is that if you instead had conditions in terms of like, for instance, that would imply strong convergence of the posterior distribution, like L one convergence of the posterior distributions, you might not need the sort of identifiability conditions of the mixture in a sense. Conditions of the mixture, in a sense? Is that something you looked at? That we haven't looked at the kind of connections with the identifiability. It seems, I think the reason we kind of struggled with the, I guess we were initially looking at some strong convergence, but we wanted to sort of get. We wanted to sort of get away from having to assume that our kind of parameter space is compact. And so that's kind of originally why we weren't looking at that in combination. Okay, yeah, yeah. And so then the other comment I wanted to make, which I can't remember. Yeah, so yes, it's about these notions of misspecification in mixtures. There is always for me a bit of an ambiguity. There's always for me a bit of an ambiguity there. Somehow, what's the component if you don't have a model? So, because by definition, unless you have some structure in your data, there is no identifiability of the problem itself. So, the number of components doesn't exist unless you pose a proper model. The only case where you can define a non-parametric mixture, in a sense, so you make no assumptions on the emission distributions. Emission distributions is if you have conditionally on the number of the states, so conditionally on the individual, you have three blocks of independent observations. Then you can identify everything. But if you're not in these sort of situations, then the problem is not identifiable. And so the number of components is not identifiable itself. And so it doesn't mean anything to try to estimate the right number of components. So we're always a bit uncertain. So, we are always a bit uncertain about this sort of aspect of things. Right. I think that's a good point. I think we're definitely thinking about the setting where you kind of assume there is some finite mixture that your data are generated from, but you sort of don't know what that component family is. And so, I think in a lot of these applications, kind of practitioners are still hoping to kind of recover. Are still hoping to kind of recover that structure and kind of what that number of populations is. Hi, Diana. I'm Louis. So, the message of your talk is that The message of your talk is that it is very difficult to get the number of components in a mixture if you misspecify the model. And from the simulations you showed, your data was Gaussian and your kernel you used to fit the model was Laplace. So did you consider using other kernels like something more like smooth, like the normal instead of using the Laplace to see whether you get the right number of To see whether you get the right number of components? So, actually, the kernels we used were all Gaussian. And then the data were generated from Laplace distributions. And we mostly looked at Gaussian kernels kind of for the ease of computation, but we're currently kind of working on additional simulations and looking also at some. And looking also at some more like realistic models that practitioners use for biological data where you might have different kernels. For instance, some non-sort of conjugate, non-conjugate mixture models where you might have heavy tails in the kernel. But yeah, here all we looked at in our current simulations were things with Gaussian mixtures as our model. Used as our model. Robert Walter raises his hand. Okay, Robert, please please talk. Robert, do you want to ask a question? He's gone. He's gone. Okay. Oh no, he's still on there. Oh no, yeah, it's all audio, it only has video. Robert, can you hear us? Do you want to unmute? He doesn't have audio, his audio is disconnected. So I should write the right question. Yeah, write the question then. In the meanwhile, I have a tiny question. What happens if not the true distribution is outside this boundary of the finite mixture set? So if you are first. Uh, set. So, if your first condition doesn't apply, if f nut is too far, then anything can happen? Um, yeah, so we did originally consider that case. And so I think, you know, under additional conditions, you might, you can kind of think about, or one thing you could think about is the concentration of the posterior on kind of the closest kind of. Kind of member in your family and what's happening there. But I think kind of there's some work, more work to be done in terms of kind of the literature on this concentration with concentration of the posterior under misspecification. And so there, I guess either you would look at, I guess you would want to look at the set of closest mixtures. Set of closest mixtures kind of in your model class and analyze that case. Okay, yeah, so I just want to ask if there is nothing on this, since this is a workshop, right? I sort of agree with the idea that if you're interested, if you pose a model and the truth is. And the truth is different, right? Probably there is not, I mean, the issue of finding the correct number of components may be a bit tricky to formulate, let's say. But so as a practitioner, I think that what we do typically is using Banders loss of regional information procedures. So my question is, is there any result that shows, you know, the posterior, this is, let's say, the results by you? The results by using, you know, the perceived decision, let's say, that can come from the user binder's loss or the operational information if that is consistent. Is there anybody that has done anything like that? So I couldn't hear everything that clearly, but I believe you're asking about, are you asking about model selection with information criteria? Yeah, like what we typically do is to use band. What we typically do is to use Banders Laws for HM Information criteria to select eventually the number of clusters. And I'm wondering if there is any result about the posterior distribution of the selection or the number of clusters that came up from those criteria? I think there must be some work. I can't remember precisely what these results look like. What these results look like. I know that in this book, for example, they've mentioned, they do mention that you get kind of the like using some of these information criteria that you'll learn too many components. And I think what they need to do is have methods that kind of depend inversely on the number of samples. And so that's kind of And so that's kind of like allowing you, it's kind of you can think of as like adjusting your prior. And so one thing we're interested in is kind of thinking about like these types of priors as some sort of prior or sorry, these types of information criteria, some sort of prior on the number of components and seeing kind of what the relationship is there. Okay, Robert, still not on audio. Any other question? Okay, well, let's thank Dennis again. All right, thank you. 