I'd like to thank the organizers for inviting me to the workshop. I couldn't be there because of these issues. So I am Supravat. I am a postdoc at Northwestern NTTI Chicago. And today I'll just be speaking about this work titled A Characterization of Approximability for Bias CSPs. This is This is joint work with Wyung Li, who is one of the organizers. So, yeah. So, this is the, let me start with this mandatory slide for Boolean Max CSP. So, this is the second out of three talks in the morning, which have CSP in the title. So, let me just go over the definition super quickly. It is mainly to introduce the notation. So, Boolean max CSP or Boolean maximum constraint satisfaction problem. So, it's we all know what it is. So, it's basically a combinatorial optimization problem. An instance of it is defined in terms of a constraint hypergraph where the vertices are variables and the edges are constraints. And the The kind of constraint that is associated with a fixed edge is given by a predicate. So the predicate associated with the edge will identify the set of assignments that are acceptable for that edge. And with that also comes this notion of labeling satisfying an edge. So a labeling of the vertices will satisfy an edge if the string forms. Edge if the string formed by the assignment to the vertices of the edge evaluates the predicate to one. And as usual, if you look at the max CSP problem, then the max CSP objective is to find a global labeling of all the vertices which satisfies the maximum fraction of constraints. Again, Amai talked about this. So max CSPs, why they're important. Again, this is like a short. Again, this is like a short set of motivations for why we study them, right? So they are everywhere, they are ubiquitous in TCS. They are very general and expressive. So you have several well-known examples such as MaxCut, TableCover, MaxSat, all of these problems have their own dedicated lines of works which study their best possible. Which study their best possible approximation algorithm, hardness, all those things. And of course, some of these problems have also like central open questions built around them. For example, you have the Uni-Kames conjecture, which is about the satisfiability of the Uni-Kames problem. Then you have the ETH exponential time hypothesis, which is about the satisfiability problem. And there are several others like this as well. So this This talk deals with a slight variant of Boolean Max CSPs, which we'll refer to as biased CSPs. So what are these? So an instance of a mu biased CSP is defined almost as that of a Boolean max CSP. So you again have a constraint hypergraph on a set of vertices which represent the variables and you have edges which represent the variables. Represent the variables, and we have edges which represent constraints. The constraints are identified by the predicates. This is just as before. But then, and again, the objective is to find a labeling which satisfies the maximum fraction of constraints. But subject to this additional global constraint that we want to just look at labelings which set at most mu fraction of variables to once. To once. So, again, going over the objective, what is this? So, is there a question? No. No. Okay. Yeah. So the objective is that we are restricting ourselves to look at just the set of labelings, which set mu fraction of variables to once. And out of those labelings, we want to find. Tablings, we want to find one which satisfies as many edges as possible. Throughout, we shall use Î¼ to denote this upper bound on the relative fraction of vertices that should be set to 1. And we will refer to it as the bias of the instance. And so this is the setup. So you have basically a Boolean Max CSP and then the object. And then the objective is to find the best labeling out of this restricted set that is identified by this fraction mu. Now that we have introduced this parameter mu, this adds an additional degree of freedom along which you can study this problem. So for example, various parameters that you can think of related to this problem become a function of mu. The best possible value. Possible value that it can achieve for this instance becomes a function of mu. More generally, the approximation factor that you get for problems subject to this constraint becomes a subject function of mu. So various relevant quantities that you can think of will become a function of mu, right? So it adds this additional degree of freedom along which you can study this problem. So is this Is this set up clear? Okay. Okay, good. So let me take a couple of moments to convince you that this is not really a made-up definition. In fact, various problems that you might already be studying fit naturally into the language of biased CSTs. So our first such So, a first such well-known example is the Max K coverage problem. So, let me define it as follows. So, you're given a family of subsets over a universe of size and then the objective is that given this parameter k, we want to find a set of size k that hits the maximum number of sets in this family. When I say hit, I mean that this set must have non-empty intersection with a subset. So that's what hit. Subset. So that's what hitting means here. So how is this biased CSP? So I'll just tell you that this is a biased CSP with R constraints, the edges having R predicates. So how is that? So let me construct a biased CSP. So my constraint hypergraph is basically going to be the hypergraph corresponding to the set system of this family. So my vertices would be the elements of the Vertices would be the elements of the universe, and then the edges would basically be the subsets from the family. And the question? Okay. Yeah. So vertices are elements, edges are subsets. And then what we will do is for every edge, we'll put a R constraint on it. So every edge predicate, every edge constraint is a R. Predicate every edge constraint is an odd constraint, right? So, this is my construction. So now, if we think of this as a CSP, right, as the bias CSP, then if we look at the labeling of the instance, then when does a labeling satisfy an edge, right? Because the constraints are constraints, then a labeling will satisfy an edge if and only if it assigns at least one of the variables. Assigns at least one of the variables in that edge to one. So now, if you think of labelings as indicating a subset, then this exactly means that labeling will satisfy the edge if and only if the set indicated by the labeling hits that edge. And once we have this interpretation, you can naturally cast the objective of this CSP that we constructed as trying to find a set of size k which hits the maximum number of edges. The maximum number of edges. Now, note that I'm not, so I did say that in the definition, the objective was to find a labeling which sets at most k values to once. And here I'm talking about finding sets of size exactly K. But this instruction here is, I mean, it's not different because the R constraint is monotonic. So, whatever you get by. You'd get by choosing at most k values to set to one, you'd get you'd get the same thing by setting exactly k thing set to one. So this is exactly the max k coverage problem. Another example that's good to keep in mind is the densest k sub hypergraph problem. So again, this is extremely well studied. The instance is just a hypergraph, and then you have this parameter k and the objective is to find And the objective is to find a subset of size K which induces the largest number of edges in the hypergraph. Again, how is this a biased CSP? So let's construct this. So let me construct the instance as follows. So my constraint hypergraph is exactly going to be the hypergraph from the densest case of hypergraph instance, right? So this is my constraint hypergraph. And I'm going to put an And I'm going to put an AND constraint on all of these edges. So each edge is an AND constraint. And this is my construction. So again, what does it now, if I think of this as a CSP, what does this mean for a labeling to satisfy an edge? A labeling will satisfy an edge here if it assigns every value, every variable in that edge to one. And that just To one. And that just again means that if you look at the set indicated by the labeling, that set must induce that edge. So again, with this interpretation, you can naturally interpret the objective of this CSV as like trying to find a set of size k, which induces the largest number of edges, which is exactly the Uh, which is exactly the densest case of hypergraph problem. So, um, apart from this, uh, looks like you have a handful of examples which fit into the framework of bias CSPs. Why does it make sense to study it more generally? Right? So, firstly, there are actually like, so if you consider more variants, like the equals version of the problem where you have to choose exactly k instead of at most k values to be set to one. At most k values to be set to one, and then you can also think about the mean and set problem as well. So, all of these variations express actually a lot more number of problems which are well studied. So, problems like max bisection, min bisection, a balance separator, small set expansion, and several others like this naturally fit into the framework of bias CSPs. And these are problems which, of course, don't fit into the frame. Which, of course, don't fit into the like as is, they are not max CSPs, the max CSPs that Amay spoke of. And therefore, the kind of beautiful theory that is being developed for max CSVs isn't immediately applicable for these kinds of CSVs. So, Ragovinda's seminal result and optimal algorithms, optimal hardness, none of those apply here. Here. So, secondly, it gives us a way of asking beyond worst case analysis questions. Remember that a key thing here is that we have this parameter mu, which gives this additional degree of freedom. And due to which we can ask these kinds of questions. For example, let's take density case subgraph. It's well known that it's a really hard problem to apply. Hard problem to approximate. But then, is it hard to approximate for every value of k or is it easy for some k's and difficult for some other values of k? So, this gives us a platform of asking these kinds of questions. And finally, you have this connection that we have seen in some of the previous works that CSPs, bias CSPs, are more general. Or more generally, TSPs with global cardinality constraints seem to be related with smallset expansion hypothesis, which is, of course, tightly related to the unique Games conjecture. Now, as much as we know about the unique Games conjecture, we know relatively less about this hypothesis. So, this gives us like another avenue to explore the small set expansion hypothesis, right? So, okay. So, now we also have reasons to study. So now we also have reasons to study biased CSPs. So now that we have done the why, let's see what, right? So what do we want to study about bias CSPs? So the first thing that comes to mind is this course characterization question. So let me motivate it as follows. Let's take the two examples. Let's take the two examples that we saw: max K coverage and then syscase of hypergraph. So, max K coverage is well known that it admits an easy one minus one over E approximation algorithm. There is a greedy algorithm that gets this factor. There is a linear programming-based algorithm that can get a similar guarantee. And this is also tight by a result of phi g. But the key point here is that there is a constant factor. There is a constant factor approximation algorithm for this, which is which works for every Î¼. On the other hand, densest case sub-hypergraph, it is known under various assumptions that there is no constant factor approximation algorithm for this. You can show this under various assumptions: small set expansion hypothesis, ETH. More recently, there are results based on average case assumptions. Yeah, so no constant factor approximation for. So, no constant factor approximation for density casal hypergraph. So, what do we have here? So, on the one hand, we have biased CSPs with R constraints, which admit constant factor approximation guarantees. And then you have biased CSPs with AND constraints for which no constant factor approximation algorithm exists. So, the question is whether we can do this classification simultaneously for every predicate, every Boolean predicate. Predicate, every Boolean predicate. Can we give a dichotomy of sorts so which can cleanly classify whether a predicate admits a constant factor approximation algorithm for its biased CSP problem or not? And in case we were able to answer this question, then why stop there? We can even go to a more quantitative version of the question where we want to understand how much does the approximation factor The approximation factor depends on the various parameters involved. So, in particular, we want to study the bias approximation curve of the predicate, which is defined as follows. So, we have this quantity alpha mu of psi, which is basically meant to represent the best approximation factor you could hope to achieve for biased CSPs with biased. with bias parameter mu and predicate psi. And the hope is that we get explicit upper and lower bounds as of for this problem as a function of the bias parameter mu and the predicate structure. And an ideal scenario would be that we'd get matching upper and lower bounds. Upper bounds will be obtained using hardness results and lower bounds will be obtained using approximation. In using approximation algorithms, right? So these are the two questions, two broad questions that we study. Are there any questions about the problems that we study or anything else? Go ahead. Okay. Oh, sorry. I couldn't get like, is there a question? Or? No question. You can proceed. Okay. Okay. Cool. So yeah. So obviously, so these our results are basically answers to this question. So we give like almost complete answers. All right, um, so before I stay, I'll state the results. So, let me uh set up like a common setting for stating the results. So, some notation and some terminology, right? So, let's for throughout, let me fix a predicate, R I Boolean predicate, right? And let me associate a couple of quantities with it. So, firstly, we have this pre-image of one with respect to the predicate. Of one with respect to the predicate psi, which is basically the set of accepting strings of this predicate. Easy. So within this set of accepting strings, I'll define a set of minimal accepting strings for the predicate. How do we do that? So the set of accepting strings are R length Boolean strings. Boolean strings represent subsets. Strings represent subsets, and subsets have a partial ordering defined on them with respect to the containment relationship. So, let me lift that relationship, that partial ordering to the set of accepting strings. So, given to accepting strings beta and beta prime that are present in sine inverse of one, I'll order beta before beta prime if the set indicated by beta is contained within the set indicated by beta. contained within the set indicated by beta prime. And so now we have a partial ordering on the set of accepting strings. And from what we will do is we will pick the minimal elements from the set of accepting strings with respect to this partial ordering. And that is going to be my set of minimal accepting strings. Right. And one last notation that we'll use is that uses that fifth s of i basically represents a set of all our length strings of hamming weight exactly i so this is this is all the notation that we will need uh to describe the results so um yeah uh so let me now then state uh the easier to absorb result which is the coarse characterization result so it goes as follows so assuming these Assuming the small set expansion hypothesis for a predicate, the bias CSP problem is going to be constant factor approximable for every mu if and only if its set of minimal accepting strings is contained within the set of strings of Hamming weight at most one. So, in other words, you give me a predicate, the bias CSV problem on that predicate. Problem on that predicate is going to be constant factor approximable if and only if all its minimal axioms accepting strings have Hamming weight at most one. So in order to absorb this a bit better, let's try to see this in action. So let's take our favorite pair of examples, max k coverage and then CSK sub hypergram. Recall that Recall that a max k coverage was just a bias CSP with all constraint, and the or predicate accepts every string except the all zero string. So you can check that the set of minimal accepting strings for the or predicate is exactly the set of Hammingweight one strings. So these are just the standard basis vectors, right? So this is the set of accepting strings for the R predicate. And then this is clearly a set that's This is clearly a set that satisfies the condition, right? This has weight, all the minimal accepting strings have weight at most one. So our characterization implies that you have a constant factor approximation algorithm for this problem. On the other hand, our denser CS sub hypergraph was biased CSP with the AND constraint. The AND predicate has just one accepting string, which is the all-one string. And therefore, its minimal set of accepting string is just again the singleton set, which is the all-one string. And if the IAT was at least two, then clearly this is a set which violates this condition, right? So it contains elements with weight larger than one. And therefore, the characterization implies that you wouldn't have a constant factor approximation algorithm for this problem. Now, of course, we knew. Now, of course, we knew this behavior for these two specific problems from previous works. In fact, we know much more fine-tuned versions of them. But the nice thing about this is that we get this class kind of like, does it or does it not admit constant factor approximation classification simultaneously for any predicate, any Boolean predicate? So using a simple, easy to check combinatorial condition. Check combinatorial condition. So give me a predicate, and this will tell you that whether that predicates by CSP problem admits a constant factor approximation algorithm or not. Now, yeah, that was now that we have a course characterization of things, so let's dive deeper. So now we'll talk about the bias approximation curve of the problem. Approximation curve of the problem, and it's going to get a bit messy. So, before going into the exact details, so let me just like the take-home message is this, that the bias approximation curve of a predicate is basically related to the curve of the densest case of hypergraph problems. So, if you understood the curve of densest case of hypergraph problems, The curve of denser scale sub-hypergraph problems, you would understand the curve of any predicate. So, but then what is the explicit dependence between the two things? So, it's this slightly messy looking thing. So, let me take a moment to parse what this is saying. So, on the left-hand side, you have this curve for the predicate, which basically represents the best approximation. The best approximation factor that you'd get for bias CSPs with bias parameter Î¼, predicate psi. What this is saying is that this is saying is that this is related up to a multiplicative factor of r, quantities just depending on r, to this quantity on the right-hand side. So what is this? So the inner argument is just the Argument is just the best approximation factor for denser scale sub-hypergraph problem of RT beta, a weight of beta. And this beta is basically cycling through the set of minimal accepting strings of the predicate. So it is not super important to absorb this exact form of the relationship. The key thing to note is that the best approximation factor that you should get for a predicate is Should get for a predicate is the approximation factor of some densest case of hypergraph problem whose RET comes from the weight of strings from the minimal accepting set. Sorry, could you say that? Okay. What is R? R, if you recall, was the idity of the predicate psi. Was the RT of the predicate psi, right? So psi is the R Boolean predicate. Okay. Yeah, so what is this saying? Okay, so this is making things explicit that if you understood the curve of tensor scale sub-hypergraph problems, you would understand the curve of a predicate. So, but then this is not immediately useful unless you actually had. Immediately useful unless you actually had explicit bounds for the denser scale sub-hypergraph problem. So, can we actually give explicit bounds for this curve, which is like the third main thing that we do here? So, we give almost matching bounds for the denser scale-hypergraph problem when k, the parameter k, is linear in the number of vertices. So firstly, when k is mu fraction of the vertices and the idity of the densest case of hypergraph instances are, we show that there exists a mu to the r minus one log one over mu approximation algorithms, ignoring some constant factors. And we show that assuming small set expansion hypothesis, this is tight up to a factor of additional multiplicative factor of r. Multiplicative factor of R cube, where again R is the RDT of the NASA case of hypergraph instance. So if in particular, so in if R is constant, then you get like up to matching constant factor bounds approximation and hardness for density by paragraph. When the important thing is that when k is linear in the number of vertices, so we shall think of Vertices. So we shall think of mu as strictly being a constant and not sub constant, like something which doesn't reduce with the instant size. So let's now put everything we have seen till now together. So we have this course characterization, which relates the constant factor approximability to the predicate structure. Then you have this explicit relationship between the curve of Between the curve of a predicate and the curve of densest case sub-hypergraph problems. And then finally, we have this explicit bound for the curve of densest case sub-hypergraph problem itself. So if you put all of these things together, you have this some total contribution, which is an explicit bound for the bias approximation curve of any predicate. So, in particular, this is saying. So, in particular, this is saying that for any predicate psi, the best approximation factor at bias mu is going to be mu to the weight of beta minus 1 log 1 over mu, where beta is basically the largest weight minimal accepting string for that predicate. And yeah, this is, of course, ignoring factors which are multiplicatively dependent just on R, where R is. On R, where R is again the RT of the predicate. So, this is the sum total thing that we get for any predicate. And when R is constant, you get matching upper and lower bounds up to constant factors. So, and yeah, this is worse for any Boolean predicate. How much time do I have? Probably two minutes. Sorry? Two minutes? Two minutes? Okay. Yeah. Okay. So let me just skip what happens under the hood. Like, so broadly, we'll first need to relate the curve of a predicate to the curve of a densisque sub hypergraph problems. So it happens using two and flow reductions. And then, yeah, of course, the point is to derive explicit bounds for the SSK sub-hypergraph. For density hypergraph, the approximation algorithm is almost similar to an algorithm from max KCSP by Manurang Sinakian and Trevisa. This is in the fixed LET and large alphabet regime. And then the hardness requires more work. We have to construct this inner verifier, which is basically a noise stability test for the immune-biased hypercube. And then we'll have to. And then we'll have to combine this with small set expansion as the outer verifier. So, yeah, this is a one-minute description of how all the results are derived. So, what do we do in the end? So, we roughly give a characterization of when do predicates admit a constant factor approximation for the bias CSV problem. And if they don't, we also give explicit bounds for. We also give explicit bounds for how much the approximation factor deteriorates as a function of Î¼ and other predicate structure parameters. And a nice byproduct of this is that we get almost matching bounds for the density scale sub-hypergraph problem itself, which should be of interest. I'll conclude with some open questions here. Obvious question is that, can we remove these? Question: Is that can we remove these multiplicative factors of our gap and get like exact optimal algorithms and hardness? Another thing is like it's also interesting to study if random guessing itself is like an optimal algorithm for some of these problems. And if so, for what values of mu it happens, for what values of mu it doesn't happen, and finally, And finally, of course, like through in and throughout, I spoke about the atmospheric problem where you set like at most k values to once. There is also an analogous question of studying the equals version as well. And there are some partial progress with Jung, which will appear in a recent paper, which basically generalizes Raghavendra's result to the setting of CSPs. Setting of CSPs with equals cardinality constraints. So, yeah, so yeah, this is all that I had to talk about. So, yeah. One quick question while the next speaker is setting up. Thank you, speaker, again. Thanks. Thanks. Maybe I should stop sharing. Hello, everyone. Can you see my slides? Yes. All right. I guess since I'm the only thing preventing people from getting lunch, let me get going on this. Thanks for the invitation to the workshop so that I could come in person. but I couldn't come in person, but um