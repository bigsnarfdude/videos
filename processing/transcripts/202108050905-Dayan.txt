To be here and talk today. I really, really much appreciate it. So, today I would like to talk about some well-known results on interpolated sequences in multivariable settings. And I would like to go over some to review Agler's and McCarthy argument on the poly disk. And then I will break the rules a little bit because I will just connect that to. I will just connect that to a problem that I was interested in during my PhD thesis, which was studying interpolating d-tuples of matrices. And of course, when you have d-tuples of matrices, the components might commute or might not. And that's the main differences with scalars. And so we will analyze those two cases separately because in one case, we will use the functional calculus to apply a holomorphic function to them, whereas in the non-cumulative cases, one has to change. In the non-community cases, one has to change completely the class of function to apply to this. But the main motif is that interpolated sequences are kind of related to some risk system type conditions in each of those two cases. Okay, I will start with some backgrounds. And for the next few slides, I'm going to say something that probably all of the audience know better than I do. So I apologize if it's going to be too boring. But let me start by. Too boring, but let me start by saying what the main actors on my talk will be. So, reproducing kernel Hilbert spaces. If you have a domain of the complex of C D, Hilbert space of a Market function on X, is a reproducing kernel Hilbert space if point evaluation at each point of X is a continuous linear functional, which means, of course, that by risk representation theorem, for any X, you can find a function in the Hilbert space that represents point evaluation. That represents point evaluation via this colour, this k scalar product, like in this formula. And each one of those reproducing for a whole space comes with a multiplier algebra, which is the space, the algebra of Polymarket function on X that multiplies H inside itself. And the norm of a function in the multiplier is going to be the norm of the operator that sends F. The operator that sends f to phi times x. So the norm of the multiplication operator has a bounded linear function on h. Some example, well, the first most known example is, of course, the Hardy space. So we are in the unit disk, and H2 is the space of all function with square summable coefficients, Taylor coefficients. And in this case, one can do an easy computation and see that the kernel looks like this. So the well-known Ziego kernel. And of course, Well-known Ziego kernel. And of course, the multiplier algebra in this case is H infinity. So bounded analytic functions on the unit disk. And that's anisometry. So the infinity norm of a function is exactly the operator norm of the related multiplication operator in H2. But since this is a conference on multivariable function theory, I will consider here the two most natural generalization in D variables of In the variables of the unit disk, which are the unipole and the polydisc, of course. In the uni ball, what can consider the Drury-Harbison space, which has this, of course, this nice definition with its Taylor, given by Taylor-Quick coefficients of a holomorphic function, or is given by this Jury-Harverson kernel over here, which of course generalizes the Ziegler kernel with his equal. The zygote kernel will be equal to one. And in this case, though, the multiplier algebra is strictly contained in H infinity in the bounded analytic function of the unipole. Whereas this is not the case for the poly disk. If you consider the hardy space on the poly disk, then, well, its multiplier algebra, it is exactly H infinity. So the bounded, the space of bounded analytic function on the polydisc and the kernel. Poly disk and the kernel is just the product of the D Z eagle kernels in the D variables. Okay, so that now that we know now that we review what a reproducing kernel of Liber space is and its multiplier algebra, we can define what an interpolating sequence is on X is with respect to its two objects. In particular, a sequence lambda will be interpolating for the multiplier algebra if that's the very famous thing. That's the very famous definition. If for any bounded sequence Wn of targets, there exists a function within the multiplier that interpolates the nodes lambdas with the W's. Or else, one can say the lambda is interpolating for the reproducing kernel of Cooper space, and Nichols talked about it a little bit yesterday. In particular, this definition here corresponds to what he called universal interpolating. If basically the normalized kernels at those points, at the points of the sequence satisfy. points at the points of the sequence satisfies a partial equality up to some constant, namely this chain of inequality. And of course, well, when I say check here, I mean that this vector is normalized and this condition is also called sometimes, most of the times actually is a resistant condition. This constant here, see the least constant for which this is true is the Is the wrist bound of the sequence of vectors because that's, of course, is true in every it can be defined in any Hilbert space. And whereas if you only require the first, sorry, the second inequality to be true, well, that's going to be a Bessel system, in particular, of course. Any risk system is also Bessel. Okay, so. Okay, so the main property I think that connects interpolating sequences to the study of reproducing kernel Huber space is that if you take any functions in a multiplier algebra and then you then any kernel functions in HK is an eigenfunction of the multiplication by phi star of its adjoint. And the eigenvalue is exactly the conjugate of phi at the point. conjugate of phi at the point x. So in particular, having a bounded function in the multiplier with respect to the norm that sends some points to some targets will be equivalent of studying a bounded linear operator that has some eigenfunctions and some eigenvalues. And that's why really the geometry of the reproduced internal Huber space plays a role on the study of interpolating sequences for the multiplier. On interpolating sequences for the multiplier algebra. And this fact, in particular, this fundamental fact implies after some using some well-known characterization of risk systems on filter spaces, that if a sequence is interpolating for the multiplier, therefore it's interpolating for the filter space. And so one notion is stronger than the other. And of course, given this, one can ask, when are those two equivalent? Can ask when are those two equivalent in spec, and one case in which that is true is when the reproducing kernel space has something called that's called the peak property. Now, the way that I want to state the peak property in this talk is the following. If we have finally many points x1 through xn in x, and finally many targets w1 through wn. And if you know that the linear operator t that goes from this finite dimensional space. that goes from this finite dimensional span of kernels to itself that sends any kernel at xi to w bar i bar times the kernel itself well if you know that this t is a contraction then you can basically extend it to be the adjoint of multiplication by some t in the multiplier and in particular this p will do the interpolating job and he will have more he will have more less than one If we have less than one, also. So, basically, it's an isometric extension property. And of course, it's easy to see, thanks again to this fundamental fact here, that if there exists such a P, then this condition, so this T is a contraction. But for some spaces, the converse is also true, and it's very important. Now, there is a fairly good number of spaces with the Number of spaces with the peak property. H2 is one of those. The Dirichlet space is one of those. The three Harveston space is one of those. But the hardest space on the poly disk, when B is bigger than or equal to two, it's not. And one way to see that is by looking at this paper of Denizo and Erica Mar, in which they show that there are sequences on the biodisk. Actually, they proved it. That are interpolating for H2 for the hard space, but not for H infinity, so not for its multiplier algebra. And that in particular implies that, since given what we said before, that the hard space on the politics does not have all the big properties. So one cannot really characterize interpolating sequences in terms of the geometry of the kernel in the hardy space. So how can one approach the problem? So interpolating sequences for the multipliers intuitively are separated sequences because you want to specify arbitrarily the value of a bounded analytic functions in this case on the nodes. And so one can define a way to separate the points via multipliers. And we say that given in any reproducing Fernande Huber space actually Fernande Huber space actually given two point X and Y, their pseudo-hyperbolic distance or the Raison distance, sometimes it's called, is nothing but the maximum value that P of X can get, given that P is a multiplier in the union pole of the multiplier algebra, and P vanishes at the other point. And with this notion of metric in mind, it's pseudometric in mind. It's pseudometric in mind. One can define a sequence to be just separated if the Squaresman distance between two any distant points of the sequence is uniformly bounded below, or uniformly separated if given any point in the sequence, you take the product of this Glacient distance between that point and the rest of the sequence, and this quantity is uniformly bounded below. And given those two notions, Carloson Given those two notions, Carleson, in a very classic and famous result around the 60s, was able to prove that in the unit disk, so for H2 and for the case when MK is H2P and you have the unit disk, then a sequence is interpolating for both the multiplier algebra and H2, because we saw the H2 as the big property, so that's the same. If and only if it is uniformly separated. It is uniformly separated, or if and only if it is just separated, and the sequence of kernels, the normalized sequence of kernels, is a peso system. Now, of course, as I was saying now, one can add if you want, or like say lambda is interpolating for h2. And actually, the equivalence between two, three, and four is true for any reproducing. true for any reproducing uh for any reproducing kernel filber space with the peak property in particular between three and four oh sorry i don't know what happened see i told you that i wasn't really practical yet sorry about that so the equivalence between three and four is due to um uh like carpilitter uh hearts and alimon like three or four years ago i think three okay so Think three. Okay, so what happened when the dimension grows, though? Well, that's not the case anymore. In fact, Burst and Chang and Lin proved that, well, if you take the three conditions in Carloson theorem, as I stated above, well, you still have a series of implications. Of course, if a sequence lambda, the poly disc is uniformly separated, therefore it's interpolating for H infinity. And if that's the case, then it is separated clearly. And it's also the Clearly, and it's also that the normal aspirinal functions in H2 are a vessel system, but not on the converse all. And the counterexamples are actually pretty easy. I think that the counterexample for one to two is a Cartesian product of a sequence in the unit disk itself. The counterexample from two to three is actually just a sequence of the diagonal of the poly disc. So that to me at least suggests. That to me at least suggests that there is a really large gap between those three conditions whenever the dimension grows. And so the situation, as per usual, is dramatically different and more difficult in the polydisc rather than the disk. Okay, but when d is equal to two, John and G. Magler were able to give a characterization of interpolating sequences. Of interpolating sequences for H infinity of the pi disk, stated in terms of Euclidean properties of some kernels. The main difference with the one variable case, though, is that instead of considering only the Zigo kernel, one kernel, basically the Zigo kernel, so the Harvey space, one has to consider a whole class of different kernels at the same time on the polybuse, on the bi-bask. And so this class of kernels is the class of admissible kernels. So we say that Admissible kernel. So we say that a kernel on the polydisc is admissible if in the associated reproducing kernel Huger space, multiplying by a coordinate function is a contraction. And of course, there is also a more algebraic way to say this property, but that's essentially what it is. And an example, an easy example, I would say that of those kind of kernels are those sort of weighted. Those sort of weighted siego kernels on the polydiskin, which you basically multiply each ziggo kernel in each variable by an exponent greater than one. And that's going to give you that the kernel is of missing. Of course, well, if the multiplier algebra of such reproducing kernel Hilbert space is H infinity, since the coordinate functions in H infinity are coordinate functions in each infinity are norm one, then k will be admissible. And the output, the converse is known to be true, at least, only for the info two. So it's not trivial at all. And this is something that is called Ando's inequality for operator two. And whether, of course, Hando's inequality failed for d bigger than or equal to three, but he might not. But he might not fail in this particular setting. So, whenever you choose a d2 plus to be just multiplication by coordinate function, so that's as far as I know, it's not known whether the fact that the kernel is admissible is equivalent for the multiplier algebra to be H infinity for greater dimension there, too. Okay, so really what works as a peak property is this. Property is this theorem by G. Magler that he has in his notes that, as far as I know, never got published, but I found it very, very interesting and enlightening for my research during my PhD, which basically proved the following. So if you have a condition, it looks like the sufficient condition for the peak property, namely, if you have finally many points in the... points in the in the bi disk and you have finally many targets in C and you have that this for any kernel so for any admissible kernel k on the by disk the function tk it goes from this finally dimensional spans of kernel and each one of those is contained in a different reproducing kernel Filber space of course when you vary the kernel so if each one of those TK in each one of those reproducing kernel In each one of those reproducing kernel privilege space, such that again those kernel functions are eigenfunctions and eigenvalues are those targets conjugated. If each one of those is a contraction, then you can find the function C in H Infinity, which is a contraction and it does the interpolating job. So there should be a two. Sorry. Find my first type. And so as I was saying before. So, as I was saying before, this is a sort of analogous of the pre property, but the main difference really is that the right condition has to be checked on lots of kernels at the same time. And as a corollary, well, one can get that a sequence in the bi-disk is interpolating if and only if the associated sequence of kernels, of admissible kernels, is a risk system. It has to be true in each admissible reproducing kernel. Each admissible reproducing kernel Hilbert space, and the risk bound does not have to depend on K. So if you have, if all the admissible sequences of kernels in those admissible reproducing kernel film spaces are have a uniformly bounded risk bound, essentially, which is great that's a characterization of interpolating sequences in a multivariable, you know, in a positive is equal to two, but it's also a fairly hard condition to. Also, a fairly hard condition to check, as you can guess, because there's a lot of currents at the same time that you have to consider. And of course, there is also some more hydrobrah and nicer way to state this condition, but I wanted to focus like on this duality between like resistance and interpolating sequences. And so, as I was saying before, I found this result very, very interesting during my thesis, and I found it also very versatile. Namely, I found out that I found out that one can extend this kind of result to a more general interpolation problem, in which instead of considering point, one wants to consider objects which are algebraically a little bit more complicated, like matrices. So the aim, the part of my PhD thesis, was to try to understand and characterize those interpolating sequences of V tuples. Sequences of d-tuples of matrices. And of course, a d-tuple like this might, so the main difference between this and a point in a polydisc on the uniple is that those matrices, those coordinates might commute or not. And if one wants to apply an H infinity, so a holomorphic functions to such an object, well, you will have to ask if those matrices actually commute. If they don't commute, we will see later, then one can have many of the speakers. As many of the speakers in this conference talk about, there is the whole theory of non-commutative function theory, and one can basically do the same thing. But let's start with the commuting case, because that's where we're going to extend basically the same proof of Agler and McCarty results. As I was saying, yes, so you apply a holomorphic function to a d-tuple of commuting matrices by just this analogous of the Cauchy integral formula. Analogous of the Cauchy integral formula. And so, for this to make sense, in particular for these inverses here to make sense, you have to ask that the point, sorry, yeah, that this joint spectrum of A, of the D-tuple, is contained in the poly disc. And that's an assumption that we're going to make from now on. And okay, so as far as I knew when I started doing this, there wasn't really a definition of interpolating CPU. Um, definition of interpolating sequence of interpolating sequences of matrices. And so, the first thing that one had to do is like to come up with such a definition. And one first attempt was to do the following. So, you can see that a sequence a n is interpolating of b2 plus an is interpolating. If for any bounded target, where by bounded, I mean that is bounded in the operator norm. And of course, you want those matrices WN to have the right dimensions, so the matching dimensions with the ones in the sequence AN. dimensions with the ones in the sequence a n. Well, if that's the case, you can find always a function p in h infinity that sends the nodes to the targets. That's one first attempt. But that's not really going to work because matrices have a more complicated structure algebraically in theory. And so in particular, a holomorphic function will always preserve a matrix invariant subspace. And therefore, even in the one dimension, Even in the one-dimensional case, and even with a one-point interpolation problem, if you're given a diagonal target and you want to send it to a non-trivial Jordan block of the right size, even if the Jordan block is bounded in the operator norm, of course, well, that's not going to be possible because at least not with a holomorphic function. So one has to kind of change the definition of interpolating matrix. The definition of interpolating matrices to make this work. And the way that one can do it is to identify a target with a bounded sequence in H infinity. So the sequence A n will be interpolating for H infinity if, given any bounded sequence in H infinity, so with respect to H infinity norm, Vn, there exists one function phi that agrees with each Vn at the dots a n. And this, of course, extends also the definition for scalars. Scalars and it turns out to be kind of the right definition. Now that we have definition for interpolating sequences, one might want to separate them as well, as we were doing in the scalar case. And well, you might separate those two matrices to compute sort of a glaison distance between two d2 Poisson matrices of possibly different sizes, of course. One can take all the functions in H infinity that. Take all the functions in each infinity that sends a to that entity and b to the zero matrix, take the one that has the least the infimum of all the norms of those functions, and take one over that number. And that will extend the definition for scalars. And we can use this as our class and distances between two matrices with spectrum in the positives. And once we have this row, we can again define what a separated sequence and what a uniform. Separated sequence and one uniformly separated sequence of d tuples of commuting matrices is as we did before. So separation will be pairwise uniform pairwise. The distance will be bounded below if you take any of two distinct detuple and same for uniform separation. That product will have to be uniformly bounded below. And So now that we have the distance, we have the right definition of interpolating sequences of matrices. One, if you want to really extend the same notion, the same theorems as for the scalar case, one will have to come up with a by definition of a kernel of what a kernel of a matrix on, sorry, at a matrix, at a d2 polar matrix might be. And so if you have an admissible kernel on the poly disk, you can take all the functions in HK. you can take all the functions in hk that vanishes at a and a is a detuple of commutative matrices and take its orthogonal complement in hk and then you realize that if a is just a point as scalars then that's exactly the line spanned by the kernel function at that point so this kind of extended them what a kernel might be in this matrix node problem and actually when d is equal to one uh This has like some more, I can say that more concrete constructions. Because, well, if HK is the hardest phase, then one can take the characteristic polynomial of A, take its quotient between PA and its sort of Schwarz reflection, and this will always give you an inner function. And in particular, it will give you a Blaschke factor at the eigenvalues of the matrix A according to the right multiplicity. A according to the right multiplicity. And in particular, since any functions that vanish at A will be a multiple of this Blaschke factor, then HA, so this subspace that I defined before for the one variable case, yeah, is nothing but a model space. And since it is a model space, it's easier, for example, to see when those model spaces are separated because there is a Model spaces are separated because there's a lot lots of literature on those. Oh, and I did say that, but when d is not one, of course, those are not model spaces in general anymore, but they're still a linear span of some kernel functions. It's the joint spectrum of the d-tuple and up to some certain number of partial derivatives. So, and kernels that represent some partial derivative at those points. Derivative at those points. So there is still some concrete way kind of to see those model spaces, those subspaces HKK. And since in the one variable case, you can see those as model spaces, one can prove more things. So it's easier to extend Carlotta's own interpolation theorem. So it was possible to extend Carloson interpolation theorem to those sequences of matrices. So if you have a sequence of square matrices, so not detoups. Matrices, so not detox anymore, with spectra in the unit disk, then that would be interpolating for each infinity if and only if it is uniformly separated, and if and only if it is just separated, and it is the sequence H of those subspaces is a vessel system. Here, of course, if you have a sequence, now those H n's are not vectors anymore, are subspaces. So, what I mean by saying that they're a vessel system, or in this case, a risk system. System, or in this case, a risk system, it's just that if you have a sequence in H2 in which each point belongs to one of different subspaces, then yeah, then extend the those normalized vectors are a resistant with the same risk bound. So the risk bound. So the wrist bound might not have to depend on the sequence that I choose. And same for a vessel system, of course. And so yes, that's what happened in the one variable case. But then the question is, when d is bigger than or equal to two, can one extend also the first implication of Burston Chang and Ling theorem? So can one prove that if a sequence of d tuple submatrices is uniformly separated, then Then is interpolating for each of TB. And I do not know the answer of this question, but I was able to prove some kind of partial result in which instead of taking the infimum of the product, I just take the double product of those instances. And if this is uniformly bounded below, sorry, just bounded below, then this is true. This is true. And this is, of course, a very, very strong condition. So I was looking. Oh, and I was able to do that only when d was equal to 2 for the reason that we're going to see later. And but in general, this question has still no answer. But in particular, the thing that I wanted to focus on is that the argument of Agler and the Carthage. argument of Agler and the Carthy for d equal to 2. So extend very nicely. I mean, this one has to change those kernels with those like kind of more general kind of sort of multi-dimensional kernels that I was defining before. But the proof just go through. And then one can prove that a sequence of pairs of commuting matrices of with spectra in the bi-disc will be interpolating for each infinity if and only if each if and only if each uh in for each um admissible kernel those subspaces h k of a n form a resist and with a response that is independent on the kernel of course that you choose and this third condition i found it um fairly interesting because he says that basically um in the definition of interpolating sequences of matrices one can all can also just choose Can also just choose diagonal targets, or if you want constant function Pn, constantly equal to Wn, because it says that for any bounded target in C, you can find one function in each infinity of the validist that sends each a n to W n times that entity. And of course, if you take En in the original definition of interpolating sequence of matrices to be constant function, that's exactly what's happening. That's exactly what's happening here. So it's not trivial that three implies one. It's trivial that one implies three, of course, but not the three implies one. And it's actually a consequence of the fact that one and two are equivalent. And that's why I included those three conditions in the same theorem. Okay, so that's kind of the first part of my talk. Now, as I was mentioning before, does decouple... Or does decimal matrices might commute or not? And so, depending on that, one can apply a certain kind of functions or a different kind of function. And my plan for the second part of the talk is to briefly talk about what's an NC function is, an NC domain is, and then go over some results, mostly by Shalit, Shamovich and Shamovich and in which they prove that basically there is a non-cummutative version of the hardy space that has some non-commutative version of the peak property. And that will kind of together with the observation that we've made before for commutative for the commutative setting will give characterization of NC interpolation. Of NC hinterpoly sequences after we'll define that. Anyway, so again, this is going to be an introductory, a little introduction to NC functions. So it's going to be probably very boring for all of you. So I'll try to go over them fairly quick. So when you consider NC function, you first have to define what's the right domain for an NC function. And in this case, we consider like matrices. So for any n, one can consider this mn. One can consider this M and D to be the set of all d tuples of m by n matrices and can take the norm and can put on this set the row norm is essential. And this will give it a topology. Now, if you take the union of all those levels when n goes from one to infinity, then you will get the set of all possible details of matrices. D-tuples of matrices of any size, and there are many topologies that one can put on this set. Here we're going to consider the disjoint unit topology, namely a set in MD is open if and only if it is open at each level with respect to the norm given by this, sorry, the topology given by this norm over here. And okay, as set of A subset of omega. So that's this M D is kind of our handed space. A subset omega is an NC domain if it's close under direct sums. Here, direct sums between Z and B is nothing but this. You probably all know that, but just to be clear. And the example that we're going to consider here, one of the most like study examples of such an of the most like study example of such an NC domain is the non-community version of the unipole. So all the d2 poles in MD that have row norm less than one. Okay, now which kind of function one can define on this on these sets? Well, the variable might not commute. So if you want to even define a polynomial, you want to allow the variables of the polynomials to not commute either. And in particular, such polynomials have In particular, such polynomials have three very nice properties, which are those three derived here. So they're graded, namely, if you apply an NC polynomial to a d-tuple of size n, then you will get something of size n as well. It respects direct sums and it respects similarities as well. And those are kind of three properties that are so nice that one wants to have it. One wants to have it for all the class of functions that we want to consider. And so we call a function f, which might not be a polynomial, of course, to be an NC function on omega if we respect these three same properties. One of our domains, one function, and then one thing talk about analytics. Now, if your domain is Is uh nice enough. One can consider also a power series in particular. One can do it in the unipole, in the no community unipole, and being equal to its NC powers to an NC power series. So being in a certain sense analytic is in this non-commutative case equivalent to just asking that the function is holomorphic, locally bounded, which is a very like, which tells. Which is a very like, which tells you that, you know, those NC functions are very rigid in a certain sense. Those three conditions are very demanding. And okay. So as I was saying before, one can study, define an analogous of the horror space of the Druid Harveston space for in this non-community setting, which is just the NC Drew Harveston space, which is a space of. Harrison space, which is a space of holomorphic function, NC holomorphic functions on the NC unit bold, whose coefficients are R square symbol. Here, I didn't say that, but this W V is the set of words with the generators, of course. And so, whenever we apply Z to the L you need to multiply each component to the right exponent. Okay, so. Okay, so it turns out that this NC Druhi-Harbertson space is a sort of reproduced synchronous Hilbert space because it's the closure of the span of those NC functions, Kv of UV, where sorry, Kw of UV, where W is just any matrix in the non-community unit pole. And U of V are vectors in C N with N as the right. C n with n has the right is equal to the size of w essentially. And well, those kw of uv has this very nice reproducing property over here, which kind of oh, that was too much. Sorry about that. Oh, well, yes, it's this nice reproducing property over here. So the value. The value of a function of an NC function and a matrix value is represented by its inner product in H2D with this kernel function. And those two parameters are just there because we're evaluating a function and a matrix. And in the case of this kernel, in this non-cumulative zigzag kernel, one could give an explicit formula to that, which is this one over here, which of course is This one right here, which of course is d is equal to the and n is equal to one, you will recover the uh the ziggle current. This is white. Okay, that was not supposed to be there. Okay, never mind. Okay, so once you have a reproducing kernel Hilbert space, you might want to define also what a non-commutative version of a multiplier algebra is. Of a multiplier algebra is. And of course, one can do it by saying, okay, a multiplier algebra is just a set of NC holomorphic functions on the unit ball that sends the multiplies from the left, because now you have to be careful. The Drury Harvester space to itself. And Salomon, Shalit, and Shamovich proved that this actually in extending what's true for one. True for the one variable case for the commutative case. What is true for the unit disk, for example, namely that this multiply algebra is nothing but the set of NC uniformly bounded holomorphic functions on the unit pole. Whereby bounded, I mean that they're bounded with respect the operator norm, of course. And then once you add your multipliers, you can separate two points in an NC union ball via those multipliers in the very Via those multipliers in the very exact way that we did before. So, again, you take functions in the multipliers that vanish at one point W and they are the identity at the other one. And then you take the one that has the least H infd norm. And you take one over this number, that will be your non-computative version of this lattice of distance. And then you can defy separation and uniform separation as per usual. Okay. Okay. And also you can define what an interpolating sequence is because the definition that I was giving before for detriment of cognitive matrix has the advantage that it's pretty versatile because you can extend it to any kind of algebra here. So a sequence Zn in the uni in and in the NC unipole, and of course any of those Zn might have different size, but they're all d tuples. Tuples in the NC in the pool. It's interpolating if for any target, and a target here has a bounded sequence in the NC H infinity, then there is one function in this H infinity that does interpolation job, basically. So that agrees with Pn and each Zn. And well, here we have kernels already, the kernels KZN, UV. Well, that Well, that thanks to the reproducing properties corresponds to the analogous of what we were studying before. So, corresponds to the orthogonal complement of all the functions in H2 that vanish at that point. And also, one can find, let me see if I can highlight things properly this time. Because if I like everything, it's like not highlighting anything. No, I just can't. Whatever. Okay, so. Whatever. Okay, so sorry about that. So this property here works as the kind of fundamental property for reproducing kernel Hilbert space in this non-commutative setting. So it says that if you take the adjoint of multiplication by phi or any phi in HNE to one kernel function, now a kernel function is parameterized by Zn, a vector u and a vector v. Then you get back one of those kernel functions at zn but you just At Zn, but you just change the second parameter, and that's going to be of Zn star times V instead of V. That in particular, it's not very clear from here, but it's a linear transformation because those kernel functions here, I didn't say that before, they are linear in V and they're conjugately linear in U. And this, again, thanks to some general General characterization of risk systems in Huber spaces gives you that if you have an interpolating sequence for the multiply, the antsy multiplier algebra of the Harvardson space, then the sequence of those kernel functions are resistant in H2D. And the opposite is true because now you don't even have to consider lots of kernels at the same time. consider lots of kernels at the same time as was for the positive for the poly disk because i didn't say that at the beginning but the fact that h2 has the so there is a very nice and elegant way to prove that the hard space on the unit disk has the peak properties which is by using the communalist in theorem now popescu in 92 i think proved a non-commutative version of such result and then and also uh as uh salmon shalit showic As Solomon Shaliti Chonovich pointed out, this can be used to prove a non-commutative version of the peak property for the Drury Harborism, for the NC Drury Harborism space, which they don't state like this. But this is a way to rephrase it in a way that kind of fits with the definition of interpolating sequences, NC interpolating sequences that I was talking about before. And so, and I'll just And so, and also it looks really much like the one that I was giving for the commuted case. So, it just tells that if you have n targets, sorry, n points in the non-commuted unit both z1 through zn and n targets, your targets again are n functions in each infinity, such that this map t that sends each kernel at each point zn with parameter uv to kzn. u v to k z n u v star n z v which let me recall to this is exactly and v star like k z n u v so if you have if you take uh each phi n you see what it what m phn sorry m phi n star does on each one of those hi and it does this now you take the linear Now you take the linear span of all those HI, and this will define a linear operator between this pan to itself. If this is a contraction, then those maps here actually come from a one function, m phi star. And so, in particular, so this is again an extension problem. Again, an extension property, and in particular, there exists an NC function phi that agrees with each EI to each CI. This is, of course, non-trivial. They show it in their paper, and then they also said that one can get it from this community pink theorem. But the idea is the same as for the pink property, essentially. And that's what's called like this. That's what it's called like this. Okay, and so if you rephrase the NC peak property in this way, it's just easy coloring to show that to characterize NC interpolating sequences with respect to a resistant condition on the NC Tree harvesting space. And again, these equivalence here, as for the commuting case, in the very same way implies that one, this condition one and three. That one, this condition one and three are equivalent. Well, it's obvious that one implies three, namely, that for any bounded sequence in C, there exists a function phi that sends each Zn to Wn times identity. So again, here I'm just asking, I'm just asking that the definition of interpolating NC matrices, NC interpolating matrices is true only in the case of Only in the case of being constantly equal to some function w to a constant, essentially. And so one implies three trivially, but the fact that three implies one is not trivial in principle, but it's just a consequence of the fact that one and two are and some characterization of these systems. Okay, so this, of course, that's not all this story. There's still a lot of questions that one might want to. There's still a lot of questions that one might want to answer. In particular, one can ask again as for the Burston theorem: is any uniformly separated sequence in the non-cumminative unit ball interpolating in this sense? And again, here, I could have just like a partial result in which, again, it would in the very same way, you just consider the double product of all those distances. And if this is instances and if this is uh dn p j if this is bounded below then this is true but there is still a gap between this condition that i just wrote and the uniform separation condition of course and one can even ask is some sort of like carbon theorem true like is it true that if z is just separated and the sequence of those kernels is a Bessel system then the sequence is interpolating that idea That I don't know the answer. I suspect that that's not true, but that's but I wouldn't know how to prove it. Okay, let me conclude with an example because, as I was saying before, the condition, the characterization for matrices in the even for sequences of points in the bi-disk is a nice characterization, but it's very hard to check because you have to check at a restart condition on lots of kernels. On lots of kernels. Here, instead, you have one kernel that you have to consider, one anti-kernel that you have to consider. In a text and resistance, then the sequence will be interpolated. And so I tried to do this like kind of toy example. So the easiest kind of D2 poles of non-committed matrices that I can think of was this one. So d equal to two, first of all, and Z1N and Z2N equal to those lower and upper triangular matrices. Uh, lower and upper triangular matrices. If you compute the row norm, so if you want them to be in the unit in the NC unit ball, you will get this direct matrix. And so you will have that that belongs. So this sequence is a sequence in the NC unit ball if and only if both. Basically, AMBN belongs to the, alpha and beta n belongs to the biodisk. In particular, their product belongs to the unit disk. Unit disk. And one curious fact is that, and I think that I don't have time to try to explain why that's the case. The sequence is interpolating according to this definition that I was mentioning before, if and only if the sequence of the product of alpha and Bn, which is indeed in the unit disk, is an interpolating sequence in the super classic sense, so in the sense of H infinity, I should say, for H infinity. I should say for HV. And so I think that I'm out of time. And so that was the last thing that I want to say. And I thank you for your attention. Well, thank you very much, Alberto. Any questions for our speaker? Yes, hello. May I ask a question? Yeah, of course. Yeah, so I didn't, did you think about operators also interpolating? So you're interpolating matrices and that's You're interpolating matrices, and that's going up from points. But did you consider operators? Yeah, like one can consider. Yeah, I thought about it. I got this same question about your time. If you want to consider, for example, a sequence on Tn of operators instead of An being matrices, and let's say with spectra in the unit disk. Okay. Okay. So if this in this is your unit disk, which is the color. Well, you can do basically the same thing. So basically what you do with matrices is that if you have separated spectra, then you have interpolating matrices. That's the idea. Here, an operator that's not a matrix will have a spectrum as well. So you can ask if that's the case. But I think that, for example, if let's say this is Let's say this is the spectrum of T1. So there are some things that you have to be careful with for the problem to be like interesting, I think. Because for example, if suppose that the spectrum of T2 is a sequence in the unit disk. In the unit disk. Okay, so if, for example, if this is not a Blaschke sequence, then any function that vanishes on T2 will vanish on the world unit disk. And so, really, you will probably want to. So, if those spectra, so if so, one kind of requirement that you will have to have for the kind of requirement that you will have to have for the problem to be interested at least is that each one of those things separately forms a Blaschet sequence and then they will have to be separated as a wall and then you will have probably the same exact result does it make sense or should I go more in detail yeah it it it I see it's a little more complicated than maybe I guess but but you're saying you you can make the assumptions to make the the same thing work The same thing works, yeah. For luck, like the last case, okay, thank you, yeah. Like, the matrix case might be even more interesting than the general operator case. Like, an operator might even have a continuous spectrum, and that's you know, it's a function. If you specify the value of a function on here, then you know all the functions in a certain sense. So, yeah, okay, okay, thanks. Any other questions for Alberto?