Okay, thank you so much. Thanks a lot. Thanks a lot, Noemi. Thanks so much to all the organizers for organizing this workshop, which wasn't easy in these very strange and challenging times. And although it's online, it feels so good to see so many familiar faces. It's really, really nice. It's great. Thank you so much for doing this. Doing this. Yeah, so maybe before I start, please, you know, I think what is very nice about this workshop as well is that it's an opportunity for us to exchange our research, but also exchange research ideas. And also, what I'm really looking forward to is feedback also from you and any ideas, any, you know, any also any criticism that you have. Also, any criticism that you have about the things that we have been doing, especially at this interface between inverse problems and machine learning, deep learning, there are so many unknowns and so many open questions and so many things that we need to be aware of that it would be really good to know what your thoughts are. This is a big open field that requires more mathematics. So, and yeah, I would be interested to know what your thoughts are. So, yeah, so please interrupt. Are so, yeah. So, please interrupt me anytime. I rather have this as an interactive thing than me just speaking all the time. Um, and then I will set the timer just because I'm really bad. If you don't interrupt me, I'm just I'm just losing track of time. So I'm going to set the timer for like 35 minutes. So I know then I have like five more minutes to leave. Yeah, okay, to leave time for discussion. So I'm doing this now. If it rings, it's because 35 minutes have passed. Because 35 minutes have passed. Okay, so let me start. I always start with thanking my research group in Cambridge, the Cambridge Image Analysis Group, for all the wonderful work that they're doing, which is really an inspiration and which actually, you know, is the content also is making up the content of this presentation. So thanks so much to all of them and also to all the funders who are funding our research from government. Funders who are funding our research from government and private trusts and also industry. Okay, so what is the background for this presentation, let's say? So I've been working on this in this framework of inverse problems interfacing machine learning now for I think. For I think like eight years or something like this already. Starting with very kind of low-level machine learning approaches like bi-level optimization was the first encounter for me that where I looked at optimizing parameters in variational models with training data. And then with this advancement of deep neural networks, I got more and more interested in deep neural networks. More interested in these very high-level parametrizations. And so, this is more or less the background. And I'm still fascinated by it, but I also acknowledge that there are lots of things that still need to happen. So, okay, bullet points. First one, really what I think from this work that we have been doing is that machine learning and, in particular, deep learning offers very interesting opportunities for inverse problems. In particular, I would say through the ability of I would say through the ability of neural networks to capture information and data to a very high accuracy. And that will appear in my talk in particular, where I exploit this to learn priors, image priors, regularizers through training data. The thing is, they only work in practice. Work in practice for inverse problems when combined with mathematical and statistical modeling. And this, you know, I'm preaching to people who will probably immediately agree because these, you know, the inverse problems that we are working on, the main challenge is that they're ill-posed and so we need appropriate mathematical treatment. In contradiction, or you know, in contrast to this, actually, because Because it's to a certain extent, you know, quite tempting to just throw the deep learning machinery onto everything. There have been a lot of works which came out in the last couple of years that lack, unfortunately, this mathematical scrutiny. And yeah, this definitely something we need to change. So the main thing. So, the main thing, the main framework that lives at this interface between deep learning and inverse problems, and in particular, you know, more knowledge-trim approaches to inverse problems that I've been focusing on in the last years was to couple deep learning with variational regularization models. And this is what I'm going to talk about today. Okay, and maybe I should say that two years ago, together with also people that probably many of you know, so with That probably many of you know. So, with Simon Arich, Peter Maas, and Ozan Akten, we have been writing a kind of review paper on what exists in terms of data-driven approaches to inverse problems, which is now already outdated, but which might give some ideas. So, the prototypical inverse problem that I'm targeting is computer tomography. Computer tomography. So, this is a prototypical problem for this class of linear inverse problems that I've been focusing on mainly. Where, just to fix notation, where my measurements are called y, they live in some function space, usually Banoff space, and they're linked to the unknown, which in my case. Which, in my case, is an image. Often the examples that you show are all CT, but of course, it's a more generic idea. But in the examples, you see lots of medical images. So where U is a medical image, and then the measurements and the medical image are connected to each other via a forward operator that is in the case of computer tomography, a sub-sample drive. A sub-sampled Radon transform. And we will focus in particular in the numerical examples that I show you on sparse angle tomography, where we are under-sampling in the angular direction. So in the angular dimension, sorry. Okay, so this is an ill-posed problem. It's underdetermined. We have noise in the measurements. Yeah, I should also say this. So we have additive noise here. Noise here. And so, what we need to do usually to still be able to solve this in a robust fashion is that we regularize the problem. And a very well established approach for regularizing an inverse problem is through variational regularization. And the idea here is, and I'll keep this brief because, again, I assume many of you know this. Many of you know this, is that you compute an approximate solution to the inverse problem you set out with by you compute an approximate solution as a minimizer of an energy functional that usually consists of two terms. One captures the discrepancy of your measurements to the forward. Our measurements to the forward model. For instance, this. Do you see by the way my hand or this cursor? Okay, good, great. Thank you. So where this discrepancy in a lot of cases is a least squares fit between the measurements and the forward model. And sorry if you hear this now, but the blinds in my office are going down automatically. So yeah. And then this is complemented and this is. And then this is complemented, and this is where the regularization comes in. This least squares fit, let's say, is complemented through a regularization term that, in addition, introduces this prior information about the type of solutions we want to compute of the inverse problem, that they have a certain regularity smoothness property. And a typical regularization that people in inverse problems in imaging and image pros. Inverse problems and imaging and image processing and so on have been working with a lot are these spicity promoting regularizers, such as, for instance, the total variation. So the, you know, if you think in finite dimensions, would be the L1 norm of the gradient of U. So it would be promoting solutions that have a sparse gradient, meaning that you are promoting intensity functions. Intensity functions which are piecewise constant. So images which are piecewise constant. And then these two terms are balanced against each other with a regularization parameter alpha that basically is determined by how ill-posed your problem is and how much noise you also have in these measurements. So the more noise you have in the measurements, the So, the more noise you have in the measurements, the more you will need to regularize and the more you will need to smooth in order to get a good reconstruction. Okay. Okay, so this is basically the background. I think I haven't forgotten anything. Maybe to say, you know, before I tell you that we need learning to improve things, maybe to say that actually. That actually, these types of what I will later, you know, also call what I also call knowledge-driven regularization approaches, like total variation regularization, for instance. They actually, you can do a lot with them and they are very powerful. And, you know, all of these applications that I'm showing you here, they have at some point in their approach, variational regularization is an integral. Regularization is an integral part for actually solving these types of problems. Yeah, and here I'm just showing you a few application projects that we are working on in Cambridge in the Cambridge Image Analysis Group, from object tracking, object detection and object tracking, from videos to digital art conservation, which is an impainting. Art conservation, which is an impainting, so an image restoration example. Where in this case, I'm not telling you a lot about these applications because I could give a talk about each of these separately. But here, this is about removing overpaintings in illuminated manuscripts. And then, of course, computer tomography, as we have seen before, is a prototypical example of what. Prototypical example of what happens a lot in biomedical imaging, where tomography appears in many different places. Here, this is positron emission tomography, for instance. So making basically, you know, getting the most out of the tomographic imaging data with variational regularization approaches. And then even in contexts like where, you know, the solution of your problem is not an image itself, but is, for instance, a classification of a data set. A classification of a data set of images. Again, variational approaches appear, where in this case, for instance, they come in terms of the Graph Laplacian. So they really work. I mean, they work. So they are actually really quite diversely applicable. Because, I mean, interestingly enough, this idea of putting Of putting most of the emphasis on edges in an image, like what we do with the total variation regularization. So, saying that, you know, everywhere else the image can be nicely approximated by a constant and what I really want to preserve are the edges is something that is common to many, many problems and is something we can exploit for solving all of these different problems here, even in the Problems here, even in this graph classification approach. Still, I mean, there is a point to why we have been looking at learning. These type of handcrafted or, you know, knowledge-driven handcrafted type of regularization approaches are limited to a certain extent. They are limited. I mean, because they are also so generalizable to all these different settings, this is. These different settings, this is on the one hand their strength, but this is on the other hand also their weakness because they can't be as powerful as a regularization that is really customized and really bespoke to a particular problem. This is one thing. And then if you think about the variety of structures that appear in images, even in Even in simpler images like these computer tomography images of the chest, even then you appreciate that having a piecewise constant approximation like we have here with the total variation regularized reconstruction is sometimes too crude. And we might be able to do better if we would be able to model this. Model these structures and images in a better way. So, I'm just making here this point by showing you from sparse angle CT. I told you this will be with us for this presentation now. So, sparse angle CT. I show you now the pseudo-inverse reconstruction, so the filter back projection reconstruction. Then, I show you a variation regularization reconstruction with total variation regularization. And the last thing. Regularization and the last thing, and this is you know already a teaser for the type of thing I want to talk to you about. This is now a variational reconstruction with a learned regularizer. This is called AR because we call this episodial regularizer and we'll talk about it in a moment. So, even though it's clear that we can never be perfect because it's an ill-posed problem, we don't have enough measurements, we have noise and so on. Enough measurements, we have noise and so on. In a way, even if you look at the filtered back projection image, you might be able to do something better than total variation regularization yourself, even in your mind. Okay, so this is somehow a little bit conceptually the idea. And so how can we do this? Okay, now I have these individual images. Maybe I just quickly go over them. But here you just see this is TV, just zooming into this a little bit, the TV reconstruction. And below. TV reconstruction. And below here, what I'm showing you is because these are simulated examples. So we took a high-dose CT reconstruction and just simulated the sparse, so a full angle, high-dose CT reconstruction. We just simulated these sparse angle, the sparse angle data. What you see here are the quality measures. So the difference basically. The difference basically to the ground truth, which is this high dosity. This is the PSNR for this TV reconstruction, and this is the so-called structural similarity index measure. The higher those are, the better. Okay, so okay, so this is CT. This is now the learned reconstruction. So you can see if I go back and forth a little bit, I haven't yet shown you what the high-dose CT image is, right? But I'll show you in a moment. But you might appreciate that you can see. You might appreciate that you can see much more detail, although it's the same data, right? It's really the same data. And if you compare this now with the ground truth, which is this high dosity image, then it is somehow, you know, it is actually quite impressive in a way. Okay, so somehow this is the motivation. And so, this is also the motivation what lots of people picked up when they were thinking about how. They were thinking about how to solve inverse problems with ideas from learning, learning from data sets. And here I'm in particular focusing on deep learning for solving inverse problems. And I really want to talk about the last bullet point here. But I very, very briefly on one slide, I just give you a rundown of what these are the different types of approaches are. Or in particular, I focus here on the two middle. I focus here on the two middle bullet points because the first bullet point I can briefly tell you is something that is this crude thing where people just use deep learning to solve the inverse problem from measurements to image without any physics-based modeling apart from putting that into the architecture, for instance. So there is no, for instance, radon transform or something like this wouldn't appear in these fully learned models. In these fully learned models, the solution to the inverse problem is fully data-driven. Okay, the only place where, in fully learned models, information about the forward operator appears is in how they set up the neural network architecture. Like, for instance, if you have a Radon transform or a Fourier transform, they appreciate that this is a global operator and hence they can't just capture this by a convolutional neural network, but they would need to add a couple of full. Need to add a couple of full matrix vector multiplications, so what is also called dense layers in order to capture this and makes it actually computationally almost unfeasible to train this for realistic sizes of data. Okay, but very briefly, what are the other two routes that are really the most popular, I would say, in how people use deep learning to solve inverse problems? So, one is Is what some people also call learned post-processing, where the idea is that you reconstruct with a baseline reconstruction approach, which if you have an inverse problem with the pseudo-inverse could be the pseudo-inverse reconstruction. So you have your data, you apply the pseudo-inverse, for instance, but it could also be exchanged with a kind of baseline reconstruction. And then you use a neural network to basically correct all the things that you introduced in terms of artifacts and so on afterwards. So you do a regularization afterwards. And this is here just one example where people have done this for MRI. I hope I have. For MRI, I hope I have actually, I don't have the references are on the previous page. Sorry, where people have done this for MRI. This is actually Jinky on Seoul's group, where they did just an inverse Fourier transform on the zero-filled Fourier measurements and then used the U-Net to regularize. Yeah, it's typical kind of thing. The other route that has also gained a lot of attention is to build a neural network architecture for image reconstruction, for instance, modeling the neural networks after how iterations would look like if you would solve If you would solve a variational problem iteratively, so let's say you have a variational regularization problem with your regularizer and your data discrepancy term, and then you minimize it by a gradient descent, for instance. Then instead of, you know, in your gradient descent, having a fixed regularizer and a fixed data discrepancy term, you parametrize each of the iterations of your gradient descent with With you over-parametrize each of these iterations, and you can think of this basically as layers in a neural network, and then you just have a fixed number of those. Or the other thing would be to actually parametrize each of these iterations with a neural network that has kind of two channels, one for the image. One for the image. So, one would be regularizing the image with convolutional operations, and the other one would capture kind of the gradient of the discrepancy term with the actual forward operator being part of the neural network parametrization. So, that is called learned iterative reconstruction, or also learned unrolling, which has, I would say. Which I would say actually produces nowadays almost this kind of state of the art in deep learning for inverse problems approaches. Now, the advantage of these type of methods for deep learning inverse problems are that they are really very powerful. So, I mean, it's really super impressive for people who have been fighting with inverse problems and getting good solutions out of them for many years. Solutions out of them for many years. It's super impressive what a little bit of data-driven components can actually do. And of course, another advantage of these type of approaches is not just great performance in terms of qualitatively really good solutions, but also computationally. Because once these approaches are trained, it's just an explicit application of linear and non-linear operations, which could still be computationally costly. Could still be computationally costly, but not like the type of problems that we solve where we need to solve non-linear optimization problems, non-linear PDEs, and so on and so forth. The disadvantage, though, is that these type of approaches are mostly a black box, apart from a few actually theoretical results about those that have appeared now in the last year or so. But so, what do I mean by that? So, what do I mean by that? So, there are almost no theoretical underpinnings like the ones we wish to have for our inverse problems type of solutions. For instance, it's not clear what type of regularization they introduce. Is it really a well-posed regularization? Is it a well-posed problem? Is it going to give you a stable solution? Interpretation is, of course, also a big issue. For instance, what we know for some of these approaches, like for instance, learned iterative schemes, is only an interpretation in the limit, for instance, that they converge to a conditional mean in the infinite data limit, which is, you know, I don't know how useful this is somehow. Data consistencies in general are not guaranteed, which is a little bit so. A little bit. So you don't know whether your reconstruction is actually in the range of your forward operator. And again, there are exceptions. So there are, for instance, these null space neural networks that Markus Heidmeier has introduced a couple of years ago. But generally, it's not the case. And also, usually, these type of approaches require a lot of supervision. Require a lot of supervision, meaning a lot of ground truth data that, you know, in many problems that we are looking at, we don't have. Okay, and so this was the motivation why we wanted to look into these learned regularizers. Okay, so let me now go into those. And so this framework of learning a regularizer with deep neural networks is something that we've Is something that we've started about working on about three years ago with my then PhD student, Sebastian Lund. So he was really pioneering this within my group. And then along the way, you know, a couple of people joined. So Marcello Carioni is a postdoc with me in Cambridge. Suarend Dittmer is a kind of joint postdoc with Peter. Of joint postdoc with Peter Maas in Bremen and me in Cambridge, which is possible nowadays because we work remotely anyway. Shubol Mukherjee is a postdoc with me in Cambridge, and I would say he's at the moment really one of the main, main drivers for these learned regularizers in the group. Ozan Agden, many of you know, is from KTH in Stockholm. And Sak Shumialov has been quite an amazing undergraduate student who has worked with us for two summers and has actually done a lot in this. And so the framework of learned regularization that we have been working on, we called learned adversarial regularizers. And I'll explain now what this is. Okay, so when you think about somehow how you would train. How you would train a regularizer. Then, yeah, so let's maybe think about how would we train a regularizer. And so let's look back at this optimization problem here. So, one route that we didn't take for this adversarial regularizer, which I mentioned at the very beginning, was bi-level optimization. So, this was this idea to Is this idea to basically find optimal, you know, parametrize your regularizer in some way, let's say through a deep neural network, and then optimize the parameters of the regularizer by minimizing a loss function, an objective function that evaluates solutions to your minimization problem. Your minimization problem dependent on the three parameters in the neural network. So, this is a constrained optimization problem where you have basically, as you're constrained, another optimization problem. And so many people in kind of PDE constrained optimization and optimal control are very much used to this kind of setting. So, we, for the deeply learned regularizers, we didn't want to do this because this is, you know, it's also there is a certain amount of Amount of computational toughness, let's say, when you solve these bi-level optimization problems, because usually you need to solve the lower-level problem to a very high accuracy. And Noemi is nodding because I know that she has been working on these type of problems also a lot. So yeah. So we wanted to decouple this somehow. But then how do you do it? And so when you think about it, and maybe, you know, as the more I talk. The more I talk about it, and I have this slide up, you figured it out now already somehow what the concept will be. But the idea is that we took, the route that we took is that we wanted to train the regularizer almost like a classifier. So a penalty term that would differentiate between what I call here the good guys and the bad guys, that would penalize, so be very large for the bad guys for reconstructions that I want to. For reconstructions that I want to attenuate and discourage, and that should be small, so should encourage reconstructions that I call here the good guys. And this is basically my training set that I set up. Okay, so in the CT case that I said we will stick to today, we took, for instance, as the good guys. For instance, as the good guys, full angle high-dose CT reconstructions. And we took as the bad guys reconstructions from low dose, sparse angle simulated data that I've reconstructed again with a baseline reconstruction, which in the CT case we took as the filtered back projection. Okay, so. Back projection, okay. So, a pseudo-inverse reconstruction. So, this depends on the problem you want to solve, how you do this. But this is how we did it for CT. And then, now I need to put into a loss function, what I said in words, right? We actually used the one or an approximation, something, a loss function that, let's say, like this, a loss function that is motivated by the one waste. That is motivated by the one Wasserstein distance. Yeah, and so if you maybe before thinking about the Wasserstein distance here, how this works is the following. Maybe I should say I call the distribution of the good guys P U and the distribution of the bad guys PN. Okay, like N for noise and U, like the use that I want to reconstruct. Okay. And so what I do now is I want And so what I do now is I want this is already the continuum formulation of the loss function that I want to use in the end. So I'm looking for regularizers R that if I feed the regularizer with samples from the bad guys, I want the regularizer to be large, right? I'm maximizing here. And if I feed the regularizer And if I feed the regularizer with samples from the good guys, I want the regularizer to be small. And then I have an additional one Lipschitz constraint here that comes in handy also later for properties of my variational problem that I get, but also that links that to the von Wasserstein. It comes really from the von Wasserstein distance. So the nice thing about So, the nice thing about this is that why do I say here this is an unsupervised loss function? Well, because we are never actually, when we evaluate this loss function, we are never comparing the good guys and the bad guys in a paired way, right? We take expectations before. So, we are only looking at this in a distributional way. So, the good and the bad guys do not need to be paired, meaning that. Be paired, meaning that you could have, you know, high, for instance, high-dose CT reconstructions from some application where you can do this, maybe, or even phantoms or something like this. And then for the real data, you don't need the ground truth. So this is actually quite nice in a way. Okay, and then neural networks come in in how we parametrize. How we parametrize these Rs, right? So we are not optimizing all one Lipschitz functions, but what we do is, and this is where we now deviate from the one-basserstein systems, what we do is that we parametrize the R's through a neural network. Okay. And so for this, we had to have two scenarios, actually. So we have one scenario where we say the regularizer that I then later put into my variational regularization approach is the sum of a neural network. approach is the sum of a neural network regularizer plus a little else a little l2 squared penalty which gives me um you know enough coercivity such that i have um you know an actual um uh that i can prove existence later but in that case uh the neural network um is just a convolutional neural network which could uh which actually is uh non-convex okay so in this case uh science So, in this case, psi theta would be non-convex. But what we also did with Shubo, in particular, is that we looked at parametrizing the neural network as a convex function through what these which references in this one, right? Which these people called input convex neural networks, ICNNs. So, this is this is, yeah. So, this is this is yeah, this was quite nice because convexity is nice when you later solve the variational problem. So, there are these two settings, and in the one setting we call the regularizer AR, which is the free non-convex setting, and in the other setting we call it ACR, adversarial convex regularizer when we have this convexity parametrization. Okay, and then once I have parametrized the R in that fashion, I then I then, this is actually the loss function that I'm using, okay? Where this is just what we had before, and then here I have turned this one Lipschitz constraint into this penalty term. Okay, now recipe, basically, what's the recipe of now training a regularizer? So, first you need to create the training data. We talked about this. Then, we train the regularizer, so then you need to. Train the regularizer, so we then you need to decide which kind of parameterization for the regularizer you choose. You train the regularizer with this loss function, um, and then once you have trained it, you plug it back into your variational problem and yeah, and you solve it with whatever. I mean, we did gradient descent, but I mean, maybe you can do something better, but uh, yeah, and and then um, the nice thing about this, and this is why. The nice thing about this, and this is why I wanted to stay with the variational problem, is that this is, of course, not amenable to analysis. And actually, on the one hand, embarrassing thing, but on the other hand, reason why we stick to this in the first place was that all the analysis that you see here, the properties, they come from the general theory. So we didn't need to do anything. Okay, so now. didn't need to do anything okay so now this is good we we actually didn't need to do much so i have five more minutes i'll try to watch the space now um so in the non in in the in the so this is the non-convex uh case this is the convex case um in both uh both settings are in uh both settings are in banoff spaces um and in the convex case of course i can show more and in particular what is nice in the convex case what we could do Case, what we could do is that we could actually show that this is a converging regularization. I mean, again, we could show it because it just comes out of the classical theory, right? Okay, any ideas by the way to extend some of this would be very welcome. Maybe I well, I just say one word: one disadvantage of us doing it like this is that we lose. Like this is that we lose when compared to this, for instance, learned iterative reconstructions, is that we lose this computational advantage, right? So we still, in the end, need to solve a variational problem. And one thing that we tried recently is to marry those two approaches, to kind of marry this learned iterative reconstruction with With variational regularization by basically doing the following. So, for the bad guys, so far, so don't maybe look too much at this slide because I don't have time to explain it. But so far, the bad guys in this training approach, they have been fixed, right? I said I have one baseline reconstruction, and then I optimized the regularizer to attenuate these. Attenuate these bad baseline reconstructions versus enhancing the structures that the good guys show. Now, in this approach, we are actually training two networks, one for the regularizer and one, which is now a generator, that updates the bad guys. Okay, so that makes the job. The job for the regularizer harder and harder as we go along. Okay, so here is the same actually as we had before, this second term, only that instead of having here the bad guys, we have images generated by this other network that we also train. Okay, and we also train this by saying this other network should explain the data. you know explain the data um and then um also have also also should be should should should have a small regularizer should should be small in the regularizing term okay but i and the nice thing about this is once you've trained this then you could also decide to use the generator in the end to solve your problem on the one hand so you could use the neural network in the end to solve the problem but another nice thing is But another nice thing is that you actually get much better regularizers because you have challenged the regularizer more. So, you so I'm now showing you mainly what I'm going to show you now examples where we use the variational regularizer in the end. Okay, so this is so now these are the examples. So, here we used the Mayo Clinic Open CT data set from which we simulated sparse angle. Sparse angle tomographic data. Here are the details. And I'm going to show you now the results that we got for this CT data for the CT reconstruction with the following algorithms. Okay, so the one is filter back projection. So this is basically what the bad guys, the baseline bad guys are. Then compared with T. Um, then uh, compared with TB regularization as a kind of prototypical handcrafted regularization approach, then um, the result with the adversarial regularizer, so really the non-convex regularizer, but I'll show you straight away the one with the unrolled updating, okay? So, the one which has been challenged more, okay? Um, and then I also show you the results we get with the adversarial convex regularizer. Regularizer and that I compare with what is called learned primal dual, which is one kind of state-of-the-art approach within this learned iterative reconstruction business. Okay. Okay. So let's go into it. So I jumped. I think, okay. So here on the left, I will show you the ground truth. And on the right, I show you the reconstruction that we got from the sparse angle data. This is the filter-back projection. And again, this is the same as I said. Projection again, this is the same as I said before: PSNR and structural similarity index measure. This is TV. This is what you get with the convex adversarial regularizer. This is what you get with the non-convex adversarial regularizer with the updating. And this is what you get with this learned iterative reconstruction, so the learned prime. Learned iterative reconstructions of the learned primal dual. So, if I jump between those two, I mean, still, you know, quantitatively, this learned primal dual is better. But then there are two things to note. First of all, we have theory. And second, it's unsupervised. Our case is actually unsupervised. It's not, you know, it doesn't need dispared data. Okay. Okay. So a few words about, you know, things that you might do next with these type of things. So first of all, you know, extensions of what I presented. So as I said, Sebastian was the one who really pioneered this in the group and he has written his PhD thesis on that, which contains much more information about adversarial regularizers in particular. In particular, one thing that he made a huge effort on in his last months of his PhD was to do more empirical investigations for generalizability, right? So if you train on one type of image, if you train the regularize on one type of images, how does it generalize on another type of images and so on? And so if you want to know more about this, you can go to his PhD thesis. Thesis. Another thing that we recently did together with Martin Berger is to look at training the adversarial regularizer with the source condition, which I guess this doesn't come as a surprise also from Martin. Then we were looking also to impose more qualitative properties into the regularizer, like for instance, something that is deeply rooted in the image process. The image processing in the image processing literature, in particular, by the people who came from PDEs and went into image processing, like Jean-Michel Morel, for instance, is this idea to impose certain, you know, to think about what regularization procedures should fulfill for images. For instance, certain invariants. Images, for instance, certain invariance properties. Like, for instance, that the regularizer shouldn't do anything different depending on how I rotate the image, for instance. It should always give you the same. And you can build this also into neural network architectures. And this is also called equivariant neural networks. Another thing that is, you know, the moment you train something, it's somehow tempting to think, well, maybe we shouldn't just optimize for the Maybe we shouldn't just optimize for the best-looking somehow image, but we could also optimize for other criteria. For a lot of times, the reconstructions that we get from our inverse problems, we do something else with them afterwards. And why not take this into account when we train a regularizer? For instance, segmentation or something like this. So, what is also called task-adapted reconstruction. Again, this is not a new idea in the inverse problems. New idea in the inverse problems, you know. I mean, you know, people, you know, many, there, you know, these are very, very classical works in tomography where people have also looked at feature reconstruction, for instance, and so on. But the interesting thing now with deep neural networks is that this makes it computationally feasible somehow to do this. Yeah, training regularizer still for very large inverse problems. Very large inverse problems, if you go to 3D or even 4D, it becomes challenging to train the networks. And so, here we need to think about strategies of how to make the training computationally more feasible. And we have looked, for instance, into invertible neural networks for that. And then I skipped the uncertainty quantification because actually we haven't really started on that yet. Yeah. Um yeah, so takeaway messages. I really think that um deep learning offers very interesting opportunities for us to be thinking about in the context of inverse problems. Data-driven regularizers, I mean, I'm clearly biased because I've been working on them, but I like them a lot because they, you know, they still to a certain extent understand and can. Understand and can understand what is going on, but I can also trust in the solutions that I get. And then there are many open questions. One, for instance, is the convex atversal regularizer is great in that, you know, it's convex, but it didn't give super, super great results. And one of the things that Shubo has been looking at in particular is to look at better parametrization of conduct. At better parametrization of convex neural networks, and you can do there a lot. So, this is just a teaser to say, you know, this is now for using or training the regularizer to do image denoising, just as a playground thing, where Shubo used the classical input convex neural network parametrization that I've also used for the CT reconstructions I've shown you before. I've shown you before. And then he used this new parametrization, which you know, you can see without telling you exactly what this parametrization is, but you can see that how you parametrized the network makes actually can make a huge difference. So we are still working on this, but I think I really like the convex setting. So we need to make this work better. And then improposing more qualitative properties would also be nice. Uh, would also be nice. Actually, just this morning, I've been talking to a postdoc I met at a workshop in Leiden a couple of weeks ago who works on multigrid and who had this idea of why don't we somehow use multigrid to differentiate, you know, to train the regularizer on different scales of the image. Of the image differently with different loss functions. I mean, we actually don't know yet how we should do this, but somehow conceptually is kind of the idea. And then another thing that, again, these learned iterative schemes, they really have the advantage of being computationally much more feasible. So it would be great if we could prove more about them. And that's a really big open problem, I think. And so, oh, and this I keep for the panel discussion. And this I keep for the panel discussion, okay? I'm not telling you what this is. And I thank you very much for your attention. Thank you.