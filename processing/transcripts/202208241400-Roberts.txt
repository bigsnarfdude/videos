Yes, thank you to the organizers. I'm really happy to be here and get to participate. So my name is Emily. I'm a new assistant professor at the University of Iowa. I just finished my PhD at Michigan. So this is some work from my dissertation with Jeremy Taylor and Mike Elliott. So, you know, I don't need to go in depth, of course, at this point about surrogate endpoints, but just to sort of set up the notation that I'll be using. To sort of set up the notation that I'll be using throughout the talk. So, surrogate endpoints I'm going to denote as S. Those, of course, are some intermediate outcomes that were related to some true clinical outcome, and I'm going to denote those as T. And, of course, usually S is measured earlier on during the follow-up or is easier to measure or is in some way superior in making the trial more efficient and helping patients overall. The approach that I'll be taking is the causal inference framework with potential outcomes. With potential outcomes. So here I'll be using Z to denote treatment. So the notation I'll be using is S of Z and T of Z to refer to the potential outcomes of the surrogate and the true outcome if the treatment had been assigned to level Z. And we'll be focusing on a binary treatment here. So of course the notation that we've been seeing, T of zero for treatment zero, T of one for treatment one, those would be the true outcomes of interest. Of course here we want to incorporate the surrogate outcome as we've discussed the surrogate Outcome, as we've discussed, the surrogate is measured after the treatment is given, so it's a post-treatment variable. So we need to do something a little bit more complex than simply conditioning on S. That would give us a non-causal estimate. So the strategy that we'll be taking in this work is principal stratification, which we've discussed some, or also known as principal surrogacy in this setting. And the idea is that we'll be able to incorporate the potential outcomes of both T and S under treatment Z. So, of course, the title of my So, of course, the title of my talk is about survival endpoints or time to event endpoints. Building off of our discussion a little bit yesterday, and just to give what I think is some intuition about the principal stratification framework, I'm actually going to start in a more simple setting where we can assume that these are normally distributed endpoints. So broadly, we can consider the joint distribution of four potential outcomes that I've denoted here. As we've talked about, we're only going to observe some set of these based on what treatment is actually given in the clinical trial. Is actually given in the clinical trial. And then basically, what we want to do based on this principal stratification approach is that we're interested in this conditional distribution. Try out the pointer. This conditional distribution. And so based on the theory that's been developed that we've discussed based on causal average sufficiency and necessity, the distribution and the conditional expectation in particular that we're interested in for continuous endpoints I've shown at the bottom of the slide. At the bottom of the slide. So it's basically, you know, the expected treatment effect conditional on some subgroup of patients that are defined by strata defined by the surrogate. So in the setting that kind of is precursor to this work, where the outcomes are continuous, we assume that they follow some joint multivariate normal distribution. This is, of course, a very simplifying assumption, but it's going to be nice and analytically tractable and give us some intuition about. And give us some intuition about these metrics that we're going to be trying to build to validate the surrogate. So, again, here's the conditional expectation. Oops, that was the wrong one. The conditional expectation that we're interested in. And since we're in this nice normal distribution setting, we can actually make this assumption or write this out analytically as being equal to a function in terms of these two key parameters, gamma naught and gamma one. So essentially, what we're trying to do is. What we're trying to do is estimate this conditional distribution or this conditional expectation, and that is equal to estimating these parameters, gamma naught and gamma one. And based on these gamma naught and gamma one estimates, we are going to make some determination about whether or not the surrogate is valid. So in short, before I show the plot that we looked at yesterday, there's two key conditions of a valid surrogate. The first is that our quantity gamma naught is equal to zero, and then the second is that our gamma one is not equal to zero. Is that our gamma one is not equal to zero. So, this is the CEP plot that we talked about yesterday. Again, so on the x-axis is our basically treatment effect on S based on individual level surrogate. Of course, we can't actually observe this treatment effect on the individual level for an individual trial, but that's what's on the x-axis. And then on the y-axis is our conditional expectation that we're interested in. So, I think this is a nice. So, I think this is a nice plot because we can understand our quantities gamma naught and gamma one graphically. Essentially, the condition that gamma naught is equal to zero corresponds to the intercept term. So this condition is basically corresponding to the idea that in the subgroup of patients where we don't have any treatment effect on S, so when X is equal to zero, we'd also want that there'd be no conditional treatment effect on the true outcome. So we'd want the expected value. Want the expected value line to go through the origin. And then our other condition is that gamma one be non-zero. Specifically, we want it to be positive, probably. But so that condition corresponds to the idea that for subgroups of patients where there is a treatment effect on S, there's also a treatment effect on D. So you can kind of understand intuitively why these are good properties of what we would think about a good surrogate. And then, of course, you know, something I won't get into a lot here, but, you know, not only are we going to be concerned. You know, not only are we going to be concerned probably about the point estimate, but we're kind of also going to be concerned about the confidence intervals around these quantities, because these are really the key quantities that we're interested in for determining if the surrogate is valid. So this is a slide that kind of lays out a little bit more mathematically what this joint distribution looks like and what these quantities really are in terms of a statistical model. So again, we're considering the joint distribution of these four potential outcomes. Of these four potential outcomes, assuming that they're multivariate normal, and then we'll assume that there's, of course, some mean and covariant structure. And then the gamma naught and gamma one quantities are functions of these parameters. So since it's multivariate normal, you can derive the conditional expectation and find the form that we're looking in. So of course, the problem that we run into immediately is that while many of the parameters are identified, the means and the variances, for example, four correlations in particular. Four correlations in particular are non-identified. So I've listed those here. I've also shown them in red. You know, the correlations between the two surrogates under opposing treatments, we're never going to be able to identify that from the data. So that's sort of a concern that we're going to run into. So again, this is kind of like the base case of like a nice multivariate normal setting, how you could implement principal stratification method when you can assume some multivariate normal distribution. Assume some multivariate normal distribution. You would try and estimate this joint distribution. Again, a difficult part of this whole process is dealing with non-identified parameters. But by doing the estimation process, you can estimate gamma naught and gamma one and then make some determination based on our conditions that we've laid out whether or not the surrogate is valid. So some of my previous work that I've done in this multivariate normal setting that I won't go in detail, but we can talk about if we want. Detail, but we can talk about if we want is incorporating covariates into this model, specifically to make some conditional independence assumptions. Something I've looked at quite a bit is using different algorithms for actually trying to estimate this model. Something that's frequently done in this type of setting is that since we don't actually observe all of the potential outcomes, a lot of times people will do an imputation strategy. So they'll try and fill in the outcomes that you can actually observe. So, for instance, you could do like a So, for instance, you could do like a Bayesian method with an MCMC. You'd iterate between imputing the missing counterfactuals and trying to estimate the parameters. So, something I explored a lot in the Bayesian setting is different prior distributions on these non-identified correlation parameters and trying to understand if we need to do this imputation strategy or if we can just use the observed data, for example. And then also trying to incorporate longitudinal measures by extending this to a mixed modeling framework. Mixed modeling framework. So that's kind of like the lead up to the time-to-event outcomes: is that I think this is a really nice framework. I will say, obviously, it's very parametric. I haven't done a lot of work with like non-parametrics. I think that'd be a great way to extend this. But, you know, in general, not a lot of trials are actually like S and T are not normally distributed. So the work that I'll be talking about today is focused on when the data are actually time. Focused on when the data are actually time to event, which of course is pretty common in many clinical trials, certainly in oncology, which is an area I've worked in a bit. So the motivating trial for the work I'll be focusing on more today comes from a prostate cancer clinical trial. It's a reasonably sized study, so over 700 men that are being treated for prostate cancer, and they have some recurrence of their PSA rising after initial treatment. And the clinical trial is trying to. And the clinical trial is trying to assess this addition of anti-androgen therapy to some radiation therapy that's given, sort of, as a usual treatment for patients. And so prostate cancer is interesting because people have really tried to have PSA measurements work as a surrogate, and they've sort of failed repeatedly to be a valid surrogate in the validation process that we're all really interested in. So the next, you know, they're trying to understand. So, the next, you know, they're trying to understand what we can use instead. So, interested in some different time to event endpoints, for example. In this work, what I'm going to be focusing on is trying to evaluate whether time to distant metastases or like the spreading of the cancer to other areas, if that could be a valid surrogate for overall survival. And so, I think that this is really an interesting and like useful area. To my understanding, at least when I started this project, is that principal stratification had not. Principal stratification had not barely been fully used to address this type of problem. Certainly, there's like different methods out there that we've been discussing that you could use, but it was sort of a goal of mine in this project was to try and see if we could use principal stratification in particular. I think it's an interesting concept because the data do become quite complex, and that we're dealing with censoring, we're dealing with semi-competing risks. It's not necessarily straightforward, like what our estimand would be. We talked a little bit about how. We talked a little bit about how principal stratification can be used for things like truncation by death. And so, just dealing with survival endpoints when you don't necessarily know what event's going to happen first, if someone will live long enough to actually see the surrogate or not, are all things that we need to try and take into account in a method like this. So, the proposal for this work is to use an illness death model as a natural way to model the data and to try and deal with some of the issues that I just mentioned. Some of the issues that I just mentioned. So, if anyone's not familiar with illness death models, this also might look quite familiar, similar to like prentice criteria if you're not. But essentially, the idea is that we can think of the different events that can happen as different states. And individuals can go through different pathways through the states based on what their health occurrences are, of course. So, here in this illness-death model, it's kind of as simple as you can get, where you have some baseline. Where you have some baseline, and then you have two paths you can take. Either someone will experience the surrogate, which again is distant Mets in this setting, or someone may not experience distant mets and they may die first, unfortunately. Of course, another possibility is that an individual will first experience distant mets and then the terminal event afterwards. So there's different states that individuals can take, and this allows for that possibility. So we don't need to have data on both endpoints for each individual. Both endpoints for each individual. Though, of course, the more data we have about all of these events, we're going to be able to do better with our estimation. So, in between each of these states, I'm going to use these T subscript 1, 2, 1, 3, and 2, 3 to denote the different gap times or the different times between states. And then, of course, each gap time corresponds to a transition hazard. So we'll be modeling basically the hazards between each of the states in our illness death model. In our illness death model. So we wanted to make this like a causal inference approach, right? And we spent some time trying to understand what it would mean for this to be like a causal estimate. Many of you I know are aware already about trying to model hazards and calling it causal is like very fraught. And so what we decided or what we came up with was this idea of having counterfactual illness death models. So now basically, you know, take one of these illness death models for treatment. Of these illness death models for treatment zero, and it's the same as on the previous slide. What I'm proposing here is that we actually have two illness death models: one that corresponds to treatment zero and one that corresponds to treatment one. So basically, there's one illness death model that's potentially we can identify, and then the counterfactual one. So, the proposal of this method is to use these two illness death models, the observed and the counterfactual one, and incorporate fragrance. One and incorporate frailty models. So, essentially, what frailties are, it's you know, essentially equivalent to a random effects in like a longitudinal setting, where we have these terms that sort of capture individual heterogeneity. Another purpose for using the frailties is that they can link the different transition states for a given individual. You can think of it as like an individual's like propensity towards having these bad outcomes that can sort of relate that. Um, and so frail. And so, frailties is one way to do that. There's a lot of different decisions that you can make about frailties. So, I'll just go ahead and first show: this is the notation we're going to use for our hazard models. So, we have our lambda jk. This represents the frailty for a given transition. And then we have our baseline hazard. And then here in this term, we're going to have our frailty, which I'm going to call omega. Omega. And so basically, there's different assumptions that we can make about all of these frailty terms, both for sort of like interpretation, simplicity, computational feasibility. These were things that I wanted to explore in this project to sort of see how well can we take this, what I think is like a fairly nice theoretical idea, and can we actually make it practical? So, here I just, in case people are sort of like seeing models, I just wanted to go. Like seeing models, I just wanted to go through exactly how many models we're sort of proposing because, in some ways, it is a little bit ambitious, you could say. But just to go through the setup of the problem. So, here I'm just showing the two hazard models that would correspond to the lambda jk, where we are going from baseline to the surrogate. Here, I'm not focusing on any baseline covariates. And I won't talk too much about these coefficients, kappa, here, but the main part. But the main part that we're focused on is that we have these frailty terms. And then for the transition from baseline to T, we again have baseline hazards for that and the corresponding hazard model. And then this is the slide is, I broke some up because it's a little bit hard to take in all at once. And then we have the corresponding model that goes from the surrogate to the true outcome. This, I think, is one of the more interesting hazards to try and model. Hazards to try and model in this illness/death setting for a few reasons. So, of course, if individuals never experience the surrogate, we don't actually have any data about this hazard for that individual. And then, you know, it's an interesting idea, I think, because, you know, when we're talking about this idea of whether or not the surrogate is good, if you can't actually observe the surrogate for very many people, you may argue it's maybe not a very good surrogate. Surrogate. But so, as it is, these are the six hazard models that we're proposing in this framework. And then we sort of have all of these frailty terms, the omegas floating around, both in the observed and the counterfactual arms. And then we have some other terms that I'm proposing to model. We're proposing like some specific Markov assumptions like time reset. You can see that captured in some of these terms. And then we're also proposing some additional terms in the Proposing some additional terms in the model, such that the time that an individual experiences the surrogate may be informative for how long it's going to take for them to go from the surrogate to the true outcome. So in this case, going, you know, how long it takes for someone to experience distant metastases may be informative to how long they're going to live after they've experienced distant metastases. So these are the set of models that we're proposing to try and capture that structure. And then And then, you know, kind of a focus of this work is like, okay, how can we actually estimate these models and what sort of assumptions can we make or should we make? The main one that I've been focusing on in this work is this, basically on the previous slide, you know, I mentioned we have all these omegas. Here there's six different omegas. And so we wanted to kind of consider if there's some simplifications we could make. So some of the ideas that we've So, some of the ideas that we've considered in this work and also kind of tested the sensitivity of is making some assumptions about some of the frailties being equal. For example, these would correspond to the frailty from either pathway from either the surrogate or baseline directly to the terminal event. And then, what we decided to do is to make some independence assumptions about the frailties and then assume that the frailties are normally distributed. So, we can talk more about these. So, we can talk more about these. But, you know, given these assumptions, we've kind of come to this place where we have these normal distributions that we're assuming on the frailties. So they may look quite similar to the work that I showed on the first slide or so when we're actually working in the continuous setting when we're assuming that distributions are multivariate normal. This is similar in some ways that we're assuming some joint distributions on. Join distributions on sets of frailties or parameters that we can't actually observe together, and therefore, there are some non-identified parameters. This is the framework that we're trying to put forward. So talking a little bit about the data, so there's, you know, there's a couple ways we can think about the data or how we want to use our notation. The first way that I'm putting up here is sort of standard what you think about in a survival model that we have S is our time to the surrogate and corresponds. Is our time to the surrogate and corresponding sensoring indicator, as well as T and the corresponding sensoring indicator. And then, of course, we can have baseline covariates in our treatment. We can also have some sensoring time. That's basically what that is saying. The way that I've been laying out the model so far is actually in this notation with these gap times that I mentioned, where we have the subscripts that denote what transition we're actually going through. So, this is the notation done. So, this is the notation down at the bottom that I'll actually be using here. Again, we're going to be making some distributional assumptions about the outcomes. Here, we're assuming that the baseline hazards follow Weibull distributions that I'm showing here. And then this is sort of the diagram that ties together all of the different parameters in the data that I just described on the previous slide. So, I'm not going to go too in-depth into the estimation, but In depth into the estimation, but essentially, what I'm proposing is a Bayesian model, Metropolis-Hastings algorithm. And this sort of just lays out: you know, here's the data that we're observing, all the gap time information for both the observed and the counterfactual arm. These are all the parameters that I've laid out in the hazard models. And everything is basically connected by these frailty terms. And then again, the frailty terms are linked via these non-identified parameters. These non-identified parameters. And so, our goal is to basically use our Bayesian algorithm to estimate all these parameters. And those are, you know, our models. That's just some details about the Bayesian strategy. So then I think, you know, I talked a lot about the parameters that we're trying to estimate. So since we're in this more complicated setting with survival data, we had to spend some time thinking about what our surrogacy metrics were going to look like at this point. Going to look like at this point. So, if you remember on the CEP curve, we basically have measures of delta s on the x-axis and delta t on the y-axis. When those outcomes are continuous, we propose to just take the difference. And I think that's pretty straightforward. So, in this setting, it's not entirely clear if they're, you know, what delta S and delta T should be in the CEP plot. So, these are the two metrics that we're proposing to use on the X and Y axis for a CEP curve. CEP curve. Here for delta t, we're proposing to use a difference in probabilities. Basically, you know, for each of the observed in the counterfactual arm, we're trying to estimate the probability that an individual would still live past some fixed time that I'm calling tau t. And then on the x-axis, I'm proposing that delta s will be this ratio or the log ratio of cumulative hazards. And again, so these are just decisions that we make. Again, so these are just decisions that we made about how we wanted to define delta s and delta t. Certainly, we could transform, I guess, delta s to also be a probability is something we've discussed. But kind of the key point of why we wanted to think a lot about this is that we really wanted our y-axis delta t to be able to deal with the fact that not everyone is going to experience the surrogate. So, you know, in the multivariate normal setting, you're conditioning on. Normal setting, you're conditioning on delta s. In this setting, when some individuals might not experience s, it's not really clear what you should be conditioning on as far as the principal stratification. So here we're using this quantity, the probability of t greater than tau t. You know, this is kind of a complex expression, but in some ways it's like integrating over whether or not the circuit happened. We're coming up with this like more marginal probability, whether or not an individual is still alive. Whether or not an individual is still alive at tau t. That's your question. Yeah. So you're saying you could have chosen for the delta S to be like basically like what you have for delta T, but with S is in there, right? Because S is a temperature of an outcome. Yes. But you, is there like a, but you just didn't but it wouldn't change things dramatically? I don't think so because I think you can just, you know, like a simple transformation of the cumulative hazards to probabilities. To probabilities. I don't off the top of my head have like a better reason for why we made that decision. Yeah. So again, we can talk more about these. So the idea is that we wanted to, you know, create the CEP curve based on this delta S and delta T. So something that I think is interesting that we just started talking about yesterday is that each individual really has their own delta SI, delta T. Delta SI, delta Ti. And so, really, what we're dealing with here when we don't have closed form, like closed form for the CEP curve, is that we have like a scatter plot. So, our idea is to plot the CEP curve based on a scatter plot of delta TI and delta SI. So, I showed what our delta T and delta S are. Remember, we have individual level frailties in our models. So, really, each individual will have their own delta S and delta T. their own delta s and delta t and then we still think that ideal sort of qualities of the cep curve would be to go through the origin and have a positive slope and then again of course you know so in the multivariate normal setting we had nice closed form expressions and this idea of going through the origin makes sense here we may have to do some type of extrapolating there's just sort of some different consequences that are interesting to keep keep track of i won't go too in depth into this but I won't go too in depth into this, but here I just do want to mention, so there are still some non-identified parameters in the model that we're proposing. You know, based on the work that I've done so far, the algorithm that I'm proposing is really just using the observed data. We're not doing any imputation of missing outcomes, just using the observed data to try and fit the models. And then, really, what we're doing here is taking the non-identified correlations and simply fixing them. Fixing them. There's, of course, other strategies we could use, putting a prior over them is like one obvious solution. But I just do want to mention that really the counterfactual estimation that we're doing is of the frailty term. So we estimate the frailties for a given arm, we fix the correlations and draw counterfactual frailties. And these counterfactual frailties will be necessary for our CEP formulation because delta S and delta T rely on counterfactual hazards. Rely on counterfactual hazards. I might just skip over this a little bit, but we did try and do some exploration of like defining what a valid CEP curve would look like, like what conditions would exist such that we have different treatment effects that may exist. This is a table about whether or not baseline hazards are equal between counterfactual arms and trying to understand, like, you know, sort of like French's criteria, but you know, which counterfactual frail. Which counterfactual frail hazards would be equal in order for us to have a valid surrogate? This is a little bit more about the data. If people are interested in sort of how common the outcomes are of experiencing SNT, not a ton of individuals in our data set did experience ST, you know, a decent amount, which obviously better for the patients, but there will be implications of that during the estimation. So this is. So, this is an example of what the data results would look like for a prostate cancer example and fitting the sort of models that I've mentioned and doing our estimation strategy. This is what you could get, which I think is really of interest based on yesterday. So here, you know, on the x-axis, we have delta SI and delta TI. And then here, what I'm showing is each dot is the posterior mean of delta SI and delta TI over the MCMC iterations. MCMC iterations. And so I think that this is a really, it's kind of an open area for how to like interpret where all of these dots will fall for an individual. As we mentioned, it's really also going to depend on how large the treatment effects are. But here I think there's some interesting discussion that we could have about, you know, this is our estimated CEP curve. It seems like it sort of goes through the origin and has a positive slope, but there's individuals that fall in different. Slope, but there's individuals that fall in different quadrants, and we can talk more about that. I just want to mention, so this was more of like trying to figure out like what the CEP curves would look like under different settings, but I did make an R Shiny app. It's not really like published yet or anything. And so this is more of like a prototype, maybe you would say. But so the idea here is that, you know, I have all these assumptions I've talked about, different correlation parameters, and then within the app, you can sort of slide either what the hazard. Of slide either what the hazards look like or what the correlations look like, change your assumptions, and you can get a different picture that will change on the spot of what that CEP curve would look like under that setting. And then I'll just wrap up quickly. There's sort of a lot of directions that I think are really interesting with this. I didn't talk a lot about covariates, but I have done some analyses where we incorporated things like PSA and Gleason score. This is interesting to think about how the CEP curve may change, maybe able to look. Of may change, may be able to look at subgroups and things like that. I do think that this idea is really interesting to use the illness-death model. I think it sort of is like a mechanistic approach, and it would be really interesting to compare this to other methods. I do think there's a lot of similarities to like the mediation approach, the apprentice criteria, methods that we talked about about assessing the surrogate paradox, and different things that I'd like to better understand how these are all connected. And then some other extensions. And then, some other extensions that I'm personally interested in is kind of, you know, better understanding how we can sort of make causal interpretations with survival data. And, you know, maybe this will play into what we choose to use for delta S and delta T and some other areas like incorporating historical data. But in general, I think it's important to sort of, you know, make use these rigorous methods and extend them as much as possible to be applicable to different types of endpoints for different trials. Trials. And then here's a subset of all of the work being done that's been done in this area. So thank you. Thanks, Emily. I really like that talk. It was nice to see an extension of the multivariate normal framework into the survival setting, I thought. I'll open it up for a question or two now, either from the people here in Oaxaca. The people here in Oaxaca first. Yes, we have a question. Yes, just to make sure, just to make sure I understand correctly, I'm familiar with the original illness death frailty model by Shint. That's not me, so they had a single frailty for all three transition, but you instead have one for each, and that's to fit eventually the purpose similar to your multivariate. Similar to your multivariate normal framework. Is that true? Yeah, so that's the question that we have been asked before. And that is something I've also looked into where it's like a shared frailty and you assume that they're equal across all of the transitions. I do think that's another, you know, an assumption that you can choose to make. We were sort of trying to, like you mentioned, like allow for them to be correlated, but not necessarily equal or, you know, different assumptions you can make. You know, different assumptions you can make. So, we did assume that some of them were equal and that others were independent. Conditional on all the other parameters in the model. I think it'll be easier if I pull it up and like talk about it later with you, but that's a good question. Any other questions? Oh, okay, Layla. Just really back. I feel like I maybe was writing something down. Did I miss the, like in your application, was it a good surrogate or not? Surrogate or not? Yeah, so as far as you know, we have some posterior distribution of our like estimated CEP slope and intercept, and it did go through the origin and have a positive slope. I think that for me, it's a little hard to look at that plot and see, as we mentioned, where all of the dots are falling and to think about both the surrogate paradox and how confident we are in saying that it is a good surrogate. Confident we are in saying that it is a good surrogate when there is such variability in the estimates and in the frailties that I mean, yes, technically it passed the criteria, but a little hesitant, I would still say. And then my last question for the shiny app. So is the idea that I know it's not like ready for public people, but is the idea that someone would like upload their data that's like, you know, a person per row and it has the treatment indicator. It has the treatment indicator, you know, whatever, like S and T, and then I guess sensoring or X and sensoring, you know. And then it would pop out like the curve, and they could go and play with all the those little tuning things. Yeah, I think ultimately that is what it could do. Right now, I will say it doesn't have estimation is not involved. So right now, I was first using it as a tool just like. Was first using as a tool this like under true values to see how the CEP curve would change. So I guess you could sort of plug in estimates of identified parameters and do something similar. I think it would be ideal to have a do estimation, of course. So I have not written any sort of like R packages that do MCMC to have it be like super efficient for other people to do that, but I think that would be a goal. Okay, thank you, Emily. I think we'll move on to the next speaker now. There might be time. Speaker now, there might be time later. What's that? Oh, someone has a question online.