And of course, we could I prepared five things that I thought we could talk about. And of course, I'm going to call on people to say something. So one has been warned, four haven't. And so these are, and as you know, I like varieties. So one of them, we're going to, at least one of them, we're going to follow up on the theme of yesterday. What's your favorite variety? So the first one, so let me. The first one, so let me put some names here. So then I was warned that I'm going to call on her in a few seconds. Then there's the LCN variety. And so I had Guido here. But unless you grab him and bring him here in the next couple of minutes, we'll have to have his postdoc to be to take over. So then, since Guido, so either you find Guido or otherwise, you know, we're going to do. You know, we're going to do. It's okay. Okay. And then I like to talk about three by three determinant. So this is a particular, so these are three by three matrices of rank at most two. And I think I'm going to see whether Lara can maybe say something about this or lead the discussion. Then there's Crasmanians. And of course, Crasmanians and And of course, Crasmanians and low-rank matrices are related. So I'll put Fatemeh down. Is Fatemeh here? There she is. Okay. And then the last item here, of course, the star of the show, everything is Carlos. So maybe at the very end, we'll get Carlos to tell us about his favorite PDE. So the first four are about favorite varieties, but maybe Carlos has a favorite PDE. Okay, so the binary five cycle was discussed yesterday. So in part this talk, So, in partis talk, we saw directed graphs that are possibly having a cycle. And so, this is the directed five cycle. And she explained a certain model for this, a conditional independence model based on colliders and all that kind of thing. And then, afterwards, we had a discussion with her and Pratik and I on what's this model actually mean, and then there was a And then there was a expert jumped in right away. You know, Piotr, who knows everything, he pointed out that this is the same. This is actually the same as the undirected model. So this is a variety in projective 31-dimensional space for binary states. So we have binary states. We have a 2 by 2 by 2 by 2 by 2 tables with non-negative entries that sum to 1. And I would like to ask Denai to tell us all about this variety. Tonight, to tell us all about this variety. And you're on five minutes. And do we really need this? Are you there, Elena? Can you hear us? Is there anybody here? Go ahead. Go ahead. Don't wait. Go ahead. You can bring friends, you can bring enemies, but get started. Oh, go, go, start, start. Oh, go, start, start, start, start. I'm going to cut you off after five minutes, which is 300 seconds. Go, go, go. I guess we're going to just additional intended statements before we start thinking about the varieties. Just try a couple of them and then the pattern moves like this. So things like one using another. I think my point is. Four and a half minutes is fully right, but I asked whether this has myself. Go, go, go. Okay. To what? I guess you should be. Okay. So that's what. Right, so to illustrate the variety. What questions do we ask? So we learned that you ask what is its dimension, for example. And for this particular variety, we have an associated polytope because because there is a total speed and then we could look at the associated polytope and find its dimension through a paper of certain and certain so we discovered that the dimension is 10 so the dimension is equal to the sum of these of the faces Of the faces of this graph here seen as a simplicial complex with five vertices and five edges. And we subtract vertices for the empty state. So we have, yeah, in the end we have for the dimension of 10. And for the degree, we can also use the quality associated with. So we can use shapes to compute, for example, the f star vector, but whose like That whose last or the star vector is the sum over all the coordinates, or the f-star vector whose last entry is exactly equal to the degree of that, alright? So in the end, we have, let's say, 1232. Sorry. And we were also asked to compute the F vector of this polygon. To compute the vector of this polytop, so I cannot double the f vector here, but we can just say it's the end. This FX well, it says it has 32 vertices and thirty-six facets. Thirty-two vertices. This is uh so this so usually I would like to know what are these thirty thirty two vertices. Two vertices as a stand-dimensional column. How would you write them down? Fine. So it would be the columns of a matrix that has, let's say. Let's work this for you. Matrix called A. And this has how many rows and how many columns? So I have 32 columns and 20 rows. So the thing is that each of the rows corresponds to a value of a passet. So the polyglot here has, sorry, the simplicity complex has these five passets. Right, so we can think of the first four rows as corresponding one, two, first. And then essentially, the entries here correspond to the first To the first to each of the possible choices for the binary members. And now we think of the 32 rooms as reported out from somewhere for 30 variables. This would be zero after the end one month. So you have a x, you have a 20 quite difficult way. Twenty by fifty two late and you're filling it with zeros and ones some formula for what's the rank of your nature 20 by 32 nature of which rank markets that also think so. What else do we know? Do we know anything about this ideal? The generators are called bases. Do we know anything about the Markov basis? So learn that markup basis has 120 elements of three different kinds. So some are from directly interpreting these types of, 40 are coming from these types of conditional independence relations. Then another 40 are from this kind of conditional independence relations. And 40 are called out from The I called out from this stuff. I don't know what we cover. Any questions or comments? Anything else you'd like to have? So pardon me, out of the theory of the non-dAD directed graph. Are there any new so this wasn't so this was very interesting. We had this directed cycle, but then we learned was already also covered by under. Already also covered by so can you make new for you a directed graph that meets something that you have a second I think I can make it but let me think you'll bring it along this afternoon yes very good thank you maybe there's a good sorry now you have to So, sorry, now you have to fill in for Guidel. So, LSCN is linear convolutional networks. So, the way it works is you have a matrix that multiplies like another. So, you make these networks. So, there's, except in between, you have this annoying activation function, right? So, you have a matrix, activate, matrix, activate, matrix, activate, catadog. Okay, that's basically how it works. And let's kind of And let's kind of ignore the activation. I know you guys like Reyloo and tropical job, but let's forget about this for a moment. So, no activation at all. We're just going to multiply matrices. And in a linear convolutional network, the matrices represent the linear maps multiplication by a polynomial. So, multiplication by a polynomial is a linear map that eats some polynomials and makes bigger polynomials. So, the lin and so that's going to be a linear convolution network. And the grind that walks into the door is the following. So, I'm going to look at polynomials. I'm going to multiply polynomials in one variable. So, I'm going to multiply a linear polynomial. A and b are unparameters. Then I'm going to go a gradic polynomial, but just cx squared plus d and then I'm going to. D and then I'm going to go exq plus f. I'm going to be interested in polynomials in one variable of degree six that look like this for some choices of af. So this is called the function space. So this is based on a recent exciting paper by Guido and Katlin Kohn and Wahid and Matthew Traeger. So look this up. This is called the function space or neural manifold. Or neural manifold. So we're interested in polynomials i from 0 to 6, x c i x to the i. And what are the conditions that we have on these seven coefficients? So we have a projective space. So variety just walked into the door. So we have this variety in P6, we have this neural manifold, we have this LCN variety, and I just told you what it is. It's a set of all. It's a set of all vectors, homogeneous coordinates here, C0, C1, C2, C3, C4, C5, C6 that have this representation. And while, so Julia will compute the ideal, and everybody else can also compute this. So first question, dimension, second question, degree. So this is a variety that comes from Bayesian networks, convolution networks. From convolution networks of the model for kind of learning situations. Any questions? So, Julia, get going. We need the variety. Exactly. So, for example, C0 is P times D times F. C one is A D F. But not everything is monomial. So if this were all monomial, it'd be happy rhetoric, but it isn't. Okay, so first question. So please compute. Anybody else have Macaulay? Okay, Angelica, you go. Little competition between really friendly competition. So let's compute this variety. So let's compute this variety, and everybody who doesn't have a computer can philosophize about the dimension. What is the dimension of this neural manifold? The definition clear. Rosaire, the definition is clear what the variety is. I've parametrized it. So the sub-variety of P6. I've given you, you know, ABs, I've given you parameters, ABCDF. ABCDF. Laura is already looking for cluster coordinates, but any questions? Daniel, you ready to go? Okay, so anybody who is not currently typing into Macaulay 2, what do you think the dimension is of this? Julia, what do you think? Any guess? Zero. Okay, so Julia. Okay, so Julia says the dimension is zero. So all polynomials like this are the same. So what you're saying is that this is a zero-dimensional, irreducible variety. So Julia's claim is if you take any choices of the numbers A, B, C, D, F, you get the same polynomial degree six. That's what you just said. Map six-dimensional space one, okay. It's a curve. So, okay, there's one degree of freedom. So, in this big six-dimensional space of polynomials and one variable, there's a curve. But this neural manifold is a curve. What's your reasoning? What's your reasoning? So we've got in the last one. So I have these equations. So this is the parametrization. I have seven equations in. Equations in 7 plus 6 is 13. Yeah, okay, so it can't be. Can't be one. It could be one. Could be something else, too. I'll stick with one. You stick with one, okay. Seems like it's okay. I go for three. You go for three. Okay, why three? Why are we producing okay? This is for the students, okay? This is no, no, this is for some time. I'm gonna, I'm gonna phrase you, okay? So, you know why she knew this? Because when a while ago, like when she was young, like last week, she studied with a guy named Hartzon, who wrote a book on algebraic geometry. Wrote a book on algebraic geometry, right? And of course, if you an algebraic geometer, then A and B are homogeneous coordinates on P1. Don't look at me, you guys are supposed to compute, okay? Okay, so this is A and B are homogeneous coordinates on P1. That's very natural, because if you scale AB up simultaneously, up to scale, this doesn't change. Likewise, we have homogeneous coordinates C D equals Homogeneous coordinate Cd for the second P2, and then you know we have Pf. So we have this map, and so really, and this is it. So the dimension is the most three, and dimension is actually very good. We have this threefold, so this neural manifold has dimension three, and we're still, Daniel is very, very engaged. We're still, now we want to know more, right? Polynomials, P. I want to use this for learning. So we might be interested in the ED degree or the fragment degree. I don't know. ML degree Anybody know that anyway? What about the base locus? Is this map defined everywhere? Image housing. Is this one to one? Is this one-to-one? Experiment presentation? It's identifiable. Looks identifiable. So how do you identify in this model? Well, it's called factoring. So suppose you have exact data. It's when it gives you C0, C1, C2, C3, C4, C5, C6 exactly from the model. Well, then you just factor the polynomial and you recover the factors. How are we doing? I want to eliminate the parents, maybe. Is it degree six? Okay, tell me one point only. How many generators does your ideal have? Minimal generators. General. There's nine generals. Tell me one of them. C1, C5. Okay, so there's some total. Tell me one non-code. 4C5 squared. Minus C3 minus 3. Okay, some users to mind generate. What's the degree of this variety? Six. It's a very low, it's a variety of minimal degree. It's a very low-dimensional variety. Looks like a cube to me, right? I mean, if this were a toric variety, this were a toric variety. As a toric variety, this is a cube, but this sits in P7 of degree six. But this is maybe just a projection. Okay. But this is maybe just a projection. Okay, are we done? Okay, so please take a look. Make sure to speak to Guido about make sure to speak to Guido about neural manifolds, about linear convolutional networks, and the rest of it. And now, Laura, you're ready about a conversation about the three by three determinant. And Jose, don't go away. You will help, Laura. Don't go away. Just relax. I'm going to say something first. You don't need to come right away. I'm going to say something first and then we're going to. Sorry, I can't do it. Sorry, Elena. Sorry, I can't do it. Sorry, Elena. I'm really sorry. That's okay. So, here, what's the object? Well, we're looking at low-ranked matrices. So, we're looking at three by three matrices of rank at most two. Okay. But, of course, we're going to look at this either as a complex variable, we're going to look at this in the statistical sense. We're going to look at this in the statistical setting. So, what we're going to have to remove, we're going to have to remove the variety where all the pij are zero, one of the pijs for the entries are p11, p12. These are probabilities. But then, since we're in statistics, we also have to remove the sum. We have to take So we have to take 10 hyperplanes out. So we're looking at 3 by 3 matrices of rank 2, such that the entries are all non-zero and the sum of the entries is all non-zero. That's what we need to study. So this is a variety. And this has ML degree 10. And this was in yesterday's featured. Yesterday is those featured and yesterday's favorite varieties, and somebody really, really likes the likelihood correspondence. So, who was that? Who said that favorite white is the likelihood correspondent? Was that you or something? Okay, very, very good. Mark, you know what I'm like to correspond. Mark, you know what a like the correspondence is? Any guess what it could be? And why this was Jose's favorite variety? No. Okay. Well, then let's ask some other questions. And we could maybe ask, let's take this variety possibly with some of this removed. So some people, you know, so this is called a very affine variety. So very affine variety is a sub-variety of a pork. Something that sometimes people do, they take sub-variety, of course. They take saproid of corres. Something that other things that people do, they take unions of tori and make varieties. Some people, what they do, they say, Well, let's we really like tori, we like tori varieties and all that, but let's take many, many, many tori and let them put them all together. And this is then what you get is called a cluster variety. Cluster variety is a controlled union of Union of many forum. Now, I've recently gotten a little bit into something that's physicists. So, physicists study something called positive geometry. So, we could ask questions, is this a positive geometry? So, Lara, if you wanted to know whether there is a cluster, can you say anything about cluster varieties for three by three matrices of rank two? And do you think Do you think anything of this could be useful for statisticians? And in the meantime, Jose will prepare a 30-second lecture on the lecture. So what would a cluster of positive geometry person say about this version of three by three matrices of rank at most? So, well, okay, so for to see if it's a cluster variety, I guess. see if it's a cluster variety i guess i would start to look for algebraically independent variables among my generators and yeah well okay so the problem is this is a very kind of schemy way to construct a variety so we really do like take tori and kaboo them along charts we don't get a global description so one question to start with just is like find charts find child Find child ideas set maximum set of algebraically independent variables among the PIJs. How many there are nine PIJs? That's called P1, T1, 2, T1, 28, P21, P22, P21, P2P3. So how many of them are action on the alphabet? So what's the dimension of alpha right? That's okay. We want this as a let's make it as an affine. So we have a nine-dimensional space of three by three matrices and we have one equation. One mysterious equation of degree three. Degree three six terms for the determinant. So we're looking for eight variables dropping one. Drop one. And then by summit, let's drop the last. Okay, and then, okay, so let's take. And then, okay, so let's take that's good. So, we have a torus, right? We have a torus that's like where P11, you know, everybody is zero, non-zero, those sides all non-zero, something like that. Okay, well, okay, so one problem I see. A very special part about the cluster and the positive, I think, positive geometries in general, is that, well, everything should be positive, right? So, we take one chart of these coordinates. So we take one chart of these coordinates and we want to go to another chart. So say like these matrix entries are positive. They're probabilities. This has to do with people with air and what we sock on air and all that. They're positive problems. But okay, so now we can do this, right? We take a torus, intersect it with our variety torus in eight coordinates. But if we want, if we go to another torus or we want to express the last missing coordinate, like Like subtraction free in the others. And I think this is a bit tricky because we have very many signs in our equation. We just take out one variable. Maybe we can reparametrize this. We can think about this as a co-dimension one positroid variety in V36. Okay, yes. Yeah, okay, okay. So we take it into. Yeah, okay, okay. So we take it inside the Grossmanian 3.6 and we focus on the cells of the union of all cells. No, I'm sorry, I don't see it. I mean, the rank is less referred to. We're not in the Grasmanian prefix, are we? So, what's a positive geometry in your mind? What is the opinion? What is a positive? Well, okay, to be fair, I don't know what the positive geometry is. Okay, to be fair, I don't know the positive geometry part, I know the cluster or the positive spaces part that's, I think, a bit more as restrictive than positive geometry. So it's some variety that I can cover with Tori. And when I want to move now within my variety from one local chart to another, I need the transition functions to be sign-free, like all positive. So whenever I have a coordinate that is coordinated in one of my That is coordinated in one of my tori, I want to express it without any subtractions in the coordinates of any other tori. So the transition functions from one to another are in the semi-field polynomials or rational functions without any minus sign to them. I find this very exciting. So for me, since you are here, we have the great fortune, not only are you We have the great fortune. Not only are you local, you also participate in the whole year. I think it would be super, super cool to relate cluster varieties and positive coordinates and law phenomena, all that to algebraic statistics. And if you like to discuss this, maybe you can sit at lunch with Lara and me. Sounds good? Jose, what is the likely? What's the likelihood corresponding and why is this your favorite voice? I don't know if you're going to be able to do it. Okay, so very seconds. Well, here I can get points K and U. Both are three by three main. Three by three main picture is the points in Karl C8 such that Such that if I say p, such that this monomial here, which is the product of pij's raised to uij's, so you should think of these u's as integer vectors, is a critical point when you switch to H2R variety. So here we ripped out some points. So this so we rip out these coordinate hyper points along with this. About these coordinate hyperplanes, along with the sum the one we sum the zero condition. So, this allows us to rescale our p's, so it makes sense to consider enough to get to the eight. And now, what's nice is that you fix the data, you count the number of critical points, and as the value said, think the ML degree of revival is equal to 10. And then, what's even nicer is that you can, because we have this product of projected spaces, we can ask, well, what is the biodegrade of this? What is the biodegradation of this likely? What is the dimension? What is the dimension? Okay, so let's say again in English: an ideal corresponding consists of pairs: model point, data point, such that the model point is the MLE for the data point. That's the cause. It's a natural thing. What's the dimension? Well, the dimension is eight. Dimension is eight. Likelihood corresponds to V. Likelihood correspondence of v is equal to eight because we have a map onto v so you can map this uh p8 and this can think of it in the u query and so when you solve um the likelihood equations what you're doing is you're computing fibers so you pick some data vector u inside of here and you find the points upstairs that matter pick a generic point there you go every data vector has some ml Every data vector has some MLE that's on and has 10 solutions, so it's a 10-to-1 map, so it has dimension. Very good. So, what is the biodegradable? Now we have a sub-variety in P8 times P8. Okay, so what is the biodegrade? This, I don't remember it off the top of my head, but I can give you a geometric way of thinking about it. You can, how do we think of this map? We intersect this eight-dimensional variety with a codimension influence. Variety: the co-dimension notate this. I don't know, like here. Right, so if we intersect these, we get at the MLD points. But this is, I don't know, restrictive. We can take a linear space that involves a linear equation in p and let's say seven linear equations in e. And when you intersect, you get a finite set of points because we're mapping an eighth-dimensional variety to an eighth-dimensional space. And or yeah, Or, yeah, in some sense. And so, the number of points of intersection is one coefficient of multi-degree or phi degree. And then you iterate. Yeah, there's some number here. I don't call it five degree zero seven by degree zero. So you can think of this as a one seven, thank you. You can think of this formally as a polynomial that captures. As a polynomial that captures whose coefficients captures the number of intersection points under certain types of spaces. Do you happen to know the prime ideal? And so here we have a prime ideal in 18 for unique variable 9. Do you happen to know the prime ideal? It depends on what we mean by no. I have only two readable files of the primary. I don't have one. Would you like to confuse one? It's confusing. Oh, it's one. Oh, it's fun. Okay, thank you very much, Jose. This was wonderful. So, uh, so if you're interested in discussing this, uh, Jose will, of course, happily to tell you. And Serka and I are very, very interested in tropicalizing this likelihood correspondence. So we will maybe in some discussion tropicalize this likelihood correspondence. Of course, the conversations with Lara will be ongoing. But time has come to talk about the Grasmania. To talk about the Grassmannian C36. So, this is a variety. Let me tell you about this variety. So, this Grasmanian, V6. So, it lives in a 19-dimensional projective space. So, it has fluok coordinates: P1, 2, 3, P1, 2, 4, P1, 2, 5, blah, blah, blah, P4, 5, 6. So, it has 20 flucco coordinates. So it has 20 flucco coordinates, it's cut out by many quadratic 35 independent quadratic equations, and its degree is 42. 42 is both the Catalan number and the meaning of life. So 42 is the degree of this variety. It's a nine-dimensional variety. So now we could ask: is this in some meaningful winds, is this a statistical mark? Does it make sense? Does it make sense to think about this as a statistical model? I'm going to ask Fatimi this question in one minute. So, is this a statistical model? So, for example, one thing that people teach in the statistics department are called determinantal point processes. Do you guys know about determinant or point processes? So, if determinantal point processes were to make an appearance in algebraic statistics, this probably would be it. But, of course, we could. But of course, we could restrict this to the positive simplex. We could say, what is the Grassmannian? What is it in here? And then we get the positive Grassmannian. Why would anybody study such a foolish thing as the positive Grasmanian, right? Now, we could also say what happens when some probabilities are zero, right, that then you go to the boundary. Why would somebody said we have these happy positive coordinates? Why would some of them be called zero? Well, you know. Why would some of them be called zero? Well, you know, I don't know. You know, then what you get, some people call these things positroids. And so my question is, is this an interesting object? What is the ML degree? What do you think about this? What can you say about G36? Is this a statistically interesting offer? And I don't mean rule. And I don't mean Rudy's question. I thought that's related to Rudy's question, then I could say a lot, but this is new. Okay, so what can we say about the probability? The state space is three-element subset of the six-element set. So is the user of nature is that? The usual matrix like the P I JK, like a tensor, like you know, or your independence. So, some stylist film is for example that I put this. So, that's a P I J to this is the metric, we have a derivative type. Now sometimes statisticians they like positive parametrization. So, maybe is there a way of parametrizing this so that that This is so that the parameter statisticians like to call that parameter theta. You write down 90 parameters: theta 1, theta 2, up to theta 9. So these pijka actually evidently positive. All right, so both nodding and looking at it. And if it returns to the city,   Okay, and we've okay we have certain variables class size and then we define the same argument phase on the code of that disguise because there are certain paths here uh that we can't use so So then we write something as a product of the starry body. So we calculate paradigm. So that would be the key column. And in other parts, you just have to... So you draw this lane graph. So this XI is the probability of choosing a particular region. Each region in this lanar graph has an unknown probability, would you say? Well we can we get it decomposing and we have each where do you actually keep the right part of the breathing also so this from this graph so you can get certain that has the thing over today so these are the so we have the input and then we have the old so these are okay so like in three six I guess you have three and kind of source word he says and then source requests and then sync and then compare we can field of paths and then okay we don't do this so should we compute so this is really nice right because if we have zeros like if we have probability model zeros as they would call them that means we're setting some of these p's to zero and this parametrization gen gracefully generalizes to these boundaries so this is a very happy This is a very happy positive geometry. But of course, we need to interpret the likelihood function, the log likelihood function. Well, some people call this a scattering potential. It's very closely related to a Lando-Ginsburg potential. So I'm proposing that we discuss this model and that we just compute the MLD. Sounds good? And if that turns out to be hard, we go to some positive outbound, we make it easier by setting some P0. By setting some P0. Sounds good? And of course, you can participate because this really tropicalizes very, very, very nicely, and Lara knows everything about it. Let's do PD. So one of the most exciting papers, thank you, one of the most exciting papers in algebraic statistics was posted two, three weeks ago. Carlos had no chance, no chance to give it. I thought we should give it five minutes. One of the most exciting papers is about the ML degree one problem for Gaussian law. One problem for Gaussian models. So, the day after this came out, I got an email from Serkon: hey, here's an exciting data, and I had already known about it. So, let's hear what this is about and why this is so exciting in five minutes, and then we'll yeah. Yeah, well, I thought I was going to be immune given an organizer status, but I guess not. Okay, yeah, so as very much Yeah, so as Bern mentioned, so we're studying Gaussian models. So Gaussian concentration models M with ML degree one gross model bigger and no slightly bigger. Right, so um Right, so the main result in this paper is that we characterize these models, right? So, these would be models, let's say, n by m. So, these are multivariate Gaussians and with mean zero. And so, they have some positive definite covariance matrix m by n. And so, these are in bijection to solutions to a PD. A PDE and the PDE is the following: Phi, so you're looking for solutions phi that are homogeneous, rational homogeneous Almost linear function. And the PD is that phi has to be equal to minus the permanent of minus the gradient of log phi. Okay, so you have this phi homogeneous function, you take the logarithm, you take the grade. logarithm you take the gradient so these are s coordinates but these are the s and then minus the permanent and you should recover the original the original pi and so what is the bijection the bijection is non-linear it's a non-linear pd exactly because the right hand side so first order non-linear pd exactly so first order non-linear yeah Non-linear. Yeah. And well, you can see that if such a pi is a solution, this pi has to be homogeneous of degree minus m. Okay, so m is the dimension. And so what is the actual correspondence? If you have a model of ML degree one, then you consider its maximum likelihood estimator, right? So this would be some psi that goes from That goes from matrices S to S hat with the MLE. So it goes to the model. This is a retraction into the model. And so what would be the phi? The phi is the determinant. Sorry, the phi is the determinant of psi. Okay, so you'll stick the determinant of the maximal estimator. So psi is the map that takes data to be estimated to the MLE. Yes. Yes. And then the solution to VDE is the determinant of that. Exactly. Dodo will do all this for brain. Yes. Great. So this is interesting. And maybe people here are a little rusty on PDE. But this is weird, right? Because we all know the very beautiful theory ML degree one for discrete models. So for discrete models, there's Horn and Kapranov and Junior. And Capranov and Junior, and we like. And what does this have to do with that? So, well, I want to say what's the opposite direction, but maybe it's not like obvious, right? So, if you take a pi that is a solution to the PDE, then just take the parameterization by this minus gradient of pi. So, take this map and then take serious closure. This map and then take serious closure, and then you will get back a model of the mode of V1. And so, yeah, okay. And then you can check that many, many models that we like will satisfy these things. So I had one example here. I want to give you an example. So if you have a DAG, there was a DAG mentioned, right? So let's say you have a DAG and A DAG and like a pentagon, for example. Well, that will not be a DAG. Wait, so the partisans thing was a DAG. So the partisans thing had degree 1232, right? That's degree 12 tonight, 1232. But ML degree is one. Maybe it satisfies. So okay, then, yes, so then the pi you can write very nicely in terms of the minors of no the. Note the terminant of the submatrix of S given by the parent of the node, and then here the terminant of the parent union, the original node. So very good. So if you have a directed graph and you write down this thing by moralization and parents and so on, you get a solution to the PD. You get a solution to the PD. And then in particular, will you get an M of degrees one? Get that M of the Grease one, which we know. What about the display? How is this related? Have we seen anything like this in the suite case? Yeah, well, so yes, so Junho has this nice classification, as Bern mentioned, with horn matrices. And yeah, in a sense, there is also a PDE. So we know Junior. So we know Junior Hurley is one of the new super cool. But who's Horn? Max Horn. Very good. That's it. So homework for those who like this stuff. Look up who is Max Horn. So he was a professor at Darmstadt University a few years ago. And okay, we're ending eight minutes earlier. So I guess we're meeting at 12:50 for the photo down there. And now there's Down there, and now there's intense. So, in the next, so between 12:30 and 12:50, the schedule says it's like intense group. So, you are invited to think more about this and active, active group work. Thanks, thank you, Ben, for moderating. And then I'm getting older than this.