Thank you very much, and I really appreciate the chance to speak at the workshop. It's been really interesting this week, and especially the variety of topics that were covered. I'm going to focus on something that's been touched upon a few times, which is how do we actually go about a computation of high-dimensional posterior measures, say, you know, coming from Measures, say, you know, coming from a nonlinear inverse problem. And this is based on some work I did with Richard last year. Okay. So at the start, I'll just introduce a more general setting, but very soon I'll focus then on one particular nonlinear inverse problem coming from a Schr√∂dinger equation. And hopefully we'll have time to look at some proof ideas. We'll have time to look at some proof ideas in the end. All right, so the basic setup for the talk is that we're given a d-dimensional posterior distribution. Okay, so we're finite-dimensional, but we can think of this as, say, like a finite-dimensional truncation of some infinite-dimensional problem. Okay, and we have a, you know, kind of the standard Bayesian statistics set up. Bayesian statistics setup where there's a prior distribution pi, and because we're in finite dimensions, we can write everything nicely in Lebesgue densities. So we have a prior density pi of theta, which is the parameter, and then we have a likelihood function e to the ln, where ln is the log likelihood function. Okay, and here and the data is Zn. Okay, so here I've already kind of So, here I've already kind of implicitly smuggled in this parameter n that stands for sample size. So, okay, and capital D will throughout be the model dimension. And the setting that I'm most interested in is one where it's not just sample size going to infinity, as is usually considered in statistics, or dimension going to infinity, as is sometimes considered in the applied math literature, where people try to Applied math literature where people try to study phenomena that are dimension independent. But actually, I'm going to adopt the situation where they both simultaneously scale in some fashion, right? So oftentimes, as you accumulate more data, you kind of want to appropriately refine your model, say your mesh in your PDE solver, or the, you know, like you want to fit more wavelet coefficients. So kind of D and N grow simultaneously. n grow simultaneously. And I also want to look at situations where this function ln you know is somewhat complicated. So it's not just nice and quadratic, but comes from some complex statistical model. And the prototypical theme for this week is, of course, nonlinear inverse problems. Okay, and then I want to ask kind of the question of Bayesian computation. Okay, and the first goal of Bayesian computation, perhaps the most Perhaps the most naive way to phrase it would just be: well, can I generate a random variable that has the same state space as my parameter lives in, r to the capital D, that has approximately as its distribution, the posterior distribution, right? So L of theta is approximately equal to pi. Okay, so this is kind of in order to approximate the whole law, and for that, I Uh, the whole law, and for that, I equip the space of probability measures with some metrics, say the Wasserstein metric. Okay, another question I can ask is, actually, I'm maybe interested just in a specific aspect of the posterior, right? Some one-dimensional functional or some actually like the maximizer. So, these could either be defined, you know, as an average quantity, so an integral against the posterior distribution, or variationally as a As a maximizer of the posterior density. And these quantities sometimes have a specific statistical interpretation. For instance, for point reconstruction, maybe I want to use the posterior mean, which is just the integral against the identity function. And the question is: can I compute those quantities actually with quote-unquote reasonable computational cost? And I'm Computational cost. And I'm going to focus on a very stylized notion of this reasonability, which is: can I do it in polynomial time? In practice, it may still be infeasible, but this is like one initial step to saying that it can be done at all. And arguably, the most successful approach in recent years for doing such a Bayesian computation has been MCMC. Computation has been MCMC. What's the basic idea of MCMC? Well, it is basically to engineer a Markov chain that I will throw out denote by var theta k. And I will engineer the Markov chain in a way such that the invariant distribution of that Markov chain is equal to the posterior. So that I kind of, if I run it for long enough, I will always arrive at the posterior distribution. So then both of these. Then both of these basin computation tasks are addressed. So, the first, which is to approximate the whole law, is just I run it until some mixing time, J mix, okay? And I trust that after enough iterations, I've converged, I've mixed, and then I just take the law of the kth iterate as an approximate random variable. Okay, the second task: say I want to compute an integral against the posterior distribution. Compute an integral against the posterior distribution is also naturally addressed, right? So, here what I would do is I would wait until my Markov chain has mixed. So, this is a sum starting at J mix plus one, and then I take J more iterations, actually from then onwards, and I just average evaluations of my function h that I want to integrate against, kind of along the empirical measure given by the states. Empirical measure given by the states of the Markov chain, right? So I replace the posterior by the empirical distribution of the Markov chain. So these are erodic averages. So to rephrase the previous question of feasibility of computation, really what we're looking for is we're looking for bounds for the mixing time and for this number of iterations j that guarantee me that I've approximately computed the quantity. Okay? A quantity. Okay? Right. So unsurprisingly, the class of models I will focus on is kind of inverse problems, right? Specifically, inverse problems with regression data. What do I mean by that? Well, so we just assume that we have kind of the standard regression setup where given every parameter in Rd, I'm given a function on some domain. I'm given a function on some domain O. Okay, so this is like in inverse problems language, this is a forward map. Theta mapping to, oh, this is a typo. This should be G of theta, of course, right? G mapping to G of theta. G of theta is a regression function. And the data that I'm given is then a point evaluations of this regression function. And similarly to Hanna's talk, I'm going to assume that the point evaluations happen at Evaluations happen at uniformly sampled points across the domain. So, this is random design regression where xi are the design points. And then I have these noise variables, epsilon i, that I will assume to be iid normal. Okay, so so my data consists of these tuples, yi and xi, so point evaluations, and I know where I evaluate them, okay, of this forward object g of theta. And yeah, throughout the week, we've seen actually a lot of different forward maps, a lot of them arising with different partial differential equations. So, you know, a lot of those maps are non-linear and arise with some kind of, say, coefficient to solution map or tomography problem, which is described by a PDE. But actually, I also want to flag that a related class of Related class of models that is really doesn't exactly fit into this framework because there's a time variable, are kind of filtering and data assimilation problems and be very interesting to see and to think about what can be done there. Okay, so there we have a similar setup with indirect observations of like, you know, latent states, but then there's also there's a hidden Markov structure. And we saw this in And we saw this in Judith's talk, which I found really interesting. Okay. And then if you try to solve this inverse problem in a Bayesian fashion, what you do is you place a prior distribution on the parameter theta. And we'll focus on Gaussian priors, which are commonly used. So, and then the posterior surface looks like that. You just have two terms, one that You have two terms. One that comes from the log likelihood, which is just the square data discrepancy coming from the Gaussian likelihood. And the second term is just a given. Okay. Now, basically, I'm going to, then we want to base our inference on the unknown parameter theta on this posterior density. And numerically, Density, and numerically, then I exactly need to do these computation tasks of, say, mean, map, et cetera. Okay, what are the things that make this computation task hard? Well, now if G is non-linear, then this whole expression in the likelihood will be non-convex in the parameter theta, right? And so the whole posterior distribution will be non-log concave. And in some sense, we may think of non-log concave samples. Sense we may think of non-log concave sampling as an analog in the sampling world to non-convex optimization in the variational world. So this can be expected to be hard. But there's more going on. It's not just not long concave, but actually as we look at the scaling when n tends to infinity, we accumulate more and more terms here. So in some sense, we can expect the posterior distribution to become more and more spiked. Become more and more spiked. And this may actually deteriorate the Lipschitz constants and everything. So this is simultaneously also happening. And as a third thing, there's this kind of common wisdom that if I am in a non-lock concave setting, so I don't have very strong geometric assumptions on the posterior distribution, there's this curse of dimensionality phenomenon that when my dimension grows that I consider my problem in, then I will need, you know. Then I will need exponentially many, the problem becomes exponentially harder unless I have very, very stringent assumptions. Okay, and so in order to overcome some of these scaling phenomena that, especially in dimension, that make this problem hard, there have been many recent works on sampling in high dimensions. So a lot of algorithms, MCMC algorithms, are based actually on discretizations of continuous time stochastic. Of continuous time stochastic processes. And one prominent example is the PCN algorithm, which is based on discretization of Onstein-Uhlenbeck processes. And here there's a paper by Helga, Stewart, and Follmer that under certain circumstances, they get a dimension-independent spectral gap property for those Markov chains, right? So basically, meaning if you have at the top of your hierarchy an infinite-dimensional sampling problem and you can verify Dimensional sampling problem, and you can verify some abstract conditions on this infinite dimensional problem in the Hilbert space setting, then no matter at which level you truncate your problem and discretize your problem, you will get the same convergence guarantees for the Markov chain. But the thing is that here there's still a lot of dependencies, implicit dependencies on other quantities like the Lipschitz constant of your problem and various other things that in the end. Various other things that in the end lead to exponential dimension when you actually want to get these non-asymptotic guarantees. Okay, and then another class of very popular algorithms are kind of based on discretization of Langevin diffusions. And here there has been some work in recent years, starting with the Law concave setting by Arnac d'Alalion and other people in Paris, that for the first time kind of derived explicit bounds. Explicit bounds that kind of polynomially with explicit dependence on dimension and Lipschitz constant and everything in the high-dimensional setting. So this was kind of significant progress. And since then, there have been various extensions to non-lock concave settings, right? But so you can kind of like vary the assumptions that you impose on the distribution you're trying to sample from. Distribution you're trying to sample from. For instance, you can just assume directly that your distribution satisfies some log-Sobolev inequality, or you can assume your distribution is convex outside some ball, or log-concave outside some ball. But usually, once you kind of differ from this log-concave setting, the penalty that you pay in terms of, for instance, the size of the ball that you allow here is usually exponential in how much you allow yourself to deviate from. How much you allow yourself to deviate from lock-on cavity, right? And so, to illustrate just briefly what can go wrong not just in high-dimensional scaling, but actually in these other scaling phenomena that I was mentioning as the distribution becoming more spiked and stuff. There's actually a very simple one-dimensional example, which is just like a double well. So imagine you have Langevin, Monte Carlo. So Langevin Monte Carlo is based on Monte Carlo is based on discretizations of a diffusion process moving in a potential, where this potential is given as the logarithm, the negative logarithm of your density, okay? Of your posterior density. So if this has a double well, right, then the Then, and if I am on one side of the well with my stochastic process, actually, the time it takes me to jump to the other side is exponential in the height of the well. Okay, so if this height is h, then the time, the exit time kind of will scale like uh e to the h. This is now grossly oversimplified, but this is something that's called the Iring Kramast. Kramast scaling. And it's also, yeah, you can see this, for instance, in a paper by Eberle in PTRF in 2015. And now, if you imagine this to actually be the posterior surface for, so this is the log likelihood surface for a single sample. Now, if you have n samples and we accumulate more and more terms. And we accumulate more and more terms, the height of these wells will actually scale with n, right? Now, this is, of course, not a complexity theoretic hardness. So, in 1D, you can always optimize or sample using other means, but it just means that for the most commonly used MCMC algorithms, actually, in any dimension, you can run into, you need to be careful, you can run into difficulties. Okay? So, we kind of started asking ourselves, well, Of started asking ourselves, well, what additional structure is there that we can leverage to prove computational feasibility? And there's a few things that you know we've understood much better over the last decade that are happening in these statistical inverse problems. The first thing being kind of understanding more about the local geometry of the forward map. So this comes through PDE tools, right? So I linearize my problem at some point, and maybe I can have some stability. And maybe I can have some stability properties, et cetera. And we've seen this mentioned a few times throughout this week, in Richard's talk, in Jan's talk. And then we also know the posterior concentrate. So actually, maybe only local phenomena end up mattering in the end. And then also we know that, you know, there's concentration of measures. So as we accumulate more and more samples, these empirical quantities will concentrate around the average. Around the average. For the rest of the talk, I will focus on the Schrodinger equation. So far, I've only spoken about generalities. But as we've seen in Jan's talk, some of those results can actually be extended to other settings like the PN non-abelian X-ray transform that in X-ray transform that in terms of tomography has a different name. Okay. Any questions so far? All right. So now I'm going to introduce a concrete forward map that comes from the Schr√∂dinger equation. Okay, and it arises as follows. So we're on some domain O. So we're on some domain O, bounded domain, with a smooth boundary, and we have some function g on the boundary that's given and known. So this is not, this is part of the statistical model. It's not what we want to infer. And now this is the key elliptic PDE that kind of describes the relationship between my unknown and the regression function that I observe. Regression function that I observe. Okay. And this map I call G and maps any function f to a corresponding PDE solution UF. And UF solves this elliptic PDE. That's just the Laplacian minus some potential term. Okay, so arguably one of the simplest elliptic PDE examples that you could, yeah, in the non-linear setting that you could look at. And now Look at. And now, whenever f is sufficiently regular, you have a unique solution, so that's okay, right? And I just briefly want to illustrate why g is non-linear, because you know, like this, this is a linear differential operator, so it's linear in the right-hand side of the data. But actually, why it's non-linear can, for instance, be seen from the Feynman-Kutch formula, a probabilistic representation of solutions to this PDE, okay? Solutions to this PDE, okay, which is just given as the expected value, where the expected value is taken over some stochastic process, a Brownian motion, started at x, okay, over the boundary data evaluated at the hitting time at the boundary. And then how f comes in, and this is where things are non-linear, is this exponential. Okay, so actually f plays the role of this attenuation coefficient. Role of this attenuation coefficient. See, for instance, also Jan's talk. So, this is where the nonlinearity comes in, right? So, we have this PDE map G, and maybe you've noticed this is straight G, not curly G, as I wrote before. And this is because I still need to kind of parametrize this F using a finite-dimensional discretized model, right? And actually, I need F to be positive. This is something I didn't. This is something I didn't say, but just wrote. Okay, so, and in order to use kind of some finite dimension parametrization, okay, we're gonna do the following. We're first gonna, oh, sorry, this should be, this is wrong. This should be f theta, and f theta is phi composed with the series. Okay? Okay, so what we do is that we first take some series expansion, D-dimensional series expansion, which then obviously will live in a linear space, so we don't have any positivity constraint there yet. And in the second step, we concatenate with a link function phi that ensures positivity, right? Okay. And I'm gonna here I take a very specific basis expansion, which is kind of the eigenbasis. Which is kind of the eigenbasis of the Dirichlet Laplacian, because it's especially compatible with elliptic PDEs. And here, as I said, phi is a link function on which we need some conditions that I don't want to go into. And then this kind of full forward map, curly g, that really maps from the Euclidean theta parameter to my regression function is theta mapping to. Is a theta mapping to the PDE solution corresponding to this positivized f sub theta function. Okay, and okay, so now I'll start discussing the results and all the results that I will be presenting are kind of under the frequentist assumption. So we assume the data kind of comes from some ground truth parameter, theta zero. Right? And the first result is And the first result is somewhat of a qualitative nature. So it doesn't specify the specific algorithm, it just says that a polynomial time algorithm exists, right? And the result is going to concern the computation of the posterior mean, which is this integral against the posterior. So here's how it goes. We need some assumptions on model dimension not growing too quickly. Model dimension not growing too quickly on the prior being a specific Gaussian prior. And then, okay, I also fix myself a precision level, epsilon, up to which I would like to compute the posterior mean, right? And the condition here is that epsilon, I can't compute up to arbitrary precision in polynomial time, but up to any polynomial precision. Right? And then what we see is that there exists an algorithm that has polynomial computational cost in three quantities, sample size, dimension, and epsilon, such that I actually can compute the posterior mean by this output of the algorithm that is called theta hat epsilon up to precision epsilon. Up to precision epsilon. Okay, and this computation guarantee holds under high probability. Now, this is under high probability of the joint law of the data, okay, the kind of really stochastic data generating process, and the randomness of the algorithm, the kind of Markov chain probability that is implicit in the algorithm. Okay? So, let's say a few words. So, let's say a few words about what the actual algorithm is. It's actually a Langevin-type Markov chain that I mentioned before. So, it's the discretization of a continuous time stochastic process moving in this kind of potential given by the logarithm of the posterior. Okay, and so here we need an initializer, which is non-trivial, but in the Schrodinger setting, doable in polynomial time. And then the update. Time and then the update is kind of this gradient step where gamma is a step size, and then I go along the gradient of the log posterior, except this is actually not the posterior, but it's some kind of proxy posterior. So that's why it's called pi tilde. Okay, and then I add stochastic noise. So in some sense, this is just stochastic gradients descent, except that the noise variable variance doesn't go to zero, right? So Right, so what I just said are these three things, okay? So the initializes, computable, and polynomial time, these are kind of these Gaussian innovations, as Richard called it. And here I have a proxy posterior distribution that is given by, like the posterior itself, but with the log likelihood replaced by this log likelihood tilde, which is locally the same and can be constructed. Same and can be constructed also in polynomial time, okay. And uh, yeah, if there are questions about that, I'll elaborate later. But so with this algorithm, we can now kind of make more concrete our results. So these are kind of the concrete hypotheses that I put a little bit under the carpet in the existence statement, right? So the precision level is the same, okay? And now here I choose. Name, okay, and now here I choose a specific step size gamma. Okay, and this expression looks horrible, but all we need to take away from it is that gamma can be chosen polynomially depending on precision level, sample size, and dimension. Okay, that's all we need to take away from it. And then we need some assumption on dimension not growing fast enough. Okay, and then kind of the precise version of the theorem we just saw is as follows, right? Is as follows, right? So suppose I want to integrate my posterior distribution against some Lipschitz function. That's h, okay. And this is just the ergodic average of the Lipschitz function over my MCMC path that I will call mu hat of H. Okay. And then we get this concentration inequality for this ergodic average, right? Which is saying. Right, which is saying that the mu hat deviating from the true posterior evaluated against h, okay, that deviation being larger than epsilon, my precision level that's prescribed, okay, this probability is bounded above by this exponential term, right? Where in the numerator, I have the number of iterations that I average over j, and in the denominator, I have some polynomial quantity, right? I have some polynomial quantity, right? And it's crucial that this g d and epsilon is polynomial in everything, because I have to choose j of larger order than that in order to make the probability on the right-hand side small, right? So this is kind of the precise non-asymptotic version of what we saw previously, okay? And this is under hyper probability and everything. And one And one nice corollary of this, so you know, one particular functional I can compute, one Lipschitz functional is the posterior mean. So, what do we get for the posterior mean is actually that this being the ergodic average of, you know, this just being the one over j, and then just the Markov chain states averaged as a, you know, as an estimator. As an estimator for the posterior mean, is able not just to compute the true posterior mean, but actually the ground truth parameter up to a polynomial precision in polynomial time. Okay, and I think this is kind of a neat message that in this nonlinear inverse problem, we're not just saying the Bayesian methods are actually able to, in principle, recover the ground truth. Recover the ground truth up to a polynomial rate, but actually it's also computationally doable. Okay. And we have analogous results for a kind of Wasserstein contraction. So I'm going to go fast here. What do we get for the Wasserstein contraction? Well, so here we're trying to do task A. If you remember kind of from the second slide, task A was to approximate the whole posterior. To approximate the whole posterior to generate a global sample, and what we see here is that the Wassersten distance between the kth iterate of the Markov chain and the posterior satisfies this exponential decay. And the key term on the right-hand side that we should focus on is really like this red term. That's why it's red. And so we see here that the rate of decay is really dictated by. dictated by the c times gamma n d to the whatever power, right? And again, what's important here is that the way we've chosen gamma is polynomial in everything, such that this spectral gap deteriorates also only polynomially, right? And then we can choose k kind of of the same order as that to achieve a given a given precision level that we desire, right? Desire. Right? And two comments. So the first one is that really, yeah, this term can be thought of as a spectral gap. So even though it's not dimension independent or it's not sample size independent, it is quantified in a polynomial way. And then maybe you've noticed that we have the second term here, epsilon squared, which really can be thought of as all kinds of. Can be thought of as all kinds of approximation errors incurred in the process. So, one error comes from the Euler discretization of the Langevin diffusion, because basically the discrete scheme we look at doesn't exactly have invariant distribution pi, but some approximation of it. And then also, we have, we're actually following not the We're actually following not the gradient of pi, but we're following the gradient of pi tilde, right? This proxy construction. And both of those errors are kind of encapsulated in the epsilon squared term. So in the remaining, I guess, four minutes, I just want to show you briefly: it's four minutes, right? Gabriel or Ankana? Yes, that's four minutes. You can take, you know, I think you actually began a few minutes later because of the iPad glitch. So, yeah. Thank you. So, one of the key results that we show in the course of the proof is actually an approximation result for the posterior distribution. Okay, so the title says of this. So, the title says of this slide: globally log-concave Wasserstein approximation, right? So, basically, even though the posterior distribution is not globally log-concave, we prove that it kind of concentrates in a region where it's locally log-concave, right? And then you can kind of modify the likelihood landscape outside of that region and construct globally local. Construct a globally log concave approximation. And then, of course, you have to quantify the approximation error, but it turns out to be very, very nice and well-behaved, right? So, this is the third part of the statement here. The Wasserstein distance between this pi tilde posterior and the pi posterior, the real posterior, is exponentially small. Okay. And where does this come from? Well, it kind of comes from the fact that, okay, That okay, the posterior distribution is locally log-concave on some region B, okay, and has a unique mode that also lives in this B. And then you kind of alter things outside of that region B and then you get something globally globally log concave with the same mode because the mode is also inside. And once you're globally log concave, and you know there's one. You know, there's one extremal point where the gradient vanishes, it cannot vanish at any other point. So, this actually also proves that in this particular nonlinear problem, the maximum a posteriori estimator is unique. There cannot be more than two global minima. There can be other local minima, but only one global minima. Okay. And this is, of course, somewhat related to, you know, like Laplace approximations or Bernstein for New. Plus approximations or Bernstein von Mises approximations. But it is different because actually the approximation that we have is constructed by hand and it's at finite sample size. So it's really like it's not Gaussian and it's non-asymptotic. Okay, but of course it's all related to stability properties of the gradient of the forward map and everything. So we use similar We use similar kind of facts and geometric properties of the problem, but it's a different approximation. Okay, so what are the proof ingredients for this Russia standard approximation? Well, so you need to show actually different things that we have gradually understood better and better over the last years. So one of the ingredients is that you actually first need to prove that the posterior concentrates Posterior concentrates more and more. So, in particular, that it concentrates on this region where you're a lot concave, right? So, and there was some progress. You know, van der Faad and van Santen studied kind of general posterior contraction for Gaussian priors. And then Gabriel and Francois and Richard kind of, yeah, did the same for non-linear inverse problems. Okay, so you need to understand the Okay, so you need to understand the convergence properties of the posterior, and then you somehow after that also need to understand the convergence properties of this proxy posterior. And for that, you need to study actually concentration of the mode. So the mode needs to be like in that nice region, right? And then you need also that the forward map is kind of the gradient of the forward map has the stability property. Has the stability property inside of this little region outside of which you modify. And that kind of gives you such strong curvature that also the proxy posterior concentrates in this region, right? So that you kind of do by hand. You say the mode is inside of B and then you have curvature and B has a certain radius. So, you know, any distribution with a certain curvature has to concentrate around. Curvature has to concentrate around its mod at a certain rate. And then, once you know these things, okay, you know that pi tilde is globally log-concave, you can actually use existing high-dimensional MCMC bounds to get convergence of the MCMC algorithm. Okay? Right. So I think I'm gonna skip. I think I'm going to skip my. I'm going to spare you another drawing of mine and just mention in the end that a lot of our proofs are really of statistical nature rather than specific to sampling algorithms. So, you know, the results that we obtain for sampling are obtained via studying the geometry of the statistical problem in detail. And in particular, this means that actually under those same geometric insights, you can also derive. Geometric insights, you can also derive results for optimization methods, right? So, the MAP estimator, which is just the posterior mode, can actually also be computed as a corollary of the things we just went through in polynomial time. Okay, so here you just discard the Gaussian term in the update. You just follow along the gradient of pi tilde, and then you get then you get a spectral gap for the iterates of this gradient descent towards the map estimate okay and in particular the map estimate also recovers so you can also recover the ground truth by means of optimization as a result and that was it just to mention some follow-up things that Some follow-up things that numerics we've started looking at that with some people here in Yusuf's group, and also Jan and Richard have follow-up work on this. And yeah, there are some really interesting questions, I think, that one could look into, like initialization, as Jan mentioned, is key. And then, you know, other models in which Other models in which these considerations can be made outside of the PDE setting, I think, that one could look into. And yeah, thank you very, very much for listening.