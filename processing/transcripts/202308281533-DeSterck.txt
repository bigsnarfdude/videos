In the speed environment, and thanks for the organizers for the inviting me. I'll talk about asymptotic convergence speed of Anderson acceleration. This is work, part of which I did with Yunri Her, who was mentioned before. She was my post in Wadelu for two years before moving to Hensgroup for another two years, I think. And now she's starting an assistant professor position at the University of Minster. And then over three seconds. And then Olvik Trisik is my former PhD student and now my postdoc. And Adam Smith is a master's student at the University of Washington. Okay, so Anderson acceleration is a very classical and old method. So the papers appeared in 1965. So basically you have some kind of fixed point iteration, like some iterative methods. Iterative methods, you try to find the fixed point. Q is the iteration function, and then you're going to try to accelerate the convergence by basically taking linear combinations of differences of this q of x k between previous iterates. And then in every step, you're going to try to find optimal acceleration coefficients delti by solving a small least squares problem of this type. Of this type. So that is like the standard, most basic form of understone acceleration. This r is double residual, so it's x minus q of x. And so basically this is pictorially. If you have your q of x that doesn't really point in the right direction to get to x star, then you you keep a history of this q of x k values and then you take those linear combinations and instead of the green steps you And instead of the green steps, you try to compute the better updates that hopefully will get you faster to this time. And this is typically implemented in a windowed way. So M is a window size, so you have a sliding window, so it's a bit like LPFTS, and unlike we started GMRS, where you have your evolving window. Okay, so that's the algorithm. 9065. 1965 and also has been reinvented by different people in the how many years is it? Almost 60 years between then and now. So it has been reinvented many times. So it's used a lot. So answer acceleration is known empirically to accelerate asymptotic convergence. For example, electronic structure calculations and other applications in alternatingly square shifting of canonical density decompositions or Canonical tensor decompositions, or for example, also ADMM optimizations for machine learning. And for a fine iteration function, so when the Q of X is just a matrix M times X plus B, and infinite window size, it can be shown easily that Amerson acceleration is essentially equivalent to GMRS with infinite window size. And this has been formalized in this paper, for example. And like I said, A is almost always window, so it's not my style. That's why it's not my standard. And for finite window size, there is basically no theory to quantify how much the conversions really accelerate. So despite the method being solved and being used widely, there is not a lot of understanding. So like with other methods, we typically see improved convergence for the first few iterations, but then also asymptotically we see better rates. And there is very little the And there is very little theory about that. Of course, the reason why this method is popular is because it works also for non-linear recognition, functions. And so it's normally used in the non-linear context, but it's also useful to think about how it would work in the linear case. Okay, so there is some convergence theory, and actually it was only in 2015 that in this paper by Tolstoy and Kelly, they showed that you get root linear. You get root linear convergence with a factor C that is not worse than the convergence factor of the underlying fixed point iteration. So that doesn't show there is acceleration, but at least it shows that there is convergence with a rate that is not worse than the original rate under certain conditions. And then there's some other work that tries to somehow quantify how much you gain, but really asymptotically there But really asymptotically there is not much at all. One example is for example this alternating these squares where you have a canonical decomposition of a rank R decomposition of a tensor and you try to compute these A, B and C vectors for this low rank decomposition and a standard method is alternating squares where you have a guess for the B and the C vectors and then Yes, for the B and the C vectors, and then solve a linearly squares problems for the A vectors, and then keep the A and the C constant, and then you find the optimal B, then the optimal C, and so on. And alternatingly, squares is typically better than steepest descent. And then if you apply understone acceleration to that, you can actually improve the conversion speed substantially with non-large costs. And this is actually an example that. And this is actually an example that converges relatively quickly even with standard ALS. There are also many real problems that converge much less quickly. So these problems can be really ill-conditioned and so this acceleration can really be useful. Another fixed point iteration that you can apply this to is ADMM. So if this is the standard form of ADMM with these alternate Form of ADMM with these alternating steps. Then you can view one full ADMM step as a fixed point iteration. And basically, you can then apply Anderson acceleration to them. And that is something that we've also done in some of our papers. So this is our paper we have in GSE in 2021. And then we see, for example, for the LASA problem, that if you just do ADMM for the particular problem that we looked at, you need a lot of it. That we looked at, you need a lot of iterations, and then if you do Anderson acceleration with memory size one, two, or three, you can really improve the convergence. This is a problem where you get a linear linear convergence behavior and the root linear factor really improves. But as I said before, there is no theory to actually predict or compute what the improvement would really be in that area. And the problem, I think, is really the windowing. Really, the windowing nature of this because even in the linear case, this is close to GMRS, and for GMRS, there is some conversion theory, or there is extensive conversion theory. If you have infinite window size, or if you have restarted GMRS, there is some convergence theory, but for windowed versions, it seems that it's really hard problem. Now, just to give a bit more insight into where this comes from, you may see that immediately This comes from, you may see that immediately, but you can think a little bit as this acceleration process as finding an optimal affine combination of the xk hat, sorry, an optimal affine combination xk hat of your previous approximations, and then use the next iteration, the next value as the q of lab xk hat. And if you work that out and you do some linearizations, then you get exactly these two updates for once. Once. Okay? And so the affine combination would be, for example, if you have three previous iterates, then this theta would be the parameters in your affine combination where for it to be an affine combination, they have to sub to one. And so in 2D, that's just the line between those two points. In 3D, it's the plane spanned by those three points. And because of this condition, you can also eliminate one of these parameters and write it in these data. Parameters and write it in this difference form, right? That would be the same thing. And then if you say, okay, now we want to find this new xk hat such that the Q of that XK hat will go to X star faster. So the residual has to go to 0 faster. So then you can take the residual of this XK hat and linearize it, and then basically compute this theta i's such This theta i's such that this linearized residual is as small as possible. And that gives you the dispersion. And then once you have those coefficients, then you can do the update by saying, again, by linearizing this Q of this xk hat, and that gives you this. And that's basically this combination of two formulas. So that's one way to see how the algorithm works. Because of this linearization, we can kind of Because of this linearization, we can kind of expect that it probably will work better when the Q, the original Q, is differentiable. And also, because of the linearization, we will need some kind of a globalization mechanism like Newton. When we're far away from this X star, this solution that the least squares problem gives you may really point in the wrong direction, and you have to somehow detect that. That is easy to do if the problem that you solve and the fixed point algorithm that you use is used. One algorithm that you use is used to solve an optimization problem because then you can, for example, do some line searches or things like that. But globalization is in many cases useful for these non-linear problems. So there's links with other methods. So this is Anderson acceleration, the Standard method. Then in around 1997, these two authors derived a non-linear version of GMRS, which turns out to be almost the same, except that instead of be almost the same except that instead of the q of x k minus i k minus i they write xk minus i and because we are looking for a fixed point you can kind of see that they will both be consistent in the linear case and and so they're both methods that turn out to perform very very similarly right and they both reduce the gms essentially gms when q of x is linear and of course there's also links with things that are known in optimization like nestor of acceleration also momentum methods Also, momentum methods for SGD in machine learning, where you use previous iterates that you keep track of, and there you only use one previous iterate, here you use n previous iterates. This method has also been derived or is also known under other names like DIIS or Pool A mixing, depending on the application area, but they're all very closely related. Okay, then there is also a link with nonlinear preconditioning. So, in some sense, if we go back to the first slide, in some sense you can also view this as a non-linear peak and conditioning iteration for a GMS-like optimization or root finding method. And one way to see that is as follows. That is as follows. So, if you have an unpreconditioned linear system and this is the residual, then gm rest can be viewed as a root finding method for b minus ax equals zero. And you have some kind of an inner iteration, which is just residual iteration, if there is no peak in the chat. So, the new x is the old x plus the residual. And then if you look at the residuals, then you can see the new residual is i minus eight times the old residual. And if you then do this uh recursively, then you get the trial space. Then you get rectal space. So that is TMRes applied to as a, you can view TMRES as an acceleration of this original residual iteration. And then when you, for example, you left precondition, you have the left precondition to be zero. And then if you just apply this GMRS to this equation and this zero, then you get this update. This is like a preconditioned update. Update. And then basically, if you look at the left preconditioned residuals of this iteration, then you see that there is this relation with the PA. So then you get your Kraal space with powers of PA. So in some sense, normally we think about GMRES as the precondition in helping to accelerate GMRES, but you can also view GMRES as a method to accelerate this. Method to accelerate this preconditioning iteration. And instead of having a linear preconditioning iteration, you can now have a non-linear preconditioning iteration, like xk plus 1 is q of xk. And then you can just apply something very similar to this TMS approach to accelerate that. So that's just another way to look at this. And once you understand that, then you can also apply many other methods to solve non-linear. Nonlinear equation systems or to solve optimization methods, you can also apply those to accelerate these basic fixed-point equations. For example, you can also use a non-linear conjugate gradient to accelerate this, or LBFTS to accelerate this, or a Nestrovs method to accelerate this. Or in the case of Nestrov's method, instead of accelerating gradient descent, you're going to accelerate this somehow preconditioned version of gradient descent, which is this x-capus. Which is this xk plus one is q of xk, right? Which gives you a better update than the send. So those are all related methods. And then for example, in some of our earlier work here, we do the so-called Tucker decomposition, which is another type of tensor decomposition that you can accelerate with. So the basic iteration methods you can accelerate with LBFGS and, for example, also NCG. For example, also NCG with standard density decomposition. And there are related papers, for example, 2015 Simon Hugh paper by Brunertal, and also some ideas of non-linear preconditioned new term. And you can also interpret the method as a multi-sigant method and also extend it further in those directions. Right, so basically there is just a whole zoo of acceleration methods, classical acceleration methods that Methods, classical acceleration methods that exist in this way. For the rest of the talk, I will focus on this Einstein acceleration and on what we can learn about its asymptotic convergence. But maybe I'll ask if there is any question so far. No. Okay. Okay, so back to the asymptotic conversions of wind notes AN. If you don't really understand what's going on, then it's useful to look at the most simple case. And one thing we can say, okay, let's look at the And one thing we can say, okay, let's look at those affine iteration functions. And like a very simple one, where m, which is this iteration matrix here, so there is no right-hand side, so the solution will be simply zero. But then for this upper triangular matrix, the speed of convergence will be clearly, the worst case speed of convergence will be the largest eigenvalue, right? So in this case, it will just be the two-states. And for almost all random initial conditions, you will get. Random initial conditions, you will get a two-third speed of convergence, except if you're in the direction of this other eigenvector, you will get a one-third. So that's easy to understand. So now when we apply Amazon acceleration to this, some strange things that I think happen. And as far as I know, we don't know what the worst case, even for such a simple 2x2. Even for such a simple 2x2 system, even with a simple upper triangle matrix, we don't know what the worst-case convergence factor is. So we have to define a little bit our convergence factors. So if we have a sequence of points that converges to x star, then the root linear convergence factor of that sequence is given by this. So it's basically the kth root of the difference between the kth iterate and the point. Case iterate and the point we converge to, in, for example, the terminal, the limit of that, limit superior. And if that value is between 0 and 1, then we say that we have our linear convergence. And it would be our superlinear convergence if that were always 0. And so that is roots, our linear convergence. And then if we look at the method with many possible initial guesses and many sequences, and also And many sequences, and also possibly more than one fixed point, then we can define for convergence to a given root x star the worst case root linear convergence factor of the sequences over all possible initial guesses, for example, that converge to that x star. And that's what we call the worst case linear convergence, root linear convergence factors of the methods when it converges to. Of the methods when it converges to a certain point next time. Okay? And so then there is this very classical theorem again that says that if you have a fixed point iteration and the Q is differentiable, the iteration function, then this worst case, so it's differentiable at x star, then the worst case linear convergence factor is just the spectral radius of the Jacobian of the Q function evaluated at the x star. So that's the easy case. And sure enough, if we again go back to this very simple problem, then the spectral radius of the derivative of this will just be that matrix will just be the 2 thirds there. And so this is all very easy. If you then take a bunch of random initial guesses in the plane, then all of these trajectories will converge to this two-thirds. will converge to this two-thirds value for that for that matrix, right? And so this is the sigma k that I put here is the kth root of x star minus xk, right? And those are the green curves. But then if we apply American acceleration with memory size 1 to this very similar problem, then we see that depending on the initial guess, we get very different root linear convergence factors. We get kind of like a spectrum, and it seems like there's even Spectrum, and it seems like there's even a gap on that spectrum. And maybe that's not the difference here now that what do you choose to differentiate? Yeah, so what we did is we now apply Anderson acceleration, this algorithm, to that xk plus 1 is m times xk with memory size 1. So it's just by keeping one memory thing. One memory thing, and we solve the linear disquiet problem in every step, and then okay. And so we see, well, we see all of these different root linear convergence factors numerically, which is interesting. But maybe the good thing is that it seems like there is a worst case, like if we do like a Monte Carlo thing, that there is this worst case that is really much better than. Than what we get from the metaphy itself. And so when we apply Amazon acceleration to this very basic problem, then we see what people also observe in real life big non-linear problems that you do get successful improvement. But the problem again is that we cannot compute this this gap, this difference. So we don't know how to compute that. Can I just say anything locally? Locally within the window? Can't you say anything? Like if your initial condition is close enough to the X time? No, I'm just curious to see whether you can... Yeah, I'll get to that. So there are some bounds from the residual from one iteration to the next, but then it turns out that it's kind of very irregular. So there are many iterations where there's almost no progress, and then there is regular iterations where there is a lot of progress. So bounce from one residual to the next. So bounce from one residual to the next and whether this could help you get just topics the four tenths that you see the yeah well we'll get into some some of that but before you go on is there is there some sort of color coding or going on with the additional condition no that is just matlap that puts them in all different colors so yeah another thing that we Another thing that we find is that this beta k for all of these different profiles, it seems to oscillate. And even when the error is very small and you actually converge, it keeps on oscillating. And that's a bit strange also, right? Because if beta k is the solution of that least squares problem, so why would it keep on oscillating? Even if you converge to okay, and so one thing that we And so, one thing that we set out to do is maybe try to understand this a little bit by looking at the fixed-point analysis of AAM itself written as a fixed-point method, right? So, the first thing you can do is, well, you solve this dispersed problem, so you can write this vector of beta coefficients explicitly as this. Now, of course, you also have to think a little bit, because as you converge, these residuals become smaller and smaller, and you take differences. And smaller, and you take differences of residuals so that will become more and more ill-conditioned. So, and it could even happen in certain examples that two consecutive residuals are the same, and so you have a difference of residuals that's zero, and this may not be full rank. So, you really have to think what to do in the case that this is not invertible. And one thing we could do is to use the servo inverse here. So, in general, you can write that as the server inverse, which means if the least pressure problem is Which means if the least pressure problem is degenerate, you're going to look for the minimum rank solution if you make this choice of pseudo-windows. And if we do that, and then for example for m equals 1, so then the beta is simply a scalar. And this is the formula, right? And you can kind of see that this becomes ill-conditioned because you get 0 times 0 divided by 0. times 0 divided, or 0 times 0 minus 0 divided by 0 minus 0 squared. So that's not very good, right? So does it even work? Well yeah, people use this all the time in these big physics codes. So it does work. So what's going on here? Now one thing we can say, well, when Rk is Rk minus 1, if we think about this If we think about this interpretation of the pseudo-inverse, then when you get zero times zero, then the pseudo-inverse of that is just zero again, right? So we just take beta equals zero in that case. That's one choice. And now what we did is you can actually write this AA1 iteration where you have xk plus 1 as a function of xk and xk minus 1. You can write it as a fixed point iteration. As a fixed-point iteration, a one-step fixed point iteration by just augmenting the system and considering two consecutive intervals, right? So this zk plus one is some function psi of zk, and zk now is like a vector of double the size of xk. And then by itself, it's a spawn iteration, and it has this psi iteration function, which is just this, right? With this beta, and maybe also this condition. And now you can. Condition. And now you can ask the question: well, if this psi were differentiable, then we could just go back to this very classical theorem, take the derivative, and look for the spectral radius of the derivative, and then we know everything. But of course, it's not going to be differentiable, right? Because otherwise, we wouldn't see all this funny behavior. And so that's what we. And so if you look at, first of all, the beta also. First of all, the beta also. So the beta is not continuous at the solution, right? Because you get this zero of zero thing. But if you actually look at the psi, which is this iteration function which has the beta in it, but multiplied by this, then that is actually continuous. And it's even Lipschitz continuous, and it's also directionally differentiable. So this fixed-point iteration method, in some sense, is reasonably robust because you can take directionally. Rob both because you can take directional derivatives of this, but no derivatives, so it's not differentiable. Because if it were differentiable, then the problem would be solved. So we're in a case where we have a fixed point iteration where the right-hand side function is directionally differentiable, but not differentiable. And the beta itself is also not continuous. And yeah, this is just some basic calculus, really. Calculus, really, analysis. Okay, and then yeah, you can derive some results. You also know explicitly what that directional derivative is, if you want. And so, what does this tell us? Well, first of all, because this size is not differentiable, we don't expect behavior like this, but maybe it can be behavior like this, where we have all of Behavior like this, where we have all of these different root-limit convergence factors. And then the other thing is when xk converges to x star, if beta is not continuous, then beta can keep on oscillating. Because if beta was continuous, then it could not keep on oscillating if xk goes to x star. So that gives, that teaches us a little bit. Not a lot, but just a little bit. And then we can do the same kind of analysis also for AN. So we just write it as a big augmented vector. Augmented factors, and then we get similar results. So, beta is not continuous, but the psi is still directionally differentiable, but not differentiable. And then this is maybe a little bit of an aside, like there are some papers, so this method, this Armstrong acceleration also is gaining some interest in machine learning. And so, for example, in this paper, they say, well, you should never do Answer or this. Should never do answer or this kind of acceleration without regularizing. But then, of course, if you regularize with a certain factor lambda, then you get fast convergence up to that level. But then, as soon as you reach that level, then you go back to the original slow convergence. So, it's not very useful to do that regularization. And also it's not it's not necessary, I think. So if you as long as you solve the least squares method in a num uh the least squares problem in a numerically uh stable way. Numerically stable way, like using rank-revealing QR or related methods, basically, just also what is implemented in MallOp, then you never have any problems with solving this first problem. Okay, but we still don't know how to compute that gap, okay? Okay, now one thing you can Okay, now one thing you can do is you can do this iteration, but basically freeze these coefficients instead of solving a least squares problem in every step. And if you do that, then of course you get a much simpler iteration. It doesn't have that singularity of that right-hand side. It's actually differentiable. And then you can actually compute the best possible beta, constant beta factors, constant over k, such that the spectrum of the original fixed point iteration, which is the blue. Fixed point iteration, which is the blue points here, like with the spectral radius close to one, so it relatively close to one, so it will converge slowly. If you then compute the optimal betas, then you can basically improve the spectrum by doing these multiple steps and basically get a better method. Now, of course, to compute these optimal betas, you need to know what the solution actually is of your problem, and you don't know that in practice. So, this is not very practical, but this is just. It's not very practical, but this is just a situation where it would be easy to understand or compute the acceleration that you get. But you can, of course, not do this for the actual method where you solve linear disquires problems in the system. Now, the problem that I showed you was just a simple upper triangular linear problem, but that behavior really actually seems to be quite generic. Seems to be quite generic. So, if you, for example, do a 2x2 nonlinear problem, then you get very similar behavior. So, for the method just by itself, it's differentiable, so you get this single spectral radius of the Jacobian. But here again, we have this spectrum, and we also have the oscillating betas. And then, also, if you go to larger linear problems, for example, these are 200 by 200. Are 200 by 200 with like a certain aim value distribution? Then you also see, for example, for AA2 that you really get dependence on the initial condition. And also for AA3, AA5, AA8. And then of course you can also do a restarting version of Anderson acceleration and you get similar. And you also get yeah, and then you can wonder, okay, is restarting actually better or worse than Better or worse than windowing, and of course, there are memory considerations. But one thing is that if you do windowing, then you always take the previous M iterates and you get better convergence than kind of building it up starting where on average you only use half of the m. I don't know how significant that is, but that is something that you can observe. Okay, and even, for example, for lice problems, you still have to. Example for large problems, you still get an oscillation of the data. Okay, so we've learned a little bit, but there's still a lot that we don't know. We don't know how to compute the asymptotic grades. Let's think a bit more about the linear case. And if you look at the linear case and work it out just by inspection, you can see that A1 is actually, or AM is actually a Fryloff method, right? Because you have powers of those iteration matrices. Matrices. The only difference is that you don't get the optimal polynomial like in GMRES, but you get some polynomials, a polynomial that you build up by local optimization formats. And then you can have recurrence relations. And one interesting thing is that because of the windowing, there is always a power of m that kind of keeps on working and keeps on increasing, working on the original R0. On the original RZL. So basically, the window method will never kind of forget what it learns about components that may converge slowly because you always have this m to the power s plus 1 where s will always increase. So that can kind of point out in certain cases it may be beneficial to do this rather than the restarting. Then there were some other things we were able to prove in the linear case, for example, that there To prove in the linear case, for example, that there is a lower bound of minus one under certain conditions for the coefficients. You can also explicitly write the new residual as a function of old residuals without the betas. We can basically just eliminate the betas or write them as functions of R. And the same with these polynomials. So we have explicit formulas. And so we were hoping. And so we were hoping deriving those results will maybe help us in the linear case to learn something more about the convergence. Okay, then another thing, maybe I'll show these things in terms of the example. So again, this is the simple example, but now rather than choosing random points, we take a grid of points, and then we see that there is actually a nice structure in this initial condition plane. This initial condition plane, so all the initial conditions that are along a ray, they have the same root linear convergence factor. And so there is a region where the root linear convergence factor is bad. That's this 0.4 region. But then there's also regions where the root linear convergence factor is much better. And if you go in the directions of the eigenvectors, then you will um the condition in the direction of the eigenvectors, then you can easily show that you you converge uh into steps, so then you can find it. Into steps, so then you can find the components. And then there's also this question of this gap here. It seems like there is almost like a discontinuity in the surface, almost like a jump. But again, that is just numerical evidence and you have to be careful with accuracy and things like that. So there are no definite answers, but I guess more more questions. So I was wondering whether it's just an inc it's accidental that It's gentle that uh that the red curve is about about uh the square of the green curve at the top. Yes, I'll show some more plots when we change the lambdas on this diagonal. And then there seems to be a pattern, but there is no simple relation. So this is what happens in a raw linear case. So, this is what happens in the nonlinear case. So, we get a similar kind of thing, except that because of the nonlinearity, we get these. So, there are still preferred directions, but it's much more complicated. Okay, and then your question about a result, a local result, so the next residual compared to the previous one is bounded above and below by the minimum and maximum singular values of m and then square root of a beta. Square root of a beta factor, which is kind of related to the geometry of the least squares problem, where the y is just the ratio of the sizes of the two previous residents, and the phi is the angle between the two previous residuals. And so with this beta, and then you have this kind of a plot, and if you're on the red curve, then beta is one, so then basically you won't do better than the fixed point iteration by itself. That's this yellow part. That's this yellow part. But then, if you're away from this red curve, then you really have a lot of potential for it to move. And so, that gives you a little bit of an idea about what can happen. And this is an even simpler example where you have a diagonal. So, even in a diagonal case, you cannot compute that gap for 2 by 2. But then you see typical behavior is that you have very slow convergence, similar to the fixed point method by itself, but you have like a regular drop that is much faster. Like a regular drop that is much faster, and it's this regular drop that will determine the linear convergence spectrum. It's interesting that with the simpler values there that counts rather than the eigenvalues. Yes. Why is that? I think it's just like the two normal collaborations. Probably in the in the when you talk. When you talk about the eigenvalues, you don't take into account the eigenvectors, but here you do. Yes, you do. And this is, of course, I think, similar to the case of GMRES, where the convergence also depends on the eigenvectors and the monomality. And the convergence will very much be dependent on the case, just like in GMRES. Okay, and then we find some very interesting, like almost periodic behavior. So, this is like a discrete dynamical system, and we find something like a limit cycle. And this limit cycle, part of it is close to this curve where there is no programs, and part of it is far away from it. And so, because this repeats many times, if you then look at the root linear behaviors, you get a real improvement. So, the result. Movement. So there is some funky, loneliness, discrete dynamical systems going on. And so there might be theory out there about convergence of this kind of iterated discrete systems when the function is not differentiable. And that is what we see here. But we haven't been able to find anything. Okay. Now we try to go even simpler, right? We try to go even simpler, right? So instead of two by two upper triangulation, what about just diagonal? Now, it's still too hard for windowed, but we had some kind of a breakthrough in the analysis of some sort, because if we restart AA1 instead of window it, then we can actually compute the root convergence factors for any initial condition and also compute the worst case factor. For the restarted version, we can compute it, but not for the window. We can compute it, but not for the window. These are results that, I mean, things we're actually still working on, so we haven't written that out yet. And then, okay, can we make it even more simple? Well, yes, we can make it just a multiple of the identity, but then it conversions into iteration. So that's the same. Okay. So then we're stuck, right? So this is too hard, and this is irrelevant. What can we? And this is irrelevant. What can we still do? Oh, there's one more case when we do lambda minus lambda. That's not completely trivial, even though it may look trivial. It still has this kind of behavior, very surprisingly, this very simple system. And so what we can do now is, in this case, actually, the restarted and the windowed, they give the same iterations. And because we knew in this general case we can compute everything for the restarted one, we can now also compute everything. Can now also compute everything for the window one in this case. Now, the only problem is that in this case, there is no gap, because in this case, the worst case is always lambda for AB1. So that's a little bit of a... So this is the... Yeah, so if you change lambda between minus 1 and 1, so this is the convergence of the fixed point by itself. And the worst convergence is always the same. But at least we can now compute, for example, the average convergence. Compute, for example, the average convergence overall initial cases, and you still have an improvement on average. So, in this case, you have an exact bound of an exact evaluation for the ratio of the similar values is the same. Yes, I think that's basically how we get everything. How we compute everything. Now, that's not the end of it because when we The end of it because when we go back to this case and we have the lambda one and the lambda two, so the yellow curve is basically what we get from the fixed point iteration. And then the red curve is what we get from the we started AA1 where we can compute everything. So we can compute the functional form of, we know the functional form of this red curve. And then we find numerically that the windowed one is always better. So basically we have a bound now. So basically, we have a bound now for this gap in this very simple case, except we cannot prove yet that the blue is always below the red. So that's the only step that's missing. But numerically, we find it's always below the red. So I think we're close to, at least for this most simple case that we can come up with, to find an upper bound for this red that is better than the blue. But the missing step is that we have to prove that the blue curve is evolved. So that basically the The window is better in the worst case than when we started. So, I guess those are some problems we're working on. And I find this fascinating that for such a mode of method and for such a problem that seems very simple, like two by two linear diagonal, that it's not possible to compute this. And it should be possible, I think. So, if anybody has an idea, then please let us know. And then please let us know. Okay, so conclusions: numerics show that wind.A1 improves the worst case factor and depends strongly on the initial condition and the beta-osolate. And when you look at this differential properties, then you see that the reason why bin is maybe less than this uh more in a more complicated way than the simple cases is because this kind of singularity in that iteration function, right? So it's not differentiable. So, it's not differentiable. And we've looked at a linear case, tried to get some properties. And even in very simple cases, it's hard to compute, but maybe in the most simple cases, we will be able to compute something. And then I think the message is that even if you go and apply this to very large nonlinear models, whatever you learn in the linear case, we see similar things in the non-linear case as well. So that will be a first step to try to understand what's going on in the case of the On in the case of the real problems where people use these methods. Okay, so I think I'll finish then. Yes. So you showed the interesting plot for Henderson acceleration one in the past, where it was a very just Where the it was invariant just based on the starting factor. Yes, in the linear case. Am I right in thinking that in the linear case, if you do a history of a little bit longer, it's going to be a function only of the subspace spanned by the previous n residuals? Possible. We haven't really looked at that case or thought about it. So I think you've got, you're taking this linear combination of residuals, you're saying what the same. Combination of residuals, you're saying you want the same limit of combination of errors. If you move around the residuals, so you move around the points, but you're getting the same space by the errors in the residuals. I think you would probably get the same. Similar properties. Yeah, yeah, yeah. Exactly. That's a good suggestion. That's a very stupid question. I've just woken up. So, when you do the first step, this bit seems a bit tricky to me. So, you just run these. Seems a bit tricky to me to just run the iteration together. Yeah, just the original Q iteration. No, so we also, in some of our papers, we looked at, you could also take two random initial guesses and then work from that and then we find similar behavior, like not necessarily much benefit or much detriment from doing that. Can you I was just wondering if you could cook up a second guess to get you some behaviour that you're trying to predict in a particular Republic that way round. At least also does it that way around? I think so. You could probably do that, but then I think in the cases that we looked at, the problem that you have to solve then, especially in these very simple cases, is as hard as the original problem to find like a desirable second initial guess. But there may be some things that can be done there. Hard case of the two by two triangular and the circle. What happens if you just have two like a defective matrix basically? Values that are the same on the diagonal, but a non-zero value at the top. So you don't have the gap. A Jordan block, sorry, yeah. I'm not sure if Jordan. Because that's kind of like an interesting thing to establish or not. Okay, that might be good to look at. I'm not sure if, I don't remember. To look at. I'm not sure if I don't remember very well if it looked like that. Questions? Something on the literature because as I see it there, so if you say you accelerate ALS, but does it qualitatively change the behavior of ALS course? Or would you say that, I mean, there is slight improvement in completion? Or is it do you see different convergence in the shape of the source? So I think you get a better asymptotic rate. And with ALS, for tensors, there are these, like sometimes people observe these plateaus like swamps and things like that. But it seems to help with a lot of those things. And it normally converges for the same initial guess to the same because those are non-convex problems, right? So you can have m many local minima, but sometimes you get different local minima. I can have one. If it is a better or not, I'm not sure. But you do get better asymptotic grades and you typically get them much faster. Right, I think that's thank you answer. Yeah, thank you very much. 