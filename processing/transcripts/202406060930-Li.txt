So, are you going to give me five minutes notice? Okay, so I'll also kind of switch in case I go over time. Okay. So, firstly, I want to thank the organizers for making this workshop possible, especially I feel as a PhD student who just started, it's really eye-opening to hear different perspectives and also to learn from everyone. So, I want to thank everyone for sharing your. So, I want to thank everyone for sharing your knowledge generously. So, in the past few days, we have seen different aspects of quantum compelling. And if we put everything into perspective, we realize that we are all trying to answer one question, even though we're probing from different angles. That is, we're trying to explore the potentials of quantum computing. No matter if it's for verification, classical simulation, or like trying to understand different groups of unitary operators, we're all trying to see what can we do about quantum computing. Trying to see what we can do about quantum computing. Now, I want to thank Niels for giving such a great introduction, so it saves a lot of my work. So, basically, we'll always start with a very powerful algorithm, but all of those are on the very abstract level, and we don't really know what to do with it. So, this comes into, we need to do some resource estimation and to quantify how expensive it is or how reliable it is to implement the algorithm on a specific quantum hardware. Now, based on this data, we want to do resource estimation. Want to do resource estimation or optimization. So, um, today I wish to tell you one of our recent work. Oops. And it is based on like the our case study on IBM's quantum computer. So we want to improve the fidelity of CNOT circuits when we route them on the NISC hardware. Now you may wonder: well, we have been interested in more general quantum circuit like More general quantum circuit like Clifford plus T, CNOT plus phase, why do we only do CNOT circuit here? Well, there are two reasons. Firstly, if you look at the encoder circuit, CNOT play an important role. And there are also a lot of very important quantum algorithms that use a large sub-circuit that is composed of CNOT gates. Secondly, quantifying the noise in a quantum circuit is not easy. So we can think of our work as a first step towards quantifying the noise level and like the reliability of a And, like, the reliability of a noisy scene, a quantum circuit. So, I want to put quantum compiling in the context of NISC and tell you what we are talking about. So, suppose that you have an algorithm and, oh, by the way, I forgot to mention one thing. What is NISC? Sorry, I moved a bit too fast. So, NISC is short for noisy intermediate scale quantum. Now, you may wonder, like, nowadays people are talking about air correcting qubits. Why are we still care about this? Well, that's because admittedly. That's because, admittedly, in order to realize some very fancy algorithms, such as Schwarz algorithm, you need millions of qubits. This means you need like a larger scale quantum computer. However, experimentally, this takes years or decades to realize. But at this point, what is available to us is a NISC hardware. And this is usually stand for a quantum device that has hundreds of qubits that are noisy and have limited coherence time. Now, how do we leverage this? Now, how do we leverage this current hardware is a very important problem for us because this will help us to explore the potential of quantum computing at this stage. So, suppose you have a quantum algorithm, let's say it is based on some quantum chemistry and it is specified by an instruction set. And you want to translate this algorithm into something that we understand, well, the computer understands, like using Kids, Kids, Circuit, and Clipper. Well, the issue is, even when you have this logical You have this logical circuit, it is still not physically executable. In order for it to run on an actual device, you have to map it to a quantum hardware. However, the current quantum hardware, like superconducting circuits, have a lot of restrictions. For example, your qubit have very low quality. This means it's very prone to error. And also, if you look at the grid that you're working with, like the actual physical grid that is connecting by the physical qubits, they have limited. Physical qubits. They have limited connectivity. You're not doing all-to-all connection. And also, finally, you have limited gate set, and sometimes you need to discuss, like, you know, how do you leverage them to mitigate the errors, depending on your error model. And then on the last level, you're mapping like an executable quantum circuit to machine code. And those are physically controlled pauses that will allow you to actually carry out the computation. So for today's discussion, we're only going to focus on the second level. We're only going to focus on the second level. This means that we're looking at a quantum algorithm that is entirely logical and we're going to map it to the hardware that is based on IBM's quantum devices. And at the end of the talk, I have some questions for the audience. So if no one have questions for me, I have questions for you. Okay. Yeah. Are you on here? Oh, yeah, that's. Thank you for this question. Well, in order for us to Well, in order for us to define a cost function, we think it's better to tailored to a specific hardware. And most importantly, we need to use IBM's calibration data. Well, compared to other hardware, IBM calibrated your data twice, like I'm talking specific to Ali, you calibrated your data twice a day. So we always have very fresh data that we can use. And also, like, we have very complete information about the hardware, like up to 127 qubits on the Eagle processors. So, I think based on IBM, it's the most convenient. The most convenient case study for us at this point. I think we can have to be optional. Why not be more general and then we can instantiate an idea? Oh, well, because, well, firstly, this is a good point. We should think about this. But when we are doing benchmark, we need to target on a specific device. To target on a specific device because we need to use their noise profile. So that's why we start with this particular hardware. Oh, and also I want to mention that this kind of problem about miscrouting is not new to our audience. For example, Arian and Alex and Vlad and Priyanka and Simon and Benua have all worked on this problem. So this is a very important problem for our community since like a long time ago. So let me first explain what we mean by quantum. Let me first explain what we mean by quantum circuit routing. So, when you're looking at this logical circuit, you see that you can actually do arbitrary operations like C naught gate on any two qubits. However, is this really reasonable? So, suppose you have C naught acting on qubit two and three. Well, if suppose that your logical qubit is mapped exactly to, you know, identically to the physical qubit, and you see that if these two qubits happen to sit in the IBM architecture and they are connected by this. And they are connected by this edge, it means that coupling them is really cheap. So you can reliably perform a CNOT operation in this case. However, if you're looking at another CNOT gate like this, the second one, then the qubit, physical qubit one and three, they're not connected. So coupling them will cause a big issue because you will have a lot of noise. So in order to do this, you need to move the logical value stored in one quantum register to an adjacent quantum register. For example, like you move to Register, for example, like you move to one to two, like logical value, and then you can perform the operation. But the issue with this is you're inserting a swap gate, and one swap gate is equal to three C naughts. So over the past few days, we see that, well, although, asymptotically speaking, you don't care about the constant, but when it comes to practical implementation and benchmarking, this constant becomes non-negligible. Therefore, we want to find a better way to do such kind of routing decision. Decision. So, for our contribution, we did two things. Well, if this is a TLDR of today. So, firstly, we designed a cost function to quantify the reliability of a NISC circuit, meaning that it is a quantum circuit that is executable on a NISC device. And secondly, use this cost function, we instruct our routing algorithm to pick the optimal choice at each step so that when we derive the NISC executable circuit, it has the improved reliability. It has the improved reliability and the reduced gate count. And this work is based on Arianne and my work in 2022. So basically, we generalize the connectivity-aware C-naught synthesis algorithm permalocall to noise-aware CNOT synthesis algorithm and a permalocal. So, I'm going to first tell you how we quantify noise in this device. So, let's take a look at IBM calibration data. So, in this case, you have 16 physical qubits, and every vertex corresponds to a And every vertex corresponds to a quantum register, and we label them by 0 all the way to 15. And every edge corresponds to some coupling between this register. But you'll see that every edge has different color. That's because different edges have different noise level. So the darker it is, it means that the more reliable it is to execute, like a CNOP between these two qubits. And the lighter it is, it means this is less. It means this is, you know, like less reliable. And we also have coloring on every vertex, and this corresponds to the head more error rate. Well, so I learned something new from Ali yesterday. So if you have questions, I'll get to that. But for now, let's just focus on only the CNOT error part. And you can see that basically we will use this calibration data map to find a noise profile for this device. And this is something that we're going to route on, like the hardware topology. Topology. So, in order to quantify the reliability, let's look at what has been done previously. So, usually there are three approaches. The first one is you know that there are error rate on every edges. So, when you're walking your graph, you sum up the edge weight. But what is the problem? The problem is, if you're thinking about the reliability of your quantum computation, you're thinking about either the success probability. Thinking about either the success probability or the failure probability, right? So this means that this rate should be always bounded between zero and one. However, if you just naively sum up the error rate, what would happen? When your CNOT gate accumulate to a certain level, the accumulated rate would actually go beyond one. Then your quantification no longer makes sense because it does not satisfy this idea of a success probability. Or you can also look at the synthesized CNAL count. Synthesized CNOT count, but the issue with that is you may become too greedy. For example, if you're looking at two paths to do the routing, so one gives you five CNOC count and the other gives you 12 CNOC count. Well, you may say, well, from the synthesized CNOT count perspective, I should pick the five CNOT one because it's cheaper. However, what if this path had very expensive edges at each step? Then in this case, you're blindly only looking at the count instead of the actual accumulated error. Of the actual accumulated error rate. Therefore, we need a better quantification of the noise level in a computation. So, like, we, in order to conduct our research, we did a thorough literature review on the state of the art up to 2024 February. And as far as we know, no one up before us has provided accurate quantification of CNOT circuit gate error rate as a good cost metric for routing problems. So, we are the first people doing it. First, people are doing it. Right, so our quantification is based on the average gate fidelity. What is that? Well, this is basically just quantifying how close the exactly computed result is to the desired computational result. You can also think of it as how closeness your desired unitary operator and the actual implemented operator. So, all of this calculation is from Nielsen and Truang. And you'll see that in order to calculate. And you'll see that in order to calculate the fidelity, we can either think of the state factor, the state density operators, or we can think of like the evolved density operator acted on by some channel. So we're actually doing something really general. We're not just thinking about some unitary operation. We're looking at the error model that is represented by a quantum channel. And in our case, we assume a generalized poly channel to model our noise. So, how do we do approximation? So, how do we do an approximation of gate fidelity? Well, firstly, you may ask, why do we need to do approximation? That's because of the unscalability if you really want to calculate the exact fidelity of quantum circuit. Well, if you're looking, if you're, so I did not represent, I did not show the like the calculation here, but in our paper, in the appendix, we have a very extensive calculation showing that in order to find the average gate fidelity of your CNOT circuit, you need to calculate the super operator representation. Calculate the super operator representation of your circuit. Well, suppose that you have n qubit, then the super operator representation will scale exponentially with the number of your qubits. So if you want to route a large C naught circuit and you want to do this decision at every step, you always have to do this 2 to the power of n by, well, it's actually not 2 to the power of n, it's 4 to the power of n by 4 to the power of n calculation at each step. So this will very quickly become unscalable for qubits beyond 7. So this is the reason that we want So, this is the reason that we want to find a good quantification of the era in the CNAL circuit while maintaining the scalability that has that like that owned by other cost function that was naive, well, not naively defined, but intuitively defined. So what we do is we think of a C naught circuit as being composed of a sequence of a C naught circuit, each of which occupy a precise time size. Occupy a precise time size. And then, because we assume every signal circuit is noisy, we have an ideal operation followed by a noisy channel. And this channel is a generalized Pali channel. And then by calculating the fidelity for every channel and then like do the composition, we're able to find certain patterns saying that, well, when you are deriving the error probability of your noisy channel, you need to consider the dimension of your channel. This means that it's not just simply adding. This means that it's not just simply adding up the error probability, you also have to use the scale factor here, alpha. So, alpha, if you look at this, it is some number that is between 0 and 5 over 4. Now, what happens if we do not have this alpha, if we do not scale the probability? Well, it means that we're calculating the success rate of executing a C naught circuit. But from our observation of Study's Simpler Channel, we noticed that this is. Channel, we notice that this is too much of a simplification. We need to consider the channel dimension. So, this is why we introduced this scale factor alpha here. And then, yes. So, these channels, like the first one that we represent, I assume it acts on qubit 0 and q bit 3, right? Not on 1 and 2. Yeah, that's a very good question. Is it a depolarizing noise? Is it a depolarizing noise? Yeah, it's a depolarizing noise model. And actually, this is a good question. Thank you for asking it. So, we also have a simplification in this scenario. We assume that the errors happening only on, let's take the first CNOC as example. We assume error only happens on Q0 and Q3, and we assume identity on Q1 and Q2. And you may say, well, how could you do this? This is so much of a simplification. We also think so. So, therefore, we do the benchmarking, and our benchmark result actually. Benchmarking and our benchmark result actually show that this is a valid assumption to make because our cost function approximate the average gate fidelity really, really well. Yeah. The voice channel you consider always two cubit devoid channels. Yeah. Okay. And that's why it becomes attractable to keep track of it. Because, I mean, to make it more complicated, you could imagine those two first pictures are the same, but if one, the CNOTs are applied simultaneously. One, the phenotypes are applied simultaneously, and the other one they're not. And in reality, what happens on the hardware, there is some crosstalk. And so, really, like, think of that layer as a four-qubit moisture for the whole layer, which might be not complicated by it, but you can also kind of like what we did with one of the sun kind of analysis, which just set up this. So, instead of trying to run it for four possible pages, volume scale is measurable, we think it just deployed and then do one. Thank you. Thank you for the remarks. I'll keep that in mind. Okay. So basically, as I said, we need to compare our cost function with the actual thing that we're trying to approximating. So we basically use a previously defined. Previously defined a way to do the FK fidelity. And we find out, like, this is the way that you calculated. And this is the super operator representation of your channel. And then we say that, well, the probability of error of an error channel, like having this error, is one minus the average key fidelity. And we use this as our reference point. So we compare the cost function value produced, like the value produced by our cost function, with this reference point to see how closely we approximate it. How closely we approximate it. And then we, oh, and also, like, we're using IBM 7-qubit device to do the benchmark. That's because if you go beyond 7-qubit, like, the real average gay fidelity calculation become intractable, so we cannot get any result. So, um, here you can see these two figures. So, this one is using the root mean square error. Yes? How is your cost function different from estimated something? So, what would you say? Estimated. What could you define estimate? What could you define? Estimate? Oh, yeah, no, actually, yeah. Thanks for asking. Yeah, yeah. So our cost function is a scaled version of the average success probability. That's because we added this alpha. And this alpha is like a scalar that considers the dimension of your error channel. So if we're going to do like modification as Li and Like modification as Ali and Vlad suggested, this alpha will likely change because right now, like this alpha is based on you have you assume no parallelization of your C naught error channel and you assume identity on the ideal qubits. But if this is not the case, then we have to adjust the alpha. But we do believe that this alpha is important because our, you'll see, you'll see that actually in both figures, like, so let's look at the easy one, like the right one. So the green dots are the value produced by our cost. The value produced by our cost function, and then the and then the blue and orange. So, so this one is what you're talking about, right? The cost two. So, and then the blue one is the one that we just simply sum up the CNAL error rate. So, we derive the value in this figure by doing the following. You first randomly produce 100 CNA circuit for certain parameters, and then you calculate the real average key fidelity compared to. Real average key fidelity compared to using our cost function to calculate the values. And then for every data point, you do the subtraction. And then among all of these 100 differences, like absolute differences, you find the maximum weight. So this is a maximum difference between the exact output and our output. So you can see that our output is always like very close to the desired output, just up to 10 to the power of minus 3. But the others is up to 10 to the power of minus 2. To 10 to the power of minus 2. And so, something that we did not do actually in our research is we should actually take into account the error profile of the hardware that we are benchmarking. Because if your error is already like 10 to the power of minus 5, then this kind of difference, well, it's sort of like negligible. No, no, sorry. If your error is 10 to the power of minus 3, then this error is negligible. So, and in fact, this is the case for the benchmark. And in fact, this is the case for the benchmarking data that we have. We just didn't show it here. Does that make sense? Okay. And then for this, we're using the root mean square error. So this is a statistical method used very cumbling physics to show that how well your function approximate the actual fidelity. And the longer the bar, the better, like the worse the approximation. And because it's actually hidden here, the green bar, which is this guy, is really, really short for every data point. So because for every data point, For every data point. So because for every CNOC count, we generate 100 random circuit. And then for all every, and then we take the average of the like the cost for this for every data point. So you'll see that across all of the data points, our rooming square error is always like the lowest among all the other models. So we conclude that our cost function is the best approximated one compared to the other cost functions. And we have also done the And we have also done the comparison of circuits of width 6 and 7, but I just decided to skip them because they are kind of like a similar conclusion here. Okay, so what is a summary? We did two things. We compare our cost function with the other previously used cost function on IBM 7 qubit hardware, like the Nairobi architecture. And we use the calibration data as input to the cost function and to compare how well our cost function. And to compare how well our cost function approximates the average gay fidelity. So, we also compare this on other IBM backends, such as the Cairo backends, the Quadaloop backends, and we all get the same conclusion. So, because our cost function is compatible with different backends and different randomly generated CNAT circuit, so we conclude that our CNA cost function is a relatively good approximation with good scalability. So, this is the first part. So, this is the first part. Okay, so linear reversible circuit synthesis, because this is like a primer. So, I'm just going to go through very quickly. Usually, we represent the CNAL circuit as a unitary operator. And then, this is not very scalable because it's not like a 2 to the power of n by 2 to the power of n in terms of the qubit numbers. Okay, and then so what an alternative is to use a parity matrix represented. So, you can think. matrix represented. So you can think of a parity matrix as a bi-adjacency matrix representing the information propagation between the input cluster and output cluster. So because we know that CNA is entangling this input and output, so what you can do is you can just track, you know, how every input wire contributes in every output wire. So as you can see, this input and output clusters will give you a bipartite graph. And this is a biodjency matrix. Look, this is represented by a biodiversity matrix, which is this one here. This one here. So, in terms of the C-naught synthesis, we know that a parity matrix is a non-singular square matrix which can be reduced to an identity. This means that you can perform a sequence of Gaussian elimination on it so that you can reduce it to an identity. Well, in this process, you will output row operation. Since every row operation corresponds to a CNOT gate, when you concatenate the CNOT gate, you will get a synthesized CNOT circuit. Now, what Ariane did in 2022. Did in 2022 is we lose this restriction by saying that, okay, why do we really have to synthesize up to identity? Why don't we just synthesize up to a permutation matrix? This means that you can extract a bunch of CNAL gate from your synthesized circuit because those CNAL gate are used to put your logical value to the previous physical register. But if you don't necessarily do that, you remember it in your logical memory, then you can factor out the C naught and Can factor out the C naught and effectively reduce the synthesized C naught count. So, right. So, noise-over CNOT circuit routing means that you're mapping a logical CNOT circuit to NISC hardware while you want to account for the connectivity and the noise. And you want to reduce the synthesized CNO count while improving the reliability of your circuit. And since we used the cost function to quantify the reliability, we have everything that we need. So, now we can start doing it. So now we can start doing it. So, because I'm running out of time, let me, how much time do I have? Okay, so let me go to, oh yeah, so let me go to the part where we use the cost function. All right, so okay, so here is a TLDR of our center. So here is the TLDR of our synthesis algorithm based on the permalocal. At every reduction step, you pick a pivot row and you pick a pivot column. Because you know that you're reducing the pivot row and pivot column to the basis vector. This means that you need to work continuously. Suppose, in this matrix, you want to reduce this column and this vector, this column, this row to basis vectors. Now, you know that every vertex in the Steiner, like in this graph, corresponds to a physical register. Corresponds to a physical register, and every physical register corresponds to a row in this matrix. So, this means that you could think of every vertex here, which is a topology graph corresponding to your niscaler, will correspond to a parity element living in this column. And what you're saying is, well, suppose I want to reduce this vector, like this column vector, to one, zero, one, zero, zero. This means that I want to reduce this guy. Reduce this guy to zero. So you're essentially, and because if you look at this two position, it corresponds to quantum register labeled by zero and three, like here. So you want to essentially move the parity from the root, which is zero, to the leaf. And because if you have one through one, then you get zero. So you canceled out your parity, and this will allow you to reduce your column vectors. So when we perform a sequence row operation, we usually do this. We usually do this. And now, if you want to look at it as like a sequential process, you start with a column vector where these two contain ones and everything else has zeros. But you don't want this one to be at the leaf. So basically, you need to move the one to the leaf. So what you do is you first move this leaf upwards so that every note carries a one. Now, because now every note carries a one, you can move from like from like this. From like from like this, also like you can you could go to the right-hand side and add every parent node to the child node, as you can see here, so that you can cancel out the parity node. And ultimately, you have this one only stored in the root node, which corresponds to the pivot that you're trying to reduce to. And this completes the column reduction. Now, if you look at this process, the idea is that if you go upwards here, you could either go from the leaf. You could either go from the leaf or you can go from the root, meaning that if you want to move the parity from either the leaf nodes to the terminal node, which is this, the blue ones, or from the root nodes to the terminal nodes. They are all feasible. But in our previous method, in promo-call, we only consider one direction. We just by default move from the leaf. So what we do in the promocall is that we consider the different combination of Different combination of moving the parities from the leaf to the terminal nodes or from the root to the terminal nodes. So, if you look at the first one here, if you want to move the parity from like zero to one, you take the cost of A edge. But if you want to move the parity from three to one, you take the cost of B C edges. Now, the edges, the cost of these edges will be calculated by our cost function. So, essentially, Barrel cost function. So essentially, for every intermediary node that needs to carry a one, we always find all options that you could possibly move them. And we build this kind of table, as you can see here, like option. So the row corresponds to moving parity to the terminal nodes, and then the terminal nodes one, and the column corresponds to moving parity to the terminal two. And then we also show in the next lemma, like in a technical lemma in our paper, that there's no contradiction to do such combination. There's no contradiction to do such a combination. Therefore, we can always find the optimal solution by just calculating the cost of all possible combinations of row and columns, and then we find the minimal one. So this means that we use the cost function to find the local minimal value to route a circuit such that we can effectively reduce the column and the row. And then, of course, local optimal does not imply global optimal, but because these are heuristic, so we're still doing reasonably global well. So finally, Well, so finally, let me show you our benchmark result. So, we benchmark on three different quantum IBM hardware. The first one is a seven-qubit Nairobi, the second one is 16-qubit Cairo, and the last one is 27 qubits quadruple. So, for all of our benchmark comparison, you'll see that Kistig just basically has the highest synthesized CNOC count and is the most expensive one, as you can see here. As you can see here, so in order to further see like what is our algorithm compared to the state-of-art algorithm that is not noise-aware, so we kind of throw away the skiski figure. So we zoom out, we zoom in the comparison. And you can see that our algorithm, NA promocal, has always given the lowest synthesized CNOC count and the lowest cost, which is this guy. And then I only showed a 5 and 7 CNOT circuit on the Nairobi hardware, but we also did a 5, 7, 15, and 25 CNOT circuit, like a qubit CNOT circuit on the larger hardware. And all of this data show that our algorithm have very obvious advantages compared to the state-of-the-art algorithms. So finally, just to sum up, we use our cost function, which is a more scalable and accurate version. More scalable and accurate version compared to the other cost function to quantify noise in our CNOT circuit. And then, using this cost function, we are able to make the local optimal decision when we are walking on the topology throughout our CNOT circuit. So, I do have questions for the audience. So, let me ask if that's okay. So, my question for the community is: what should be an appropriate way to benchmark NISC compelling? Like, the problem that we encounter is partially like what NAO. Counter is partially like what Neil is saying: is there's so many different standards for quantum circuit and also like hardware. Like, what standards should we use as a consistent and uniform standard so that we can say that, you know, this is something that is not going to change. So, no matter what algorithm you're going to design, we'll always compare to the standard, like, to the standard framework. And also, like, how do we quantify whether your algorithm is doing actually well? Because, like, as you can see, our benchmarking is based on our computation. Is based on our computation capability. This means that based on like 27 qubit hardware, we're doing fairly well. But how do we say anything about working with 127 ego processors? There's nothing we can say about it. Well, so this is something we are not sure. And secondly, in order to quantify noise in a Clifford plus T circuit, what are the things that we can ignore? Like, for example, does crosstalk really matters when we're thinking about like other noise? Are you thinking about like other noise, like for example, a headmark error rate or the breath out error off? What things can we simplify and what things we should really care about? And finally, when do we need circuit routing in fault tolerant point accommodation? Because in the space-time, like the circuit distance discussion yesterday, people mentioned about, oh, you need to do circuit routing. But I wonder, is there any general discussion about thought-tolerant circuit routing in our community? And that's it. Thank you. Community, and that's it. Thank you. Yes. So, to what extent is this, are the methods here really specific to the IBM architecture? Like, for example, you mentioned at some point that you are trying all combinations of different paths, like sort of along this line, right? That seems like something that's like maybe works for linear connectivity, but for other kinds of Connectivity, but for other kinds of architecture, you know, other kinds of connectivity might be extremely expensive or impossible to do. So, to what extent is it specific to IBM, or to what extent can you generalize it to other parts? That's a very good question. So, firstly, I want to explain IBM's architecture. So, they're using the heavy hacks architecture. It indeed is a relatively simple connectivity. Well, when we were doing our benchmark, we didn't notice any delay for finding the optimal. Delay for finding the optimal results for up to 27 qubits. So, in terms of scalability, we're thinking for working with even more larger IBM hardware, our algorithm could still produce, like, could still find the shortest combination of paths in a meaningful time scale. This is first part. Second part, how do we generalize it to different architecture? Well, firstly, I'm not really sure what are the other architecture out there because if there are some symmetry. If there are some symmetry in the graph itself, then we can leverage it to find some maybe some shortcut or some smart comparison. So we don't need to do this greedy approach. Admittedly, this greedy approach is not scalable if you're looking at a more complex graph, but we really have to see what are the inherent graphical structures that we could leverage before we talk about scalability issue of our algorithm. Maybe I can I can ask the like for make uh general algorithms just like Essentially, when you're trying to find this path between the qubits to get these ones in the right locations, you build a signer tree, which is NP hard, but you have polynomial approximations for it, so it won't be then very as good as it could be, but it's still doable. So I guess it is using that choice algorithm. It's usually we're using Dexter's algorithm. We use Dexter's algorithm. Okay, yeah. Yeah, no, yeah, exactly. So you just try to find the search path. Yep. And then in principle, you can, for example, store LBR search path and try and find that. And essentially, you're, yeah, I mean, underneath that's what you're doing. So now I think this would work on arbitrary connectivity and arbitrary electricity. At least like these model is, only your C notes are bad. Is only your C nodes are bad, right? So the qubits, the coherence is missing, and these not is missing. It's not complete in that sense, but there's no real way that it needs to be precisely for IBM, except for that IBM is something the only device that is very open about there. Or at least that it's very easy to get the fidelities and the connectivities of the team into your program. Into your programming language, right? Because a lot of them just have them somewhere on a website, and then you can look at them yourself, but they could, yeah. And I get rid of you have a basically, if you just have to go to your data from Qiskit, and if you have a device from a different manufacturer and they have to evaluate and set it as part of Qiskit, then you should be able to just use it as straight out of the box. Maybe I can address the third point here. So there's a bit of an elephant in the room for basically every quantum hardware company at this point. And it's that there's kind of a limit to the size of how big these chips or whatever the architecture can get before you actually need to basically have either an entirely different chip or a different chip. Different chip or a chip in a different fridge, or basically, you start having to talk about distributed architectures. And the second you talk about distributed architectures, doing operations between two chips in that architecture, generally speaking, is going to be either an order of magnitude slower, an order of magnitude worse fidelities, an order, basically just generically going to essentially. Going to essentially set the time scales at which algorithms are running if you need to run a lot of operations across those links. And so to meaningfully do quantum computation at the scales that you need to do, like for the algorithm or things like that, figuring out how to compile and route to avoid those slowdowns, kind of in the same way that I think people have done in the classical world. People have done in the classical world to a large extent is really crucial. So I think that's a very natural setting to start looking at questions. And I think there are people from the community who are starting to get interested in those problems, but this would be a great example of why it's important. Thank you. We had a similar discussion on this topic in a meeting once, and there was a bit of a debate: if everything is fault tolerant, does it matter that? Tolerant, like, does it matter that you add a bunch of slots because in principle tolerance, you're not going to add any overhead, but then, yeah, you might hugely increase the duration of your computation. But are we overlooking something and just assuming that they're fault tolerant and it's just time? Like, are there any other? I mean, swaps are gates. So, if you can reduce the number of gates that you need, then that reduces the overall error rate that you required, which then the number of resources you require is going to be lower, right? So, just because it's Right, so just because it's a spot doesn't mean it's free, it still costs, you know, you still need it to be essentially perfect, right? So it's counting against your total error rate that you require, right? And the other thing that I would add, really true, is that there's also no reason to expect that even on a single chip, that, say, if you have surface code patches, for example, that those patches are going to have homogeneous fidelities, right? You might have a patch over here that's a bundle. Like you might have a patch over here that's a bubble where the physical qubits are worse, you have a patch over here where the physical qubits are better, and the logical fidelities are going to abundant. And also, like, if you're really bad with your routing solution, the amount of time that you're incurring can just be to the point that you might discimulate. So yeah, it's uh it it's not as very important as well. Yeah, you might like how much Yeah, even though it's like in code, I guess it's slightly different. But like I guess like the it's it's not simple enough to be like, we go, we have a code and then like errors don't match anymore because that's fast. But like, there's still visible units, and they don't just disappear. We have like actions we have. And like, so you have a threshold, however, that's calculated by some piece. Like it says if we reach this threshold, then we can do error regression limits on two things. Can do their requirement, but there's two things there, which is like actually getting to that threshold. Is it just like, you know, not just that's kind of like kind of where some of the still have purpose in terms of like, we actually, how do we put our physical system like over that edge into the threshold regime? But then even in the regime, there's a big difference between just past threshold and like achieving the threshold. So like it's not just like one or the other. So I think even like even at I think, even like, even I think in all tolerance, at least in the first couple of years or decades of tolerance, there's probably something going to be nuanced discussions of the low-level errors and optimization to that, even if you have problems. So, I think also what's quite overlooked from like a filter point of view, it's like, oh, you have a code, you just run the code. It's like, yeah, but the code also needs to be compiled, and that runs on noisy hardware. And actually, this is noisy hardware on the level of, you know, On the level of, you know, some qubits don't have a good day, so today they're out sick, right? They just don't, don't, they don't feel like it today, so now you lost a qubit in your service code, like a physical qubit in your service code. How are you going to deal with that today? Yeah, so you're still going to need to do that on the low level as well and deal with all the types of bullshit that Cubans are up to. Yeah. Do you take fidelities into account when you're finding the standard? Like is it going to use it like a weighted treat the thing like as a weighted graph for purposes of treatment? Yeah, yeah, that's a very good question. So when we so we start by looking at the matrix and we find the pivot column and pivot row. So we look at the pivot column, we know that, oh, these are the vertices that we want to. Are the vertices that we want to reduce the parity? So, those will be our terminal nodes. So, those terminal nodes will form the steiner tree. And then, when we look at the topology, we're looking at a subgraph of this topology to find the steiner tree. And we need to find all possible walks to connect all of these terminal nodes to form the steiner nodes using the cost function. So, this is the part that I did not show in the talk, but this makes our But this makes explaining our algorithm a bit complicated because, well, it's almost like when we choose the pivot column, well, we actually did not just choose it like randomly. We choose the best one we can find because that one gave us the cheapest minimum standard tree based on our cost function. And then this minimum standard tree, the minimality comes from the minimum of the cost based on our definition of the, you know, like the cost function. Minimum cost, the minimizing. Minimum cost, but minimizing also overall these different combinations of ways that you can build the tree again, I guess. Well, we so because we know that our routing strategy for column reduction is fixed, right? Like, you know, you first do a walk and then you move the parity from the leaf node to the intermediary nodes, and then you move the root node parity to the intermediary node until everything canceled out. This sequence. like this like this sequence is like a choreographed dance in the in the in the in the in the topology so we know how to obtain every step's cost we just need to compare different combination of the first walk does so we have two walks sorry sorry sorry sorry sorry maybe should hear some talking yeah oh yeah yeah i'll explain thank you very much