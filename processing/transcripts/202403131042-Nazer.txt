And continuing with the talk game gave as a nice tutorial in the morning. And if you're trying to watch this, you know, later or now, I mean, I really recommend going back and watching what Galen was saying for the future. And we already saw some nice talks about spectral and memory AMP. And so this is going to be kind of a different perspective. It's a joint work with Riccardo Rossetti at Duke. He's a PhD student of Bayern, but he's also at Duke, who's in the audience. I wanted to thank the organizers for including me. It's very special to be here, not just because I love being at Beers and I was at Beers a decade ago. Decade ago. I actually grew up here, so I'm a real local from Alberta. It's essentially me from when I was a kid. And then I wanted to connect to the other talk, so I asked Henry to generate a ChatGPT image of Galen in this image, because Galen had an image of all of us. So I wanted to insert Galen as a child, and what ChatGPT did was this. So we tried, but that's what it didn't want to make a real child image of Galen. So that's it. Okay, so I'm going to keep things simple at the beginning. So, I'm going to keep things simple at the beginning, and then I'm going to go to something a bit more general. So, what I was interested in is the power method. So, the power method we already heard about in a couple of the previous talks. This is kind of continuing the case that Galen was looking at. So, let's have a symmetric matrix, and let's say I want to estimate the dominant eigenvector. So, the eigenvector corresponding to the largest magnitude eigenvalue. Here's a very simple look at the power method. There's no amp perspective on this yet, okay? So I just. On this yet, okay? So I just keep the hitting my estimate, I normalize the estimate every time. That's my power level, okay? And there are different ways of writing this, but this is a very simple thing. And just to make it a little more visual, you can think of it as this kind of block diagram with a delay. You keep going in through this projection, the one that you like, and then you end up refining your over time. And you know, they're classical error bounds. You could look at Gollup and Boundloan or any kind of older numerical learner algebra textbooks. You're going to get something like spectral. Yeah. Yeah, way they are. Okay, so that's very well known. But let's say I want to distribute this power method because I have a very large matrix to deal with. Okay, so I have a huge matrix. I don't want to put it all into a machine. I want to do something distributable. Okay, so one thing I could do is I could partition the rows of the data matrix into, let's say, J equally sized submatrices. And you could permute this in some way, it could be non-equally sized, it doesn't really matter. So this is just, you break it up in some way that you like. Break it up in some way that you like. And I'm going to give each submatrix to a server. And so I'm going to end up with some kind of distributed power method where at each step I'm kind of stacking my previous things, normalizing them, and then farming them back out. And that's my distributed power method. Okay, and that's fine. You can do that. But what starts to happen, so here's a visualization of what I'm suggesting. You have a projection, everything goes through its own server, which has a part of this matrix. So if you stack those matrices together, you get the full matrix. And then they go through, and you have to re-stack those. And then they go through, and you have to restack those estimates, re-normalize, and then keep going. But what happens in these kinds of contexts is you have stragglers, and you often end up with one or more servers that don't respond by your deadline, or they kind of lag, you have a long tail in response times. And ultimately, in practice, it's as if one of those servers did not respond. Okay, so what do you do? Well, there's a lot of interesting work in the last few years on coded computing, which is coding, kind of developing MDS-like codes for matrix multiplication, so matrix vector, matrix. Matrix multiplication, so matrix vector, matrix matrix multiplication, with these kinds of erasers. There's a lot of interesting work. These are kind of the initial things that started that development, but there's a lot more stuff about spaces. And what I want to argue in this talk is I kind of wanted to think about these problems, but from this power method situation, where you're not just doing an arbitrary kind of adverse, you know, just an abstract matrix multiplication, but it has a task. Matrix multiplication, but it has a task to refine this estimate. So every time I hit with a matrix, I just expect that my estimate of the eigenvector is getting a little bit better. So, from my perspective, why do I need to do things perfectly? If I do it a little bit wrong, or if I lose a part of it, it shouldn't matter that much. I'm just not refining my estimate that much more. But I should be able to keep going. I shouldn't need to come and reach with this very powerful tool set, which is kind of protecting any matrix competition. But I know what I'm trying to do, so why can't I just ignore it? So, why can't I just ignore it? Okay, so I'm going to start with very silly things. And, you know, so I started with a very kind of naive and basic understanding of this, and we kind of, as you'll see, we've moved up the abstraction stack mostly through Galen's insight. So we'll see how that goes. But for now, I want to put everything on the same framework. So I don't want to do this for abstract, you know, for any matrix. I want to do this with a spiked matrix model. It's the same model that you saw in Galen's talk. Okay, just to make everything that I'm going to say very consistent in terms of the pictures. You can do this. Victories. You can do this for other things, but I want to be very consistent. There's a matrix that has a single spike, just theta, theta transpose, and then we have some SNR, like lambda over n factor, plus GOE. So as Galen was saying, so the GOE matrix is symmetric, and it has independent Gaussian one above the diagonal, and on that diagonal is 2 over. So it's just basically a symmetric Gaussian matrix. We're going to care about the correlation between our estimate and the true theta. And you can figure out the exact thing you can. And you can figure out the exact thing you can get from finding the dominant vector. But okay, fine. So here's what happens. If I just ignore the erasures, and this is the silliest thing I can do, I can just set the missing entries to zero, the ones that I didn't get, I just say, I insist that there's zero, that's not going to go very well. That's actually not going to converge. Okay, well, that makes sense because I'm just destroying information at every round. I'm destroying a lot of information, like a linear amount of information, like n over j, every time. So that's just not going to work. Every time. So that's just not going to work. Okay, that's and this is kind of our setting for the whole time. We're always going to keep lambda equals to root two, we're always going to keep n equals to five thousand, we're always going to assume the spike is plus minus one. Okay, uniformly drawn from that side. Okay, and we're always also going to say that effectively 10% of what you want comes through. So that's a very low amount, but it just helps to make all the pictures consistent. You could change all of these as much. Okay, so what's the next natural thing we could do? Well, first of all, I'm kind of moving this a little bit more towards. First of all, I'm kind of moving this a little bit more towards amp. Let's take this delta vector to be a 0, 1 vector, which is 1 if you actually computed that row, and 0 if the row is erased. So then what I'm really doing is I have an iteration where I'm trying to hit with m, and I take this kind of element-wise product of this delta t, and then I have some normalization. Now you're, I'm kind of more of this amp sphere normalization, but it doesn't really matter. Okay, so this is what. Okay, so this is what we were doing. So, what's the next thing I should do? Well, why don't I just stop throwing things out? I can hold on to the previous iterative. Okay, so this is maybe what you were expecting me to do. This is what I should do. This is the starting point for the toxic. It's the natural reaction in my mind of saying, well, you lost some computation. I say, well, I just don't update those coordinates. I don't throw them out, but I just don't update them. It should work. It works. Okay? So it does work. It's slowly converging. It slowly converges. Like, if we were to keep going here, they all reach the same fixed point. You know, for my own reasons, I just decided we want to keep all the plots are going to be the same because it's always the same plot, so we're not adjusting the time scales or anything to get this. So eventually it'll converge. So keeping the past iterative is better. Okay, that's not surprising. But what we want to do with this is we want to prove that this is okay. And surprisingly, And surprisingly, this is a lot harder than you would think, at least for me. So maybe it's kind of in some convex combination of these, and you add some insight and you'll get it. And I would love to know about that. But there's a lot of work in the literature on noisy power method, coordinate-wise power method, momentum, adaptive, missing data and sub-pace tracking, OJAS methods. There are a lot of things kind of around this problem. To the best of my understanding, the specific thing that I want, which is a very precise guarantee. A very precise guarantee on what's happening with this trajectory is not there. That's my understanding. So, what we're going to do is we're going to take this AMP perspective, which does give you these per-iteration performance guarantees, coupling to a Gaussian process. And we saw from Galen's work how you can get an even tighter coupling. And we're not actually going to use that coupling here. We're going to use kind of a more kind of a more basic one. Ultimately, we're going to get simple correction terms, where you're going to see them kind of go up and then back down. Going to see me kind of go up and then back down, kind of like this picture that Galen had. We can then go up and then back down. We're ultimately going to do that. And I also want to say something towards the end about maybe this also says something about efficient computation. Just hold on to that, but we're going to leave that for a while. Okay. Okay, so here is the basic AMP iteration in my notation here. So I have this data matrix M, it's n by n. I have the noising function, it's non-separable. It's non-separable. Okay, so we're focused on non-separable. So this is actually acting on the entire vector and producing a vector. It's not acting according to my coordinate. It could, and there's a de-biasing coefficient, this BT. So you have this correction term. And what you want to do, as Galen was saying in the tutorial, or our talk in the beginning, is that you want this to kind of give you a connection to some Gaussian process. And the early work was motivated by compressed sensing. Motivated by compressed sensing. And there's a lot of interesting recent work, and we were even seeing yesterday, you know, starting with Wei Yu's talk, connections to massive random access and so on. And I'm not going to take, I'm not going to have the time to summarize all that work here, so I'd point to Cindy's recent survey that appeared now published recently 22. You can also find it in an archive, and you know, so there's a lot of stuff out there that you can see, but I'm just going to leave it at that. So, this is the basic setup, okay? Please, why do you Please. Why do you uh middle bit more with the norm? Yeah, because the thing I mean, strictly speaking, I'm dividing by the norm. Now you could say, like, well, because it's high-dimensional, I could replace that with the scalar with concentration. Because you are with power method with beta, you can push. Yeah, yeah. But anyway, we're gonna get it. I mean, we can get it. We're gonna get a general theory that includes that. It's not, I it, yeah, so, I mean, for this little example, you're right. I'm kind of overdoing it a little bit, but that's, it's, I do want to hold on to cases. I do want to hold out the case, but then too. Okay, so to make this more consistent with AMP, we're going to center the data matrix. Galen was showing how to do that. I'm not even going to show it. So I basically pull the spike out into some other G thing that's kind of suppressed here. Okay, we have these noising functions, we have my T-biasing coefficient. So the basic setup here is like you have some deterministic initialization, and then you end up with this covariance recursion, so this recursively constructed covariance matrix in a Gaussian process. And then we assume that the Fs are And then we assume that the F's are ellipsis, and the first one is bounded by some constant. And then ultimately, this is a standard kind of result that you get a coupling between the sequence of iterates that you have. This is a kind of concatenation of them, so you can take the norm, and this sequence of Gaussian processes, which, you know, as we were seeing starting with Galen's talk, that they're coordinate-wise independent, and they have a scaling or a correlation between iterates that kind of depends on the scale. Is that kind of depends on this neural problem? And this convergence notion is actually one of the theorems in Elon's talk. I'm not going to reproduce that here, but this is kind of equivalent in a way to the pseudo-Lipschitz test function. Convergence, it's just simpler to write down. We kind of like writing it. Okay, so we have this. And so now I'm going to write, what is the AMP power method? No orators. Okay, so I'm just trying to write the AMP power method. We're catching up to where Galen was in his talk. We're going to show that that converges a little. We're going to show that that converges a little quicker. So I have this correction term now. This minus eta hat is really this, okay? It just works out. Wait, that should be T. I'm sorry? T minus one? Are they both T minus one? Yes. Okay, so it converges a little faster. We expected that. We even saw that at a given stock, so it's not surprising. And in fact, the Gauss, so this, for me, the marks here are the images. The marks here are the empirical evaluations. So these are the iterates, the X process, and then the curve, the line, is the Gaussian process. So I'm just showing that they line up. And so this is the power. So you get accurate predictions from that state of function. So that's well known. Is this for a block model erasure or is it an individual model erasure? There's no erasure here. We're still not. Okay, so we want to deal with this. We want to deal with this. So, this is outside the AMP world. We don't need to know anything about AMP to talk about this. Now, I'm bringing in AMP. I'm trying to put things in a context. So, this is the power method. There's no state evolution for that. Now, I'm saying this is my baseline AMP state evolution. The sped up version. So, now I'm going to introduce the erasures. Okay, there we go. How do I have erasers? So, I'm just going to say instrumentation should have been. This notation should have been the delta. So, sorry, this is like the delta t hitting this, and this is the one minus delta. I kind of left in this projection form. Sorry for that. Yeah, so basically, so just think this is delta t times this. Okay, so that's what we're ultimately computing. Okay, I'll fix this offline. So, this is delta t element-wise product. This is one minus delta t element-wise product. We're retaining the memory. Can we find a good correction term here? A good correction term here that gets us the state of OG. That's what we want. Okay, so here's our first naive approach. Maybe it just doesn't matter. We just reuse the basic AMP correction term. And you can convince yourself that you should only apply it on the fresh update. It's already kind of been applied here. I'm not going to talk about that. So this is the empirical performance, the iterates, that wrong correction. It turns out it's wrong. It's not good. So the performance decays. So, you know, the performance decays. So, it's even worse than no correction term if you have erasure. So, again, we have erasing all the 10% of the coordinates. So, how do we derive the correction? So, that's one of the things that we got interested in here. How do we get a correction term? Can we get this to kind of go above here and start approaching the performance of the centralized method? That's what we wanted to do. Okay, and so we struggled with this for a while. I don't have a slide showing all the different things that we tried. We did try a lot of different. Things that we tried. We did try a lot of different things. It wasn't really clear where we'd land up. So we convinced ourselves it's not in the literature. Although, I mean, I think it's always hard to convince yourself that something's truly not in the literature. So we converge, you know, to maybe here, one of these points not in the literature. That's the fixed point. And then we ultimately landed on something like this. Okay, so this is a generalization of what we want. So what's happening is I have my data matrix, and I'm saying that every time a linear operator hits Operator hits my data matrix. And the way that we're going to express that is with this non-unique decomposition that you hit on the left with this LTK, which is n by n, and then this RTK, which is n by n. So those are hitting on the left and right. That's how we're going to express these linear operators. You can do this in multiple ways. We're going to assume that this k is essentially finite. It doesn't depend on kernel. Okay, so this is how you can express it. What we're talking about before fits into this framework for sure. And then we have full memory. That's just like something that we can't. We'll come back. That's just like something that we can have. And then we have these matrix-value debiasing coefficients that, in general, it seems like you need at this level of generality. Okay, so we have the same assumption that the functions are Lipschitz, and these are also not, their operator norms are not too big, so right, in this linear operator. And then if we just have these two assumptions, we get a state evolution. It doesn't matter what the form is, it's a little more complicated. And we have these matrix-value debiasing coefficients. We get the same kind of conversion. Coefficients with the same kind of convergence theoretically. Don't worry about remembering this. It doesn't matter. For the purposes of the talk, it's not important. Okay, but it took us a while to kind of converge on this thing. And in fact, it took us a while to understand that the easiest way to prove this, and this is really kind of connected back to what Galen was saying here. So this was Galen's insight that the thing that actually makes this easy to prove is to lift this process, this linear operator process, onto a full memory AMP recursion. AMP recursion. So if you think about just drop this LT and just have a Z here, that's like a full memory AMP. And you can construct one here with this doubly indexed thing. So basically, you just construct something that every time you're going through these functions g and hitting them with z, and you have some correction terms. And this g just has this particular form. And if you think about this for a little bit and write down on paper, you can convince yourself that the x that you really want is a linear combination. To really want is a linear combination with these matrices on the left of the double-index process. The point is, this is in the literature in a way. You can convince yourself that this is kind of a version of full memory amp. It just happens to be doubly indexed. And then you can project its state evolution back down to get the state evolution that you want. So you establish a u, which is a state evolution for this w, and then you just draw it back down. And that's it. Okay, so that's an instance of what Gale. Okay, so that's an instance of what Galaxy said. Okay, and you know, we're using this for our guarantees, but you could use some other frameworks. So, I mean, there are other things in this space that we could use that they just need to have full memory. Okay, so now let's start dropping back down a little bit. So, we went up to, so we kind of went up to linear operator AMP, and we used full memory AMP to capture that. Now, we're coming down to a special case. I want to show that this, the reason I wanted to show this is that the fact that I have these. The fact that I have these matrix values and biasing coefficients, that's not fundamental in general, or sorry, in this special case. So, if you have a projection matrix hitting your iterates, right, so it's not just any linear operator on Z, it's a projection matrix. It doesn't have to be diagonal. So, we were implicitly talking about diagonal matrices. It doesn't have to commute. So, the erasure patterns that we had commute and are diagonal, but okay, that's fine. Anyway, so and in this case, for example, And in this case, for projection matrices, you can get scalar debiasing coefficients, and they have a nicer form. So you can kind of write these C matrix, which is the product of projections, and then you get this kind of form, which is pretty similar to what you see in the scatter now. So even though we're not interested in this level of generality for the talk, you could think about projecting your data matrix now, like sketching it or something, and trying to work out something for this. Don't have an example like that here, but you could think about it. Okay, so what are we really interested in? Okay, so what are we really interested in? So ultimately, we wanted to come back and talk about the power method. So this is the op amp, right? So operator amp power method. So we have the erasure pattern hitting our update, which I've now kind of returned to the full data matrix, so it's more of a method, but you could put Z there if you want to get the Ethereum. And then you have your correction term, and then you have your memory. So you're remembering what happened on the past iterate, where you were erased. So these are the, this represents the erase coordinates. You remember what happened. Grace coordinates, you remember what happened, and you correct what you're duplicating. Okay? It turns out, one thing that kind of was happening on the previous slide is that you need to have full memory, but like kind of going back in time for your correction. It's not enough to just look one step backwards. Even though we're doing a one-step update, the oddness of this kind of memory is that it forces you into this kind of correction. That's something I didn't believe before, but that's really important. So, in some cases, it's quite simple. So, if this is Is it's quite simple. So if this is element y'all's IID per layer gamma, ratios of y just have IID orators, then you can study deviating coefficients in this way, and you end up with this kind of nice form, right? So three simple quick concentrations. You can just convince yourself, like, this is enough for the debiasing coefficients. This will get you where you need to be. And the state evolution also has a simple form, but I'm not tension on it. Okay, finally, let's see that, you know, if you were to remember the points that I had on the previous plot, these are basically in the Plot, these are basically in the same place. They're a little bit higher because of this correction term, but now it has an accurate state of function. So we got what we wanted. We showed that remembering what happened before basically allows you to purchase something. Okay, so that kind of answers this question that I have at the beginning. Okay, so what else can we do? Well, the other thing we started to get interested in, once we had this framework, we could start asking other questions. So, one thing we started to ask is: well, what if So one thing we started to ask is, well, what if there are no, so now there's no straggling circle, okay? Let's just think that there's a giant matrix. And for my own reasons, for maybe computational complexity, I don't want to hit with the full matrix every time. Maybe I'm sitting on one machine, I want to extract a fraction of the rows and hit with that only, then go to the next fraction, next batch, and so on, just because I can't hold the whole thing in memory. How does that do? Well, that fits into this framework as well. In fact, it fits into this. This framework as well. In fact, it fits into this nice delta framework. I just have a slightly different form for these deltas, right? So instead of them being idenouli, it just you basically just partition the rows into whatever subsets you like. And if modulo j, the number of subsets, if you fall into that subset, then you get computed, otherwise you don't get computed. So it's just a different form. The correction term is also simple. The state evolution is also simple. I'm not showing mostly. I just want to ask what happened. I just want to ask what happens. Well, intuitively, it should be better, right? Because if you think about the IAD case, when I'm erasing things, some things are going to lag for a long time and never get processed until many, many iterations go by. Whereas here, I'm kind of operating in the cyclic structure. At most, J times can go by before I hit everything. Okay, you can see that. So let's kind of rename these. So full matrix is when every time I come and apply the full matrix to do my refinement, To do my refinement. Random update is what I had before. IID sample the rows. In this case, let's say 10% of the rows. And I use those. And it could be that a long time goes by before I resample a different row. And round robin is that I kind of move, you know, cyclically through the rows. Okay, and so obviously, intuitively, this should be faster. We can see that it's indeed faster. The empirics match the state evolution, and that's great. Okay, but this doesn't really deal with the. Okay, but this doesn't really deal with the straggler case. This is kind of the thing that models a straggler case. This is like a planned, scheduled, partial comprehension. Okay, once we have this plot, then we start asking another question. And this comes back to something I was asking the other day. Sorry, before I do that, it also works for sub-Gaussian, just in terms of the simulation. If you just replace Z with ID Ryanmacher instead of GOE, then, okay, I still have the state evolution from GOE. The state evolution from GOE here, but you know, it's not that sensitive to Z as long as Z is ID subcouncil, but still. Okay, great. Coming back. So I want to show, so far I've been showing you performance with respect to iterations. Like every time I go through a new iteration, I get some performance. And obviously, hitting with the full matrix there is going to be better because I'm applying more of the data matrix. I'm kind of refining more of the coordinates. But what if I want to plot in terms of But what if I want to plot in terms of the total amount of computation? And I'm going to just say large-scale computation. Basically, how many times do I multiply with an n by n matrix? That's the only thing I'm going to count. I'm not going to count normalization. This is distributed. I'm not going to count communication or coordination. That's also for us. How many times do I hit with now? Total. So if I'm breaking into two blocks, I'm doing half at one time and half at the other time. Time and half at the other time, then that takes two iterations to do one matrix multiplying. Whereas the full matrix always does one matrix multiplying every iteration. So what that means is for the plots that we have, we have a ten-fold speed up because we're only working with 10% at the time. So it's a ten-fold computational speed up. So let's see how that looks. We're going to just track this number, number of matrix multiplies. So full matrix applies a full matrix. applies a full matrix, round ROM bit cycles through, and random update, you randomly pick IIT of rows each time. And actually what we see here is that the amount of computation you need to do is less, especially for round raw. Okay, so you need fewer matrix multiplies to converge towards the fixed point. Random update here, it is better, but we found some instances, depending on initialization and just like randomness in the problem, it can slip a little bit below this. Okay, so it's not. Bit below this. Okay, so it's not always strictly above this. For a round robin with a Bayesian denoiser, which we're not showing here, this is a kind of a sphere projection denoiser, but for a Bayesian denoiser, we can prove that this is going down the full matrix. But that's not in the topic. But this is kind of an interesting perspective. For me, it was interesting because, to the best of my knowledge, I don't think people have been thinking about AMP so much as a way to analyze these sort of distributed computational steps, but it seems like it can be done. It can be done. And so you could go and try to think about this for other problems. How much time do I have? Oh, five minutes. Okay. So I'll just kind of move towards my conclusions. So what we showed is an AMP perspective on the distributed power method with erasures. We got a simple state evolution eventually in scalar biasing coefficients. Eventually, we kind of went up, moved back down. We got the same fixed points in no erasure setting. We got a computational speed up for partial updates. We found a way of Speed up for partial updates. We found a way of understanding that. And you could also consider other denoisers. We have plots for the Bayesianoiser. I just don't show them here for time and consistency. Right, and so the way we establish is first we generalized up to linear operator amp, and then we kind of brought ourselves back around. So some follow-up questions. So what about orthogonal ensembles? So we heard from Brian how you can use memory amp to kind of deal with these ensembles in an efficient way. So we don't have anything in this talk. So, we don't have anything in this talk or in the associated paper about orthogonal ensembles. You could think about adaptive updates. So, for instance, let's say things are getting erased, but you're allowed to schedule computations. So you could say, well, this coordinate hasn't been processed for a while, so I'm going to request that twice or three times. More generally, you could think about some kind of coding over this process, which we don't have in here, but you can think about it. We think about first-order methods with erasures, noise instead of erasures, and there might be a connection to this idea of mini-batches in SGD, where it actually is. Mini batches in SGD, where it actually is, in some regimes, effectively less computation to deal with that than the full grade. I want to thank my PhD student, Iksako, and also my colleague and former postdoc, Guy Konambe, for a lot of valuable discussions on the power method, but we're trying to do this outside of the AMP framework, and we're still trying to get a handle on those kinds of theoretical balances. So I just want to thank them for all those chats. Makes sense. 