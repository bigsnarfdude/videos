And resolvents and things like that. So, somehow, just from the title, whoever did this inferred the right things. This is joint work with my collaborator, Yuval Kluger from the Yale Medical School. And I want to start by kind of briefly describing the motivation for this work. It comes from an experimental procedure in genomics called single-cell RNA sequencing, or in short, sRNA-seq. So, in sRNA-seq, So, in sRNA-seq, we have a large collection of cells, and each of these cells is described by a high-dimensional vector. This vector is called the gene expression profile or the gene signature sometimes, and it kind of characterizes the cells. So you have many cells, you have many vectors, you concatenate these vectors, and you form a matrix that looks kind of like this. So, the different rows correspond to different genes, the different columns correspond to different cells, and each entry is just the number of times you observe the specific gene. Times you observe the specific gene in a specific cell. Okay, so the columns of this matrix are these gene signatures that I talked about. And when practitioners analyze the data, they basically want to find meaningful structures or patterns in the data, like you see here. So very often the cells are clustered into distinct groups. You have distinct cell types. And there are also groups of genes called marker genes that sometimes characterize the cells. They're kind of like indicator functions, if you wish, on the cells. And there are many other And there are many other interesting things that practitioners want to find in the data. Now, when analyzing the data, there are two main challenges. The first one is that these data sets are very large. So typically you have several tens of thousands of genes and a similar number of cells. And also, these data sets are very noisy. So it may be challenging to identify these structures directly from the raw data without suitable pre-processing. Now, to address these challenges, a popular first step is just to perform a low-rank approximation. To perform a low-rank approximation, you project the data on a small number of leading components using PCA or SVD. Practitioners like to use that. And that can compress and denoise the data set simultaneously. So if you want to use low-rank approximation, of course, you need to choose a rank for the approximation. And what you see here are the sorted singular values of a real data set called the purified PBMC data set. That's a popular data set for benchmarking. And what we see is that we have this kind of practice. And what we see is that we have this kind of rapid decay of the singular values at the beginning, and then we have a slow, gradual decay, corresponds to a heavy tail of singular value. So this matrix is clearly not Laurent, and it's not even approximately Laurent in the sense that you cannot get a good approximation to the raw data using a small number of components in the S V D. So instead, the questions that we pose are how many informative signal components we can detect in the data and also how to recover them accurately. Recover them accurately. If we want to answer these kinds of questions, we need a model. So here we assume a signal plus noise model that looks like this. We have a data matrix Y size M by N. And here, without loss of generality, we assume that M is smaller or equal to N. So the matrix is either square or it is kind of wide. And of course, you can always transpose the matrix if that's not the case for your data. And the data matrix is composed of a signal matrix X. Composed of a signal matrix X, which is deterministic and low-rank, plus a noise matrix epsilon. So, this is a random matrix whose entries are independent, they have zero means, and they have some positive finite variances, Sij. Okay, so each entry has kind of its own variance in the most general case. Okay, so remember these two quantities, sorry, three quantities, they will follow us throughout this talk. Now, given the data matrix of this form, there are two important tasks that kind of There are two important tasks that kind of correspond to the questions that I posed before. The first one is to estimate the rank of the signal matrix, which is denoted by R, or at least to provide a lower bound on the rank. And the second is, of course, to recover the signal matrix X itself from the corrupted observation. So let me start by briefly discussing an important special case, which is called the case of homoscedastic noise. And this complicated word only means that all the noise only means that all the noise variances are identical. So to understand what happens in this regime of noise, it is convenient to consider this asymptotic regime where both dimensions of the matrix grow to infinity simultaneously and the ratio, which is called the aspect ratio, converges to some number gamma between 0 and 1. And then we look at the empirical distribution of eigenvalues of this normalized gram matrix. You can think of that also as the covariance of the data. That also is the covariance of the data across the columns. And by empirical distribution of eigenvalues, I just mean the normalized histogram of the eigenvalues. So if you look at that, you expect to see a picture that looks kind of like this. For the majority of the eigenvalues, the bulk, we expect to see something very close to this red curve, which is the Marchenko-Pastor law, the Marchenko-Pastor distribution. It has an explicit expression that depends only on the aspect ratio and the noise variance. Aspect ratio and the noise variance. And it is supported on a finite interval between two numbers, beta minus and beta plus, and these are known as the lower and upper bulk edges. And also they have a very simple expression depending only on the noise variance and the aspect ratio. Now, if you have only noise, then by these results in random matrix theory, you do not expect to see asymptotically any eigenvalues that exceed this threshold over here, the upper edge of the Martian-Kopster law. And if you happen to And if you happen to see these eigenvalues, then they must come from the signal, the low-rank signal. They cannot possibly come from the noise. And so, a simple approach to estimate the rank is just to count how many eigenvalues you see here that exceed the upper edge of the March and Kopster law. And to recover the signal matrix itself, the most standard, most popular approach is just to truncate the SVD at this estimated rank. And there are many theoretical guarantees for this approach. But in some cases, you can also do better. You can shrink the singular. Do better. You can shrink the singular values towards zero. You want to do that sometimes to account for the noise in the singular vectors themselves, because they're also noisy. And there are kind of optimal formulas to do that for various loss functions. And there are many, many works on that topic. This is just a very short, by no means a classic. So the purpose of this slide is just to show that in the case of homoscedastic noise, we have a fairly good idea or understanding of how to do signal detection and recovery. And recovery. The problem is that, in practice, for many data sets, the noise is not almost kedastic. Specifically for single-cell data, we know that the noise levels or the noise variances can change considerably across the rows, which are the different genes, and across the columns, which are the different cells. And this is what is called heteroscedastic noise, where the noise variances can differ across the data, which is probably the more realistic scenario. And in this case, we don't expect to see the And in this case, we don't expect to see the Marchenko-Poster law. Indeed, if you compute the empirical distribution of eigenvalues for the purified PBMC data set that I talked about before, you don't see a good fit in the Marchenko-Pastor law. In fact, the empirical distribution of eigenvalues is much, much more spread out. So instead of assuming homoscedastic noise, we will assume a slightly more complicated model, but still simple, for heteroscedastic noise. And here we assume that the noise variance matrix is run quantum. Variance matrix is rank one, okay? So it is equal to some column vector x times a row vector y transpose. And you can think of these vectors as describing the noise levels for the rows and the noise levels for the columns, respectively. Okay, so suppose that you have this heteroscedastic noise. Okay, what's the problem? Why can't we just use the standard, simple techniques that we have from scedastic noise in this case? Of like the scales of the gene? So, I will try to answer this question first by showing results, empirical results later on. And then this answer might not be satisfactory. And then the last slide, I will mention a few things that might be more satisfactory. Okay, so I want to highlight, first of all, a few challenges associated with extra scedastic noise in simulation, but to explain the simulation, I want to start with homoscaling. I want to start with homoscedastic noise. That's kind of the baseline. And in this simulation, we have a signal matrix with rank 20. And out of that 20, we have 10 strong components and 10 weak components. And we're adding on top of that completely a Muscadastic Gaussian noise. And I will always show two complementary views of the spectrum. On the left, you have the histogram of the eigenvalues, which is the empirical distribution of eigenvalues. And on the right, you have the sorted eigenvalues. I'm showing kind of the lead. I'm showing kind of the leading eigenvalue. So on the left, we see an excellent fit to the Marchene-Koposter law. As we expect, we see some spikes here that come from the signal. And on the right, you see the 10 strong component, the 10 weak component, and then the rest of the spectrum, which comes from the Marchenko-Poster law, or essentially the noise. And all of these eigenvalues are below the upper edge of the Marchenko-Poster law, which also coincides with the largest empirical eigenvalue. The largest empirical eigenvalue of the noise. So there are two lines over here. It's kind of hard to see, but they're almost on top of each other. So what we did next is we took five rows and five columns and we amplify the noise in those rows and columns. So the noise is extra scedastic because there are five rows and columns where the noise is much stronger. So what happens when we do this? If you look at the distribution of eigenvalues, it actually looks almost the same. This makes sense because we only modify the small number. Because we only modify the small number of rows and columns, so it's kind of a low-rank perturbation. There are standard results that explain why this does not change the distribution of eigenvalues. However, if you look at the sorted eigenvalues, then something strange is going on. And if you count how many eigenvalues exceed the upper edge of the Marchenko-Poster law, which is this green dashed line over here, you will see that there are 30 eigenvalues over here and not 20. If you compute empirically, the largest eigenvalue that comes just Largest eigenvalue that comes just from the noise. So, this corresponds to this red line over here. You will see that it's much larger than the upper edge of the March-Confuci law. So, what happens here? These five corrupted rows and five corrupted columns essentially contribute these five eigenvalues over here and these five eigenvalues over here. So, we have these outlier eigenvalues, which basically look like signal components. If you inspect this figure, you might be confused or mistaken to think that there are Or mistaken to think that there are 30 signal components. So, this is a problem for rank estimation or signal detection. It's also a problem for signal recovery because where would you truncate the S V D? If you truncate the S V D over here, you're removing the noise, but you're also keeping these purus components that come from the corrupted rows and columns. If you truncate the S V D over here, the largest eigenvalue of the noise, which many methods try to do, you're removing these porous components, but together with the noise. These porous components, but together with them, you're removing the weak signal components, which might be interesting in applications. So that's one scenario. In the second scenario, what we did is we generated heteroscedastic noise where all the noise variances kind of fluctuate considerably across all rows and all columns. In this case, we no longer have a good fit to the merchant cluster. And in particular, the distribution of eigenvalues, this blue histogram, is much more spread out, which is similar to what we Out, which is similar to what we see for real data. And if you look at the sorted eigenvalues, now you have an even bigger problem because you can no longer see the weak signal components that we could see before, because they're now buried under the noise, because of this heavier tail of eigenvalues. So they're kind of masked underneath, and you cannot see them. So hopefully, what I try to convey here is that if you have data that might have heteroscedastic knowledge, That might have heteroscedastic noise, then it might not be a good idea to rely on the spectrum of the observed data. You might want to do something else. So, a simple thing that you can do, which as I will show later, works very well in applications, is to rescale or re-normalize the rows and columns to kind of stabilize the behavior of the noise. So let me elaborate on this and explain why this might be a good idea. Suppose that you have two positive vectors. Suppose that you have two positive vectors, u and v, which are deterministic, and you just use them to rescale the rows and the columns. You just multiply the rows and the columns by these numbers. So you can write this down by taking the data matrix, multiplying it from the left and the right with diagonal matrices. And we call this new matrix Y tilde. Because of the signal plus noise model, you can write this as a rescaled signal matrix plus a rescaled noise matrix. Now, these rescue matrix matrix matrix Now, these rescaled matrices satisfy kind of the basic properties that we required in the signal plus noise model. The first property is that the rank of the rescaled signal matrix must be the same as the rank of the original signal matrix. This is just by basic linear algebra. You're multiplying the rows and columns by positive numbers. You're not changing the dimensions of the row space and the dimensions of the column space. So the rank of the signal is preserved. Also, this diagonal scaling does not mix between any Does not mix between any pairs of rows or any pairs of columns. So the entries of the noise after rescaling are still independent. And they also have zero means because these vectors are deterministic. Now the advantages of this is that if you write down the noise variances after rescaling, they have this expression over here. And if you choose the scaling factors carefully, you can make all the noise variances in this model to be exactly one, okay, because of the rank one model for the noise variances. Model for the noise variances, you can make the noise exactly homoscedastic. You can cancel out the variability of the noise completely. Okay, so if you do this, you preserve the rank, you preserve these statistical properties of the noise, and you are back in the regime of homoscedastic noise, which is very convenient for us. Okay, does that make sense? Okay. So the problem is that we don't really know the noise variances. We need to know these vectors x and y to know how to read. These vectors x and y to know how to rescale the rows and columns to cancel out the heteroscalasticity. So we need to somehow estimate them from the data. Now, there is one kind of immediate naive approach, which is that let's ignore the signal. Suppose that we have only noise in the data. So what would you do in this case? You would take your data and you would just compute the empirical variances across the rows and across the columns. That would give you kind of the average noise variance across the rows. Of the average noise variance across the rows and the columns, and from that you can easily recover x and y. The problem is that you do have a signal, and oftentimes it is strong, or at least it has strong components. Okay, so that will give you highly biased estimators. And from my personal experience, this kind of approach does not work for real data. And also, it does not work in the simulations that I discussed previously, because we had this strong signal component over there. So instead, we will take a different approach using more advanced random. A different approach using more advanced random matrix theory, and our main contribution is to show that you can actually estimate these x and y accurately in many models of signal plus noise, kind of regardless of the magnitude of the signal, even if you have strong signals or weak signals. So I won't go into all the technical details, but I just want to highlight the main ideas or tools that we use there. So remember that we have this signal plus noise model. The variance of the noise in each entry is denoted by SAJ. Is denoted by SAJ. And then we define these two matrices over here. So the first matrix is a symmetrized version of the noise. And the second matrix is a symmetrized version of the noise variance matrix. So these two matrices are symmetric and they have dimensions m plus n by m plus n. Now for the symmetrized noise, we can define what you've already heard about, the resolvent. So we take the symmetrized noise matrix, we subtract Symmetrized noise matrix, you subtract some multiple of the identity, you take the inverse of that matrix. And this scalar is just some complex number in the upper half plane. It's a fixed number. And there are results in random matrix theory that characterize the behavior of the resolvent for large matrix dimensions. Essentially, they provide a deterministic equivalent for the resolvent. And what these results say, that the resolvent for large matrix dimensions is approximately a diagonal matrix. Approximately a diagonal matrix with some vector f on the main diagonal. So that's a complex-valued vector. And this vector satisfies a deterministic equation known as the Dyson equation. So it's a non-linear equation. This one over F here is just entry-wise division. That's how you should read this. The important thing is that this equation only depends on the noise variances. It's kind of universal. It doesn't depend on the distributions. Depend on the distributions. You can have very different distributions of the noise across your data. And this result holds under some mild conditions on the boundedness of the noise. Okay, so there are a few references over here, but there are many other people who derived similar results. Okay, so what, sorry? It's it's uh it's not the same Erdos. It's Laszlo. It's uh i actually checked and it's not even related to that Erdos, to the best of my knowledge. To the best of my knowledge. Yeah, yeah, yeah. It's a big famous guy in random matrix theory, I think, in Austria. Yeah, yeah. He's very prolific, has very important results. Okay. So, okay, we have this universality result. How does that help us? What do we do with it? Our first observation is that if your variance matrix is rank one, as we assumed before, then it turns out that then it turns out that this equation is in a way invertible in the sense that you can write down x and y explicitly in terms of f, in terms of this solution. I'm not writing these formulas over here, but trust me that they're not hard to derive. And because of this result, this vector f is approximately the main diagonal of the resolvent. So what does that mean? It means that if someone gives you the resolvent of the noise, you can take Resolvent of the noise, you can take its main diagonal, compute some explicit formula that I did not write here, and that will give you approximately x and y. Okay? Okay, but there is a problem. We don't have access to the resolvent of the noise. So what do we do? What you might already guess, we replace the resolvent of the noise with the resolvent of the data. Okay, so we construct a symmetrized version of the data matrix, define or compute the resolvent of that. The resolvent of that, similarly to what we did before for the noise. And then we treat the resolvent of the data as if it was the resolvent of the noise. First, why are we allowed to do this? Basically, what we show is that in a way, the resolvent of the data is robust to low-rank perturbation or to low-rank signals regardless of their magnitude. And the reason for this, or the intuition, is it's because of the inverse over here. Okay, if you had strong components in your signal, what would happen is. Signal, what would happen is that they would be mapped to components that are weak or have a small absolute value in this resolvent. Because you're just subtracting some complex number and taking the inverse of that. In fact, the stronger the signal is, the less influence it has on the resolvent of the data. So in a way, the resolvent of the data allows us to look kind of under the hood of the data, to look at the structure of the noise, irrespective of the segment. Irrespective of the signal. So, this allows us to estimate these vectors with theoretical guarantees. Overall, our algorithm looks kind of like this. We call it the Dyson equalizer. Basically, you have your data matrix. You just compute S V D. Then we have some formulas that depend only on the singular values and singular vectors of the data. And these formulas just come from the representation of the data resolvent using the S V D. data resolvent using the SVD. Okay, so you can from the SVD, you can go directly to the resolvent, it's computationally more convenient. And then we have explicit formulas for these estimators for x and y. And these formulas over here come from the inversion of the resolvent for the Dyson equation, what I talked about before. And lastly, we just use them to re-normalize the rows and columns to make the noise approximately homoscedastic. And there are several advantages to this procedure. It is fully automatic. Procedure, it is fully automatic. You don't need to know anything, there are no tuning parameters. It is easy to implement because you just need to compute the SVD and some explicit formulas. And we have theoretical guarantees for this procedure under some assumptions, but I'm kind of writing loosely over here. And under these conditions, we can prove that indeed these estimators for x and y converge entrywise to the true vectors x and y that describe the noise variances in the data. In the data. And we also have rates for these, and they depend on some constants in these assumptions. But the key feature of this result is that the convergence and the rates also do not depend on the singular values of the signal. You can have signals that have a very strong or very large singular values that are kind of increasing with the dimensions. You can have others that are fixed. It doesn't matter. We also allow the random to grow with some rate. We also allow the With some rate, we also allow the dimensions to grow disproportionately, so it can accommodate for many, many different models. So, let's see what happens when we apply this approach to the previous two examples. Remember that we had this example where we had five corrupted rows and five corrupted columns, and we got these spurious eigenvalues over here. And what happens after we apply this approach that these spurious eigenvalues essentially disappear because we re-normalize the rows and columns, including those five, where the noise was much stronger than in the rest. Was much stronger than in the rest of the data. So now it's very easy to detect that we have 20 signal components by just counting how many eigenvalues exceed the upper edge of the March and cluster law. And you also maintain a good fit to the Marchenco cluster law in this case. Then we had this example where we had kind of severely varying heteroscedastic noise and we didn't have a good fit to the Marchenco cluster law. And also we could not see the weak signal components because they were buried under the noise. And after we applied this procedure, now we can. And after we apply this procedure, now we can easily identify these weak signal components. And we also have a good fit to the Marchenbookster law. So let's see what happens for real data. Remember that we had this purified PBMC data set, and it also had a very poor fit to the Marchencoposter law. And after we apply this procedure, now we get an excellent fit to the Marchenkoposter law, all the way down to the upper edge. And we see these spikes over here that, at least according to the theory, should belong. According to the theory, it should belong to the signal component in the data. And if you look at the sorted singular values, now at least kind of qualitatively, they have a better behavior than the singular values before, because we have a sharp transition and then a flattened region. And the threshold is roughly where we want it to be, at least by inspecting this figure. And it turns out that this procedure works very well, not just for single-cell data, but for many other. For single-cell data, but for many other different kinds of data sets and technologies, spatial transcriptomics, DNA methylation, calcium imaging. For all of these data sets, if you inspect the empirical distribution of eigenvalues for the raw data, you never exactly see the Marchenko-Coster law. Sometimes you see something similar, but not exactly. And once you apply this procedure, then we observe very accurate fits to the Marchenko-Coster law. And remember that this procedure doesn't do anything too crazy. It just rescales the rows and the columns. It just rescales the rows and the columns of the data matrix. The key here is just to know how to rescale the rows and columns. You need to know something about the structure of the noise, which is what we're inferring. Okay, so I think I will stop here, but I just want to briefly mention two important things, very important things that I did not have enough time to cover. So the first one is that, okay, so we estimate the rank or the number of signals. We also want to recover the signals. So what do we do? The signal. So, what do we do? And what we suggest to do is to exploit this renormalized data matrix where we stabilize the behavior of the noise and truncate the SVD of that matrix and then undo the scaling, unscale by what we estimated previously. And we showed that this provides a kind of approximate maximum likelihood estimator for the signal under Gaussian noise. And also, it has a nice interpretation as a weighted Laurent approximation, where you try to find the Laurent. Where you try to find the Low-Rank matrix closest to the data, but under some weights, like a weighted loss, that re-weighs each entry inversely to the noise in that entry. And you can do better things in some cases than thresholding singular values. They can also do shrinkage, and optimal shrinkage in this case was developed by William Lieb. He's in the audience. And we have empirical results both on simulations and on real. Results both on simulations and on real data that show that this kind of procedure can significantly improve various downstream analysis tasks. And the last thing, which is related to Amit's question at the beginning, okay, are we satisfied with this model? So empirically, it works very well, but academically, it doesn't make a lot of sense to assume that all data has this rank coin variance structure. So we try to analyze this a bit further and try to understand. This is a bit further and try to understand what is happening here. And what we can show is that essentially the same procedure can work for more general variance matrices. And I want to explain if I have one more minute or something. I know I'm doing in time. So I want to explain a little bit more why is that the case. So first of all, the fact. If you have a variance matrix or just a positive matrix, a matrix with positive entries. A matrix with positive entries, you can always write it as a rescaling from the left and the right of some other matrix as tilde, which is doubly regular. And by doubly regular, I mean a matrix whose average entry in each row and each column is precisely one. Okay, you can think of that matrix as a kind of rectangular variant of a doubly stochastic matrix. So this is a consequence by Richard Sinkhorn. Sinkhorn from his famous paper in 1967. And if you use these x and y to re-normalize the data, so you will get a variance matrix that is not, it doesn't have identical entries, but it is doubly regular. Then we show in the previous work that you still get the Marchenko-Pastor law. So even though the noise is not exactly homoscodastic, you can still normalize in this way and get the Marchenko-Pastor law and also get that the largest. And also get that the largest eigenvalue of the noise converges to the upper edge of the machine stern law. So you should not get any outliers. And what we show basically is that we can still estimate these x and y using our procedure, the same procedure, if this matrix S tilde is sufficiently incoherent in some sense with respect to these vectors x and y. First of all, I'm not explaining here exactly what I mean by incoherence, but what we show, for example, Incoherence, but what we show, for example, is that for several ways, random ways for constructing S tilde, okay, suppose that S tilde is sampled from the space of these doubly regular matrices in some random way. Then you choose X and Y deterministically or randomly but independently from the generation of S tilde, and you form the variance matrix in this way. Then our procedure will work. You would still converge to the true. To the true vectors x and y using our procedure. Okay? So at least in these kind of random generic settings, we expect the procedure to work. And maybe, maybe that's an explanation for why it also works well for many different real data sets. Okay, so thank you. And this is the reference. So if you wanted S to be, say, rank two or higher rank, would you just need to solve the Dyson equation for us? Okay, that's a great question because this is one of the directions that we want to explore. So if you have a kind of a higher If you have a kind of a higher-rank model for the noise variance matrix, it's not enough to solve the Dyson equation, but you somehow need to be able. So basically, you only gain information from the main diagonal of the resolvent. And clearly, if you have rank greater than one in the variance matrix, there's not enough information from one diagonal of the resolvent to somehow extract the entries of the noise variance. So you need a more Variance. So you need a more elaborate procedure. Somehow, again, this is just a preliminary type of thinking. You need something that's actually more similar to compressed sensing. But I don't have a good answer for how to do this. I think it's possible in some situations. But yeah. Maybe the people that are Related to Joe's question. So, I mean, and also related to the question I asked you before. So, like, how's the relation to the physical model? So, I would assume that variance will not multiply, but actually be additive. So, like, if I have noise on a certain set and noise for a certain G, then the variance will be the adding the variance is not multiplying. So, this would be like a rank two. Plus x times y would be x plus y, but x plus y, but so but this factor is one. So still you have enough equation to solve, right? For the variance, but so maybe your Dyson can still work because you have enough equations, but maybe this, I don't know, at least I can understand what the model would be in that case, right? You have the measure of plus noise because of this, the noise because of that. Yeah. Yeah. So so we didn't. So so the we didn't so my question was like what's what's the physical model to multiply the variance of the different yeah so let let me try to explain so first of all we did not consider the model that you're you're describing might be interesting to to think about this and see if we can infer the the variance structure I have a bit maybe a better answer about the physical model so oftentimes what people assume so there are several kind of layers of complexity in assumptions but let's say the simplest model is to say that for the original Say that for the original data, which is count data, each entry is a Poisson. It comes from a Poisson distribution. And the parameter matrix, like the lambdas that you have in the Poisson, they are low rank. So if it was rank one, if that parameter matrix was rank one, you will get exactly the model that we analyzed here. Of course, it's not rank one, it's higher rank than that. And then you can say, because it's Because it's non-negative, usually the first singular value is much larger than the rest. And so you might argue that the Ren-Con model for the variances is some approximation for that. It's not a perfect approximation. And actually, the extension that I talked about is a better explanation for what is happening there. Now, there are more elaborate models that say that you have a Poisson, but the lambda parameter, the rate of the Poisson, also has a prior distribution. Prior distribution. Usually, I think people assume a gamma distribution, and then you get a negative binomial distribution for the entries. And in that case, the variance structure is more complex. So we actually developed a previous approach that can estimate the structure of the noise variance in these cases, but then you need to know the parameters of the distribution. If it's a negative binomial, it has parameter, you need to know that. Parameter, you need to know that. So it's more complicated. And what we wanted to do here is to treat kind of the case where the distribution of the noise can be arbitrary. So you don't need to know anything. So that's kind of the main advantage of this approach. Because in practice, what practitioners do is they apply some transformation to the entries. They subtract the means, normalize, do maybe log transform or some other things. And then nothing is left from the original physical model. Original physical model. So it's very hard to analyze that. Maybe different practitioners use different normalizations. So it's very hard. And we wanted to develop something that was kind of agnostic to that.  Oh, you're even diverted at the first time at the end. Yes, so yes, so we haven't done that. I think the physical distribution results like that. I think that was going to be the first part.