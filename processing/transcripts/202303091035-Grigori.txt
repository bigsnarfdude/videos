And for allowing remote participation as well. So, my talk today will be about communication in numerical linear algebra. So, I'll start with the motivation to why we need to think about communication. And then I'll discuss some of the results obtained 10 years ago about minimizing communication for dense linear algebra. And I'll focus on two different operations. So, the first one is just Different operations. So, the first one is just computing the QR factorization of a Dolan skinny matrix, and then I'll be looking at the column subset selection problem when using deterministic or randomized rank-debilling QR. So, those results will be asymptotic in terms of communication. And then I will be looking at some more recent results in which people look at deriving tight constants in this communication lower bounds. And the focus will be on a very simple. And the focus will be on a very simple operation, which is the matrix multiplication. Then I present some of the results as going beyond three-nested loops, like for example, when we want to do tensor operations. And then how can we avoid communication in territory methods? And then I will discuss some open questions and conclusions. So, first, why do we need to think about communication? Because the algorithms will spend some time in doing floating point operations and then Floating point operations and then some time in communicating data. And so communication could be in sequential across different levels of the memory hierarchy, or it could be in parallel between different processors. And so the time per flop keeps improving because now you have multi-cores. So the number of cores can increase per socket, the number of vectorization units, or now you can have tensor processing units. So that keeps So that keeps getting better and better. But the communication, it's a different story. So, communication, we look at two different terms. We look at the bandwidth, and so the bandwidth would be the number of words that you transfer per unit of time, and the latency. And the latency will look at what's the time it takes to send only one byte of data between two different levels of the memory hierarchy or two different processors. And so both are improved. Are improving at a much, much slower rate than the time per flop. And here I just take some numbers from the latency. And if you look at the latency between different levels of memory hierarchy, it's really decreasing very, very slowly, sometimes increasing, or it's only stagnating, right? So that's a real problem. Natural latency, it's even worse because if you want to transfer some data between two processors, you would Two processors, you would use MPI, and the MPI latency is a few microseconds. So we do need to take this to account when we derive algorithms. And people did look at this in a very long time. And the first result, which is the pioneering result, is from Hong and Kung in 1981, who showed that if you multiply two square matrices and you have a slow memory, which is our size M, and then a Or size m and then a large memory of infinite size, and you want to multiply those two matrices. And how many words do you have to transfer? Then that's proportional to the number of flops divided by the square root of the matrix size. And it also gives a lower bound on the latency because you just divide this by the maximum memory size, which is M, and so you get this bound: number of flocks divided by M. Number of flocks divided by m to the power three half. So it's easy to see that the same lower bound will apply to LU just by using reduction because you take this matrix, you compute its LU decomposition, and then you see that during the decomposition, you have to multiply two matrices. And it was proven in this paper from 2009 that those bounds apply to almost all direct linear algebra. So LU, SVD. LU, SVD, eigenvalue decomposition, QR factorizations, and they are also extended to fast linear algebra when you want to use Strassen like algorithms. So I'm not going to discuss fast linear algebra. I'll really focus on only n-cube-like operations in dense linear algebra. And so now, given those bounds, we can extend them easily to be used. them easily to be used in parallel because we just say now the memory per processor if that's proportional to n squared divided by the number of processors so i have square matrices n by n and i just distribute those matrices across all processors then i get those two lower bounds on communication so that's the volume of communication n squared divided by square root of p, and number of messages. root of p and number of messages square root of p and so that's really important because if you look here independently of the matrix size that's the lower bound on communication we'd like to attain and so now if we look now at different algorithms as implemented in scale pack for example we could see that they do minimize the volume of communication when we consider this kind of a memory size but they don't attain the lower But they don't attain the lower bound on the number of messages. So, why? Because in LU, for example, you use partial pivoting. And when you use partial pivoting, you have to, for each column, to determine the maximum element in this column permuted to diagonal position. So you have as many messages as columns just because of partial pivoting. And so we introduced another strategy which is called tournament pivoting. Is called tournament pivoting, and Jim was explaining this during his tutorial, so I'm not going to discuss this. Then, if we look at QR, the one which is based on column-based householder QR, then again, for each column, you'll have to compute the norm of the column because you have to compute this householder vector. And so, you cannot attain the lower bounds on communication. And so, there is an algorithm which is a reduction-based. Is a reduction-based householder kind of an algorithm, and I'll be presenting this briefly. And then, if we keep going in round revealing QR, one would use Callum pivoting again, as many messages as columns in the matrix, so it doesn't minimize communication. And then here we use tournament pivoting to reduce communication. And then randomized QICP plus tournament pivoting can also reduce communication. Pivoting can also reduce communication. Who detains this bound? And then for eigenvalue solvers, the ones which are classically implemented in Scalapak will not reduce the number of messages, but there are eigen solvers which are presenting during this workshop, be it the randomized version that we've seen or the one which was presented during Jim Demos tutorial, and those will allow to minimize communication. Communication. So let me briefly explain ESQR because that's a simple idea, but it really works very well in practice. So suppose you want to compute the QR factorization of a toll and skinny matrix. And so we have four processors, we distribute this matrix over four processors. And now what TSQR does is to compute locally a QR factorization. Locally, a QR factorization. And now you can use whatever the best sequential QR factorization could be, which is turned out it's a recursive QR factorization algorithm. And so after this step, where most of the flops happen, we have four upper triangular matrices. And now we combine those two by two. We do a new QR factorization until we get one final R factor. So arrows here represent. So, arrows here represent messages, so we have log P messages. So, now we are independent of the number of columns we are factorizing, and so that's why we can attain the lower bounds on communication. The idea is old, so it goes back to the papers from late 80s. What's new is that you can use any kind of reduction tree. So, here I took a binary tree, but it can be a flat tree, it can be a binomial tree. It can be a binomial tree. It could reflect the underlying architecture. And when you use this to factorize a square matrix, you just replace the permanent factorization with this kind of factorization, and you get an overall communication optimal QR factorization. Easy to implement as well. And now you might think that we don't have any more those householder vectors that the classic QR would have it, but you could reconstruct them. Have it, but you could reconstruct them. So we could reconstruct the householder vectors which reflect the entire columns of the matrix. And of course, now we double the number of flops if we do this. But let me show what one can get in practice. So here, we took a matrix which has only 32 columns and Z many number of rows, and we increase the number of processors, and then we look at the effective. And then look at the effective flop rate. So, higher means a better algorithm. Down here, you have the QR factorization from Scalapa, so the one householder-based. And then you go up and up here is the TSQR. And so easily a factor of six improvement. If we have to reconstruct the householder vectors, then we do twice more flops, so we get less improvement, but still a factor of 2.7. Of 2.7. So the pure factorization is essentially unique, of course, modular round of errors. So no matter what kind of shape of the tree you take, the R factor is going to be the same. Again, module round of errors, but we did change the representation of the Q factor. So that's a simple example. That's a simple example, but it turns out that's also one which works really well in practice. It works also well for GPUs. But now let me move to something which turns out to be more complicated, and that's the rank-reville and QR factorization. So here we want to compute the factorization in which we permute the columns of A, and then we get the factorization in which what we want is to have here a well-conditioned R11. Well-conditioned R11. And in other words, I have here selected k columns, and I would like those columns to reflect the largest singular values of A. And if I look at the residual matrix R22 here, then I would like this to approximate well the trailing singular values of A. So we use this definition to say if we attain those approximations, then that's a rank-revealing factorization. Revealing factorization. So, in other words, what I said is that if we look at the ratio between the leading singular values of A and those of R11, then those are bounded by a low-degree polynomial in N, which is the number of columns of A, and K, the rank K we have chosen. And so, there are different ways you could use this authorization. So, you can say, I want to select K columns such that I K columns such that I get k linearly independent columns. So, in linear algebra, that's how you do this. You can use those to get a low-rank approximation, of course, once we select those k columns, then we just use an octagonal projection. So, one option is to use QR with column pivoting, which works really well in practice, but will not, this algorithm will not ensure we get a low-degree polynomial here. And there are many. And there are many important works in this field with many important papers. So, I took here the one from Gu and Eisenstadt, which showed that there exists such a permutation such that you can get this relation in which the polynomial here has this form. In addition, you also get a bound on R one one inverse R one two norm max. Norm max, which is bounded by a small constant. And for example, that's what is used in interpolative decompositions. So that's going to be called strong Rangrivillion QR. So how this is obtained, you do first QR with Callum pivoting, and then you do extra permutations to increase the determinant of R11, a finite number of those Callum permutations until you fulfill those two conditions. Those two conditions. In practice, QRCP works already well. So, very often, you don't need to do those extra permutations. There are cases when you do have to, usually, a very few number of those permutations. So, it's going to be expensive. And why? Because you have to do those permutations to find the column of a maximum norm. And so, all in And so, all in all, if I do, if I aim at the rank K, if I just do QRCP, I'll already do K log2P messages. So, K synchronizations among all processors. If I have to do strong rank evidence in QR, it might get even worse. And Mihao was showing, he was comparing this QR CPU with QRs, no key voting, and you could see. No key voting, and you could see there was quite a big difference in performance. So, those permutations do decrease the performance we can get. And of course, we can't use BLAT3 either. So, that's also another point. So, one option to do this in a deterministic way is to do this tournament revoting in which we do the following. So, suppose I have four processors. So, suppose I have four processors and I distribute those columns among processors. And now I do locally a strong graph developing QR to select K columns. But now I want to select the best K columns among those sets. So what do we do? We put those together two by two. Again, by using this kind of a binary tree reduction, I select new k columns, I combine those together and I got my final. Together, and I got my final k columns. And now we take those k columns and we say that that's the selection of my best columns with some definition of best. And it turns out we can prove that this does remain a strong rank revealing QR. So we have those bounds. So we got a good approximation of the leading singular values of A, a good approximation of the trailing singular values of A, and the radio. Values of A, and the ratio is bounded by this term here. And before, for strong wrong revealing QR here, I had a lowercase f which could be only 2. Now, if we do a binary rereduction, this f has this expression here, so where I have n divided by k to a power, but which is a small power, so logarithmic 2 of something which is small. So we can still say it's a low degree policy. Low-degree polynomial in n and k. It's getting worse if I use a flat tree. So then we really get something which is exponential. But it's important that we can guarantee those bounds. And the main point was that the stronger VQR was now local on one processor. So I could do these extra permutations. That's less intuitive, but we can also do this kind of a selection. So now suppose that I have. Selection. So now suppose that I have my columns distributed across processors like this. So I have blocks of rows. And now we do local selection here of some k columns, other k columns selected here, other ones here. But now we again combine them two by two. So I combine those with those, extend it to the two blocks, and I select new K columns. We put them together and we select the final K columns. Select the final k columns and we can still prove bounds. So, those still remains a strong round reviewing QR factorization. So, if we put those together, basically, now what we could do is to take the matrix divided on a 2D grid of processors. So, I have on each processor a block of the matrix A. Now, we do locally a strong RAM revealing QR, select those K columns, then combine them along a binary tree. Along a binary tree and get my final K columns. So the main cost here comes from this local strong random developing curve factorization and then those extra factorizations that we do here, but those are sort of small in terms of flops because we only do those two K columns. So that's not going to cost much more if K is reasonably small with respect to the dimension. Reasonably small with respect to the dimensions of A. So we do this. So now I just show what happens when you do this on an image. So the purpose is not to use this for image compression, but just to see what happens if I take an image and I want to select K best columns from this image. So we divide it here in two by four blocks and we call locally the strongbrand revealing QR. And in purple you have the column. And in purple, you have the columns which were selected, and then here you have the image which was reconstructed from those columns. And so, as we could see, for example, here there is not much information because almost all those columns are probably going to be linearly dependent. But here, there is more information. So, as we go, so now we combine the columns from those two. Some other columns are selected, but when we put those together with those two, But when we put those together with those two, we see that more columns are selected towards the part of the image where there is more information. And so finally, the purple columns selected are mainly around this part here. And the image is recovered pretty well. So now let's compare this with the randomized QRCP. So Miha already presented this, so I don't have to go into the details of this, but we are given. Of this, but we are given the matrix A. Now we multiply with a random matrix omega and we obtain a matrix B, where now B has only L rows. So the goal is to compute K, to select K columns, and L is going to be larger than K because there is some oversampling. Once we got this B matrix, now we compute here K steps of QRCP. We select K columns from We select K columns from B and we return the K selected columns. So, this was proposed independently in those two papers. And so, let's look at what guarantees do you get when you have this kind of factorization. So, the guarantees I've taken them from this paper. And so, what the paper says is that if I compute this. Paper says that if I compute this current evoted QR factorization, we have these relations which are satisfied for all permutations, which means that the singular values of A are bounded by the we look only at the K leading singular values by are bounded by the singular values of R11, R12, plus the two norm of R22. R22, which means that if we want a good approximation, then we'd like this to be close to the singular values of A L plus ones. So basically, you'd like this to be small if there is a gap, for example, in the singular values. So this L2 norm is bounded by this quantity here. And so if we look at QR scalum pivoting, G2 here is upper bounded by 2 to ZF. Upper bounded by 2 to the L. So that's why I was saying QRCP doesn't guarantee the properties of the rank-revealing QR factorization because you have an exponential bound here in L. If we look at the randomized QRCP, as the paper was saying, randomized QRCP is as reliable as QRCP with high probability. And so this is why, because this G2 is bounded by this quantity, and so what you have here and so what you have here is something to the power L minus one and here you have a dependency on epsilon. So of course if this is epsilon is small will mean that you will have to oversample too much because then you have also bound on the dimension on the oversampling size. If you want high probability and then again so there is this trade-off but basically Trade-off, but basically you have this dependency, you have this exponential term which shows up in the bound. So it's doing less well in terms of warranties than strong run-revealing QR or the tournament voting version. There is also a strong version which is introduced in this paper, but that would require to go back to the matrix A and do these extra permutations on it. And do these extra permutations on A until you reduce this G2. So basically, but if you do this, then probably you lose a lot in efficiency. Nevertheless, SQRCP works well in practice. Randomized QRCP as well works well in practice. So I think there is a theoretical gap here, which maybe could be improved. So now let's look how those doing practice numerically. And so here we have three matrices. So, here we have three matrices, and we look at relative Robinson norm error for different approximation ranks. And we look at QRCP in red, QR istonometric in black, and randomized QRCP in blue. And so for those two matrices, the results are superposed, so it's the same. And here, QRTP is doing less good, slightly less good, and randomized QRTP. Less good and randomized QRCP as well as QRC fix. So they all work well in practice. Now, is there a clear win or no? Because when you look at what you do in parallel, if you do the randomized QRCP, then you multiply your matrix A with a random matrix. Now suppose that that's a Gaussian matrix, easy to parallelize, and now you got this P matrix. You got this B matrix. Now the bottleneck becomes again QRCP because here you want to run QRCP. Of course, you could combine this with tournament reporting, but if you just use QRCP and you do this in parallel, then the cost you have here might be the same cost as what you're doing in tournament evoting. So again, a QRCP on a matrix, which could be the same dimension as a matrix that is getting on one processor. So could be the same. Is getting on one processor, so could be the same cost. So the point is, even if you do randomization, you do need really an efficient algorithm for the deterministic QR CP. So I guess that's the main message. Because now if we look at the performance, you can come up with cases in which one wins or the other one wins, depending on what rank you choose, depending on how many processors you are going to use. You are going to use. So, here we took two cases. We used this code from the paper by Xiao, which is available on GitHub, because that was an MPI version of randomized QRCP. And then we do two tests, a strong scaling test in which we took one dimension, one matrix, fixed matrix, and here we increase approximation. Here we increase the approximation rank. Right, sorry, it's not really a strong scale increase, but to increase the approximation rank here, we fix the matrix dimension and the number of processors. And in the other one, we do we scaling, which means that I increase the number of processors, but also the dimension of my matrix, such that per processor I keep the same dimension of a submatrix. Submatrix. Okay, so what you could see here is that for small ranks, QRTP is doing well, but then at some point when the ranks increase, then the randomized QRCP becomes better. So, as I was saying, it's really a choice of what rank you choose, how many processors, and you can make one go faster or the other one go faster. Okay, so now I'm moving to the second part in which we'll be looking at communication lower bounds with tight constants, because what I've shown so far, I didn't focus on the constants as well. So those are only asymptotically minimizing communication kind of algorithms. But people did focus on identifying tighter bounds. So let's see if it's the theory. So, let's see a bit of the theory which is behind those communication lower bounds. And so, the main inequality which is used here is a Loomis and René inequality. And so, if we do multiply two matrices, we have three nice state loops going, for example, for square matrices from one to n. And in the inner loop, we do Cig equals Cig plus Aik times PKG. And so those multiplications can be Those multiplications can be seen as lattice points in a cube in which I have three faces A, B, and C. And if I take this multiplication and I project on the three faces, I get the elements of A, the elements of B, and the elements of C. And now, given that we have a memory of finite size, M, then what we'd like to know is, given the data I have in this memory from A, B, or C, how many computations can I do? Computations can I do? And so, for this, Lumi-Sweeney can be used, which says that if I have a finite set of lattice points, V here, and I look at the projections on the A phase, B phase, and C phase, I could bound the number of floating point operations by the square root of the product of the projections on phase A, phase B, and phase C. So we have. And phase C. So we have this inequality here. And so this will tell me at a given point in my execution how many flops can I do given the data I have in the memory of size M. This inequality also suggests that when I do the algorithm, the most efficient way of using data will be to use rectangular prisps here because I might have the equality between the surface and Between the surface and the product of the surfaces and the volume. And so basically, quickly how those bounds are obtained. For example, in the case of matrix multiplication, the bound is obtained as follows. So suppose that we are instituting this code and then we broke the instruction stream into segments, such that in each segment I can only do x loads and stores. And so when I start the segment, So when I start the segment, I have already m data, and then I can store this data and reload some new data. So basically, this x is bounded by 2m. And now we can use this Lumi-sweet inequality to bound that at each segment, the number of floating-point operations is going to be bounded by this quantity. By this quantity. And so, from this, I could say I know that all in all, I have to do M and K multiplications. So, how many segments am I going to have? M and K divided by the number of loads I can do in each segment. And from here, we can determine the number of loads and stores, which then is lower bounded by x multiplied with the number of segments. And so, basically, this gives you. And so basically, this gives this kind of a lower bound where I have the precise constants in the lower bound. And so that's for general matrices. I'm multiplying A of dimensions n by K with B of dimensions K by N. So that's a bound which is published in this paper. And it's tight. It's attained by a block algorithm in which you make a precise choice. Precise choice of the block size. So it's a tight bound. So now when we go to the parallel case, we take this bound and we say, let's see what's the memory size I'm going to take in the algorithm. And so a typical choice would be to take this memory size to be such that we can store A and B and C on the P. And B and C on the P processors. So the memory size is N squared divided by P. And so those are called 2D algorithms. And so if we look at the bound we obtain in terms of volume of communication, so that's the bound I is presented at the very beginning. So it's n squared divided by square root of p. Then people introduced 3D algorithms in which now they say we don't have to use only one copy of the data, we can use One copy of the data, we can use p to the power one-third copies of the data. So I increase the memory size per processor and then we decrease the volume of communication. And more recently, in the PhD thesis of Edgar Solomonik, he introduced 2.5D in algorithms in which the idea is: let's suppose I can use C copies of the data, then what do we get in? Then, what do we get in terms of volume of communication? We decrease the volume of communication by a factor square root of c. So, you could see a trend that if we increase the memory size, then we decrease the communication cost. But there is a limit to this. So we can't increase, there is a limit to how much better we can get in terms of communication. And that's called a memory-independent lower. That's called a memory-independent lower bound. So we can derive bounds in which we are independent of the memory size. And with this bound, you obtain that the volume of communication is bounded by n squared divided to the p to the power two-thirds. So that corresponds to the 3D algorithm. So if we use more memory than this, there is no gain anymore. The communication is not going to decrease. So now, given those bounds, So, now given those bounds, let's see what algorithms attain the bounds. So, in the 2D case, that's a well-known algorithm which is called SUMA. And here, the presentation is coming from a paper by van der Geyn and Watts. And SUMA is a 2D algorithm. So, it parallelizes the algorithm along two dimensions of the iteration space. And so, in this algorithm, In this algorithm, A, B, and C are distributed over here four by four processor grids. So each processor gets a block of C, a block of B, and a block of A. And so this processor will compute this part of C. And so the algorithm will iterate over columns, block of columns of A, blocks of rows of B, and at each iteration will compute the outer product of The outer product of this block of columns of A with this block of columns of B. And so, for doing this, at each iteration, a processor which owns this block will broadcast a block to the other processors in the same row of the processor grid. Here will broadcast this block of rows of B along to those processors here. And once every processor receives the Once every processor receives the needed data, then there is a computation which is performed. And so this algorithm will attain the 2D bounds because we have only one copy of the data on each processor. Now, in the 3D algorithm, we'll consider that we have more memory and we can store copies of A and B and C on each processor. So 3D algorithm. So, 3D algorithm will parallelize the algorithm along the three dimensions of the iteration space. And so, basically, now each processor will have to compute the flops associated with this cube here. And so, at the beginning, this algorithm will start with the data A and B distributed among processors. So, if we look at this specific processor, So, if we look at this specific processor here, this processor will own only a block of columns of A, a block of rows of B. And so then this has to be distributed among processors such that after this communication phase, this specific processor will own this part of A and this part of B. So after this first communication phase, now each processor computes Each processor computes a partial result of C. So it computes the flocks which are associated with each part of the cube. And then there is a nut communication in which we sum up all these results to obtain the final C. So, with this kind of a 3D algorithm, we do attain the lower bound. Attain the lower bound on communication for the three decades. So now, as you could see here, the volume of communication decreased. And basically, if we look at the latency, that's only on the order of log P. So you can't go below this. So that's the limit. So going further a bit here towards getting tight communication. Getting tight communication bounds. Now let's consider that we have we are multiplying rectangular matrices. So A and B now have dimensions. A is m by n and b is n by k. And we suppose here that m is larger than n, larger than k. And we want to use p-processors. And so you can solve this problem and obtain a And obtain this kind of bounds, which depends on the dimensions of M, N, and K with respect to the number of processors. And so basically, the bounds say the following. We have a case in which we have one large dimension. So as you could see here, one large dimension and two small dimensions. And so in this case, what you should use for matrix multiplication is a 1D algorithm. Duplication is a 1D algorithm in which the matrix B is duplicated among all processors and then everybody computes a part of the product A times B. So basically that's what you could see here that in the communication we have to communicate B, which is the dimension and K among all processors and then once this And then, once this communication is done, all the computation happens locally. Of course, there is some subtraction because, to begin with, the processors had one piece of B. In the second case, we have two large dimensions. So, we have this kind of an outer product in which we multiply A with B, two large dimensions, one small dimension. So, in this case, One small dimension. So, in this case, the best would be to have this kind of a 2D algorithm in which A and B are communicated, but not C. And in the last case, we have three large dimensions with respect to the number of processors. And in the case, the best you could do is a 3D algorithm. Those are memory-independent. So, we assume that there is enough memory on each processor such that you could choose among those. such that you could choose among those three kinds of an algorithm three kinds of algorithms so this lower bound is tight and with tight constants because it's attained by this kind of a grid based algorithm in which you you make the optimal choice between 1D 2d or a 3d processor grid and those constants do improve the existing bounds so we I put here a couple of existing algorithms and so in all of the three cases And so, in all of the three cases, we improved the constants. Just some idea of how do you solve this kind of problem. So, that's the key optimization problem we are solving. So, we are given X, Y, and Z. So, you can think of this as the projections on the three phases A, B, and C. And we say that we want to minimize the amount of data. The amount of data which is required by a processor from each matrix. So we minimize x plus y plus z. And we have four constraints here. So the first constraint is precisely Lumi Suine. So we know that we have to satisfy this because of Lumisuine. And we have three additional constraints, which are constraints with respect to the dimensions of the projections on the A phase, on the P phase, on the C phase. P phase on the C phase. So basically, you can show that you have this inequality which has to be satisfied. And once you write this optimization problem, then you get the analytical solutions, which gives us the three bounds with the tight constants. So that was all about dense linear algebra, three nested loops. So now let's see what happens if you want to go beyond. If you want to go beyond those three nested loops, so there are bounds which were derived in 2013 in this paper here, and those bounds rely on a specialization of the Holder-Branz-Kaveli multilinear inequality, which is an extension of Lumi-Swini to higher dimensions. And those bounds are more complicated than what we've seen for the Lumi-Swini case, but they could be applied. But they could be applied to programs with any number of nested loops in which the array indices are affined functions of the loop indices. They turn out to be tight when the loop bounds are sufficiently large. And it was also shown that you can derive optimal tiling such that you can attain this lower bounds. So those more general lower bounds were used to derive were used to derive some bounds for some tensor computations. For example, the symmetric tensor contractions, the matrix size tensor times Katry-Rau product, and also for any number of nested loops and arbitrary loop bounds when the array indices are subsets of loop indices. And that was, for example, applied to CNNs in this paper here. So just to give you an intuition of what's happening, so let's consider the multiple tens. So let's consider the multiple tensor times matrix multiplication. So multi-TTM here, in which I will take only three tensors with three modes, but this can be extended to any number of modes. And so in this operation, we are giving a tensor X and a tensor Y, and we want to perform this operation in which the tensor X is multiplied along the first mode with. Along the first mode with matrix A1, along the second mode with matrix A2, and along the third mode with matrix A3. So you could recognize that this kind of operation is very useful, for example, if you want to do the tackle factorization. So if you want to compress your tensor to a lower order tensor, or if you want to do the opposite of Opposite operation, it's also the same operation. So, if we are given such an operation, so you might think that you could, of course, simply compute this as a sequence of tensor times matrix multiplication. So, you do first the first unfolding, do the operations, then you do the second, you do second unfolding, the second multiplication, third unfolding, third multiplication. So, you could get a lower bound. So, you could get a lower bound by considering that there is a lower bound for each matrix multiplication, but it turns out that you can get a better bound and a lower communication volume if we do this multiplication all at once, not in a sequence, but all at once. So, basically, if we write the algorithm of multi-TBM, we have this algorithm here where we have six loop indices and we open. And we operate on five arrays. And so, in the inner loop, we do this kind of multiplication. So, the idea here is that if we impose that this multiplication is done atomically, which means that each processor, we don't do this in sequence, but you do it at once and you do it on only one processor. So, in other words, when you parallelize, So, in other words, when we parallelize, basically each processor will compute a local multi-DTM, then we can get better bounds than when doing a sequence of those operations. So, those are the bounds we are getting. And I'm just going to show how the volume of communication compares when we do this kind of a parallel atomic multi-TTM with TTM in. With TTME sequence. And so we have here two graphs where we look at the volume of communication when increasing the number of processors. And so in red, we have the parallel atomic multi-TTM, and in yellow, you have the TTME sequence. And so we can see that we do get a lower volume of communication. Now, the number of flops might. The number of flops might be increasing, but in most of the cases, that remains a lower order term. So, on the right here, we put the percentage of increase in terms of floating point operations. So, here it's a 6% and up here is 70%. Okay, but that's kind of results you can get when you consider this lower bound on communication. You can derive different. You can derive different algorithms than the one you might think intuitively of as the best. Okay, so now let me move on to avoiding communication in iterative methods. That's an important body of work going on in communication avoiding algorithms. And so, if we look at the Krilov solvers and what we do in the Krilov solver, we generate a Krilov basis by starting from an initial. Basis by starting from an initial vector R0 and then multiplying powers of the matrix A with this R0. And in general, to maintain stability, what one would do is to orthogonalize a new vector against the previous vectors at each step of the iteration of a Krilov solver. And so the idea here to avoid communication is to roll case types of these iterative solvers. Of these iterative solvers. So, in other words, to generate a set of vectors at once. So, you generate R0, AR0, A square R0, A power S R 0 at once, and then you orthogonalize them or A orthogonalize them together. And so, you can avoid communication by doing this. So, those are called S-step methods introduced in the 80s. Methods introduced in the 80s. And then there were two theses which focused a lot on this: Mark Huyman's thesis and Erin Carson's thesis, in which they develop those ideas for different Krylov solvers, including preconditioners, and looking at the stability of those methods. And so, if we just look at standard GMRS, so in GMRAS, you would start from an initial value. Start from an initial vector, then you use modified Gram-Schmidt. For example, suppose that classical Gram-Schmidt doesn't work, so you have to use modified Gram-Schmidt to orthogonalize this vector against the previous vectors. You obtain a Hessen-Bern matrix, and then after a number of iterations, you solve a least square problem with this Hessen-Bern matrix. If we do in a communication of weighting generals, then what you do is that you generate those k vectors at once, then you use, for example. Then you use, for example, TSQR to autonomy as vectors, and then you build the Heisenberg matrix, and then you solve the least square problem. And if you manage to do this with no communication, because you can duplicate some data to have no communication here, then you only communicate now for TSQR. So you can get important speedups. So if I look at the parallel case, you have some. You have some communication and use a sparse matrix vector multiplication, and then you get messages from modified Gramschmid. So that's an order of k squared log p for k iterations. And now if you look at the communication awaiting generals, you can compute this power matrix kernel, so it's only O of one messages, and then lock-pie messages from TSQR. So it really So, it really reduces the communication. However, of course, if you just use monomial bases, then you might get in trouble because that becomes a power method. So, that's a graph which comes from Mark Hohmann, I think. And so, if you just look at the GMRES convergence for a matrix here, if you do non-started GMRES will be here, but if you GMRES will be here, but if you are going to do this a step method, then you are losing convergence. And then what you have to do is to change from monomial basis to polynomial basis, and then you can recover convergence. So there are some limitations. So how large can be this K or S in if you call this S term method? And how do you deal with this loss of precision? And what preconditioners can you use, of course? preconditioners can you use of course because you might not you cannot use now a preconditioner which will involve communications that you just managed to get rid of so some there is some more complete conditioners so you can use of course a block jacobi but there are some other propositions as hierarchical semi-separable or or deflation to to add also preconditional to this so let me now get close to Let me now get close to conclusions here and open questions. So, what are the open questions? So, I did show some type boundaries type constants for several algorithms, but there are many algorithms for which there are not yet type constants established. So, for example, for fast linear algebra, I think there is no tight constant which was derived. Another interesting point is the symmetric operation. point is the symmetric operation. So if you have something like CIRC operations here in which you multiply A with A transpose to obtain a matrix C, if you look at the parallel implementations of CIRC, they have the same communication cost as matrix multiplication, but they do twice less flows. And so there are recent results on lower bounds on communication in terms of volume of communication. And Julian was showing yesterday. Julian was showing yesterday some of those bounds that you can obtain. So, those are memory-dependent bounds. For example, for sequential circ, they have obtained a factor of two to the power three halves smaller than gem. We look again at memory-independent bounds. So if you look at the memory-independent bounds for parallel circ, then we've shown that there is a factor of two less than maximum duplication that could be attained. And you couldn't. And you couldn't make this bounds if you use this triangular blocking that you might remember from Julian's talk from yesterday. But basically, what's nice here is that you can extend this Loomis-Sweeney inequality to take into account that there is symmetry and you can get more use of the data. What's open also is many operations when you have to deal with tensors. What I've shown are next is. What I've shown are next-base loops with no dependencies, so that's also something which needs to be addressed when you have dependencies. And there is a body of work on looking at this from a compiler's perspective. So can you automatically generate those lower bounds and can you integrate dependencies? And there are recent works which look at those automatic generation of the lower bounds, and they do a Of the lower bounds, and they do apply this to Polybench. And Polybench includes some of linear algebra operations like Gram-Schmidt and Strolle-Schemidi. And those methods are sometimes available in libraries. So progressively, slowly, they get in LAPAC, ASKELEPAC, Slate. You might find some of those in GNU Scientific Library. In GNU Scientific Library. We are writing a book on communication availing algorithms with Gray Ballard, Erin Carson, and Jim Dammo. We are not working hard enough on this, so it's a bit of a slow process, but I took many figures from that book. And with this, thank you.