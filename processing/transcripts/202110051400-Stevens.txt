Five tech organizations: Google, Amazon, Facebook, Apple, Microsoft, they're each running tens of thousands of experiments per year. LinkedIn recently published a paper where they reported having in general, or they're generally running 400 or more experiments at any given time on any given day. So this is sort of huge experimentation aspect. Huge experimentation at scale. So much so that there exists this cottage industry of companies who exist to provide as a service help for other companies in running their experiments. So for example, you might have heard of Optimizely or Google Optimize or VWO, some of these things you see listed here on the screen. These companies exist to help other companies run their experiments. And Optimizely, arguably the largest of these The largest of these organizations. To give you a sense of their size, they have around 500 employees. They were recently acquired for an undisclosed amount, but prior to that, they were valued at over $500 million. So this is a big business, these online controlled experiments. So to give you a sense of sort of what they are and how they work, like any sort of an experiment, there's Sort of an experiment, there's a control. The experimenter has to control something. And typically, what is controlled here is different versions of some sort of a product, whether that be an ad, whether it be a version of your website, whether it be an ad or sorry, a headline and an email promotion. Whatever the situation is, the experimenters are controlling something and they'd like to, through a designed experiment, figure out which treatment is optimal. Which treatment is optimal? And so you might have heard the phrase A-B test before. That gets used quite often, quite colloquially. A traditional A-B test is one in which there's just two experimental treatments. We'll call them A and B. The example that I've got here on the screen is two different versions of a website layout. And I'm sure if you've done any online shopping, you've experienced these two layouts where it might be a list view or a grid view. And perhaps the goal here. Perhaps the goal here is to identify or to determine which one of these layouts increases the proportion of customers who add something to their car. And so what we do is we randomize experimental units in this context. These are typically people into the different experimental treatments. We observe some response variable on them. And by way of some sort of a hypothesis test, we decide which of these experimental treatments is Experimental treatments is superior. Or if they're not different, then maybe that's our conclusion. But that's the general idea behind these online controlled experiments. And it might seem very straightforward, it might seem very simple, it's just a two-group comparison, but one of the arguments I want to make throughout the talk today is that they can be much more complicated than this. And I think that in industrial statistics. In industrial statistics, anyway, this area of experimentation gets trivialized. People think, oh, this is just a two-sample t-test. When that might be true some of the time, but there are a lot more complicated scenarios, complicated experimental designs. And I think it's something that we as statisticians and industrial statisticians in particular should be paying attention to. So, what kinds of things are companies experimenting with? Basically, anything. With basically anything to do with user onboarding, user engagement, user retention, things with email promotions and headlines, aesthetic components of a website, the way it's laid out, where are the images, where do the ads go, where does the text go? What colors are we using? What scheme? All of those kinds of things are all ripe with opportunity for experimentation. Check out funnels, free. Checkout funnels, freemium conversion, how many Wall Street Journal articles you can read for free before you have to pay? Ad campaigns, just the content of the ad. You might play with a few different versions of an ad and determine which version was optimal by some metric. Language on buttons, machine learning algorithms. If Uber is trying to experiment with their matching algorithm, the algorithm that matches a driver to a rider, that To a writer, that could be done under the guise of a designed experiment. And if you want to gain an appreciation and understanding of some of these sorts of contexts, I've included a link at the bottom of the screen there that is an interesting website where I imagine somebody just goes in various incognito browsers trying to find experiments on the internet and uncovering them. And so they are predominantly. And so they are predominantly under the realm of user interface experiments, but nonetheless, it gives you an interesting glimpse into the sorts of things people are experimenting with. So I encourage you to explore that. And I will make these slides available. Anything you see in paint here is going to be a URL that you can click and visit. An interesting concrete example that just came out a week ago was this one with Ryan Reynolds, one of Canada's favorite celebrities. He recently He recently did this A-B test, or he was a part of an A-B test where there are two different versions of an ad for this website, mountain.com, that he was a part of. And the first version that you see on the left was one where he was basically a talking head talking about, I guess, the benefits and why you should care about Mountain.com. And then the right-hand side was the same audio, but there was no image or video of him. It was just a narration of. A narration of images and videos. And I think, you know, much to the bruising of his ego, they found that his face wasn't increasing conversions. So what they cared about here was click-through rates, you know, which version of the ad maximized click-through to visit the webpage. And it turned out it was the one where his face was not involved. And that's one of the really exciting, I guess, byproducts of these designed experiments is that. Of these designed experiments, is that they let the data speak for themselves and they can often contradict what you might expect, what the people at Microsoft refer to as the hippo, the highest paid person's opinion. All of that can get turned on its head when you use an experiment and the data that's collected from it to make your decisions. I've included a link there to a YouTube video of him being broken the bad news that his face wasn't worth very much. So if you're interested. So, if you're interested, you're welcome to have a look at that. Here's another list of various experiments that are happening or have happened at various companies. I think, again, these are all links that you can check out if this interests you at all. The one I want to sort of bring attention to as we move into the next segment of the talk is this bottom one, where Lyft, a ride-sharing app, much like Uber, they do lots of experimentation. Of experimentation, they are sort of a very mature company with respect to designed experiments. One of the more complicated problems that they're working on, and one that we're going to talk about later in the talk, is network interference. So they're operating a business within a network and issues of spillover and violation of the SUPFA assumption, all of that kind of sort of practical issues rear their head in their ride-sharing network. And so you can read at that. That link, a little bit more about it, but I will, as I said in a few minutes, talk more about network interference. So, what that is or what this conversation is leading me to is hopefully to convince you, if you don't know already, that these online controlled experiments, although they are generally colloquially referred to as A-B tests, they are in general not always A-B tests. And an excellent example of this is This is this referred to it as sort of a big win for Obama's 2008 presidential campaign, where through factorial experiments on the homepage of his campaign, through these factorial experiments, they were able to increase donations by $60 million. And so what you see here in this image is the final version of the web page, the home. Of the webpage, the homepage for his campaign. And there were two, well, I'll use DOE language, there were two factors. One was what you see in the middle here. This is what they referred to as the media factor. That was either a photo or a video. And within each photo and video, there were three options. And then this button called Action Language had four different versions. There's the one you see there that says learn more. There was, I think, sign up, sign up now. I think sign up, sign up now, and a fourth that escapes me at the moment. But they had 24 factorial combinations of these levels of these two factors. And it was through this designed experiment that they found that, in particular, this combination that you see on the screen was the winner. It increased conversion by 40% relative to what they were previously working with. I'm working with. And they also found, as you do with sort of factorial designs, not only might you isolate the optimal treatment combination, but you also learn about the various influence of the different factors. And what they found was that videos didn't help at all. In fact, they hurt. Photos were better. But then ultimately, this combination was optimal and led to an additional $60 million in donations. Dollars in donations. Another example of factorial experiments in this realm, with an example in advertising, comes from Survey Engine. Survey Engine is much like Optimizely and some of these other companies who help other companies run their surveys and their experiments online. This company, as you might, if you know German, you would recognize this as German, ran an experiment where you'll Experiment where you'll see that these two ads are more or less communicating similar things, but in different ways: the different exercise pose, different color schemes, different language used in these bubbles, different prices per month. And it was through a sort of multivariate factorial structure that they explored what is the optimal combination of all of these things that optimizes the ad's performance. I see a hand raised. Is that you, David? It is I, Nathaniel. Do you recall if any of these experiments found interactions between the factors? Not the Obama one in particular. I can't recall if they said that there was interactions there. I imagine that there were, given that the effect of I think they said something like there was one of the videos was not uniformly worse than all of the other videos. So there were like some treatment combinations where the treatment effect wasn't the same at the different levels of each of the factors, but they didn't admittedly get into that and certainly didn't show estimates or anything like that. And I'm glad you brought up the point of interaction. I'm going to address it again later, but there's an interesting, I think, belief. I think belief in this community that interactions are generally non-existent, which I'm not sure if I believe. I think interaction is kind of a natural law, but nonetheless, I think that there is not as much attention paid to them as in, say, traditional industrial settings, for better or worse. Speaking of interactions and Speaking of interactions and something along these lines, another concrete example is one that Lyft did. And if you're either a Lyft customer or any sort of a customer of a ride-sharing app, if you don't use that service in a while, you may yourself have received some sort of a notification or an email saying, hey, come on back. Here's a promotion. Have a 50% off of your next 10 rides. And so Lyft was experimenting with that kind of re-engagement promotion. Of re-engagement promotion. They wanted to figure out what was the optimal promotion, what discount should they consider, and for how many rides should that discount apply? And the experiment that they did to run this was a two to the two factorial experiment with a center point condition. So the two levels of discount were 10 or 50. And then I should say the higher low, there was a third, which is the midpoint. And then the ride cap was either three or 10 at lower high, and then the midpoint. Or 10 at low or high, and then the midpoint for the center point. And they did this to, as you do in sort of traditional experimental design settings, to check for curvature. The idea there being that you might find, as you do in response surface methodology, an optimal combination of these factors that isn't one of the ones that you originally experimented with. So they may not have tried it, but maybe a four-day 75% off. Uh, 75 percent off promotion was optimal. Um, those are the kinds of things you can do when you fit these second-order models. So, what I'm trying to communicate here is that these experiments that happen in this realm are not always as trivial as just two sample comparisons. Um, the final illustration of that is this job ad that I came across in 2016. Um, you can still look at the job ad, it's linked here. Um, this is Linked here. This is for a senior data scientist, specifically on their experimentation team. And if you read through the job ad, you'll see that the expertise that they required by the person who was ultimately successful in getting that job are things like they need to be able to analyze experimental data with statistical rigor and support internal research into new methodologies for experimentation, as well as adapt existing methods such as response service methodology to. Such as response service methodology to online A-B testing. So I saw this back in 2016, and I was at that time admittedly under the misapprehension that these people doing experiments in the data science where I'm like, they didn't have very much sophistication. They were just doing t-tests. And this job ad proved me wrong. And for the last five years since then, I've really gotten excited in the use of what are more traditional methods, but applied in this more modern area. Applied in this more modern area. And one thing that I'm also trying to do is, you know, not just think about this myself, but encourage the rest of the industrial statistics community to get involved in this area as well, because I think there's a lot of interesting opportunity there. So not only, though, are there sort of interesting problems here, interesting applications of old, what I'll refer to as sort of traditional methodology in a new exciting area, but these are Exciting area, but these are really lucrative opportunities. And just to give you a few examples, you might have heard of Google's infamous 41 Shades of Blue experiment, where you have, think about the color of blue that a URL shows up as in the Google search results. They experimented with 41 different treatments, 41 different shades of blue between two endpoints. And experimentally, they picked what they Experimentally, they picked what they felt or what the data told them was the optimal shade of blue, and it increased ad revenue by 200 million annually. Huge, huge win there. Bing similarly made a very minor change to the way the search engine displayed ad headlines, and that amounted to $100 million in additional revenue annually. Amazon, in their checkout funnel, they moved where they were advertising their credit cards. Where they were advertising their credit cards from the home page to the checkout page, and doing so increased profits of tens of millions of dollars. Optimizely, this company that I mentioned previously, who helps other companies run their experiments, in 2019 alone, they helped those companies increase revenue by $800 million. So we're talking about billions of dollars worth of money to be gained with the use of carefully designed and analyzed. Carefully designed and analyzed experiments. So, not only is this a fun, modern playground for us to play in as statisticians, it's incredibly lucrative, very highly valued. And so, you might ask yourself, who's doing this? Who's running these sorts of experiments? I showed you this job ad for Netflix, and I've showed you a couple examples of some big companies you might have heard of. But the short answer really is almost everybody, all of these internet and technology companies. All of these internet and technology companies are involved in this now. And you don't have to take my word for it. I'm just going to go through a couple job ads that I've collected in the last little while, each of which call out the need for experience and expertise in designed experiments. So there's this Netflix one. Then there's also Twitter, eBay, Microsoft, Pinterest, Game Show Network, Facebook, Optimizely, Masterclass, Cash App, Sephora, Twitch, StubHub, Glass. Sephora, Twitch, StubHub, Glassdoor, Zynga, Slack, Uber, Walmart, Indeed, Ancestry, Adobe, Macy's, Airbnb, Turo, Intuit, Third Love, PlayStation. And this is clearly not an exhaustive list, but it shows you the variety of companies that are thinking about this, not just in terms of what they do and what services they offer, but if you look in. And what services they offer. But if you look into the job ads and the specific expertise that they require, there's a lot of variation out there in terms of how you might consider applying designed experiments in this online setting. So if this is an area that interests you in any way, I encourage you to have a look at some of these books. I know that a question came up yesterday about potential reading material on this topic, and these are a couple. And these are a couple good ones in my opinion. None of them would substitute a traditional DOE textbook. They're not statistically rigorous in that sense, but they give you a nice non-technical overview of this area of online controlled experiments. So just to give you a bit of context, this A-V testing book, this is written by Dan Soroker and Pete Kuhman. They were the founders of Optimizely. Dan Soroker was the data scientist. Dan Soroker was the data scientist on the Obama campaigns. He was the one responsible for generating that extra 60 million in donations. This other one here with the hippo on it, this is written by Ron Cohave, who was originally of Microsoft, then Airbnb, and now I think he's just like a freelance experimentation guru. Diane Tang of Google, Yahju of LinkedIn, they bring together decades combined of experience in online control experiments. And so this is among the Experiments. And so, this is among the newer of these books and a really, really good read. Stephan Tonke is from Harvard Business. And then this one is very, very non-technical, but a nice, very quick read to get you into this area if it's something that you're interested in. I'm going to pause for a second here then and ask if there's any questions, because that ends my sort of high-level overview of this area before I start talking about sort of concrete problems. Sort of concrete problems. I do have a question, Nathaniel. Sure. Many of these experiments seemed tremendously under precise. You described the Lyft experiment, which they did the response surface design and looked for a quadratic surface that would give them an optimum. But they certainly have the capability to just really explore that design space. They could have tried four trips, five trips, six trips. Four trips, five trips, six trips, and lots of other combinations of things. And they would have been able to get sample size to actually be determinative. Why didn't they do that? Yeah, that's an excellent observation. And one thing that I'll talk about in a minute is sort of the size of these experiments in terms of how many millions of users they have at their fingertips. And you might wonder, you know, why are they doing simple things when they could do some sort of like a space filling design and really explore that? A space-filling design and really explore that area. And I don't have a good answer for you. And one of the things I'll comment on in a minute as a challenge is that until now, and hopefully this workshop spurs conversation, but until now, there really isn't a strong bridge between academia and industry in this particular industry. And so, you know, these examples. And so, you know, these examples and things that I've laid out here, they're ones that I found on blog posts or I've seen talked about at meetups and things like that. And the unfortunate part is that generally there's proprietary issues here where they don't get into the nitty-gritty of a lot of these things. And so it's hard to know sometimes what the thinking was. Why didn't they try something more elaborate? Maybe there was cost constraints. I don't know. But there is always this, in my mind, this sort of. Is always this, in my mind, this sort of air of mystery, unless you have some real meaningful connection and collaboration with a company or so. A follow-up question, if you don't mind. Good experimental design is great, but even better is coming up with the idea for what you should be studying or what manipulations you should consider. And I'm quite confident that statisticians can contribute a lot to the design side, but I hope. The design side, but I hope we can find ways to contribute to the creative side. And there are probably some brilliant experimentalists out there, but I don't know what they're working on. Yeah, I agree with you there. I think that there is opportunity for us as statisticians to provide assistance on that creative side. I know these sorts of companies generally have creative teams, teams of designers who are Teams of designers who are, if it's something like user interface, who sort of understand the psychology of web design and interface layouts and how people interact with them and navigate through them. But I think that there is sort of opportunity for meaningful contribution there as well. Thank you. Okay, I don't see anything else in the chat or any hands, so I'm going to carry on here to talk about some. About some open problems in this online control experimentation playground. And I've chosen some specific ones that are quite distinct from traditional experimental design problems to sort of showcase the opportunities that are available here. The first thing I'm going to advertise, and I have no affiliation with this conference, but I think that if you're interested in this, every year MIT runs this. MIT runs this conference on digital experimentation. It's been online last year and will again be this year. But this is something that happens annually. And this is where the leaders in this field, the people actually doing these experiments and running into the practical problems that necessitate research solutions, this is where it's getting talked about. So I'm going to talk about some existing problems in the field, but if you really want the cutting edge, you really want the cutting edge discussion of both the problems and the solutions, I encourage you to attend this conference. So another place I'll direct you if you'd like to dive deeper into this and some of the problems that are sort of the modern challenges in this field is this paper written by 34 different employees from 13 different organizations where in 2018, these 2018, these 13 organizations got together to hold what was the first of, I'm not sure that they actually held another one, but maybe they plan to. They're calling this the first practical online controlled experiment summit. And so you can see the organization logos there on the right. So you can see who is a part of this. And out of this summit came a paper that summarizes the main challenges that, you know, across all of these organizations, across all of the different products they deal with. Products they deal with, there are some common challenges that they face. I'm going to summarize a few of them here in this talk. I think there's some important things that were not talked about in this paper, so I'm also going to talk about those. And that's where we're headed in the final next 15 minutes of this talk. So the, whoops, I skipped the slide. So there's eight things in particular. The first four are things that are described and discussed in that paper, and then an additional four that I'll briefly touch on here. Briefly touch on here. I think through all of this, I only show you one formula, but I do provide lots of links and references if you want to dig deeper on any of this. So let's just get into it. The first one is the estimation of long-term effects. So these companies, they typically run their experiments for two weeks. And if they're doing it in an ordinary frequentist way where they figure out how many experimental units they need in each treatment, then they generally try to, whatever that number. To whatever that number is, disperse it over two weeks to basically average out seasonality that might exist in whatever business cycle they're a part of. Now, how then are you able to estimate long-term treatment effects if your experiment is only over two weeks? And that's something these companies are interested in. They would like to be able to say, you know, here's this effect that we observed in the two weeks. They'd like to. The two weeks, they'd like to be able to talk about what that effect looks like over the longer term. And a couple sort of time-related effects that they're trying to mitigate with this understanding of long-term effects are two short-term things. One is referred to in the literature as a primacy effect. And so that's something where initially something looks very bad, but overall, over the long term, it's actually something that's positive. It's actually something that's positive. And so you might imagine, or I think an example of this would be something like if you use Spotify or iTunes or Apple Music or any of these sorts of things, if suddenly they changed the way you interacted with that product, if they changed the menus and the way the product looked and felt, people generally don't react positively to that kind of change immediately, even if over the long term, it increases retention. It increases retention and user satisfaction. So, those sorts of primacy effects where something looks bad initially, but over the long term might be a positive, then you want to make sure that you're understanding that. You don't want to throw something out based on sort of misinformation. And sort of related to that is something that's referred to as the newness effect, where something that might initially look good, but over the long term is bad. So, if it's some sort of Term is bad. So if it's some sort of like a clickbaity type change where some shiny new button is added and everyone's clicking it because it's new and they want to figure out what it does, you know, if your metric that you cared about was something like a click-through rate over the short term, that might be increased, but that might not be indicative of long-term performance. And so there's very great interest in this area of estimating long-term treatment effects. And the naive thing to do would just be to run your experiment for longer. But that is generally But that is generally not a practical solution to this problem for two reasons. One, in this space, whether it be ads or any sort of version of online experimentation, things are constantly changing, things are constantly moving. These companies like to be agile and they would like their experiments to be as quick as possible. Another statistical issue is the longer the experiment lasts, the more likely. Lasts, the more likely you might have experimental units that pop up more than one time in each treatment, or they might cross over and be in different treatments if they delete their cookies. And I'm going to talk about that as a separate item in a minute, but there exists these sort of statistical issues where you dilute the treatment effect if you run the experiment for too long. David, go ahead. That was largely where I was going to go. Obviously, if the experiment persists for two weeks, they Persists for two weeks, they either have to make sure that I am always randomized to the same arm, or they're going to have complications. Absolutely. And that is a very real complication that I'll address in a minute. Another key challenge they refer to at the summit is the estimation and basically interpretation and understanding of heterogeneous treatment effects. So you might be Netflix and you're doing some sort of And you're doing some sort of an experiment, it would be very unlikely that the treatment effect is the same for all users across, say, the entire United States or across different browser types or device types. The treatment effect might not even be the same across days of the weeks or weeks of the month. And these sorts of what are referred to as user segments are important to these companies. What they'd like to do is optimize the user. They'd like to do is optimize the user experience. They want to determine which treatment is best in every one of these segments. But that becomes challenging for a variety of reasons. One is the signal to noise tends to be quite small. Even within a particular segment, there's a lot of variation. And so there's work being done in variance reduction techniques. But one of the most common issues cited here is signal to noise is small. Here is signal-to-noise small. And despite having potentially millions of users to experiment with, for the effect sizes they're searching for, these experiments tend to be underpowered. Also, the more you slice and dice this data, you're running into multiple testing problems. And I guess even at a higher level, just fundamentally, what can you conclude when you find different treatment effects in these different segments? Statement effects in these different segments. You can't randomize users to these different segments. You can't tell someone where to live. You can't tell them that they have to use an Apple device. These are nuisance factors, like in traditional design of experiments, where you might imagine this referred to as blocking. But even in that traditional paradigm, you haven't randomized to the different levels of the nuisance factor. So the conclusions you draw there cannot be causal conclusions. So there's opportunity here for causal inference and. Opportunity here for causal inference and observational design type investigation. David, go ahead. Well, it seems to me that in this situation, you would like to have personalized interfaces or perhaps, you know, sort of three different styles and you can go with the glitzy new one or the old-time traditional or something, something else. Yeah, yeah, you could definitely increase the number of. Of experimental treatments, hoping that you find the specific treatment combination that works well for a given demographic. But I think the issue there is really determining in a way such that you are confident in your conclusions, which of those really is the best in each one of these different user segments. And I think there's another talk later this week, maybe Thursday. This week, maybe Thursday, Michael Braun is talking about heterogeneous treatment effects. So, I think he is probably going to talk about some of these ideas in more depth then. So, estimation of heterogeneous treatment effects is of interest. Another big problem is experiments on networks and the interconnectedness of everybody is just more and more prevalent. Prevalent, or we're more aware of it in the sort of online framework, whether it be LinkedIn or Facebook or Twitter or Google or Lyft or Uber, a lot of these marketplaces and these products operate within some form of a network. And typically, when you experiment on a network, the stable unit treatment value assumption SUTVA is violated. And the idea behind SUTVA is. idea behind SUTVA is that the treatment assignment of an experimental unit, let's say it's me, me as an experimental unit, my treatment assignment will very likely influence my outcome or my response, but shouldn't influence David's. Just because we might be friends, we might be connected on LinkedIn. But if the sutva assumption is true, whatever treatment I'm assigned to should not impact. Whatever treatment I'm assigned to should not impact David in any way. However, on a network, that assumption generally is not valid. And so, as an example, imagine it's Facebook, and Facebook has this feature called People You May Know. And what happens is if I'm on Facebook, this feature gives me a pop-up of some kind that says, look at all these people you might know. And maybe you want to send them a friend request. So imagine Facebook is experimenting. Facebook is experimenting with the algorithm that recommends those people to me. And suppose there's the old version and the new version, and they've tweaked things with the recommendation algorithm, and they're hopeful that the recommendations it provides are of higher quality and make it more likely for me to interact with that feature and ultimately invite more people to be my friends. So, suppose that's the case and So suppose that's the case and I get that treatment assignment. Now, suppose David is also a member of Facebook, but he and I aren't friends. And I'm treated, but he's controlled. Now, what breaks down behind Supa now is suppose I am very happy with this new algorithm. It's suggesting lots of very relevant people to me. I start adding that. Relevant people to me, I start adding them. But the people I'm adding may or may not be themselves treated or controlled. That could go either way. And so suppose David is someone that I send a friend request to. He's recommended to me and I say, yeah, that's a great idea. I want to be his friend. But David's a control person. But maybe David just starts receiving lots of friend requests from people who were treated. And so that sort of instigates some reciprocity. Sort of instigate some reciprocity where David says, Okay, this is fun. Lots of people are adding me. I want to start adding lots of other people too. And so his behavior has changed, even though he was in the control condition. And so when you have that kind of network interference, you really need to account for it when you're trying to estimate a treatment effect. If you ignore it altogether, any estimate of treatment effect is going to be biased. And so the big social networks like Google, Facebook, and LinkedIn, they all have their own ideas of how to approach this. They all have their own ideas of how to approach this. They generally involve some version of cluster-based randomization where you'll try to partition the network a priori into different communities or subgraphs and run your experiment sort of in isolation in various ways on these subgraphs. But these, I think there are, having read that research, I think that there are still interesting opportunities here. They almost all assume that there's only two treatment conditions. Treatment conditions. They generally assume binary response. So I think that this is a big open area and a very interesting and important research problem. Oops, I missed one. So another practical issue here, especially when these companies are doing experimentation at scale and they're running potentially hundreds of experiments simultaneously, it's possible that those experiments are meant. That those experiments are meant to try to move the same metric. It might be two independent teams working on two independent problems, but they're both trying to improve, say, the signup rate on their email promotions or something. And it's possible that interaction could happen between two or more experiments that are all trying to impact in some way the same metric. And if that's the case, Same metric. And if that's the case, you can't be certain whether the observed treatment effect, or as I refer to it in this area, lift. You can't be sure if the lift you've observed is actually due to your treatment or maybe another team's treatment that happened to be happening at the same time. And the researchers at LinkedIn and Google, they have thoughts on this. And I've got two references listed there. Both of them are. Listed there. Both of them acknowledge that factorial designs are sort of the obvious way to think about this, but they also concede that the factorial experiment, a full factorial that they would require at this scale, can be quite infeasible in a practical way. Being able to, at the drop of a hat, move all of the different level or factors and assign all of the different treatment combinations may not Different treatment combinations may not be possible. And so they tend to opt for more practical solutions where they just try to have some sort of a like a review body who looks at all of the experiments that are being proposed at a given time and hopefully makes decisions about, oh, these two should not happen at the same time or there should be no overlap in their users because they're trying to do similar things. But it sounds, and my impression is based solely on My impression is based solely on reading their papers that that's not necessarily a statistical solution. So I wonder whether, you know, if maybe it's hard to do all possible combinations in a full factorial, is there opportunity for fractional factorial or some of these other non-regular designs where we look at subsets of treatment combinations? So that's a potential point of exist or new research. One area that I think is really One area that I think is really exciting is the idea of sequential experimentation. And David alluded to this yesterday. And it's this acknowledgement of the reality of these experiments is that they're happening in this ecosystem where you can collect a lot of data very, very quickly. So much so that you can make almost real-time decisions and analyze the data immediately as it's observed. We're not waiting the six to nine months for our crop to yield. Six to nine months for our crop to yield, like you do in the traditional agriculture experiments. And so there's been a lot of recent research and application in this area. The researchers at Optimizely a couple of years ago came up with something that they refer to as always valid p-values. They also have always valid confidence intervals. And these are basically the traditional sequential experimentation, sequential hypothesis testing idea. Hypothesis testing ideas sort of repurposed for this A-B testing online controlled experiment paradigm. So there's interesting research there with these sort of sequential tests. Something that sort of turns that all on its head is the multi-arm bandit experiment, where the idea with always valid p-values and any sort of sequential testing is that what you're aiming to do is allow yourself to. Do is allow yourself to analyze data sequentially as it's observed without inflating type one error rates. And that's why it might not have been obvious, but this, when properly handled, is italicized because there's evidence to suggest that a lot of these experimenters peek at their experimental results too early and they basically do p-hacking, where as soon as they see a significant p-value, they stop the experiment. They say, Stop the experiment. You know, they say, Oh, this p-value is significant at the 5% level, we've got our answer. And prematurely in the experiment, and obviously, we know that that's a bad thing to do. That inflates type 1 error rates, but sequential testing gives you the tools required to do that, to analyze your data as it's observed, and potentially end an experiment earlier than you might have if you had just done the a priori sample size calculation in the standard Frequentis paradigm. Your standard Frequentis paradigm. Multi-arm bandits is similar in the sense that it's sequential, but it turns all of this stuff on its head where it stops caring entirely about type 1 errors. And the two papers that I included there, the reference are Google papers. This was really popularized by Google about 10 years ago. And multi-on-banded experiments were the default experimentation type baked into Google Analytics. And I believe Google Analytics. Google Analytics version of experimentation is now what is referred to as Google Optimize. But as far as I'm aware, multi-arm bandits are still the standard approach to experimentation within those platforms. And the acknowledgement there is that if we're talking about button color, something trivial like that, suppose you're comparing red and blue. Maybe they're really the same, but you incorrectly say that blue is better. You're not hurting anybody. You're not, no one's. Anybody, you're not, no one's lives are lost, you're probably not losing money. The consequences of a type 1 error in that sort of a setting are not so drastic. And so the multi-arm bandit, rather than observing all of your data until your sample size calculation says to stop and then you draw a conclusion, it alters what is referred to as the exploration exploitation. Exploration, exploitation trade-off, where with a traditional experiment, you are exploring every treatment sort of equally throughout the experimental setting. And then at the end, you draw your conclusion, and then you exploit the winner sort of forever on thereafter. With a multi-armed bandit, you explore and exploit simultaneously through the experimental period by sort of adaptively assigning experimental units to treatments based on which. Units to treatments based on which treatments look like they have been performing well up until that point in the experiment. And so, Bayesian methods and Thompson sampling are useful heuristics there for figuring out how to do the adaptive sample size determination. But both of these methods, sequential testing or multi-armed bandits, they're gaining pretty widespread popularity in this field now. In this field now, because they facilitate quick decision making, which these companies tend to be very happy with. The next thing that we talked about very briefly a moment ago, but I'll expand on quickly here, is a problem that arises when your experimental units are not identifiable. So Tim Hesterberg yesterday talked about how in these sorts of experiments, randomization is typically cookie-based. Randomization is typically cookie-based. But cookie turn, like if you're someone who doesn't want to feel like they're being surveilled on the internet, you might delete your cookies. You might use private browsing modes like incognito mode on Google Chrome, which is the picture that you see here. If you're someone that does that, you might end up in different experimental treatments in the same experiment. You might both be a treated unit on, say, Unit on say this morning, and then a control unit the next afternoon. And that kind of pollution or contamination can seriously bias your understanding of treatment effects. It would also be problematic, though, not only if you showed up in both treatment and control groups, if you showed up multiple times in the same treatment group, but the experimenters didn't realize you were there generating multiple response observations. Generating multiple response observations, especially if, for example, you're a heavy user of whatever the product is. If you're a heavy user and you use that website often and frequently and you're in the experiment many times, but under the guise of different units, then you could inflate the, or you could bias the treatment effect. Or some combination of these things might happen. So this is a really interesting. So, this is a really interesting problem that we don't really run across in traditional sort of industrial experiments or agriculture experiments, where your experimental units are generally well-defined and easily identified. That is very much not the case in these online settings. And I haven't found any literature that gives a statistical solution to this, aside from saying randomization will hopefully make it. Will hopefully make it so that this problem impacts both groups the same, and therefore maybe it dilutes the treatment effect, but it's at least impacting both sides similarly. So I think that this is a problem that has lots of opportunity for research. This is an important one. I'm not going to spend a lot of time on it, but post-selection inference, the idea. Sorry, go ahead, David, if you have a question about the last one. I was simply going to say, if a cook I was simply going to say: if a cookie has a time stamp on it, then you can tell whether or not this is a person who regularly deletes cookies or not. And that would be one way in to try and address the problem you mentioned. Yeah, that's true. And then just make it so that that person is not re-entered into an experiment. Yeah. So I think that there's work that could be done here. The issue with post-selection inference, post-selection bias in particular. Uh, post-election bias, in particular, is, I think, well known now in the statistical community, but it's only recently been discussed in the context of these A-B tests. One thing, Ron Berman, who is another speaker at this event, he's got a really interesting paper with colleagues where they, with data from Optimizely, estimate that a huge proportion of Of discoveries in this setting are false discoveries, owing largely to misattributed true null effects. And that could be due to a variety of things. It could be peaking in any experiment early. They talk in the paper about several potential causes of this problem, but it could also be due to this post-selection bias. And the idea here is that if the treatment effects that you estimate are only those that have been found to be statistically significant. Found to be statistically significant, your estimated treatment effects are going to be biased upward. And you can show just for a standard one-sided z-test that this conditional expectation, the expected value of the difference of averages, conditional on it having been larger than some threshold, is strictly greater than the true treatment effect delta. And so research into bias adjustment methods in this setting are beginning to receive some attention. And I think it's a really important area. And I think it's a really important area when practitioners see a treatment effect that seems very exciting in their experiment, but they can't reproduce it when they apply the winning treatment to their entire user base. That, you know, obviously discouraging, but in some cases, what this shows mathematically is that it's almost a guarantee, at least on average. The final one that we'll talk about here is just the The use of causal inference and observational studies in this setting. It's not always possible to run a designed experiment, a controlled experiment. It might just be impossible, it might not be ethical. And so there's a big opportunity here for experts in the causal inference field to provide their expertise. As an example, just to give you some context or a problem where this kind of stuff happens, is a couple of years ago, most Is a couple years ago, Mozilla was experimenting with an ad blocker in Firefox, and they were trying to determine whether users who installed an ad blocker were more engaged users. Whoops. Did they have more sessions? Did they open more tabs? Were their session durations longer? By a variety of different metrics, they wanted to understand if people who installed the ad blocker were more engaged users. But you can't force someone. You can't force someone to install an ad blocker. And so, what they did was an observational study where they made the ad blocker available and then used propensity score matching to try to estimate this kind of an association. So, these kinds of experiments happen all the time. Maybe I shouldn't call them experiments, but this sort of inference and these sort of studies are happening all the time and therefore I think are good candidates for many of the tools that have been developed in the causal inference literature. In the causal inference literature. And then this also just brings, I think, awareness to a much larger discussion that requires its own talk entirely is just the question of ethics and fairness in online controlled experiments. A question of whose interests do these experiments serve? Is it the people that are being experimented on? Not likely. The companies might say that this experiment is. Say that this experiment is about user satisfaction, but really it's about money, it's about increasing revenue. And I've always found it interesting that these are human subject trials. If we try to do an experiment of any kind, any sort of study with humans in academia, we have to go through very rigorous ethics review processes. And there's nothing like that in this scenario. I think we all click the box and say you agree to the terms and conditions. The box that says you agree to the terms and conditions, and the end user license agreement gives companies this license to run these experiments on you. But I think there's this sort of broader problem of, you know, what is informed consent? Who do these experiments serve? I think that's an interesting conversation to have at some point. So just to close, the opportunities I see then for academia more generally and industrial statisticians in particular is any of these problems we just talked about. Any of these problems we just talked about, it's a sort of exciting research area for anyone to get into. And I think that industrial statisticians have a part to play here. They're, as I said, the ones who have historically done research in design experiments. And so I think they have insight that they can offer. But I make these couple comments here about it's only useful or it's only helpful if the innovation is useful. And I say that because I've noticed that there's. Say that because I've noticed that there has been literature recently coming out of the industrial statistics community that seems to be what might be referred to as sort of an error of the third kind, as Kimball once said, type three errors where we're solving the wrong problem because there isn't this sort of, as I refer to in this bullet, this synergistic bridge between academia and industry. A lot of these proprietary Components of these experiments make it so that we in academia don't necessarily have the means to fully understand the problem or the methods, or certainly not have access to the data. And so I think that we really need to bridge that gap for academia to be able to make meaningful contributions here on the research side. I think it's possible, but it's going to take some bridge building. Two things I want to say here about education is that I think that there's a lot of research. About education is that I think that there's an opportunity, and I'm quoting David in this first bullet. These were his words yesterday. I agree with them fully, but you can yell at him if you're unhappy with them. This modern application area has the capacity to breathe new life into otherwise stodgy DOE courses. And that's something that I like to do when I teach DOE. I like to bring these contexts into the classroom, show the students that, look, there's some really exciting stuff happening out there and you're learning the stuff that you need to in order to get. learning the stuff that you need to in order to get these jobs and and students love that and so i think there's an opportunity here to revamp some of our traditional doe courses um another version of a doe course is one that i think that should exist in all data science degree programs um you know we've talked today about a lot of interesting problems in this area of online control experiments that are non-trivial and and i think it's important that there be a devoted OCE course. OCE course in data science programs. I saw, I won't say which school and which program, but I was at a conference where someone from a reputable Canadian data science program said about A-B testing, oh, you know, just the example when you teach the t-test, just make it a red versus a blue button and you're done. And that to me was just offensive. It really trivializes the problems that go on in this space. And I think it's not forward. Space. And I think it's not forward-looking. I think that there's a lot more opportunity here than people give this area credit for. And so I think, you know, not only in the research field, but also in education, as I say here. So just to finish and summarize, online control experiments are, it's an exciting modern playground for people who are interested in DOE. There's a lot of interesting challenges there that require innovative statistical solutions, and we should be involved with that. And we should be involved with that, but we have to be aware that to involve ourselves in a useful way means that we have to have some sort of a meaningful collaboration. We can't just make assumptions about what these problems are. We really have to understand them. Otherwise, the work we do is not going to be useful and we're not really going to be playing in the playground to the best of its ability. Okay, I'm going to end there. Thank you. That was amazing. That was amazing. Sorry, I went a little longer than I thought, but I'm glad that you asked some questions throughout. No, this was terrific. And if you don't mind lingering just a little bit longer, I think there's a couple of things that we might sort of look at. One thing is when you're performing a couple hundred experiments a day and they're all active, you have a huge workflow management problem. Workflow management problem. And it's not at all clear to me how these companies have set up structures to manage that problem. Do you have any questions on that? A little bit. So I can point you to one or two papers. So reference four here is from LinkedIn, and this whole paper is about their in-house experimentation platform. It's called XLN. It's called XLNT. They refer to it as Excellent. A lot of these big companies who are doing experimentation at scale, they build homegrown experimentation platforms to do exactly what you said, manage this workflow that would otherwise be overwhelming. And companies like Optimizely, Google Optimize, VWO, they all help to an extent, but To an extent, but I think what the big companies recognize is that they sometimes have problems or questions or experiments they want to run that an off-the-shelf software solution can't facilitate for them. So a lot of engineering, software engineering, and sort of organizational infrastructure goes into the building of these. Of these homegrown experimentation platforms. So, what do you call it? LinkedIn has excellent, Netflix has something called Ablaze. And if you're interested, I can send you reading on all these. All of these companies write blog posts about their own experimentation platforms. They're all very proud of them and they show off how they manage these things. Intuit has something called Wasabi that was. It has something called Wasabi that was originally proprietary and then they open sourced it and now lots of people can use it. Facebook has one. The name escapes me, but a lot of these companies have these platforms that help address the issue that you raised. Thank you so much. That's fascinating. Let me ask if anybody else has anything they would like to mention. Ron, you got a shout out. Perhaps you might have something to say. I think you asked David an interesting question during the talk of why don't they, you know, why do they use only two A-B tests and not maybe try the entire surface? And what I found is that the effects that they're finding are tiny usually. They're really, really small. And this is also what you see in the data that we analyzed. So when you start doing a lot of too many arms, you basically find nothing. And even though they have millions of millions of people in their experiments. Millions of people in their experiments. Actually, I'll talk about it on Thursday. And one other thing, Nathaniel mentioned that there's not that much connection between academia and maybe those experimenting companies. And partially, it's hard to get data out of them. So the paper we wrote with Optimizely, we got permission to publish the data, at least partially. So this will come up online in a few months. And there's a paper publishing economics. Published in economics from a colleague of mine at Wharton that got Microsoft's research data published. And there's recently another data set called the Upworks the A-B testing data set where they experimented with headlines that they put online. So now finally we're starting to see public data that researchers can work with. But until now, it was actually pretty hard to get data unless you run like one experiment. It's hard to run to get data for many, many, many experiments. Yeah, definitely. And one area I'm finding that particularly challenging. That particularly challenging is network experiments. I've got a student who's thinking about these things, and to have the data means that you need to, in some incarnation, have the Facebook network or the LinkedIn network, which they might be willing to share data summaries in aggregate or for specific uses, some actual data, but sharing the actual network itself, I don't see as being something that will happen soon. So, what I've seen recently is people are running experiments. Recently, is people are running experiments physically. They're actually collecting their own data on Twitter. Yeah. That network is kind of public. And there are nice new techniques on doing encouragement design on Twitter because it's hard to randomize exactly, but you can kind of do private messages and do encouragement designs, et cetera. And that actually seems to be working nicely. Yeah, that's an interesting idea because they have a pretty user-friendly API to get the data right. Fascinating. Anybody else care to jump in? Nathaniel, you gave us a lot to think about. Thank you so much. This is perfect. No, thank you. Thank you.