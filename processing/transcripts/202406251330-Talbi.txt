Thank you to the organizers for inviting me here. So, I'm going to present two ongoing work, which has been on me for a while, actually, with Thibault and Nizar on plotting theory. So, namely, an extension of the famous Saniko's principle in a problem to the case with many errors. Okay, so three parts. So, the first part, I will just recall what is Remember, recall what is the original single problem. Then I will consider the finitely many agents, many agent problems, so n agents. And then we're going to go to the case where we have many, many agents, which is the middle case, constitutional infinity of agents. Okay, so the regular problem, you can write it like this. So we consider X the output process. The output process, which is controlled by the event through an effort alpha in the following way. So here the BRAM is written in weak formulation, as you see. You control it with the probability P alpha. It just starts thinking complex theory, but if you're not familiar with this, just imagine this conformulation. It's not really important for the talk. Okay, so very simple dynamics support the drifts and open. Uh, to control the drifts and no control on the voice heat. And this is why the supermeal will be constant. So now the agent is given a contract by the principal. So here we use a formulation by Dylan and Nizar, who somehow revisited the Senikov's problem. So in this formulation, the contract consists in three parts. So PAL, which should be the contract over time, others, the salary that the agent receives. Salary that the agents receive over time, the process. XI, the final payment, and tau, which is probably the most interesting term in this framework, which is the retirement term of the agent. And that is the principal we decide to terminate the contract. And so, given this contract, the agents want to serve the following. So, for example, the criteria. Criterion. So here's a utility of his salaries, terminal, and continuous patient. Discounted with a constant interest rate R, cost H, okay, which does depend on his effort, and up to the stopping temperature. After that, he'd return. So now, what is the principal problem? Problem. So the principal, as usual, anticipates. So here is a supernova relation. So she will anticipate the optimal response of the agent to a contract. So that we don't have the alpha hat of C. So it will see the triplet pi xi tau. And she will optimize the following. So again, a very standard criterion. So the best contracts. So, the best contract that satisfies the participation constraint for the agent, so that he accepts to enter the contract. And this, so she will pay this to the agent, so it's negative, and she will benefit from the effect of the agents. Okay, so that's Sanika Pro. And what has been shown by Zian and Isa Is that you may express the value function of the principle as a unique solution to an OD, actually, an obstacle OD, which is the following. So if you write like this, super power, some real numbers of a function, a small V, which is solution to this B. Okay? With the following integration. So you need this consistent. So, you need this consideration in a suitable class of functions to this equation, in the sense of this. Okay, so the idea already is to extend this approach to the case where you are many agents of cost interaction. Okay, so how do we define this problem? So, in a really natural way, so you have n agents. So you have n agents, so n is the integer. The appropriate size is n amateur, and the controls will be in the vector denoted both alpha. So both alpha is alpha one extra up to alpha n. And each agent k controls the kth coordinates of the process in this way. So still, if we control the drift and the constant load. And constant voices. And we don't assume common noise, just in a just equipment. So, in the variable and the shift. Okay, so now it becomes a bit more complicated because we, of course, going to put some interaction between the agents. So, I introduce the following notation. So, this set is the set of anticipated control for the K agents. For the kth agents. So it will depend, of course, of the effort of the other agents, alpha minus k. I think you all familiar with the station, which is a vector of alpha k without the k coordinate. Okay. And then given the contract CK, so which is a triplet, pi k psi k tok, like for the single agent, just depending on each agent, of course. So given this contract and the efforts of the other agents. Agents, the agent K faces the following problem. So almost the same as before. So UXIK, UIK. But we just added some interaction. So of course you could write the more general quotient, but here I just take a toy model to show how the interaction will bring the features. And this is very simple one. Bring new features, and this very simple one is enough for that. But you can do one brain value if you want. So, here I just assume that the cost of the agent K depends on the effort of the other agents, already. But you could do a gift if you want. Okay, so here I took the same coefficient R and H for all same U because I'm going to pass to the limit in the next section. But for this section, it's not necessary. But for this section, it's not an issue if you take a different UK and HK. If you're just interested in energy, it's not an issue. Just for CO2 purpose, I will need this for the game. Okay, so now, as you see, this will be typically a stochastic differential game for the energy. Similarly to the paper of Romiel Delis and Delaine from a few years ago. So here we stopped it in addition. Times in a decide. So we're gonna characterize the existence of a Nash equilibrium. So first you need the different Hamiltonians of the agents to be able to maximize them mutually, I may say. And then in order to so first to have probabilistic optimization of the existence of the Nash. The existence of the Nash, and then to rewrite the principal problem as a standard control problem, we introduce the following BSD, okay, which for the weightlessness is equivalent to the distance of a dash. So let's look a bit at this BSD. So it's an end-dimensional BSD. When the letters are in bold, it means the vector. And the original feature here, the BSD. This is standard, is the I here, the vector I. So, what is the I? Bold is the vector roughly of survival process of each agent. So, why do I call a survival process? It's indicative of tilde one and two. It means that the agent is not stopped. If ik is equal to one, the agent is still working, not variable yet. Not for your data yet. Or just look up. And you see that here you have like here you have the Z C which is just this term. So each coordinate of the BC of the Y just leaves up to token. So that's stopped. And so if you have a solution to that, you have a notch. So now I don't know, probably it's not trivial to see this BAZ because as you see it's coordinate. It's only Its own time horizon. So, what we have is a characterization. But properly establishing the presence of this is probably a tricky one. So, we should look harder on this. But assuming you have this, yes, you can say it as a dash. Yes, in that case, the value function of the agents write like this as a value of the The value of the coordinates of the solution in zero. Okay, so now the principle, assuming that everything goes well for the edge. So for CPC, I assume there is a unique Nash. I'm just not adding any terms on the slide. You can take many if you want. So the principal wants to optimize all the state of contracts satisfying the participation constraints for each agent. Participation front rates for each agent. So for all K, I want this liquid. And the bolt C is a vector of triplets. Each cardiate of C is a triplet, which is a contract to each agent. So we assume that the principal has the simple utility. So she just wants roughly to maximize the sum that she will receive from each agent. So roughly the same thing as before, you just add the sum here and the openness and k. add the sub he and we pronounce mk so she pays psi k and pi k and she receives the effort of each of it so now we want to calculate this value function with a pd approach as in a paper of delanisa and as an ecofunction so first we rewrite this problem This problem, which is the maximum of the, so here the BRM process unbroken variables, FT measurable, as a control plate, more or less standard. And so you use the classical technique by Zardian and Yakshar to rewrite the contract the set of on which optimize has this. On which optimize has this? So, roughly, we replace the xi and the z here, the incentive to change code. If you go back to the previous slide, what kind of assumptions do you have on the hat? On the what? A hat. Is it unique and also do you have like relative discussion? Here, I assume it's unique for simplicity, just but you don't need uniqueness. So, you assume it's unique. Yes, here in the slide, yes. The slide, yes. At regularity, you assume it's regular enough to avoid possessivities. So, yes. So, that's a green. Yes. Just on the slide, another sub here if you want to equilibrium. Not to take too much space. But yeah. Okay. So, once you use the technique, you can rewrite the problem as this. So, stochastic control program. This so stochastic control problem so uh eta is just this now we just post this so control of the eta z and top so it makes control and stopped simply and well and what I call a bud y is a forward versionability so the principal controls the web process of the agent with higher incentives so here the beauty for dynamic programming is that Dynamic programming is that tau is a vector of topic types. Each agent has a different stopic type, it's assigned a different stopic type. So it's multiple snowstopping features. So you have to be a little bit careful if you want to establish diagogamic here. And you need to introduce a new variable. So the bold I here. So y is a classical variable, protected value in R plus. Value in R plus, okay. You can show that y0 is a positive, non-negative. So the y of the agents, and i, what I call i, so i is a vector in with each coordinate in 0, 1 is somehow the state of the agents. That is, if i k, so the k the kth coordinate of i is 0, the agents is retired. If it's one, he's still in the gap. Okay, and you need to introduce this idea. Okay, and you need to introduce this variable to have a subhov DBP. You need those two information: the value of the agent and its location. Retired or not. So that's how you introduce the appropriate dynamic version. Okay, so you can initialize this. Initially. So why I0 minus? Because I is the jump process, so you need to start a little bit before zero to allow for. A little bit before zero to allow for image adjustments here. And then you can write VP and show that the end is solution to this equation. Actually, I say C2, but it works for viscousity. So the N is solution to this equation. So what special node sequel? So it's an obstacle, partial obstacle. So this part is a classical, the diffusion of obstacles. The diffusion operator. The interesting part is this one. You see that the obstacle I wine I fixed the obstacle problem is this one. So the semi-loop function with less players in the game. So that's what we called in our previous work with Jan Feviniza Cascade obstacle equation because each obstacle will depend on the symbol function with less PR. Depends on assembly function with less PR. So it's a case case rep. Okay, so it's like intricate of like a problem. And roughly what you do to fix this, to solve this problem is you just write the multiple optimized problem as a recursive sequence of standard optimized people. With each new cycle, is this until that there is no more preventable game? No more plugged in the game. So, this is the P. And assuming you have a solution to this PD, you can use it to derive optimal contracts in the following way. Sorry, so yeah, I'm not interested. That's the division operator. So, it just comes naturally with the dynamics of what. So, no need to talk too much here. So, how do you write the suppose the algorithm for cell contract? So how's the algorithm for the malcontracts? So we start by expressing the optimal tau given the control zine and eta. The part z in eta is just control. So roughly you have to maximize the militanyan to find them. So it's quite classical. The part in tow is a bit more original. So what you do is the following. You're going to define not the token directly, but their But there order statistics. Which one amended stop first, then which one second, etc. That's what you proceed. So initially, you fix this to be equal to zero. You assume that all the agents at zero are in the beginning. Okay, everybody's here at the beginning. And then along your value function u. So you assume that the solution, okay. And then you can compute it. I long your value function. It along the key value function u you follow the process. So you're here at the beginning, and the first time that this to achieve the first obstacle, which is the term in the PDE, okay, the cascade obstacle, so the first time to cheat, you stop. And who do you stop? To know who you stop, you look which I try maximizes the max here. So yeah, here it's a max on vectors, so it's coordinates. Here it's a max on vectors, so it's covered inequality. Uh, so here I say the smallest one because we need to pick one, there can be many, so let's say we take the smallest one, and you do that each time. And so you just keep alive the I trial. All the coordinates which are in IK but not in I trial, one here but zero here, they are retired at the time where At the time where this hits this, and you do like this recursively, and you update your vector of surveillance processes. Okay? So that's a simply recursive optimal CP problem. And for that, you don't need bothness, I work with solution. You just need some opportunity only. And then to find, so this is as a function of z and eta and the. function of z and eta and then if you want to have the super contract uh you have to do the basic control thing on the each uh on each uh stochastic interval uh by just maximizing semiton so okay that's all for the end player game so we have uh a theoretical resolution and no as usual in the small stopping it's very challenging to find the to find the non-trivial it's really Find a non-trivial solution. So more than one map. But at least we know how to theoretically derive contracts. So now we're going to discuss about the mean field limits, so n goes to infinity. And we'll see it's much more complicated. So here I'll come back to the first part. So we have a representative agent for the following dynamics, okay? What's the following dynamics? Okay, control the grease. So I didn't enter C the center plate, okay? And he wants to solve the following problem. So you have C, you have Ïˆ, sorry, you have pi, and H depends of alpha, and here the model can be general. It will depend on the measure Q, but only through its mean. So expectation under Q, which is the average effort of the other agents. Okay, so the Q bar here is just the average effort of the other agent. So just a particular case of interaction. It can be a lot of things. I'm going to keep quite theoretical, so you can add interaction here or here. It will be the same. Just a suggestion. Suggestion and the principle. So, here I did not assume uniqueness. The soup and the contract which satisfies the parts constraints, the soup of all the mini equilibrium, she will assume that she can take the best ecliprium among the among all the eclipsion. Among all the equilibrium. MFP is mean field equilibrium. So, of this. So, what you anticipate is a mean field equilibrium. So, like a generalization of the concept of Nash equilibrium to the mean field case. So, that's the problem. So, now we're going to apply the same approach as before, but to the mean field case. So, you need to assume. need to assume instance of MFE. So here it was as the following. So you just aminifying the generalization of the one before. So you can maximize the Hamiltonians with the rho here which which accounts for the impact of the other guns, which are frozen. Do you agree? And then it won't be the e-to-solution. And then it won't be easy to have solution, which encapsulates the fixed point condition. So, of course, in all generators, it's not easy to guarantee that. So, once again, it's a characterization. But I don't know if it's much easier to do this or for someone else, or for other methods. But it will be useful to write the PD for specific path, so that's why I introduced it. Okay, so we assume that we have the fixed part condition, so heat interaction is through the mean. So heat interaction is through the mean, so rotation of the control of the optimal control a hat, which is equal to the vacuum interaction. Okay. So notice that here the A hats depend on the Z tilde. Zilda is a Z times I. So it means that you just care about the agent still on the contract. Okay, so here the stopping time is involved. I the sole process of the stopping time. Okay, so how do we write it? So here as means unique, of course, you don't have to do this. It's just for simplicity. So then you can write DP as on all the y0 above the path special strength. So a V0. So, a V0, and that's the function V here, which satisfies the PZ that we'll prove in the next slide. So, again, you rewrite the problem as a stochastic controller. So, here mean field controls the behavior. Why mean field? Because of this. Because you see that here in the expectation of the dynamics of Y, you have the look. The dynamics of y, you have the law of the control which is invoked. It's expectation. And the control depends on zeno on y, on isorring. So you have the law of the spin times and the law of the control involved. So it's a mid-field structure. Although the criterion of the principle is not itself mid-field, it's fluid dynamics. It's enough to make the structure of a mid-volt. So, yes, mid-field control and So yes, midfield control on this. So midfield because of this album here. Okay. So midfield control. So how do we write a PD for this? So of course, since we are in midfield, it's going to be a PD in the session space. So we need to introduce a good idea. So Eran did it in the previous talk, so I forgot to talk too much about that. I just need to mention one particular term. So here Constantine. term so here we want star measure not on r only but r times 01 so when you define the linear actional derivatives and the linear derivative you have the variables of the underlying space that appears that is one x and one i which is the y in zero one okay so i use dissertation to be more compact uh the y the i here for for this For this, and I introduce the pseudo-derivative in I, which is the discrete difference of the linear derivative in the mouse. So, it's like generalization of the linear derivative for a discrete for all the variables taking value in discrete space. And roughly, show the variation of your function with the number of people who assume the game. Of people passing the game, not stopped. Uh, okay, so now the equation. So, this is a lot of notation, so I'm gonna say the end, I'm gonna accept it a little bit. So, assume in the eta sequel that be c2. Then applying the same method as we did in our paper with John Feg and Niza about the mean flux mass topping. What the mean fluid mass stopping, because we have the same thing, plus control. We can show that u is a solution to the following AGB equation on the bus to a mixed obstacle equation on the session space. So here you have roughly three terms. So we're going to start from the right. So here, so we did not introduce notation. M empty set means that the supermarginal of m is a delta zero. So Delta zero, so everybody stopped. You just have the component on the x. So when everybody is stopped, your value function touches the final obstacle. Okay, so that's the boundary condition. So the diu non-negative, it expresses the natural humanity of our problem. Recall that in spirit, di is the derivative with respect to Pluto or to I. To the toe or two eyes. That is the number of people in the game. And so, of course, the most agents I can control at the beginning, the greater will be my value function. So it's normal based derivative. So if you want, this is the obstacle term. In the NPR case, you have ui greater than ui prime when i prime is lower than i here the same thing but with a constant Here, the same thing, but with a constant infinity. So it's right like a derivative straight to the measurement. Okay, the same L. And then the sample AGB term. So here L is the image operator associated with the control parts. And you choose M' in COM. So I don't have to explain what it is, but somehow it's M' in CUM. It's the best way to stop people right now. best way to stop people right now at the time now how much people how many people did i keep that the end prime is the c rels okay that's why i should i should okay almost done so this is uh the equation so as i said it's assuming it's c2 if we wanted to be rigorous we should do this quickly solution because it's very likely to have a classical solution uh but it can be a bit involved because here you have the mixed control mixed control and uh and the studying time so ask a lot of work to do that Lot of work to do that and last so it was an example okay which is solvable but a bit trivial because you show that in the first base for these dynamics you have a constant value function a bit trivial not it's not easy to find not too trivial example or unseal doable examples in this problem because it's stopping so it's very challenging and the last part is just the safety check The safety check, the convergence of the employers and agents to the minfit problem, which relies on this unrealistic assumption, VC2, of course, it will be C2. But if it's C2, you can easily find that Vn tends to V in the following sense. And if we wanted to do that in our case, maybe either. So either we would need to do properly the Viscohit theory, which would be very involved, or maybe directly on the. Or maybe directly in the memory functions. I still talked about that. But yeah, at least for me, VN should tend to V. And that's one of my last slides. So thank you for your attention.