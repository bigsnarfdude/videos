So, I'm going to introduce a recent work on positive inference with bi-dimensional outcome variables. And it's a joint work with my previous PhD student, Nolan, and Professor Wibia Wu from Yu Chicago. And Professor Zhou from UIC is in the radiology department, and the data he provided. The data he provides us actually motivated the study. The collaboration with him motivated this study. So, let me first outline the talk. So, first, introducing the application that motivated our study. I'm introducing the details, what it's about, and how it's done in the previous study, and what we try to do in this new framework. And I'm going to introduce in our new framework for the causal information. New framework for the causal inference and some of the results we have right now, and but still working on some of the theoretical properties. So, I don't have all the theoretical properties, but still working on some of them. And I'm going to show you the simulation studies and then application to the diffusion MIMI data. So, the data is on Parkinson's disease diffusion MI data. So, Parkinson's disease has many. So Parkinson's disease, as many of us know, that is a disease affecting many older people. It's a neurodegenerative disease that leads to the abnormality of patients' movement. So currently, the diagnosis of the Parkinson's disease still mainly relies on clinical features, just some evaluation based on the physician. And although there are a lot of progress. There are a lot of progress using the MRI to detect changes in the Parkinson's disease. But in practical clinical applications, there's no MRI imaging biomarkers can be used in the clinical real applications because the accuracy is still not high enough. So here's the snapshot of what the clinical diagnosis. Of what the clinical diagnosis criterion used in practice are basically just some questionnaire and based on evaluation on the physician. And what we try to do in this study is try to find so-called imaging biomarkers. So what are imaging biomarkers? Imaging biomarkers are the measurements which are derived from the images that indicate the present or stages of disease. Of the disease, especially so we hope to find the biomarkers which can detect early changes in the brain, so which are indication of the Parkinson disease. So if we can find such imaging biomarkers, it potentially can be used in the clinical application. So although conventional MRI has already suggested several candidate, several candidate imaging biomarkers, which has potential to use in a clinical Potential to use in clinical applications. But so far, in real clinical studies, in real clinical applications, no MI being used because it's not enough accuracy and it's not very reliable. So what we study is the so-called diffusion MRI. So it's a type of brain imaging, try to quantify the water diffusion. To quantify the water diffusion in biological tissues. By quantifying the water diffusion, we try to understand, for example, if we compare the normal person and the Parkinson disease person, if the water diffusion in the brain are different, maybe those could be the indicator of the Parkinson disease. So the diffusion mind has great potential in characterizing the neurology disease. The neurology diseases. But convention MRI has some limitations because of the limited spatial, oh, sorry, limited spatial resolution. Okay, thank you. And just about two to three years ago, our collaborator, Professor Joe at the radiology department in UIC, and his group invented a new technology which can increase the spatial resolution in the MI, diffusion MI data. My data. So the data they collected for the Parkinson disease can achieve pretty high spatial resolution. For example, in the 2019 paper they published. So it can achieve the spatial resolution at this 0.6 multiplied by 0.6 multiplied by 3 millimeters. So this is among the highest in the Parkinson DG study. So what we are thinking So what we are thinking is that how to make use of this high spatial diffusion MI data. So I'm going to first introduce how they use this diffusion MI data in late practice, in a common use approach. And it's not public available, but yeah, we get it. So, commonly in current diffusion MI studies, Gaussian diffusion model is used the most often. So, in the Gaussian diffusion model, the so-called parent diffusion coefficient essentially is the variance in the normal distribution. It's very commonly used because this ADC is kind of assuming the free diffusion in a homogeneous. In a homogeneous medium. If we're assuming a homogeneous medium, if we're assuming the free diffusion, essentially it's a random walk. So the limit is going to be Gaussian diffusion. So they try to use this D. Oh, I'm sorry. I can use this to all the To all that stuff. Yeah, so this D, this parameter D essentially is the variance in the normal distribution or in the Gaussian distribution. So under this Gaussian distribution assumption, so the expected signal can be represented in this format. It's very simple. Is a very simple format. So that's what commonly used in the diffusion MI data. But clearly, this is not a very practical way because the human brain tissues has a lot of structure, heterogeneity, and complexity. For example, there are some human brain tissues which will prevent this free diffusion. They cannot diffuse freely in a brain. Use freely in a brain. So we need to have some model which can account for the heterogeneity and the complexity. So what we use now is the continuous time London walk model. So what is continuous time model? It's essentially a generalization from the London Walk model. But there are two things that are generalized. Or it's also called space-time fractional diffusion model. Model. There are two ways to generalize the Gaussian diffusion. One is generalizing the jump, the size of the jump. So we allow the jump to have a certain heavy tail, which is characterized by this beta parameter. So in a Gaussian diffusion, the beta is equal to two. And we also introduce another parameter, which is the alpha. Alpha parameter. Alpha. Alpha parameter is the parameter to introduce for the waiting time. For the Gaussian diffusion, this alpha is exactly equal to one, but here we're allowing alpha to be more general. So by introducing both alpha and the beta, so we can allow more heterogeneity and complexity among the different participants, among different people. So we can make the model more. We can make the model more flexible. Then, in the real application, what we will be interested in will be comparing alpha, beta, and the DM. Compared with the Gaussian diffusion model, we only compare this DM in this continuous time-run work model. We also compare in alpha, beta, and the DM3 simultaneously. The signal. Yeah, this is the signal measuring at the B value. So-called B value is, you can consider this B value as an MI parameter. So it's determined by the machine, the sum parameter you set in MI machine. Yeah, it's a valve. Yeah, it's a value for each voxel. I'm going to give more details. But in general, it's kind of the expected signal. Expected signal. Over, this is expected signal with the magnetic gradient. And this S0 is a signal without this magnetic gradient. The ratio will follow this. I mean, the signal over average will follow. I mean, the signal over average will follow this ML function. So in Luio, the data looks like this, just back to your question. So we actually observe this SK at the location XJ and with the B value Bi. So K here is the indicator for the subjects or individuals. And XJ is And XJ is the location in the brain, for example, some location in certain region of interest. And B is one parameter already set it up when you do the scan. So this is the MI parameter. So it's already set before the study. So there are seven B values actually used in this particular data set. So at a zero, Data set. So at 0, 50, 200, and so on to 3000. And in this study, we have 27 participants with Parkinson's disease and 27 healthy control participants. Yes, data N is the number of the P values. So n is the number of the b values. So for example, here is one, two, three, four, five, six, seven. So n is equal to seven. So in this particular, yeah, it's kind of you can think about over time, but it's some parameter list that before they do the scan. Yes, exactly. Yeah. So what they can commonly do is, uh, yeah, yeah, yeah, they can commonly do in radiology, they're doing ordinary non-linear discourse method for every voxel, because for every voxel, so we get seven values. So you have this equation, so you have this equation. So there are three parameters. We have alpha, d, Alpha, D, Em, and beta. So for every voxel, so we can estimate these parameters use this seven, using the signal from the seven B values. And then once you get the estimation for the alpha, beta, and the D for every voxel, what they do in practice is they actually averaging overall. Averaging over all the pixels. So that's, I think, like Lino mentioned in the morning, assuming every region of interest have the same alpha parameters. So they set the average for all the region of interest. And then they use the t-test or maybe some other classical test. Now it becomes this upper average becomes one-dimensional. Average becomes one-dimensional parameter. So you can compare between the Parkinson's disease and healthy control group, and then you can get the p-value. So that's what they commonly do in practice. But when we look at the data, because they obtained high resolution data, so they spend a lot of money effort to increase the spatial resolution. The spatial resolution, but if you do the averaging, you did not make use of all the information in the region of interest. So, what we propose is that if we can using all the parameters in the region of interest, so I be voxel, and if we can combine all together to do a high-dimensional inference, that actually motivated our study. And in addition to that, To that, so in this study, they actually collect the case and the control separately. So they folks obtain the cases from Lars University. It's a medical hospital close to UIC. And they also recruit 27 health control for UIC. So clearly, it's not a randomized clinical trial study because they recruit both separately. Both separately, and those Parkinson disease per person already have the disease. So, this is the patient characteristics in this data set. So, what we are asking here is that because this kind of is age and gender match observational study, so it's not a randomization study, so what we're asking here is suppose we find the differences among the parameters. The parameters in certain regions of interest, the difference might not be due to the Parkinson disease. It might be due to some other confounding factors. It could be the differences in gender or maybe differences in race because it's not randomized clinic trust study. So, what we think is whether we can do something better. So, then it motivates us to do something causal, causal influence. We try to Causal inference. We try to exclude other confounding factors. So the relation between the causal inference and the correlation is that for the correlation, we study relation between two variables. And in the causal inference, we try to study if the change of the one variable will cause the change of another variable. So in our case, we want to understand if the Parkinson disease will cause the changes in the The changes are in the brain tissues or in certain types of brain regions. So, that will cause the changes in the parameters. So, we try to study the differences among the parameters, but under this causal inference framework setting. So, for the causal inference, the standard approach will be the randomize the clinic trial. But, of course, in mining studies, you cannot you cannot. Studies, you cannot do randomized skinny trials. Sometimes it's very costly, sometimes it's not possible, especially for this type of study, it's not ASIC to consider the randomized clinical trial study. So, we study the causal inference on the potential outcome framework. So, this is a very commonly used framework, especially. Commonly use the framework, especially in statistics community. So, in our study, we're assuming we observed YI, which is a vector coming from certain region of interest with p values, with p different values, and each value representing, so in our case, representing the parameters, we estimated from one box. From one voxel in a region of interest. So the P here will denote the number of the voxel in one region of interest. See, in observational study, we can observe which treatment or which intervention this individual received. And if this individual received the treatment and is not going to receive control, or if this person is in a control group, it's not going to be in the treatment. Be in the treatment. So we're assuming that there are two potential outcome variables. One is the Y with the superscript one. If this individual is coming from the treatment group, another is the YI0 if this person is coming from the control group. But this Y1 and the Y0 is unavailable. We can only observe it either Y0. only observed either y0 or y1 in application. So we use the data to indicate so this data, if data is equal to one, means this subject received treatment one. So if data is equal to zero means this subject received treatment or treatment control group, treatment zero. So what commonly studied in the in the literature is the so-called average treatment effect. So what we try Every treatment effect. So, what we try to understand is the differences between the expected value of the outcome from the treatment group and the expected value from the control group. And we define the differences between these two vectors as theta. So, in current literature, a majority of the study focusing on the univalid case with t equal to one, there are few publications. There are few publications consider multivariate case, for example, this 2017 paper, but they only consider fixed dimensions. So the data dimension is fixed. The sample size goes to infinity. And the mining studies are actually focusing on low dimensional outcome, but with high dimensional covariates. So that's the difference between our study and the mining. Our study and mining causal inference with high-dimensional data, the most of the causal inference with high-dimensional data treat this high-dimensional data as the covariate. They're not treating those as the outcome variables, but in our study, we treat them as the outcome variables, which make more sense in our study. So, to the best of our knowledge, we did not find existing study with high-dimensional outcome variables. At least in the potential outcome framework, we did not find any study. Yeah, exactly. Yeah. Yeah, yeah, yeah, exactly. I'm going to mention that. Yeah. It's connected. That idea is connected. So, in a low-dimensional case, there are two common approach for estimating the mu1 and the mu0. The reason why we cannot use the sample mean to estimate the mean for each group is because typically in observational study, the expected value of the y given data is not the same as the expected value of the y. Same as the expected value of the y1 because the data might be related with y1 or y0. There will be some selection bias, so we cannot use the average to estimating the means. So there are two approach, very commonly used approaches in literature, try to collect in the selection bios. One is the propensity score. There are many method design related with propensity score. Propensity score like the inverse probability weighting method and propensity score matching and so on. And also, another type of method based on outcome regression model based on estimating the condition expectation of outcome given some observed covariance. So, to define our method, let me first define some notations. So, let me use the y to be the combination. The y to be the combination of the potential outcomes. So, if the delta is equal to one, so we observe the treatment. If delta is equal to zero, we observe the outcome. And assuming also there's a mean y is equal to c i1, so this ci1 has the mean zero, and also has mean zero. So, what we are interested in here is the mu1 and mu zero. And for the And for the propensity score, we are assuming this unconfoundedness in the missing data literature is similar to missing at random. So if we condition on the observed covariates and outcome, so is the probability delta equal to one is the same as we get rid of this y. So this is a unconfident. So, this unconfirmedness or missing at random. And so, we're estimating the pi using some parametric models, which specifies pi with some parameter gamma. And we're also estimating the outcome regression model using some parametric models here. But there's a potential to either misspecify Either misspecify the propensity score or misspecify the outcome regression model. So, the double-lobust estimator was proposed in 1994. So, in the double lobustity method, we can either misspecify the outcome regression model or misspecify the propensity score function. So, if we misspecify the outcome regression model, you can look at the first equation, but if we collectively specify But if we collectly specify the propensity score, so this will be still a consistent estimator for the mu1. And if we misspecify the propensity score, but correctly specify the outcome regression model, so you can use the second equation here. So you can still conclude that this mu one will be consistent estimators. So there are many works related with the doubly robust estimation. So here is the partial list of the So, here is the partial list of the reference. So, what we are interested here is that in a high-dimensional case, whether this is a good estimator or not. So, we study this W lobust estimator in a high-dimensional setting with the P goes to infinity, sample size goes to infinity, and we can quantify the differences between our W lobust estimator and the true mean vector. So, it turns out So it turns out these differences are depending on the trace of the sigma epsilon one. So this is the difference between the y and m1 and the sigma m1. So it's the difference between m1 and the mu1. So as you can clearly see that if the data dimension are growing faster compared with the sample sites, the low quantities on the right-hand side is not going to converge to zero. Is not going to converge to zero. So, which means that if we use the convention low-dimensional double lobuster estimator, it's not going to be a consistent estimator to the true mu one. So, we try to solve this in a high-dimensional setting and put this into this hypothesis testing framework. So, basically, what we try to do is we try to test if this data is Theta is equal to zero or not. So, theta is the differences between the mu one and the mu zero. So, the null hypothesis is the theta is equal to zero, and alternative is theta is not equal to zero. So, what is different from the classical two-sample test here is we are not just doing the two-sample test, we are actually doing causal inference because we are studying this expectation of the potential. Of the potential outcomes. So they are not directly observable. And just similar to the two-sample high-dimensional test, so we can reformulating this hypothesis testing to this hypothesis testing problem. So we can translate the non-hypothesis to the norm of the theta and this alternative to the norm of the theta. So essentially, we're testing if the norm of the theta is equal to zero or the norm of the theta. Is equal to zero or the norm of the state is not equal to zero. Then the construction of test this is seems to be pretty straightforward. So there are existing study, existing method to constructing the test this, but here we also want to achieve the so-called double robustness. So we follow the method in Chen the Ching using this used. Using this used this type of method to constructing an unbiased estimation for the norm of the stata. So the benefit of this is that we can achieve the double robustness. So under the unconfirmedness and the positivity assumptions, so either the propensity score or the outcome regression model is misspecified. So this T will still be a consistent estimator of the theta. Of the stateful. Yeah. So if you don't want to achieve the double robustness, so if you just want to use some consistent estimator, for example, if you think the propensity score. if you think the propensity score can be correctly specified then you can oh but you still need to use the X because you want to control the selection bios you want to just for the selection bio but there's an issue for if we use this estimator directly so one of the issue is the location invariance problem Invariance problem. So if you do a transformation, let's say if you transform on the y on the z to y, so because if under the null hypothesis, g and y both have, for example, g has the mean equal to zero and the y has mean equal to mu, but both samples have the same mu. So what we expect is that the statistics should have the same distribution, right? So we do not want the distribution move. Do not want the distribution move when we change the mu. So, but the statistics we just constructed directly has this issue. It's not location invariant. So, in fact, you can check that there will be such relation. So, if I use this Y, which is the location transformation of the Z, so there will be additional term which is not ignorable. So, this cannot be removed. This cannot be removed, especially the variation of this part is actually comparable with this part. So, this motivated us to do a correction on this elect estimation. So, this is non-case. Let me skip this. This is our proposed statistics. So, we first use the ELAC estimation based on the used statistics. Estimation based on the used statistics. And then we subtract this part. By subtracting this part, we can make the statistics which is invariant to the mean under the null hypothesis. And we also check what will happen for this corrective statistic under alternative. So it turns out under alternative, this statistics also has some power. Has some power, will have power. If you check the expression, check the relation between the correct statistics under the alternative, there will be one turn related with the location shift, and there will be another turn which has a mean equal to zero. And this turn, this turn is not as large as this turn, so we will have a power. So, we will have a power under the alternative. So, that's some intuitions are related with the construction of the statistics. Uh he's without this press. Without this pressure? In a low-dimensional case, usually just people just considering the estimation. I would say it's a kind of a quadratic form of the double robust estimate. By removing those diagonals. Yeah. Yeah, still bad. Yeah. Yeah. We did not check the low-dimensional case. Yeah, but that's a very good question. Okay, uh, so then we need to uh, uh, uh, uh, uh, uh, uh, Then we need to approximate the distribution on the null hypothesis to conduct the test. So, what we do first is we first do a quadratic approximation. So, for the quadratic approximation, we consider several different scenarios. In the first scenario, when both propensity score and outcome regression model are correctly specified, we can pick the W as this and the D. As this and the D as this. And when the propensity score is incorrect, but the outcome regression model are correctly specified, we can take the W as this and the D as this. And in both scenarios, so we can write our corrected statistics as this quadratic form. But in the scenario when the outcome regression model are incorrect, but the propensity score model are correct. Are correct. So, in this case, it's a little more difficult. In this case, we cannot write the statistics directly as a quadratic form. So, there will be an additional turn, which is indicated by the LED here. So, in order to ensure this LED turn to be ignorable, so we need a condition for that. So, we need a condition specified here. So, under this condition, the statistics. So the statistics will be approximated by the quadratic form for all these three scenarios. So after we approximating the test statistics with the quadratic form, we also want to understand what is the limiting distribution of this quadratic form under the high-dimensional setting. So we can show that this quadratic form Q, QN, can be approximated by another. Can be approximated by another quadratic form with normal distributed random vectors. The approximation accuracy can be quantified by this on the right bound. Although we have the asymptotic distribution, it's still very difficult to apply this asymptotic distribution. So the reason is because this S here is unknown. And if you want to estimate this S, this S. Want to estimate this s, this actually involves a lot of unknown parameters. Just for illustration, for example, if you're looking at the variance of the original statistics, so if you compute the variance under the noun, you will see, oh, this actually is the variance under both noun. Oh, this is under the noun. So, under the none, even under the noun hypothesis, so the variance expression is very complicated. So, there's no way. Complicated, so there's no way you can estimate them directly, and because it's actually involved a lot of unknown vectors, for example, here this m1x is a high-dimensional vector, and this sigma is also high-dimensional covariance matrix. So, there's no way to estimate them directly. And in addition to that, because the depending on different scenarios, so although we present in the So although we present them in the same format, we present a symptomatic distribution in the same format here. But depending on the different scenario, for example, here, when both models are correctly specified, so we have the D have this form and have W has this form. But if one of them is specified, the W and the D will be different. And basically, depending on different scenarios, the asymptotic distribution is actually. asymptotic distribution is actually different. So this makes very difficult for us to use the asymptotic distribution directly. So this motivated us to find a unified way to estimating those moments in this symptomic distribution. So we try to use in two approach. One approach is based on the symptotic normality. Another approach is based on the The kind of weighted chi-square approximation. And by in this both approaches, we need to estimate the moments. So, in the asymptotic normality, we need to estimate the inla variance. And in the quadratic approximation, we need to estimate the inla variance and the skewness. So, basically, we need to estimate the moments consistently. To estimate those moments, we propose this wild kind of wild push-up procedure. So, this So, this we found in the simulation it performed pretty well. And this estimation, based on this wire bootstrap, can consistently estimate the moments under all the scenarios when both opens score or outcon regression model or either one of them are misspecified. So, under either scenarios, on this, what bush up can consistently estimate the moments. The moments. So let me show you the simulation study. So in the simulation study, we take two covariates. So we have x1 and x2. And we assuming this outcome regression model, and we're also assuming this logistic regression model. And here is the empirical size on the type point error for the uncollect. Uncorrected statistics. The first column is the uncorrected statistics. And the first, oh, this is the uncorrect statistics without the plugin estimator for the mu. And this is one is the uncorrected statistics with the plugin estimator for the mu. And this is the corrected statistics. So what you can observe here is that when the mu is equal to zero, since all the this is All this, this is perform pretty well. When the mu is equal to 20, then when the mu is not equal to zero, so you can see this uncollected statistics with the estimated mu, so we have a very small empirical size. So, basically means that in this case, the uncorrected this is cannot control type 1 error very well. And for our For our collective statistics, it can still maintain the type 1 error reasonably well. And for comparison purpose, we also compare in the chain-the change test, which is based on the two sample tests without considering the without considering this causal framework. So, you can see in all the scenarios, the chain chains test will have very high type IF. And in order to make it more clear, so you will see when the mu is equal to zero. So for the uncorrected status, which is on the left-hand side, and for the correct statistics on the right-hand side, both of them are pretty close to each other. There's not much difference. But when the mu is equal to 20, so you can see this on the left-hand side, this is uncollected, this is statistics. This is statistics, and this is the collected statistics. So, clearly, so you can see when a mu is equal to zero, if you use the uncollected statistics, the bush shop is not able to estimating the variance correctly. We also can see the misspecified models. We can see the both outcome regression model misspecified and also the propensity score model. The propensity score model misspecified. So here's the empirical sites when we have miss specified models. So you can see when we have miss even when we have misspecified models, the collected test still can control the type of error reasonably well. And this is another misspecified model. Both cases are similar. Both cases are similar. And also looking at the empirical power of the proposed method, so we first consider the scenario when both models are correctly specified. So when both the models are correctly specified, you can see that as we increase in the signal, the power increase to one. As we increase the sample size, the power also increases. So in general, as you increase in the P, Increase in the P depending on the ratio. If the signal is also increased, usually the power also increases. And we also consider the misspecified case. So in this case, we can see the propensity score misspecified. So the power in this case is pretty good. But when the outcome regression model is misspecified, it's more difficult. is more difficult so we need a higher so you can see we need higher signal to achieve the consistency so if you're going compare with this scenario so when the m is equal to one around one so we can achieve the power one but when we have misspecified outcome regression model so we need have this m around 10 here to achieve the consistency okay now let me Okay, now let me back to the motivating example. So, in this motivating example, what we basically try to compare are the parameters alpha and the parameter beta and the dm. So alpha is the parameter related with the temporal diffusion and the beta is related with the spatial diffusion and the d is kind of the variance. Okay, so in our Okay, so in our data set, we have data from two regions of interest. And for one of the region of interest, we have 99 voxels. So we comparing all the parameters. So we compare the alpha, beta, and the D simultaneously for all the parameters in this region of interest. So basically, this alpha here is going to be a 99-dimensional. Is going to be a 99-dimensional vector, beta is also a 99-dimensional vector, d is also a 99-dimensional vector. So we compare all these parameters simultaneously, and we do multiple tasking because we have three parameters, three hypothesis compared simultaneously. As I think Lino mentioned at the beginning, so because this for brain imaging data, not all the brains are actually the same. Are actually the same. So we need to do a kind of a co-registration. So we first choose an ideal subject, and then we co-register all the subject to this standard subject. And then after that, we just use the standardized data to estimating this alpha, beta, and the D. And after we estimating alpha, beta, and the D, and we perform the test, we compare in on the We compare our proposed method and the chain change method and this conventional t-test. So as you can see that they are pretty much in this data set, they're pretty much consistent to each other. But if you consider the multiple testing, then for example, if you use this buffaloni adjustment, so let's say if you choose the 0.05. say if you choose the 0.05 and divide by 3 then you can see the turn the change test and the t-test is not going to be significant and only this our proposed method will achieve the significance so this also shows the benefit of combining all of them as a vector so it's more powerful compared with the existing methods okay so I think that's all thank you   Yeah, that's that that will be yeah that will be the second step on yeah to answer your question. So how do I go back so um so for example in in this region of interest it is all of them are not uh significant it's I think it's depending on I think it's depending on the signal to the noise laser. It's not always rejecting. Yeah, it's still depending on the signal to the noise latest. Yeah, yeah, that would be, yeah. We can do a multiple testing after that. Testing after that. If we found that two of the vectors are significantly different from each other, then you might do a multiple test to see which component or which voxels are different from each other. But in this application, because we are mostly interest in this particular region of interest, but we're not interested in particular voxels. So in this application, it's not necessary to do that. But in some other applications, we did forget. So we did, for example, if I studied FMI, so we have all the brain regions combined together. And if we see there's a difference, then we can further check which region of interest are different from each other. I think, yeah, your modeling and inference were very different. I just wanted to comment about the application. So you So, the causal framework, the way you set it up, sort of like we can't do a randomized trial, but we can imagine that we could, right, in most cases, right, if we randomized treatment. In this case, though, the interpretation, I think, is a little different, right? You're sort of imagining if we assign this person to have Parkinson's disease, or we assign this other person not to have Parkinson's disease. Yeah, you missed that. Yeah, you know, drawing from homogeneous populations. I think that's a little tricky to sort of really interpret. Yeah, it's an observational study. Well, I mean, but an observational study that I don't think it really makes sense to think of it as a clinical trial. I mean, and like, you know, to ask, I think you say, oh, yeah, by the way, we estimate. By the way, we estimate high, right? The propensity for I think if you could really estimate that well, I think you could win a Nobel Prize, right? I mean, you're really figuring out what are the costs, who's going to develop parts. Yeah, less general difficulty in this detail, I think. Yeah, thanks. Does it really make any difference? Whether you want the high dimensional doubt and then you average, and you don't drive your test, but you average and apply it, you know, the statistics for one dimension. Yeah, that's what they did in the conventional positive averaging situations where it would make a difference, whether you go one way or you do the other way. Yeah, we did some simulations that is, for example, if you design some simulation with the state averaging to be zero. With the state averaging to be zero, or with the permit averaging to be zero, then there'll be different conclusions. Did I answer your question?