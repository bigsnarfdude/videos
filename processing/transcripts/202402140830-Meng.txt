Can I guess about it? Yeah, please. You may begin, but there's 25 minutes and then around five minutes for the questions. Okay, so can you see the slides? Yeah, yes. Let me do the full mode to see if I can do this to see some of you. All right. Okay, I see. I need to see some faces there. I see Nan Singh there. Thank you, Nan Singh. So I can get some reactions. Well, thank you for the invitation. I wish I could attend in person. Benefit is always a fun place, but my schedule does not allow it. But I'm happy to have this chance to talk about some work I've been doing with my PhD students, James Bailey, and also former students, Robin Gong, at the Rock Goods. And we have been working on We have been working on data privacy, particularly differential privacy, more from a statistical perspective. So, one of the things you might have heard in terms of differential privacy is this notion of that, you know, we can protect your privacy regardless what the attacks, the adversaries, you know, knowledge is about you. And we just want to examine this question from a statistical perspective to see if that's really. Perspective to see if that's really possible. And I will start talking a little bit more broadly about the notion of privacy, then using a particular example to illustrate the points I'm going to make. All right, so let me start with this. As we know, that the privacy, particularly data privacy, has become such important topics and partly because, or mostly because the technology mostly because the technologies, particularly with the social media, with the AIs, with all kinds of tools that become potentially more invasive into people's privacy. And interestingly enough, this notion of the privacy broadly as a legal concept, which is called the right to be let alone, happened back in 1890 with this article published at the Harvard Law Review. And it really happened because another technology And it really happened because another technology advanced then, which is the ability to print pictures in newspapers and the broader circulation of the newspapers. And this was written by these two individuals. And one of the names you might recognize, Luis Braindeis, because that's the namesake for the Brandeis University. He later was also become the Associate Chief Justice of the United States. And the first person, the Sammy Warren, the reason this is. The Sammy Warren. The reason this article was written according to the history, I, you know, you always take them with a little bit of grand salt whether these things was recorded correctly. But the story was that Warren married to a socialite, a daughter of a senator, U.S. senator, and she's, you know, she's the socialite, she's attractive, she has, she's from a rich family, so she got lots of attention by these what they call the These, what they call the yellow journalism, a journalist, I think, for us is what they call these paparazzi. And to a point that Samuel Warren got really agitated, think there got to be a way, there got to be a legal concept to protect the people's privacy. So that's what happened. And you can read about this interesting story. But I start with this because, probably not surprisingly, 125 years later, 125 years later, we have the GDPR, the European, the guideline on the privacy regulations. And now we have this notion called the right to be forgotten. And you can see the right to be let alone maybe is protecting my privacy in the first place. The right to be forgotten is, you know, unfortunately, I had too much drink last night and I put things on the webs, you know, and on the Twitters or whatever called X. And the next day, I regret it. Next day, I regret it. I ask her, you say, Well, you know, can you take this thing away? Remove it. That turned out to be a lot more complicated, right? It's like it's kind of protect your privacy afterwards. So I will touch that tiny bit in the talk. Mostly I'm going to talk about definitions privacy, but the issue, the very thorny issue that will be very much related to this idea of implementing the right to be forgotten. To be forgotten, and I will say a few words about that. So that's a general notion of the connection between privacy and the technology development. Now, let's be a little bit specific about data privacy, what does it mean, because the term has been used very broadly, but for different groups of people working on different areas, they actually mean different things. I want to be clear that today I'm going to talk. Clear that today, I'm going to talk about essentially is what specifically is data content privacy. This is the protecting information that can be reviewed by the recorded data values. So this is about data content privacy. This is pretty much what a differential privacy does. There's a whole area, much, much harder area, is a metadata privacy. This is to protect the identities of the senders, the receivers, time of communication, all these. Time of communication, all these metadata, not necessarily about the content, but just about who's sending what. And this turned out to be very important in the world of intelligence, for example. In the world of intelligence, where you get the information, where you send these things, you know, can kill you, literally. And you guys probably can understand this is much, much harder than protect the data content privacy. But the other area is now really this right to be forgotten. Area is now really this right to be forgotten. Right, you want you your data has been already posted by you or by others, and you now want to have that erased. That turned out to be a really very challenge is the question is how do we operate operationalize erasure and do we erasure all copies, all consequences, right? Because you know, it's not just remove your post, it's remove all the posters, all the other people's posts of you. All the other people's posts of your post, all the people written what you did, you know, and how do you do that? That's operationally, but even harder ones that if your data has been used in machine learning, and so how do you unlearn what you have already learned, right? That's a whole field called machine unlearned learning. Some of you probably even work on it, and that is incredibly hard because incredibly hard because you is no longer individual you, you'll be connecting with others and uh. Connecting with others. And so, and you already have consequences by posting on it. So, how do we really implement that? It's a fascinating area. I'm teaching a course on data privacy and I invite a lawyer to talk about this aspect. There's lots of legal requirement. What does it mean? Lots of liability issues. It's a really terrific area. I would certainly encourage statisticians to work on it because there's a lot of thinking here is about how do you think these things jointly and counterfactually. And counterfactually, causal inference, all the stuff there. So, I will not be talking about this part, but what I'm going to talk about specifically about this protect privacy, individual privacy is actually very relevant for the right to be forgotten because it's the individual at the core, but individuals are not isolated from each other. So, now let me talk about defensive privacy using the following example. This is known. Example. This is known as the first kind of a published case of differential privacy, even that term was invented many, many years later. This was an article published in JASA, Journal of American Aesthetic Association, and by Warner. It's about how do you inject the noise in the response mechanism to protecting the privacy? So that is squarely the same idea as a differential privacy, except that. Differential privacy, except the differential privacy tends to inject the noise after you collect the data. This is actually inject noise when you collect the data. Okay, but so the timing is different, but the goal is the same. The approach, even the approach is very much the same. This was created then because the need of asking sensitive questions, right? These sensitive questions traditionally, you know, sexual behavior, drug use, you know, you know, health. You know, health status, but these days, probably political ideology, all those things that could be sensitive, and people just don't want to answer you, even they have no intention to lie or anything, but they just don't like you to have the answer. So, how do you go about that? And then in the education content, I actually tried this. I said, well, I want to know how many of my students are cheating. Now, good luck with that if you're trying to survey the student directly. But this is why I'm using this example. But this is why I'm using this example, and I'm going to share with you and talk about both the differential privacy and all the thorny issue comes out because of the connection between individuals. Okay, suppose you want to estimating the exam cheating rate in your class. Let's say that's called a p-cheat. So for each individual students, if they cheated, x equal to one, if they did not cheat, x equal to zero. All right. All right, and now each student will be given a biased coin in this particular case. In the original one setting, it's using a biased coin with P large than half, large than yeah, 50%. You will see why that's important. So the idea is that you tell them that they don't need to answer you whether cheat or not. You know, they're not going to give you honest answer. So you say, well, why don't you secretly flip that coin? flip secretly flip the coin and record, don't tell anyone, record whether it's head or the tail, right? I equal to one or I equal to zero. And then what you report is neither R nor the X, but rather if the two are equal. Okay, so what you report is you report a one, so all I know is that your cheating status and the flipping coin status are the same. Otherwise, it's a zero. So at least, you know, on the At least, you know, on the surface, um, you think, well, this at least provides something what they call the you know plausible deniability. Because if I tell you one, I didn't tell you whether I cheated or not, because you don't know how I flipped a coin, right? So at the individual level, at least you can see it's not immediate you can actually infer anything about the person's cheating status, but collectively, as we can easily show, right, by aggregation, what's Um, right by aggregation, what's the probability i equal to x? You do, because you're assuming these two processes are independent of each other, you can easily get this, you know, total probability law, you get this formula. But since the p is known, because you are the one to distribute the coin, and so you can really solve this, right? You can solve this to say p cheat equal to that. Now, you can estimate what the py is from the sample, and so here, right there, you get this estimator for the percentage. For the percentage of cheating, and you can see why p cannot be 50 because otherwise this thing does not work. You probably see why, because 50 is completely symmetric. You really get no information. You still need some information, right? So, because there is some information about estimating the overall cheating rate, it implies there's actually some information for each individual. It's just the information is weak. And so, here's an example how you apply that. You know, let's say you 45. You know, let's say you 45% of your population, your sample said, give you y equal to one, you know, you give them a coin, it's 60% chance of the head. So you can write down, so you'll be estimating it's 25% of the rate in your class. It cheated, and you can easily calculate the variance because you know how to calculate the variance of y and bar. Now, I want to mention that when you really implement in practice, it doesn't really work that well because even when you Really worked out well because even when you have this protection, there's not much incentive for the students to tell you. And they are smart enough, they suspect that you might be able to trick them or something. So in reality, when you implement this thing, you probably still tend to get very biased estimator. But at least in principle, we can see this is the notion of literally this is what the differential privacy means. By inject some noise that we can protect better at the individual levels, but we still can. know individual levels but we still can hopefully have some information left for the population estimation right so that's the so now let's look at specifically uh you know you can calculate the what's the variance and clear the variance will be increased compared to if you if they answer directly but of course you should always worry about the bias variance trade-off because if you do not ask it do not have this protection you just you get it you know terribly biased to estimate or not at all right so just a very simple So, just a very simple calculation: the variance equal to that. So, you can see that as the p getting closer to 50%, the variance goes up. As P goes to one, go to zero, the variance goes down. But on the other hand, you also, it's very clear, as P goes to zero, goes to one, the individual protection become much less. So, what exactly is this doing is in fact controlling a likelihood ratio. Now, in this particular case, for all the statisticians, we usually don't think. For all the statisticians, we usually don't think of data themselves as the estimate. But in this case, what we're trying to estimate: look, think about the individual x, think about yourself, x equal to 1. You do not want people to know whether you cheated or not. So what we're trying to do here is say, well, once you observe the y, right, what is the odds that you exactly cheated versus you're not cheated? Using the Bayes theorem, you can easily write down, right, that equals to the posterior odds equals to the likely ratio. To the likely ratio. I say likely ratio because here the parameter is X. Okay, the X, your actual status of cheating is the parameter of interest. The Y is this randomized response. So this is likely ratio, and this is the prior ratio, right? A priori, me as a professor, may have some suspicions about whether the person cheated or not, maybe not, right? But whatever it is, this is the odds I want to protect. And I cannot do much, at least from data curators' perspective. At least from data curators' perspective, I cannot do much about the pry odds, but I can try to control the likelihood. Okay, so now let's see how do you control the likelihood? Well, it's very easy to write down in this case, because of the way we set up, right? This likely ratio is equal to p times 1 minus p. Okay, that's if you think for a second, because that's how, you know, that's that's how that happens. And if you write that at the e to the epsilon, And if you write that at the e to the epsilon, epsilon becomes low jp, right? Now, that's when you observe y equal to one. What if I observe y equal to zero? Well, you can do the same ratio, and this will be flipping, so it's e to the minus epsilon. Okay, so either way, regardless whether you observe y equal to one or y equal to zero, the like ratio to distinguish x equal to one, x equal to zero is essentially controlled by e to the epsilon, either positive or negative. And so you basically have this inequality. Have this inequality saying, regardless of what the y is, I have bound it by controlling the p, I can bound it this like ratio, right? And right there, that is actually the differential privacy definition, at least the original pure differential privacy definition, right? It's to control this likelihood by saying, I want to inject the noise in such a level that this ratio will be controlled, you know, e to the epsilon. Now, epsilon is called the privacy loss by. Private loss budget. Okay, so, but you can see this is quite clear, right? You will have this, you will have this kind of a trade-off as P moving move towards half, this epsilon is going to move to zero. Sorry, epsilon is going to move to, let's see, if, yeah, if P moves to half, then epsilon is going to move to zero. That means that. Going to move the zero, that means that it's a complete protection, right? Because you don't disclose anything, but as p go to zero go to one, uh, you know, you have this bound will be going to uh going to going to infinity, essentially saying you have no protection at the individuals. So that's the typical, I'm plotting these two lines. One is the like ratio, the other is the, one is the posterior arts, one is the one is the variance. And, you know, you usually what you And usually, what you're trying to do is to try to find some kind of sweet spot so you protect enough privacy for the individuals and you do not lose too much in estimating what you are interested in. So this is all fine. As long as I'm only looking at this individual data, y equal to one, and it does protect that as defensive privacy will promise. Well, well, well, promise. So, I give you these two definitions of differential privacy as defined by my wonderful colleague Cynthia Dork and her team. And Cynthia has been really the pioneers of doing differential privacy. Their 2006 article defined it using, you know, thinking about X here is the data as the, they use the word query, but that's essentially, you think about X here is the parameter, right? Here is the parameter, right? So the T is the random mechanism, and you want to say the probability that the distinguished X versus some other X differ by one entry. The idea of one entry is representing one individual, that probability, the ratio shall be controlled to the e to the epsilon. Of you using log, that will be essentially the log ratio. And in 2016, it's the additional, the It's the additional the second kind of a revision of the first version. They define it more broadly using the more kind of a you know mathematical notation and using sets, but the essence is the same. You're trying to protect the likely ratio. Here, the likelihood parameter is the individual data. And you think about the data if they differ by a single entry. And there's a lot of discussion of what a single entry means. It could be membership, it could be. It could be a membership, it could be an individual, it could be a group, but the essence is to control the likely ratio. So I just want to mention that some of you that I've been editing the Harvard Data Science Review, we published a special issues. It's entirely free online, so you can check, including now if you want to. It's about differential privacy for the 2020 census. Part of the reason differential privacy become very popular is because U.S. Become very popular because the U.S. Census Bureau decided to implement it, which has generated lots of controversial lots of debates. As both the articles document what they actually did, so-called top-down algorithm, and all kinds of concerns, worries, and methodologies, debates, because questions like who's going to choose the privacy loss budget. We benefited one group, they could harm another. So there's a lot of issues. I'm not getting to it, but for anyone who's interested in. It but for anyone who's interested in definitely, this is a very rich source of information. I'm actually teaching a course, you know, there's a printer version. If any of you are interested in, you can certainly email me that you may actually using this whole volume to teach an entire course. Okay. So now I wanted to show, well, let's look at this revisit this problem, this random response. Random response. Let's think about it from a different perspective. Think about the ways that we statisticians will be thinking about what would be the protection means. The protection means, say, I have a pry knowledge, like your cheater. Let's say that's pry x equal to one. I want to see how much the prop, what's the posterior probability equal to your cheater once we observe your answer compared to the prime, right? And that change for most of us would be viewed as. Change for most of us would be viewed that is the key change. That you know, if I'm going to operate like a Bayesian, uh, or you don't have to be if you make any decisions, I want to say, well, how much the information in the data has changed my prior probability of your being a cheater. So, so, you know, defensive privacy or any privacy mechanism, none of them can protect the absolute privacy, right? If you put yourself on Twitter or Acts, whatever it is that tell the world about you, there's no That tell the world about you. There's nothing anybody else can do, right? Of course, you can regret, then you ask them to take out, but that's a different problem to work on. What we're trying to do, and I think that defensive privacy does do right, which is really say, okay, what is the incremental information that because of release of the data can provide so that I'm trying to make some guarantees about limit that increment. Limit that increment improvement. Okay, so if you write down this posterior to pry ratio, it's very, in this particular case, it's very easy to write down. Essentially, it's a function of like a ratio times the pri, you know, the time this pri. Because of that, it's very easy to show that the bounce on this posterior to pride is exactly the bounce on the likely ratio. Okay? Okay, so if you want this posterior to be bounded in the e to the minus f by the e to the epsilon, regardless of what the pri is, the best you can do or the only thing you should do is to bound the like equation. And that's a simple calculation and I probably made it more complicated than it's necessary. There's a general formula you can write down and it's also not hard to think about why that's the case because by changing the price for any price. Changing the pi for any pry, you can take a pi equal to zero, pi equal to one, you can work out. So, basically, this is very interesting, right? We use statistics usually don't think that way is that you can, you, if you want something to be safe against all price in this particular setting, then what you should do is really control the like ratio. Then, the worst scenario is going to all control the by the like ratio. So, this is all fine. There's only one big problem. The big problem is this assumed. The big problem is, this assumes that you only look at this individual person's answer. What if, because you know, I have a whole class answer, right? You know, what if I want to look at, you know, I have, I have a suspicion that two people tend to cheat together, okay? Suppose I have a pride to link to individuals. Then I can obviously, in the simple case, say, well, I'm not just going to look at your answer. I'm also looking at somebody else's answer, particularly the person I think is quite related to. The person, I think, is quite related to you. Now you can do this posterior to prior odds. If you do the calculation, you will see it's a very similar formula, except in this case, you will look at a different like ratio. I probably shouldn't even call this like a ratio because you see the x's of x2 are not even here. That's implicitly being integrated out. But there is this ratio you need to look at. Basically, it's a joint probability of your answer given x equal to one or x1 equal to the. or x1 equal to 0. Unfortunately, if you define the differential privacy for as they take an individual in and out, this one is not protected by the differential privacy because you're manipulating at the individual level. You did not manipulate them at the pair level. I don't want to see you what's the consequence of that. It's a very, let's take a simple case. Suppose you observe you respond y equal to one, and the other proof. Equal to one, and the other person I suspect you know tends to have some correlation with your cheating. Maybe you guys always sit together, also respond to one. In this case, you can calculate this Liker ratio, it's called a like ratio, is equal to this quantity. By the way, by this formula, you can still see the extreme you can happen for any prime is the extreme you can happen with this Liker ratio. But the problem is this Liker ratio is no longer protective to the e to the epsilon. The e to the epsilon. The e to the epsilon was only when you did the likelihood, did a sampling with a marginal, not with the join. Now, it turned out that you can show that suppose, for example, these two people are, their answers, their cheating status are always the same. They always cheat together or they don't cheat. In this case, you can show that that, you know, that like the ratio itself is e to the two epsilon, not e to the one epsilon, because you are actually, you, you actually. you are actually you you actually have another person can can leak the information for you the more interesting results is uh you can show that this like this you know join like ratio i should say join sampling ratio is bigger than e to epsilon if and only if these two uh the status are a priori positively correlated that's also pretty intuitive right if they're positively correlated knowing the other person is leaking a little bit of information for you and it A little bit of information for you. And the proof is very, very simple in this simple case. I do not believe this if and only if is true in general, but I uh we haven't worked out what the general is, and I think the general will be a lot more complicated. There's some binary property make this much easier, but there got to be results to show that the kind of a positive correlation makes the leakage, I mean, depends on how you define it. So these correlations are the ones very. Are the ones very, you know, very important here. So, because of time, let me just emphasize one thing, then I give you one slide, then I stop. The fundamental thinking here, this is kind of statistics thinking versus inference thinking versus database thinking. For a computer scientist, maybe, or anybody that does a database, you can think about the database itself as fixed, right? That's the way the Liker ratio works. The parameters are kind of fixed. The parameters are kind of fixed. You're thinking about protecting the particular value of that, the value of that data point. You're thinking about, you know, for any data value, you can manipulate it, right? You can say, I take the value out, or I could put the value in. But statistically, if we think about information, it's no longer about the value. The data are this accidental representation of the essential information beneath it. As you know, that when you're dealing with distributions, think of them as variables. Think of them as variables, you don't say, well, let me take out one variable and still treat the distribution of the other variable unchanged. You know, that's not the case, right? Because there's a marginal distribution, there's a conditional distribution. If you change to your prime, whether I know x2 equal to one or not, if they're correlated, it's going to change the information about x1. So in that sense, that you no longer kind of just manipulate these things by just thinking them as the By just thinking them as the fixed value, you need to think about them as distribution. So, we have a general, I'm not going to talk because of time, essentially, that these protections that they do not protect these marginal ones. They protect the conditional one. You have the conditional, you know, everybody else. And that's like an incredibly strong, you know, assumption. And also, in this case, you have to remember: by assuming strong information, actually makes it easier for you to protect because it's a relative ratio. Protect because it's a relative ratio, right? If you assume that you're not everybody already knows about you, then anything will be differentially private because there's nothing left for me to disclose. So you have to be very careful the relative differential privacy versus the absolute privacy you want to protect. Often people get wrong and say, well, if I assume the attack has a stronger information, that gotta be the best protection. Well, that's actually easiest to want to protect relatively because there's not much left for me, you know, for me to. Much left for me to protect. So, the fundamental question here is: what is the information unique to an individual? Now, we proved the results. This is actually, it's just a re-summary of results. In the end, what you need to show is that if you do differential privacy on whatever individual level you have, the actual protection for the posterior to pride, you pay a price of the C. It's E to the C type of epsilon. What is the C? C is the number of people connected with you a priori. Connect with you a priori. So, in the conditional, or they call the micro boundary, essentially is like when you do the conditioning, how many people? It's the smallest size of the seeds that you cannot remove because the conditioning. So, you're basically saying you are connected with others and the information units is not you. It has to be everybody connect with you. So, my last slide is really the one to illustrate this point from something we all understand, which is the COVID, right? We know that during the COVID, if you want to COVID, if you want to stop the spread, it's not enough to quarantine the person who already infected or the person who has strong symptoms. You need to quarantine everybody that connected, has in contact with that person. This is very much the same notion. If you really want to protect individual privacy, even in this relative term, your definition of protection needs to be group privacy. It's all the people related to you a priori. The problem with A priori. The problem with that, the moment you say that, it's context-dependent. So, this notion that I can protect individuals regardless of what the pride is simply impossible because it's quite obvious why it's number of people. Imagine you have three copies of yourself in the data. Just remove the one copy does not really do anything for you. So, your risk is much higher. So, let me stop here. I hope I present enough to say this data privacy in Say this data privacy in protecting the individuals is a lot more nuanced concept than say just let this person data be differentially differentially private. Thank you very much. Thank you, Xiao Li, for excellent talk. Very, very interesting. We have time for one or two questions for Xiaomi. And also, if you're on Zoom, you could enter questions in chat and then I'll check in a moment. So, I have a question. So, it's related to the theory you developed where you were looking at the covariance. So, for example, two people may be cheating often together. And I think if we sort of explain. I think if we sort of extend this so that we have n variables where there's sort of a cor correlation in if they're cheating or not, is there some sort of like correlated failure that occurs where the balance on epsilon will change? So it's hard for me to kind of formulate this question, but I'm sort of feeling like if If there's many tests and many people are either all cheating or all not cheating, then the more people there are, it's sort of like we're getting more information about the underlying value. So it could be that like no epsilon is going to work anymore. Yeah, no, that's exactly right. That's exactly this results presenting quickly trying to show that if. To show that, you know, if you have, let's say, if two people relate to you, then the price you're paying is, is, will be, is, will be three epsilon because it all depends on, including yourself. Essentially, how many, so this is what we call the smallest, we call it, you know, we call the minimum information chamber. Essentially, is how many people are operating connected with you. If, if the prior knowledge, the attacker has that, and the attacker, let's say he knows that there's three people's behavior related to you, then Behavior related to you, then that you think you are protecting him up to the epsilon, that actually that could be up to four epsilon. And so that's the price you pay. Now, I think your question has another interesting angle, which is not necessarily the number of people, but if you have on the individual, you have number of variables, and if these information are connected with each other, and what would that imply? I'm sure that can be worked out because all we're doing. I'm sure that can be worked out because all we're doing here essentially is saying how does the posterior to prior ratio is bounded by the likely ratio.