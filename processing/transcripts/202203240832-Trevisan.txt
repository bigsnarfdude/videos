Yes, yes, I would. Thanks, Matthias. So thanks for the invitation. It's a pleasure to give an online talk anyway, better than nothing. So I decided to change the topic at the end and I will discuss this quantitative Gaussian approximation of randomly initialized deep neural networks. This is a small work jointly with a student of mine, Andrea Basteri, who's completing his master's thesis, and we ended up with a nice Thesis, and we ended up with a nice inequality which quantifies this approximation, Gaussian approximation of neural networks. So, let me show you the short plan of this talk. So, first, I'll give you a motivation, and there are plenty of motivations to study random neural networks. Then, I will just give you the statement and the interpretation of our result, and tell you a bit more about possible extensions in future works. So, the motivation. So, the motivation is really concrete. Essentially, there are huge problems in understanding essentially why deep learning methods work. And on a practical level, deep neural networks provide the framework with a lot of applications in machine learning. So, speech, visual recognition, generation of samples, feature extraction. So, essentially, these are the new paradigm of machine learning. Of machine learning in the last at least 10 years, I would say. There's been a really revolution on this. On the other side, the mathematics is a bit lagging. So, understanding why deep neural networks work the way they do, it's a bit an open question, I would say. But there are some insights. I'm not really pushing new insights on this. What we're going to do is essentially to quantify some proposed explanations from. Proposed explanations from the machine learning literature. Anyway, there is a lot of new tools and results, interesting results from different fields, essentially probability, statistics, statistical physics also, but also functional analysis, geometry, essentially all the many fields in mathematics try to provide some tools and explanations for machine learning. For machine learning, and on the one side, to guarantee that methods work, but also to show some limitations essentially of these methods. These are the general idea behind it. So, what is, I hope you see, yes, okay, this slide is a bit, I don't know if you see the above. Anyway, so the basic tool in this deep learning or deep yes, deep learning framework. Yes, deep learning framework is provided by artificial neural networks, which are not a new thing because these were proposed, I think, in the 50s of the 20th century. And these are biologically inspired ways to parameterize functions. Essentially, functions with input in Rn naught and output in NL dimensional vector. You can think also the output to be one dimensional. Think also the output to be one-dimensional, but in any case, it's better to generalize. And why these are biologically inspired? Because you think of each component of a vector like a neuron with some sort of potential inside, and there are connections, which essentially are mathematically linear functions, and then there are also some non-linearities which we call activation functions, essentially. Now, graphically, these are. Now, graphically, these are always represented like this. Of course, these connections may be there or maybe not because there are some parameters. These are not very rigid. It's like polynomials, right? When you have a polynomial, say you fix the degree, but then you have a lot of coefficients. The same works for neural networks. Essentially, you can have a lot of parameters. And then typically, what you do is for your problem, for example, some recognition, you try to classification problem. You try to classification problem. You have an input which is an image, say a very large vector, and the output is one-dimensional. And you want to find the best choice of parameters so that you can, your network really classifies images, say, between cats and dogs. I don't know, you just do a binary classification. So these are the tools we want to, so the mathematical objects we want to describe. And of course, we want to move. We want to move to random neural networks because it's a bit like saying random polynomials. But why random? Because it's really a successful approach to explain many features of deep learning. And essentially, the idea is that you consider the scaling limit of large neural networks and the parameters are randomly chosen. And why you should do this? You should do this. Because there are at least three reasons. There are at least three reasons, but I think there are even more. So, neural networks are used to do inference. And if you really want to do inference, say, based on probability, you should use a Bayes theorem. So, you should do a Bayesian approach. So, you should first specify apply your distribution on your parameters, which are called weights and biases, essentially these parameters in the neural networks. And but, general framework is just specific. And but general framework, you just specify a prior distribution and then you update it after the observation, which in machine learning is called So the training procedure using the training set. But essentially, it's just an application of Bayes' theorem. So, the idea is you need to specify some prior distribution. And using so prior distribution means really a random choice for the parameters. On the other side, from a practical point of view, so this new. Practical point of view. So, these neural networks are so large that you cannot use really Bayes CRM in practice. So, what you do is you sample some initial choice of parameters and typically you sample it randomly and then you use algorithms to update. Essentially, instead of using Bayes' theorem, you do these algorithms which are called stochastic gradient descent. This is a very typical algorithm you use to train these networks. But still, the randomness is there. But still, the randomness is there, not only because the algorithm is stochastic, radiant descent, but also because of the initialization. Okay, this is quite relevant, in fact. Okay, the algorithm is also very important, but also initialization, the way you start with the algorithm. And finally, there's also another important feature, which is the following one. It was realized that if you have a random neural network and you don't do the training, so you don't apply. Do the training, so you don't apply BS CRM to all the parameters, but only say typically you do it only on the last layer, so only for a fraction of the parameters. Essentially, you apply it in a way which is very simple to apply it computationally. Still, you get something which, okay, is not fully trained, you would say, it's not really a full application of biased CRM, but at the end, you get something which is useful in computation, so very efficient. So, this is the So very efficient. So, this is the paradigm. It has many names. One name usually is reservoir computing when you do with the special structures of recurrent neural networks. So, anyway, I hope you see there are many reasons to consider neural networks with random coefficients. So, what is our, yeah, of course, we're not the first one doing this. And as I told you, this is a small project which follows many important works. Many important works. So, just to give you some references, Rilli Rosenblatt, who was one of the pioneers in the field, already proposed to sample random neural networks, essentially. Then Neil was the first to show a theorem about shallow networks, which means you have only one hidden layer, which are the layers between the input and output, but you take only one hidden layer. And if it becomes very wide, if it becomes very wide very so you you you put a very high dimensional hidden layer then he proved that in appropriate scaling you get a gaussian process now it took say 20 years to understand for the community that you can also extend it to deeper architecture why deep because in a neural network in the picture i showed you before you have a series of hidden layers and it practically turns out to Practically, it turns out that if you instead of consider a single hidden layer, but you put many, many hidden layers, things may improve. In many cases, you get something which improves. So, for the practical applications. So, it turns out that just after 22 years, they realized it was possible to extend Niels theorem to deeper architectures. And this is what we are going to follow for our result. Later on, Result. Later on, it was also realized. This is very important, but we are not going to use it. That also training, so also after the so-called training of the network, this Gaussian behavior, essentially, if you don't train too much the network, is still, so the Gaussian process is still there. It's a different Gaussian process after training, but it's still there, it's still Gaussian. And this is the so-called neural tangent kernel theory, which is very important. Theory, which is very important and also found applications besides theoretical explanations on why neural networks work. Of course, there are other mean field limits, other limits, other large-scale limits for neural networks. For example, there is the so-called mean field limit of neural networks, where you do a different scaling of the random parameters, you end up in a different setting, essentially. Okay, so you see there's Okay, so you see there's a lot of literature, but our result essentially is a small contribution on this because what we prove is that when you initialize a neural network, so you choose random parameters, random weights and biases, I will tell you a bit more now about the notation. If the network is wide, so you fix a number of hidden layers, say a fixed number, but then you A fixed number. Then you let the number of neurons in each layer grow. So it becomes a so-called wide networks, not very deep, but wide. Still can be deep essentially, but a fixed number of hidden layers. Then we can quantify convergence towards the Gaussian process. Essentially, this complements the work by Matthews and also by Eli and also later ones because there's been follow-ups on this. Been follow-ups on this. Apparently, we're the first to provide explicit rates of convergence, and quite interestingly, our rate of convergence is measured in terms of buses and distance. Okay, it's not very surprising because I've been working on optimal transport for a while, so this is a basic idea. Apparently, it fits very well, but I guess you may use other distances as well. But for mass design 2, we fixed for mass sign 2 for simplicity, we get We get a very, very nice expression, as I will show you now. But first, of course, now we need to introduce some notation, unfortunately. So just to fix the notation. So as I told you, the neural network graphically is this biological network of neurons, but mathematically, what is it? So you fix, say, an input dimension and a not and an output dimension. And not and an output dimension and L in our notation. And of course, you need a series of hidden layers inside, right? So say that you fix L plus one total number of layers, which means you have the zeroth layer, which is the input, the Lth layer, which is the output, and only one in between. Now, each layer has its own size. So N0 is the input, then you have N1, N2, NL-1. Have N1, N2, and L minus one, the hidden layers, sizes. So, which means that you have N1 in the neurons in the first layer, N2, and so on in the second and third layers. And then NL is the output. So, as I told you, there are parameters, even if I haven't shown you so far what are these parameters. These are called the weights and the biases. But essentially, these are our parameters. They play the same role as coefficients in polynomials. But we have to. But we have typically a lot of parameters because for each layer and the next one, say from the zeros to the first one, the first to the second and so on, we have a matrix of parameters which has components essentially the size n L plus one times N L. So it depends on the product of the two layer sizes. And also you have a bias, which is just a vector of size N L plus one. and L plus one for each layer from zero to L minus one. Because at the end, the output layer is really the output that you read. You don't need any more parameters, essentially. And now, of course, as I told you, there are these connections, which are these weights, essentially, but also some nonlinearities. And it's actually the key idea to introduce nonlinearities in neural networks. Otherwise, if you just do compositions of linear functions, Compositions of linear functions, you would just be in a so-called linear model. You have just a linear function at the end. So, what you do is you fix, for example, one activation function, but you can choose other, you can choose many activation functions. For us, it will be only irrelevant that it is a Lipschitz function. So, in our case, you have Lipschitz functions from R to R. And for example, the so-called rectified linear unit, these are very specific names. These are very specific names, but this is a very simple thing. So it's a positive part of the input, right? If you take a real number, you just do the maximum between 0 and z. So this is a Lipschitz function. And you use combinations of weights, so linear maps given by the weights, and activation functions, non-linear maps, to define your overall neural network, which will be a non-linear function at the end. So how is the neural network defined? The neural network defined? It's inductively defined, recursively defined. So you define, say, the first hidden layer map from the input to the first hidden layer, simply just a linear function. Okay, take the input, you apply the matrix, and you sum the bias. You multiply by the weights and you get the bias. And then by recursion, you define FL from the input to NL simply by composing component-wise the previous. The previous map, so sigma of F L minus one at the input x, so which will be a vector, but you take sigma and you apply at each component, you apply sigma. You get the same as a vector with the same dimension, and then you apply the weights, and then you sum the bias. So it's really a composition, stacked composition of linear and non-linear functions. So this is actually what a fully connected network is, because in reality, you use so-called You use so-called different architectures, so you restrict to special cases of weights, special cases of essentially to reflect some geometry of the problem. But for simplicity, typically these theoretical results people only consider fully connected networks. I guess our inequality will also work for more general architectures. So, now, what is our theorem? It's really a way to quantify, it's a sort of central limit theorem. It's a sort of central limit only for neural networks randomly initialized. So, what we do is to consider weights and biases. These are matrices and vectors that are all independent Gaussian-randoid variables. So, all the components are independent Gaussian variables. We assume that to be centered. And we also normalize the variance of the weights. You see, the weight W L is a matrix from N L from L from our so n L plus one times n L. So we normalize in this way. So one over N L, which is essentially a central limit theorem scaling. Okay. Well, for the biases, we don't need any normalization. We take, for example, variance one. Of course, this one is a bit arbitrary. You can play with other constants here. So, given this choice, which is a very common choice for initialization of neural networks, if you take any Neural networks. If you take any machine learning package in library, in Python or whatever, typically when you initialize a neural network, they choose this or a very similar initialization like this. So they take independent weights, which are Gaussians. Perhaps the scaling is not really this one, but it's very common, this type of scaling. So what happens is that for every set of K inputs for your network, you can look at the law of the output. Look at the law of the output. Okay, let's call it big X, the input set. So F L of big X is like the matrix given by all these outputs, which contains, say, N L times K numbers. So we can prove that it is close to a centered multivariate Gaussian with a computable covariance matrix. So we are not the first that computes this matrix, but we are the first to quantify this convergence. We are the first to quantify this convergence in terms of Vassalstein distance. So, the distance between the law of the neural network, which is a non-linear composition of Gaussians, okay, and the Gaussian matrix is bounded by this. We have C, which is a constant which depends essentially on not so many things, but does not depend on the size of the hidden layers, nor the output layer. This is important. Because then this is multiplied. Because then this is multiplied by the square root of the output dimension times we have this sum of one over square root of n L, where L goes between one and big L minus one, so which means that you are doing just the sum over the hidden layers. Okay, so the constant here is positive and finite, depends only on sigma, actually on the Lipschitz constant of sigma. Actually, on the Lipschitz constant of sigma, the input set and the number of layers L, but not on the sizes of the hidden or output layers. Okay, it's a very neat inequality, I would say. We can make it very explicit. It's not really. But the dependence we're interested in is this type of dependence here. Okay, so it's a sort of central limit theorem for quantitative central limit theorem for deep neural networks. Theorem for deep neural networks. Sorry, may I ask a question? Why does it make sense or why can we accept that the constant depends on the set of inputs? Yeah, I will show you later what you should do. Because if you want to do a limit in a functional space, so say take an infinite set of inputs, there is some degeneracies. It's not clear, at least to me, if this rate is sharp. There are some results for shallow networks. Some results for shallow networks for a single hidden layer, but it's not clear to me how to extend them to deep networks. So, for the moment, consider only a finite set of inputs. Of course, in applications, you have a finite set of inputs, so it could be also relevant. I mean, the idea is that the functional limits, this is when you consider infinitely many inputs, it's a different setting. It's more like, yeah, I will tell you many. Yeah, I will tell you maybe something if I have two minutes. So, what happens is that the matrix KL, we're not the first one to compute this matrix. I mean, the convergence was already known, that if you take very wide, so very large number of units in all the hidden layers, you converge to this Gaussians, so this Gaussian process at the end, because you have the input and then Gaussians at the output. So, what am I saying here? Yes, the matrix is explicit. In fact, there is a recursion. In fact, there is a recursion. You can compute it recursively. There is even libraries in Python that you just type in the structure and they compute for you this covariance matrix. And an important thing is that all the output neurons for our Gaussian approximation, not for the neural network, but for the output for the Gaussian approximation, for this multivariate Gaussian here. Multivariate Gaussian here, the output neurons are all independent. While this is not true for the neural network, because you are mixing all the weights in the neural network, while the approximation gives you independent neurons, this only at initialization, then the training is all a different story. We don't even consider that. Now, so this inequality implies very simply, right? If you take the hidden If you take the hidden layers to be wide, so the wide limit, these all become very large, and you keep fixed input and output. This will converge to zero, so you get quantitative approximation towards the Gaussian law. And we can say also something, but not really too much in the deep limit, because everything is explicit. So if you're really interested in the deep limit, you can also take out some information, but not so much. Take out some information, but not so much. So, since time is not so much, we can keep it. So, think about a result about the wide limit, so L fixed, and then you make a very large number of hidden neurons in each hidden layer. So, the idea of is very simple, in fact, because you really go back to NIL and you understand that NIL's result could be a first step in some induction. First step in some induction. Now, typically, also Matthews and the others proved things by induction, of course, but then it becomes very, very, very much complicated because they are not tracking quantitatively two things. First of all, so essentially one thing actually. So the independence of the neurons. As I told you, in the Gaussian limit, all neurons are independent of the output neurons. And this also would essentially give you a very nice induction. Nice induction argument. What do we do? Essentially, we use for the first Hiban layer, you have already exact independence, which is already nil. And then we argue by induction by using essentially the triangle inequality for buses 102. So we plug in the induction assumption and just rigorously take care of the error terms that enter for the application of triangle inequality. In fact, we also use. Inequality. In fact, we also use a complexity inequality for which I was not able to find a precise reference. So if you have any reference where inequalities like this, where you, so this is just a very simple convexity inequality for vastness and distance. But since we are working with abstract spaces, I wasn't able to find a good reference for this. Anyway, it's very elementary. If you are in fact, it's a master thesis, so In fact, it's a master thesis, so okay, we're not really going to very deep sophisticated arguments. But anyway, I think it's relevant, it might be relevant. And of course, the next step would be, as you told, to let k to infinity. Yeah, but the problem is that if you, for example, you want to obtain convergence on the space of continuous functions, because you know that your neural networks are continuous functions, the problem is that. Is that the constant, the implicit constant, now diverges as the size grows. So, for shallow networks, this has been studied. Otherwise, we can say we conjecture, we don't even really prove this bound, but it's very simple to prove it if you plug in all the details. We can give an abstract bound like this, where you pick an exponent gamma between 0 and 1, and you minimize between what? Between what? So say that you fix your X, which is a compact set, for example, a ball. And then you pick an epsilon net, which has a finite number of points, such that all the points in your compact set are very close to epsilon closer to one of these points here. So at the end, you can minimize over epsilon and you get this abstract bound, which still is not really what I would like as a bound, right? Like as a bound, right, but it's still something that implies convergences in the functional space. Okay, just two minutes about extensions. So, as I told you, this is a we try to be not so technical. We try to keep technicalities at minimum. So, you can do many things. Maybe we will do them. I don't know. So, first of all, vasustain 2 is not very special. You can do, for example, with vast sustain P, I guess, for any P. Will must assess, I guess, for any p positive for any finite p. Now, the proof also should extend to other architectures because the only idea is really triangle inequality and central limit theorem and careful estimation of the error terms. But for example, convolutional or recurrent architectures could be also investigated. An interesting thing is also about allowing for non-Gaussian initialization. The initialization of the parameters because typically, in practice, you really choose Gaussian parameters at random when you start your network. But of course, theoretically, you can ask, for example, what happens if these are discrete random variables or even stable laws. This has been done theoretically without quantitative results. If you initialize with the parameters which do not satisfy the Gaussian central limit theorem, of course, these are, say, we say, simple. Of course, these are simple extensions. An interesting question instead would be the sharpness of this bound, which is not clear to me. I'm not able to prove it. I guess it's not sharp. And then, of course, properties of the optimal transport map, because we know it exists from the Gaussian random variable to the neural network output. Since if you take a source which is Gaussian, so it has density with respect to the back, there is an optimal transport map. But what are its properties with respect? What are its properties with respect to the sizes of the layers? And then what happens after training, which is what it's really relevant for applications, because initialization is very, yes, it's interesting, but it's very simple. It's not realistic. What people are interested in is what happens to the networks after you train, after you apply Bayes' theorem in, say, theoretically or even more interestingly, in practice with these algorithms. Okay, so here, I don't know, there were some references, but okay. But if you're interested in this subject, we have a short preprint on the archive and there are all the references there. Unfortunately, he didn't print the references. Okay, I think my time is over. Thanks for your attention. Thanks a lot for this. Thanks a lot for this very nice talk. Are there questions or remarks? Okay, I will still hear, yes, Ario, can you? There is a question of Matthew Coleman. Italian the second term or the side that you just had now on with the epsilon gamma plus something. What? This one, this guy here. So you have inf on epsilon, but I don't see why is the epsilon playing a role in the second term? Yeah, the point is k is a finite epsilon net. Okay, so this is so it becomes, yes, it becomes. So this is the so it becomes uh yes, it explodes as uh as epsilon goes to zero, right? So it's uh you need to balance between the two in principle. Yeah, but I'm not sure that this is the right way to prove functional bounds for shallow networks. They use a lot of heavy machinery like Stein-Smith or the so perhaps this is not a good way. I mean our approach is really elementary in this respect, but maybe there is not. Thanks, Dario, for the great talk. So, maybe a comment and a question about the convergence to functional spaces. It's the same thing, basically. So, do you think that your bound should actually depend on the number of layers, right? Because you sum over the number of layers. But it seems to me that if you know that all layers are white, then the output of the first ones should look. output of the first ones should look like a Gaussian, or not Gaussian, but well it should look like a Gaussian, but it should not distort space too much. And if it does not distort space too much, then the last layer just outputs a Gaussian, right? So in order to say something about the first layers and we know that the last layer converges in functional space, then somehow everything should also converge in functional space. Yes, it would be, yeah, but the Yeah, but the point is that if you take as a black box these results from shallow networks, I was not able. I mean, it's really crucial independence here. If you have a square root of, so you do my induction, right? As long as when you add a layer, not here, but in our result, when you add a layer, you get one over n, l, and then another square root. So essentially, things combine, you get this nice inequality. If you play To this nice inequality. If you play directly with the functional convergence in functional spaces, I'm not able to prove a similar inequality. What would be very, very nice. I'm just suggesting maybe all the work should be done by the last layer. Somehow you just want the first layers to not be stolen space. Yes, yes. Yes, I guess there is some work to do here. I'm not claiming that these functional space bounds are sharp. In fact, for me, the interesting question will be already if the bound for k inputs is sharp, because it's not clear to me. If you do simulations, you see immediately a Gaussian, even if you take a very small number of hidden neurons. So I was a bit surprised. We are running some more detailed simulations. More detailed simulations to see it, but I guess it's not sharp. So maybe it's sharp if you take Bernoulli weights, it's sharp if you take non-Gaussian weights. But if you start with Gaussian weights, maybe you are much, much closer. Maybe. Maybe another good question. So I guess that the constant C of capital X, it depends linearly on the size of the set? Okay, no, it's very implicit. No, no, no. Implicit. No, no, no. We know it's finite, but it's very, very implicit the way it depends on X. Yes. Otherwise, it would be nice to... I mean, if it were just a dinner dependence, it would be very nice to complete here. You get some explicit. Yes, it depends on the spectrum of the covalence methods of KL. So it depends already on the spectral gap of the. Gap of the covariance matrix when you pick the k inputs. So it's very, very implicit. Yeah. Thank you. But still, this is something we're able to prove as long as K is finite. Thanks. Okay. You're welcome. We also have a question in the chat. And chat, have you considered the optimization of the neural networks as a gradient flow on Wasserstein space? So if you go to the mean field limit, the one by May and the others I cited here, maybe then you can have a look at the letters in the preprint. Literature in the preprint, this paper here. Essentially, what they prove is that if you do instead a low, large number scaling and then you do the training, this becomes a flow in the persistent space, okay, because you don't have so you end up really with a flow in the persistent space. Now, here the next step for me would be really to understand better because you can read these papers, but they're mixing a lot. These papers, but they're mixing a lot of theory and applications, so it's a bit difficult to get to orientation here. But I guess something can be said using this theory here about neurotangencarin, where they say essentially that the training is linear in the first approximation. So Gaussians are still after training, after a linear training, you end up with being still Gaussian, which is already something striking because it's something that. Striking because it's something they checked a lot in practice, and apparently it works in many, many situations. Okay, maybe then we thank Bario again and we finish the talk of Michael. Yeah, thank you for that.