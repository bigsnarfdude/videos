It means it's some sense. Now I would like to avoid the word. To me, it's something like we want to give a characterization of an open system dynamics. And somehow we want to look for a characterization that is useful. And the characterization now takes the name Markova or non-Markova according to each idea. One big group points to looking for this characterization by including besides the evolution of the reduced system itself. Evolution of the reduced system itself, which is here this solid line, with intermediate measurements that you can make on the state of the system along with time. This viewpoint, which is encoded in very different ways and worries, so advocated and put forward by many people, I hear at least a few references. I just mentioned this tensor approach, the past-future independence approach. Future independence approach and different others somehow closer mimics or tries to get closer to the classical viewpoint where we have a clear idea what is a non-Markovian stochastic process. Along somehow the very same line, there are other approaches which try to point out whether a dynamics can be labeled as Markovian or not, looking at whether a trajectory viewpoint can be applied in describing the dynamics so that we can recover it as an average of a suitable trajectory. Cover it as an average of a suitable trajectories, possibly attaching to these trajectories the idea that some measurement on the system has been performed. The other rough group, which is the one I will somehow address a bit more in this talk, is an approach in which actually you just look at the evolution of the reduced system itself. That is to say, you assume that you can do tomography of the state at different times, but somehow you don't include additional measurements. You're not really going. Measurements, you're not really going to look for something like the quantum analog of joint correlation probabilities for events or measurements taking place at different points in time. And as you might know, and as you have heard in previous talks, these approaches somehow summarize into strategies. One is to look at the evolution map itself and ask whether this map has the property of being divisible or not. We had 30 days a nice review talk by Angel Rivas. So essentially, you know better than me about it. And the other approach is about featuring a label of Markov and non-Markovan by looking at how the distinguishability of states behave in time. And I will essentially focus on how to characterize distinguishability and to see, let me put it like this. If we have to take quite seriously this viewpoint, then how stable it is, how dependent on specific. Stable it is, how dependent on specific choice of object quantified and so is the approach. Therefore, I will start trying to put into evidence what are the requirements that we put on this distinguishability quantifier in order to connect the label that we associate to the dynamics or a possible notion of memory and information exchange between the system we are looking at and the other degrees of freedom that influence its dynamics. I assume to have. Dynamics. I assume to have some quantity which here I call S sub F. We will see in a minute. S comes from a typical letter for an entropy, it is a kind of general object, which allows for associate a positive number to do quantum states. And this number has to be bounded so that we can then normalize this quantity to take the upper value one. The other natural point is to ask that this quantifier, this way of comparing states, is. Of comparing states is a contraction under the action of any map which describes the interaction of our system with other degrees of freedom. That is, in the quantum language, these completely positive stress-processing maps. So, any dynamics in which you are possibly including adaptive freedom in general leads you to lose distinguishability. The third requirement, which is maybe the most, in a sense, the most subtle, but somehow to me, Most subtle, but somehow to me important, in order to allow to associate to this label an idea of memory. That we ask this quantifier to obey what I've called here triangle-like inequalities. That is to say, if this quantity is a distance, it trivially obeys a triangle inequalities. Since we are going to look how general this object, this way of comparing states can be, we allow for some more general feature, that is to say, we are able to upper. That is to say, we are able to upper bound the variation of this quantifier if we modify only the second or the first argument. And we allow this bound to be different for the different sides, which means that we allow to consider asymmetric quantities. And the bound could be somehow more complicated than the one that we have with the standard distances. It could be that we upper bound the quantity by a function of the quantity itself, but there are important constraints to this function. But there are important constraints to this function. This function has to be positive, to be positive, and to take the value zero only if the argument is zero. And we'll see in a second why this is important. So given this object, we exploit properties one and two, and we can introduce this very well notion or label of that we attach dynamics that was introduced in seminal work by Breuer and collaborators. We say that if for any arbitrary pair, For any arbitrary pair initial states, the behavior of distinguishability quantifier is monotonically decreasing in time. We say the dynamics is Markovian. And the other way around, if for at least a pair we observe revivals, we associate the label no Markovian to this dynamics. Now, what is now the role of the third property? Up to now, we have been able to provide a definition, now the motivation for this definition. Now, the motivation for this definition. Let me introduce, as was done in further evaluation of this first suggestion by Breuer, the notion of internal information. That is to say, we call internal information the distinguishability that we have if we look, if we can perform measurements on the system only. So, it's our capability to distinguish two different time evolutions if we can perform measurements on the system only. And we call external all the rest. That is to say, the information. Rest. That is to say, the information that we have, if we could access everything, all the degrees of freedom involved in the interaction minus the internal one. According to this definition, the sum of internal and external is a constant because we assume that we have included all relevant degrees of freedom, so the overall dynamics is unitary. And therefore, our previous definition of being non-Markovian amounts to the fact that the dynamics points to a revival in the internal information. In the internal information. That is to say, we find the time s such that at the later time t the local information is higher. Importantly, we can upper bound this variation by a sum of three contributions, which can be either positive or equal to zero. The fact that we upper bound just relies on the use of this third property, that is to say, the existence either of a triangular or triangle-like inequalities. And the meaning. And the meaning of this upper bound is the following: this revival, so this coming back of information, and therefore the fact that the dynamics that we are labeling as no Markovian is related to some backflow information, is due to the fact that some information has been stored at the reference time S, either in the environment which have been affected in a different way depending on the different initial state of the system, or in correlations that have been established. Somehow, these are the degrees of freedom. Somehow, these are the degrees of freedom in which the information is stored, which can later come back. So, somehow, I think this is kind of the natural way to put this viewpoint in place. Now, the question that I want to address is which are the relevant quantifier? Before doing that, let me just recap in a picture. This is an example taken by a specific model taking as quantified address distance, which is the one which was first introduced. So, the idea is I take. Used. So the idea is: I take a reference time S. That time S, it might be that I have information stored in different states of the environments. It would be here the red line, or in correlations, would be here the blue line. And the internal information with the lapsing time from S to greater times, so with the difference T minus S grows, can go through revivals. Can go through revivals only if initially at this time S, reference time S, some information what's to outside and it then comes back. And the height of this revival is bounded by the sum of these two quantities. Somehow they provide a condition and also actually lose quantitative information about the possible hate of these varieties. Now, let me come to how to evaluate this quantity. So say, is this concept robust with Is this concept robust with respect to the way in which we compare states? This is somehow the question, the first question. And we will see there is another one, which is, is this approach related to the visibility approach and is this relationship dependent on the quantifier that we use? Now, in order to put under one cap a large amount of distinguishability quantifier, let me exploit the classical notion of F-divergence. By F-divergence, Divergence. By F-divergence, people mean a class of quantities that allow to compare two probability distributions. And we are referring to the classical case, so this Pi and Qi are the entries of a certain probability distribution. And it turns out that given any convex function f, if one considers the average with respect to the second distribution of f evaluated at the ratio of the entries of the first vessels to the second vessel. The entries of the first vessels, the second distribution. This quantity is indeed positive, actually, also jointly convex. Importantly, it is non-increasing under the action of stochastic maps, which are, say, now we are moving from quantum to classical backward, but stochastic maps, as you know, are the analog of CPT maps. On top of this, this kind of object is invariant under permutations, which are somehow most important. Which are somehow most important in the classical case, but in the quantum case, they do correspond to a unitary evolution and invariant under the extension with fixed marginal, which in the quantum case is considered the tensor product. That is to say, this quantity somehow has a very natural feature in order to compare states also in a larger bipartite setting. We consider a very simple example of this quantity. The first is an entropy, the so-called classical electrode. The so-called classical relative entropy or pullback Liber divergence obtained for the function t, which is, say, the queen of the distinguishability quantifiers among states. However, for our purposes, as some drawbacks, because as it is well known, it's an unbounded quantity, hence the name divergence. It's asymmetric and it doesn't obey any triangle inequality. It's a relative entry, it's not a distance. While we can also recover a distance, in particular, the L1 distance. In particular, the L1 distance for another choice of function, which would be now one-half the modulo of one minus t. So I've put in one cap many different quantifiers where the relevant properties are all encoded in making a choice of convex function f. Now, as I said, this issue of being unbounded and asymmetric and not obeying a triangular inequality looks like an obstacle in using this object to. Using this object to quantify this NDL information backflow, but can it be that this idea somehow is this viewpoint is not stable to the chosen quantifier? So, how fundamental is the approach? Now, it turns out that it's somehow easy to cure this difficulty by considering a simple trick of skewing, which was used in other instances in the literature. So, we can consider a skewed version of our divergences. And roughly speaking, the idea is that we modify. The idea is that we modify our function f so that, as a matter of fact, we are not comparing directly the one with the other distribution, the pi versus the qi, but the pi versus a mixture of pi and qi. And this mixture depends on a parameter new, the mixing parameter, which has to lie between strictly between zero and one. Because at one end, you get something three. At the other end, you recover again something which is unbounded, not Unbounded and not useful for your purposes. This trick of skewing can be considered in general for an arbitrary function f, and indeed it can be used also in the L1 distance. And as we will see in a second, it's actually what somehow lies behind the Hellstrom distinguishability quantifier mentioned by Rivas the other day. Indeed, let me move to the quantum setting. So I have taken this viewpoint of diversity. I have taken this viewpoint of divergences in the classical framework, where they are better and more easily defined. I now move to the quantum framework. Consider the quantum counterpart of the fuse objects. And for the case of the L1 distance, as it is well known, the quantum counterpart is the trace norm. So the norm one of the difference of the operators. For the relative entropy, it is the quantum relative entropy, which again is an unbounded quantity, asymmetric, and fades for the triangle. Phase for the trigonometry. But now, somehow we bring over the trick we used before, so we consider skewing. For the case of transistance, we recover the norm of the Airstrom matrix that was already introduced the other day. For the case of entropy, we introduced an object which is less well-known, if you want, which was studied also recently in the mathematical literature, especially by Auden Eyre. Actually, we introduced this object under the name of telescopic relative entropy and then realized that it was related. Entropy, and then realized that it was related to this other issue of skewing. And as I said, it can visualize by saying like this: I take as a standard object the quantum relative entropy, but not comparing directly rho and sigma, but rho with a mixture of rho and sigma. And this is enough to cure all the, say, the unwanted properties I had before. On the one hand, I recover boundedness. So this quantity is after bounded by a number which is finite, provided by a Provided by a new state stays away from some of the value zero. Most importantly and less obviously, I recover, or to be more precise, obtain this triangle-like inequality as I mentioned before. That is to say, now this quantity is bounded and obeys an upper bound in the variation with respect to the second or the first argument. Given that it is still asymmetric, this upper bound is different if I vary on the left. Bound is different if I vary on the left, on the right side. However, I can also find a common upper bound for the two, of course, a loser common upper bound. I notice here that this upper bound is here given in terms of the trace distance among the two operators, which appear as different in this quantity. But as we will see, we will overcome this in a minute, coming to a closed inequality. Before doing that, let me force, let me. Let me force, sorry, enforce symmetry. Enforcing symmetry means that to our purposes, that is, we let two states evolve in time and then we want to compare what happens. It doesn't matter if we associate the label one to one or the other state. So in a sense, it's quite natural that exchanging them, nothing would change. Therefore, it's natural to symmetrize this quantity with respect to the exchange of states, so rows. To the exchange of states, so rho versus sigma, and the exchange of the mixing parameter, mu versus one minus mu. It turns out that you can do this in two ways, which somehow are quite different. The first way of symmetrizing leads you back to the Olevis quantity, adapted to the case in which an ensemble of only two states. If you have an ensemble of only two states, the Olego quantity is determined by the state itself, and the probability distribution is fixed by one number. And this quantity is And this quantity is then normalized to one by dividing it by the appropriate shenometry. Or we can recall what has been called in the literature as quantum screw divergence, which amounts somehow to the fact that we are renormalizing each contribution by itself. So essentially, it's a different way of symmetrizing the two objects, which leads to different quantities with slightly different behavior. Now that we have symmetrized, we have all the ingredients. That we have symmetrized, we have all the ingredients to come back to obtain our upper bound. The last trick that trick or better property that we need to use is the fact that we can close the previous estimate in terms of a variation of relative entropy, which is upper bounded by relative entropy itself, exploiting the so-called Pinsker inequality, which tells us that there is an upper bound. An upper bound on the trace distance, more specifically on the square of the trace distance, in terms of the relative entropy between two states. So include all these ingredients with the proper coefficients. And you can write down, therefore, also for an entropic distinguishability quantifier, in this case, a suitably smooth and symmetrized quantum relative entropy, the inequality which tells. Inequality, which tells us that also, looking at things in this perspective, a revival in distinguishability can be traced back and must be traced back to the fact that information has been stored either in correlation or in environment. This suggests somehow, at least in my viewpoint, that indeed this notion of connecting information backflow to the fact that something has been stored in external degrees of freedom. In external degrees of freedom, possible revival in the local distinguishability is a good motivation to call such a dynamics nomarkovian. Now, let me also notice that, as I said, there are different ways of symmetrizing, leading to slightly different quantities. It's quite natural to consider what happens when we take the symmetric mixture, so that mu goes to one-half. In that case, we recover coming from either side. Recover coming from either side a well-known object, which is the so-called Jensen-Shannon divergence, which essentially exactly builds on this idea of regularizing the lattice entropy. And as it was shown very recently, in particular, the square root of this quantity is also distance. So that for the square root of this entropic distinguishability quantifier, we also recover in the simplest form of these upper bounds. The physical motivation remaining the same. Now, just in a picture, I point to the fact that what have I done up to now, I've simply mentioned the fact that one can look for the possible revivals in a certain distinguishability quantifier. Here, given by this solid line, the different colors refer to the different possible quantities. In particular, orange would be the trace distance, and green would be the square root of the Jensen channel. While the black corresponds to Black, the black corresponds to these other possible entropy quantifiers you recover for different mixing. And what you see here is that, okay, indeed, these upper bounds somehow follow the lower bounds, and in particular, tightness, say, is better if you consider the distance, despite the fact that the physical interpretation remains the same. Moreover, let me stress that these actually these bounds are somehow never useful. Are somehow never usefully tied, not even for the trace distance, because the highest value that could be taken is one. And you see that typically these upper bounds are above one. So I would insist on the fact that the relevance of this bound is just to putting into evidence what is the physical meaning, not just say the convenient way to deal with this amount of normal Kovian. Now, let me explore the last, I think, about. Exploit the last, I think, about five minutes or so to say, okay, what we have learned from this, apart from, say, stability of this viewpoint. Can we say something more about connecting this viewpoint about distinguishability and information backlog to the idea of this visibility that was extensively presented the other day and various other talks. Now, as it was stressed, it is known that there is a one-to-one relationship between To one relationship between positive visibility and distinguishability, if one considered a special distinguishability quantifier, which was this skewing of the trace distance in the language I introduced today, and it was mentioned, according to the standard literature, as the norm of dielectromatics, as was first put into evidence in work by Kunshin, Skirivas, and Kosakost. Now, while it is obvious that if the dynamics is divisible, The dynamic is divisible, it is a monotonic contraction just because of this composition property and the fact that each of these divisibility quantifies a contraction under such kind of maps. The question remains, is there a one-to-one relationship in general between the notion of divisibility and the notion of contraption in the sense of, sorry, the notion of divisibility and the notion of contraction in the sense of distinguishability. Now it appears Now it appears that the answer is no. Indeed, while it can be proven that there are one-to-one for the case of the trace distance extended to the astronomics, and this was already mentioned the other day, we can point to an example. So we managed to look into to investigate the issue. We managed to find an example in which one can show that taking as distinguishability quantifier the Jensen-Shannon divergence, then you can point to a dynamics which is To a dynamics, which is pretty visible, but is not a monotonic contraction with respect to this combined quantifier. Now, let me just mention what are the steps that one has to consider and point into effect why this framework is somehow more complicated. Now, it is when we compare states using the trace norm, we can compare. Trace norm, we can compare both states or arbitrary operators because this distance is defined on the whole linear space of operators. While when we consider other distinguishing identifiers, like the elected entropy and its, say, the smooth version, the symmetrized version, and so on, the situation is more complicated because, indeed, this quantifier is just meant to compare proper states. That is to say, you cannot compare a positive and operator. Compare a positive operator with an operator which is no more positive. Therefore, in looking at what happens, you have first to identify what we have called and is often called in the literature positivity domain of the matter. So we take our matter, given the time evolution, we suppose we can split it in many pieces. We assume that one of the pieces is no more positive, so that PD visibility is no more there. But if it is not a positive map, it will send some states. Map, it will send some states to objects which are no more positive. Therefore, it's in general non-positive, and only a subset of the states belong to the positivity domain. This is a feature which depends on the map only. Then, on top of this, you choose your distinguishability quantifier and you define a non-contractivity domain. It is to say, the set of states in the positive domain, so those that are transformed to states by this non-positive. To states by this non-positive map, but such that they do not get closer, but do get far apart, according to this way of quantifying what it does mean closer and far apart. So with this, say, more subtle approach, one can then consider a non-positive maps, which is, for example, you can restrict in this case to consider qubits because we managed to find the counterexample already in this low-dimensional. Already in this low dimensionality. So we can visualize the states in the block sphere. And the idea is our non-positive map is a dilation in one direction and a contraction in the other. So in the direction which dilates, it sends points outside the sphere. And these are exactly the points in the light blue area here. The whole rest is the positivity domain. So the objects that are so slightly, if you want, dilated that they still remain within the sphere. However, With the sphere. However, we distinguish the dark blue and the green area. The dark blue is the area in which, if you pick up points here, you realize that the map is not positive because it dilates the distance. While if you pick states in the green area, you do not notice that the map is non-positive because they still get closer. Now, given this fact, we managed to find out an example in which we find out an example in which we follow in time dynamics in such dynamics defined in such a way that is not p-divisible so at certain point if you split it you have a transformation which is non-positive and the idea was look adjust the the the parameters in this dynamic so that somehow the the block sphere is sent to this area in which in which you are outside the non-contractivity domain. If you manage to do this in time, then somehow. To do this in time, then somehow you are done. And this was obtained relying on nice work by Filipov and co-workers that managed to classify for the class of phase covariant dynamics for a qubit. And phase covariant dynamics somehow just do the trick that we want. They act as contraction, or dilation, and translation. So they pick the block sphere and they bring it smaller and smaller on the top. And they manage. And they managed also to provide conditions on the choice of coefficients so that you can say when this map is p-divisible or not. Specifically, you can point to the fact that these blue and red lines are a suitable combination of these parameters appearing in this limbland-like expression. When either of these parameters goes negative, and this happens at this time, which we call time-normal Kauvianity, then the map. Called time normal Kauvianity, then the map is no more predivisible. And what happens? It happens that for these dynamics, as we expected, the Jensen-Shannon divergence leads to the normal community measure, which remains zero when you cross this point in time. While considering, for example, the astrometrics, and in this case, actually, even simply the trace distance, when this point in time is crossed, you see revivals, and therefore you point to. Revivals and therefore you point to the normal Kovalianity of the time evolution. So it is and done. Essentially, the statements are: okay, text here is this idea of connecting your characterization of maps, which you can do in a thousand ways, as we are learning, to some way of motivating your label with the notion of memory. And Samaro's suggestion was: okay, there is this idea of information backflow, which is quite robust with respect to the choice of quantify, even entropies. Quantify. Even entropies well do the jobs. Then you can also ask, but is then the connection to this idea of characterizing the label via a property of divisibility still a good one? And it looks like this connection is dependent on the choice of quantifier that you consider. So I hope I didn't exploit your patience too much. I just thank the people that have been more directly involved in this work, Minia Meger, who now moved to the Finn. Work, Minia Meger, who now moved to Infinium, Andreas Mirne's there if he's not frozen, and Federico Setimo, who just finished his master and should hopefully study PhD soon. And with this, thank you very much for your attention. Should be interested, these are the practices. Thanks. Hi, I'm Manjal Rivas. Hi, Rosano. I am curious about the possibility, I mean, this equivalence between, or this equivalence, better said, between the visibility approach and the information approach. Of course, it almost holds for the, or sure, it holds for the invertible dynamics in the case of the trace distance, the trace norm of the Helstern. Trace distance, the trace norm of the Helstone matrix, but it at Yugo Mendras, it fails for entropic quantities. But is there some entropic quantities that you have the hope that for some, of course not for all, but for some entropic quantities, you can get the perfect equivalence or if there is no clue about this? Okay, of course, I cannot exclude the hope completely, but my impression would be that there is no hope. That there is no hope because you see for the trace distance, I go towards skewing and I consider this weight, which is either one half and one half or a different. And there, just the weight one half plays a special role. There, you would lose this connection between divisibility and the distinguishability, and you recover it for all other bases of the parameter up to the invertibility issue that you were mentioning. Exactly. Well, for that case of entropy, I would say that. Of entropy, I would say that there is no playroom there because all these quantities are essentially characterized by this queuing parameter. So it doesn't work in the most symmetric scenario, which we considered, which was the Jensen Shannon. So apparently there is no playground, so to say, to recover this connection. What's the meaning of this? I do not know. You can take it in two ways. On the other hand, you could say, okay, this connection is not really. Okay, this connection is not really one-to-one, it's somehow more subtle. Or the other answer could be: look, this somehow this skewed trace distance plays kind of a special role. And indeed, there are some indications in this respect, even though they still have to be understood. But as I mentioned, it's for sure that it leads to the tightest upper bound. It was also used, as you know, to detect initial correlations, and also in that case, it looks. Correlations and also in that case, it looks like it's somehow the most sensitive quantity. So, in this respect, could be that it goes towards the suggestion: okay, look, this is somehow indeed the most relevant quantity. Okay, so it's uh okay, thank you. Thanks to you. We're running out of time, so maybe just one question from the online audience, if there are. Anyone from online that has a question? Um, has a question? Uh, may I ask a question, Anton Trushichkin? Uh, thank you for the interesting talk. Could you it's clear why the notion of skewness operation for the divergence for the relative entropy because of some problems with divergence and so on. But what's the profit of skewness operation for trace distance? Trace distance is already distance is bounded and so on. Could you please repeat? Yes, sorry, okay. So from the due. Yes, sorry. Okay, so from the viewpoint of the quantity itself, there is no gain. As you mentioned, this is only relevant for the relative entry because there you can improve with respect to these features. However, the skewing is relevant for the trace distance because it's just for the skewed trace distance or say or generalized trace distance or the norm of the astrometries that you recover the one-to-one relationship for invertible time evolution. Invertible time evolution between P-divisibility and monotonic contractility. So, this is something which was already there, so to say. Now, that was the work that I mentioned initially by Darius Krushzinski, Andreiko Sakovsky and Angel Rias. That's the issue. So, it's not about, say, obtaining a more suitable quantity, because, as you mentioned, it's already very well defined, but rather to Very well defined, but rather to connect this issue, which is somehow strictly related to our normal reality business. While skewing in general was introduced for the electric entropy, not to come to a triangle inequality as I was interested in, but rather because if you go through the, that was obtained in the classical setting for studying patterns in language. Then one wanted to compare different, so that the probability with which different probability with which different say sequence of letters appear. And the point was that getting in a result unbounded, so kind of an answer which is plus infinite was somehow unstable from the numeric viewpoint. And it could also be the fact that maybe you didn't have such a precision as to in one shot discriminate the two. So that was kind of a practical reason, if you want, initially. Okay, okay, thank you. Thanks, Ian. Thank you very much. Thank you very much. Thank you. Okay, thanks a lot. Okay. We don't have to see it all. 