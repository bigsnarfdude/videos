Okay, so are you good to go? Yes, let's go. So, yeah, awesome. Perfect. Thank you, folks. All right, so after this brief technical problem session, my name is Guilherme, and I'm currently working at Google in Brazil. But before that, I was a postdoc in Universidad Federal Municipales, along with Vinicius, who should be in the audience. At least I hope so. And this work was part of Sergio's PhD thesis, which was Vinicius. PhD thesis, which was Vinicio's student at the time. And well, his broad theme was vertex operators, and we were into reconfiguration at the time. So why not both? Let's put our hands together and have something fun. All right, so a quick chat about separators first. So an AB separator is what we're interested in. And a set of vertices is an AB separator if and only if it hits all AB paths of your graph. B paths of your graph. So we have this tiny, easy, simple graph here in the picture. And well, the blue vertices are an AB separator, but the red vertices are not. They are just an AC separator, separator. And as I said, we're only going to talk about AB separators. And we're going to use the notation gamma AB of G to denote the set of all AB separators, not just the minimal. AB separators, not just the minimal, and that's important. So, our problem for this entire talk is going to be the following: vertex separator reconfiguration. I'm going to give you a graph and two ST separators, A and B. So S and T are fixed and A and B are part of the input. And what we want to do is, well, can we find a reconfiguration sequence from A to B using some sort of rules or a set of constraints in our? Rules or a set of constraints in our configuration. That's the main question. And everything they're going to do is about the computational complexity of the problem. I think that's most of the topics, but I'll just make sure that we're on the same page. Okay, and we're going to consider three sets of rules that are broadly used in data configuration literature. The first of those is token slide. So we want to get the blue AB separator and arrive at the red AB separator. At the red AB separator. So we do a simple movement from the top blue vertex to the rightmost green vertex, and then from the bottom green vertex to the not so far right red vertex. That's token sliding. We're also going to consider token jump. So again, we have the blue and red sets. And we're going to do the same thing. So we're going to jump from the top blue to the rightmost green. Most green. But note that we don't need that edge to make this jump. We can just jump from whatever to whatever. And then the same jump from the top blue, the top green to the bottom green to the bottom red. And finally, we're going to consider token addition and removal. So we can add the rightmost green vertex, remove the topmost green vertex, add the mid-green vertex, and then finally. The mid-green vertex, and then finally remove it to get to the red separator. And note that we're going to have to impose some constraints because separators are well behaved, right? So if I get a small set that is a separator and pick a superset of it, I'm also going to have a separator. So we're going to have to give an upper bound on the maximum size of the intermediate sets that you're willing to tolerate. Okay, so we have three rules, but actually... We have three rules, but actually, we just need to consider two of them. Because, well, TAR and well, token jumping and token addition removal are the same thing, computationally speaking. So if we begin with a token jump instance, we can actually get a TAR instance, an equivalent TAR instance. And the better part is that, well, as you can guess, the length of the reconfiguration sequences are really close to each other, just a factor of two. Just a factor of two, and the same thing actually holds in the other direction. If I begin with a tar instance, I can get to a jumping instance, an equivalent jumping instance. And it's just a matter of just shortening the path. So instead of the add, remove, add, remove, add, remove, we can actually replace that with jumps. So every pair of sets can actually potentially give us a shorter sequence in the end. In the end. Okay, so all in all, it's just that tar and tj are equivalent, are computationally equivalent for vertex operator configuration. All right, now that we have taken all of the preliminaries out of the way, we can actually focus on the results. And we actually have a couple of those, not a huge compendium, but it's honest work, right? So we're going to begin by showing that the problem is hard. And it's hard on a very, very restricted class of bipotet graphs. Well, not that very restricted. Of bipartite graphs. Well, not that very restricted, but it's a nice class of bipartite graphs either way. And we're going to show in particular that it is P space hard for token sliding and just NP hard for Tor TJ. After that, we're done with the bad news because, well, it's just so super hard that we're not going to worry about that anymore. And then we're going to look at a couple of graph classes. So the first of those are the family of tame graphs. Are the family of tame graphs? A graph class is tame if it has a polynomial bound on the number of separators for each member of the class. Then we're going to look at graph classes that are not tame, in particular 3P1 diamond free graphs. And we're going to conclude our talk with series parallel graphs. And again, we're going to prove mostly results on the TAR TJ case, the TART. Case, the TAR2J rules, but are also going to take a stab at token sliding for the 3P1 diamond free routes. So 3P1 diamond free may seem like a weird case to study, but we're going to get there. There's a nice motivation along the way. All right, so if you folks remember in the morning session and in the Discord chat, I promised you that we would have peanuts. And here are the peanuts. Well, sort of. Before that, every of before that most of you should be familiar with a result of Lakshanov and Ember, which I think is in the audience, that says about the complexity of independent set reconfiguration on biprotect graphs. And it goes something like this. IAS reconfiguration, is LP complete under TAR TJ and P space complete under token slide? Yes, TAR TJ are also equivalent for IAS for configuration. Equivalent for IS reconfiguration. And well, you can probably guess what we're going to do right now is that we're going to reduce from IS reconfiguration to separated reconfiguration on bipartite graphs. Now we've got the P-nuts themselves. All right, so you begin with your favorite bipartite graph G. So this one was my favorite. Why? Because I picked some random numbers and that's what came out. And well, we're going to add two vertices to this graph. To this graph, S and T. So S is going to be universal to one part, and T is going to be universal to the other, but they are not adjacent to each other. And well, okay, so we have our favorite graph, we have the S and T. And now we get our favorite, our most favorite independent set. In this case, the blue one. And well, it's an independent set. But now we look at its complement. And well, it turns out that its complement. And well, it turns out that its complement in G is actually an ST separator in H. It's not very hard to see. And just for the sake of completeness, here's the graph without the red vertices. So red vertices are there, no longer there, and ta-da, no more edges between the parts. And it's not very hard to see, but I'm not going to spend a lot of time on that. So what this nice proof by picture showed us was that independent sets in G are in one-to-one correspondent. Are in one-to-one correspondence to the ST separators of H. And well, what we get out of that, along with the result of Luxionov and Moad, I hope it didn't bosh that name, sorry for that, is that if I give you two separators of your peanut graph, your peanut-like graph, H, you can actually determine there's actually a reconfiguration sequence of bounded size. Sequence of bounded size, of polynomial size on the size of the input, which is something, goes something like n to the fourth power. And this was the nice result, right? So there's a bounded sequence. It's a very short sequence, reconfiguration sequence between these two guys. And moreover, if I can transform A into B under Ts, then there exists a poly space. Exists a poly space algorithm that constructs this reconfiguration sequence. So, again, to RTJ, we're aiming for NP hardness, as we showed here. And for TS, we're actually aiming for P-space hardness. And well, in this tiny class of peanut-like graphs, we actually can show completeness due to the corollary I just pointed out, just discussed. And what, well, one of the nice And one of the nice consequences of this theorem is that Ts and Torr Tj cannot be equivalent for vertex operator reconfiguration unless NP equals P space, which none of us see as really probable, right? So yeah, we have a clean separation between these rules. They're not just the same thing. Okay, so this is the bad news, and it's bad news on the News and it's bad news on some subclass of biprotective graphs. Now we're going to tame some graphs. We're going to tame the lions. And well, again, just to refresh your memory, a graph class is tame if and only if it has a polynomial, its members have a polynomial number of minimal centimeters. And there's this nice, there's this big worded lemma over here that says, okay, Over here that says, okay, I'm going to give you an integer. I'm going to give you a graph and it's minimal separators. Then I'm going to construct an auxiliary graph, which is just actually the tar reconfiguration graph of the minimal separators. And we're going to say that, okay, we can solve tar for a graph G, its true of its ST separators A and B, and the constant K that we picked beforehand. Beforehand, and well, we get this instance, and it has a solution if and only if there are two midable separators, one inside of A and one inside of B, that are reachable. So they're connected in the same connected component of the reconfiguration graph. And yeah, it's just a quick note that the conditions that the sizes of the minimal separator, the size of the union of the minimum separator. Of the union of the minimum separators, it's pretty much equivalent to saying that, okay, I can transform A into B safely under the tar rules. So we're good. We can actually get there with a couple of operations. And yeah, having an edge between two elements of the reconfiguration graph says that we can actually reconfigure A into B under the Tor rule. And well, how that helps us? How that helps us? Well, we can actually generate the reconfiguration graph for this in this case, because there's an algorithm by Barriot and Friends that says that the set of minimal separators of G can be computed in the size of the set to a constant power. So it's polynomial on its size, so a polynomial of a polynomial. We're good to go. And putting these two things together, we just get that. Two things together, we just get that torque, the vertex separator reconfiguration, can be solved in polytime for tame graph classes. So that's going to include probably a bunch of nice classes, but very well behaved ones. Okay, so if I bound the number of separators, I get an easy problem. Let's look at the unbounded case. That's what's left for us. And in their 2019. And in their 2019 paper, Milanishtin Pivoch showed that all but three classes that forbid graphs on at most four vertices are tame. So if I forbid like C4 and K4, I get a tame class, a tame graph class. And what classes they show that are not tame? So they have a load of separators, actually. Actually. Well, the 3P1 diamond-free as TADAG, that's why we're chosen this class to work with. And two others. The first of those, and it's the clock K4C4 diamond-free class and the K3C4-free class. We don't have results for those, but as you can imagine, these are going to be topics of interest for future research. Okay, so how did we approach the problem for 3P1 diamond-free craft? P1 diamond-free graphs. Well, we proved that we characterize these graphs with the following theorem. So if our graph G is connected, it's not complete and has at least four vertices, and it's different from a C5, then the graph is 3P1 diamond-free, if and only if it has a diameter at most 3. And one of the following statements hold. So it's either So it's either the union of two clicks with one common vertex, or it's the disjoint union of two clicks, and the edges between the clicks actually form a matching. So we have proven this in our paper, but let's focus on the reconfiguration. The argument is just induction. It's pretty nice. You can think about it and have fun in your spare time. So let's see. So let's think about reconfiguration itself. So, case number one, G is the unit of two clicks, Q1 and Q2, and they actually have only one vertex in common. So, your graph looks something like this, and S and T have to be on different clicks because, well, otherwise we wouldn't be able to separate them. So, as you can imagine, we're going to need that cut for text to be in every solution. So, we have this thread separator. So we have this red separator, we have the blue separator, and then we have this blue plus red equals green scenario, right? So we have the red plus the green is going to be one, the blue plus the green is going to be another one. So we look at this and we quickly come to the conclusion that, well, under TARTJ, so token addition removal and jumping, it's pretty much always possible to reconfigure one into the other. To reconfigure one into the other, we just have to leave the cut vertex alone. We leave it there all the time, and then we just jump the tokens and move them around to our liking. And under TS, it's even easier. We just have to check if the number of tokens, there is the number of elements in the separator in both the source and target configurations, they are the same within the clicks. Because while we have Because well, we have to keep the central vertex stuck. We always have to keep a token on it. It always has to be part of the solution. So we can only move tokens within the clicks themselves. Okay, so case number one is done. Case number two, it's the disjoint unit of two clicks and the edges between them form a matching. All right, so again, red separator, blue. Red separator, blue separator, and then blue plus red equals green. So we have two separators here. Tor TJ is the same thing, but instead of keeping the cut vertex as an invariant across the entire reconfiguration operation, we have a bit more of leeway. We can actually just have to keep one endpoint of the matching edges in the solution. As in the solution, so we can just keep the green ones here in the solution and then move the rest around. For TS, we have to be a bit more careful because, for example, in this picture we have in the slides, we can't really slide tokens from the left to the right if we don't have this middle dashed edge, because, well, we can't remove S and T from the graph. So it's a small coordinate case that we have to be careful, but if we don't have to worry about But if we don't have to worry about it, we can just push, we can just pump tokens through this dashed edge, possible dashed edges, and keep the separators intact across the entire reconfiguration process. And there we go. We can actually solve vertex separator reconfiguration for 3p1 diameter graphs under the three rules that we explored. And it's more than that, right? We can are pretty much guaranteed. We can are pretty much guaranteed that we're going to be able to do it. So, the no instances, the cases that we cannot reconfigure, they're actually quite trivial. So, we can keep clear of those trivial cases and the rest is just, yes, we can do it. Okay, so this is the simplest graph class we worked with. Let's look at a slightly more interesting class. So, series parallel graphs. Well, a graph is series parallel. Well, a graph is series parallel if and only if it has tree-width two. Let's not discuss what tree-width is, but the graph is going to look something like this, right? The maximal series parallel graphs are actually the two trees. So, yeah, tree-width two. But it's not very useful for what we're going to do. We thought a bit about it. We couldn't come with a good usage for the tree-width condition, so we explored other options. And what Other options. And what options were those? Well, there's a 1965 paper by Duffin where the series parallel graphs are characterized in a very nice recursive way. And it goes something like this. So let G be a MODI graph. So if G is isomorphic to K2, then G is serious parallel. And then we have two operations to act upon G. So the first is, Upon g. So the first is if g is series parallel, the graph h obtained by subdividing an edge of g is also series parallel. So we pick an edge and then replace it with a middle vertex and an edge to the previous endpoints. And that's the series operation. And the parallel operation is: well, you pick an edge and you just double it. You add a copy of this edge to your graph. Your graph. And a very, very nice fact of this definition: oh, and that's it. You do that, you can actually get any series parallel graph you want. You just have to delete the parallel edges afterwards. So what's so nice about this definition? Well, it gives us a full binary tree where the edges of G are the leaves of this tree. So if we begin with this UV edge as our UV edge as our first case, so the base case, we can apply a parallel operation. And then we get edge F1 and edge F2. So we consider that we actually delete the edge. It makes our life a bit simpler in our proofs. All right, so that was the first operation. And then F1 is here, nice and beautiful. We're not going to touch it again. But now, okay, I want to do something else with my graph. It's too simple. Something else with my graph. It's too simple. I'm going to apply a series operation to E2. And then, well, this series operation is going to give us vertex A and two edges, F2, which again, we're not going to touch anymore, and E3. And again, we don't like E3 very much, so we apply another series operation to this edge, and then we get F3 and F4, and we're happy. And that's our final graph. And again, every series parallel graph has. Every series parallel graph has various different trees. You just pick your favorite, and we're going to roll with it. And just a bit of lingo here. We say that an edge, I chu, for example, is the support of A because A was born, so it appeared for the first time in our graph sequence, G1 up to G, after we applied the series operation. So every time. The series operation. So every time we apply series, we get a new vertex, and the edge that we applied this operation upon is the support of the vertex. Okay, so we have this nice tree, and it's going to help us before it helps us, we can actually understand a bit better what the separators look like. So I'm going to give, again, we're going to get two vertices, ST, non-adjacent. vertices st, non-adjacent, obviously. And we're going to define epsilon of st as the number of times any st edge was the target of a parallel operation, so a p operation. And it pretty much says that, okay, the size of the minimum ST separator is going to be either equals to epsilon plus one or epsilon plus two. So the number of times you had to duplicate those edges. To duplicate those edges plus one or two. The plus one case is a poor case where st are actually the original vertices. So if we look at epsilon uv, it's going to be epsilon plus one and not epsilon plus two. And well, if we never had any st edge, epsilon is zero. Okay, so this lemma, how are we going to prove it? Well, I'll Prove it, well, a ton and ton of case analysis and sub-case analysis, but it all boils down to looking at the relative position between the supports of SNT in the in the operation stream we described before. So we look at the least, the lowest common ancestor of the supports of SNT, and that's it. We're done. We can actually cover all cases that way. So if S comes before T, if S S comes before T, if S is part of the support of T, stuff like that. I'm not gonna get into this because it's too technical, it's not worth our time. But our proof is constructive and it describes the separators themselves. And then we use the standard technique, right? We go from a separator to the canonical separator that we're describing now, and from the canonical to the target. And that's what we're done. So, just as an example, So, just as an example, we have a sequential pair, which was one of the cases we analyzed in our proof. And okay, suppose that S and A, S A is the support of T. So we have S, we have A, and we apply an operation to one of these edges, and the graph then becomes something like this. All right. In this case, we actually have to be a bit careful because S and S T could be parallelized. T could be parallelized a bunch of times. But again, we're okay with that because we have a budget to cover that. We need these three red vertices. We always need these three red vertices. In this case, because they are adjacent to both S and T. But we're also going to need another one. Otherwise, we could just, I don't know, circle around these red vertices and get to S through a strange path. To S through a strange path. So we're actually just going to pick A to be the rest of the remainder of the separator. And we can do that for every sequential pair. It's fine. But in this case, the separator was a bit on the nose, right? It was pretty obvious what we're going to do. And we can reconfigure to it pretty easily, right? We just move something to A and then move the elements to these bottlenecks. These bottlenecks in between ST. But that's not always the case. And reconfiguration may take a couple of steps. For instance, if we want to transform the blue set into the red set, they're all S T separators, right? S is right in the bottom. T is pretty much in the middle of the graph. And we're going to separate these two. We can't just move X2 to A, for instance, because then we have. For instance, because then we have a TX2 X1 unnamed vertex Y1 BS path. So it's a very roundabout way to get your target. So we have to move things very, very slowly. So we move X2 to X1, and we're fine. We move Y2 to Y1, and again, we're fine. And we keep moving. Then we can move to A, and then move to B. And that's going to be our canonical separator in this case. Canonical separator in this case. There are a couple more cases to analyze, but not much more work to do. The idea is the same. We look at the lowest common ancestor and usually pick one of them to be in part of the canonical separator. And with that, we get that under token jumping and token addition and removal by By our previous result, is that vertex operator reconfiguration is always possible for series parallel graphs. And the reconfiguration sequence can be computed in polynomial time. Because every time you get at least one vertex closer in distance, in actual distance, to the canonical separator. And we can only do that at most a polynomial number of steps, polynomial number of times. And that's it. And that's it. I didn't get into anything very technical because, well, it's late for me. It's probably late for you guys as well. It's one of the final talks. So let's get to our final remarks and open problems. So the first thing we want to understand is, is there a class where vertex operator reconfiguration is polynomially solvable, but it's also the negative cases aren't trivial. They aren't trivial in the sense that, okay, we actually have to. Trivial in the sense that, okay, we actually have to think, we actually have to do some processing. Like at some point in the middle of our algorithm, we have to realize that the problem, the instance is a no instance. We can't just trivially say that this is clearly no and we're done. So that's question number one. We believe that the answer is yes, but we haven't spent much time looking for examples and I wouldn't be surprised if it is a And I wouldn't be surprised if it is a very simple class. Finally, we want to keep working on the classes that Milanic and Pivach showed us, showed in their paper, that are not tame. So they're very simple classes, right? They exclude some very basic graphs. But we would like to understand what happens in these classes. Can we solve either token sliding or token jumping for them? We're not sure. We're not sure, we don't have an idea. And I don't think we had much success proving any characterization results for these classes, which were pretty crucial for our 3P1 diamond-free result. This one wouldn't be here because, yeah, that's trivial. We showed that how it can be done. My bad. It was actually this one that I. It was actually this one that I want to show. This should be series parallel, actually. My bad. So, can, and our main question now is, can parameterize complexity help us, even if only for some graph classes? So, can we do something when parametrized by vertex cover? So, your graph, the complicated part of your graph is super simple, just the k vertices, and the rest is an independent set. Can we do something in this case, or do we have? Something in this case, or do we have to do some crazy thing where we push tokens to the independent set and then pull them back into the vertex cover? We don't know. And it's something that really draws my attention because I'm a premature complexity aficionado. So yeah. And yeah, we were thinking about series parallel graphs for quite a while to get a result. And we began with that definition, right? That it has tried to. That definition, right? That it has true-wheel two, and well, true-width is a super important parameter in parametrized complexity and structural graph theory. And well, is it possible? Who knows? Maybe, perhaps, we can generalize our search parallel result to graphs of arbitrary triggers. There is an interplay between true-width and separators, but we're not sure how, if we can apply this. We can apply this type of reasoning to vertex separator reconfiguration. And with that, I'll stop bothering you with separators and me saying vertex separator reconfiguration. And thank you very much for your time. And I'll be happy to take questions. So I just have a quick one. Have you looked at 3WIP3 and checked what goes wrong with 3-WIP3? We haven't because I don't... So our approach for series parallel was very dependent on the operation tree, right? And when we look at 3-rout tree, we tried some examples. And so first of all, we couldn't find And so, first of all, we couldn't find a nice factorization. And we looked at some examples, nothing jumped to our eyes. And yeah. I think there's a question from Zoom, Yoshio Okamoto. Yeah, can you hear me? Yes. So have you looked at the interval graphs or quota graphs, which are also tame? So tame graphs are boring, right? Because we know that they're easy. Because we know that they're easy, we can do it in polytime for token sliding, oh, for token sliding. No, for token sliding, we have not. But yeah, that's a good question. Yeah, I have to look at those. But yeah, interval graphs sound like a very promising. Yeah, I thought it would be easy to solve. Solve in both cases. Yeah, it should be doable. Yeah, thank you. Thank you. I also have a question. I cannot figure out how to raise my hand on Zoom, though. So yeah, go ahead. So yeah, so question, maybe more of an observation as well. So have you looked at what happens for minimum separators? Like, and I mean, the reason I ask is because this is a question that's been bugged. Ask is because this is a question that's been bugging me for a while as well. Like, do we know what happens when we're given two minimum separators between S and T? Is this, does it remain hard? Is it possibly polytime, NP hard, T space hard? Yeah, I would, if I had to bet money and I would bet like a dollar at most, I would say that it's polytimes. I would say that it's polytime solvable because well I can't really think of a complete like I can't think about a complicated structure where you can move from one minimal separator to another and with one operation right just replacing one vertex token addition removal might be a bit no token addition removal yeah token addition removal is trivial right because if you add a token on the jumping i would Under jumping, I would say. Yeah, under jumping. Um, that's a good question. That's a very good question. We have not looked into that, but it's it's natural, right? To take a shot. Yeah, I'd be happy to work on that. All right, thank you. Thank you. Thanks, folks. Thanks, folks. And the next presentation.