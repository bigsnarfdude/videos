I hope you all had a good lunch and have a little bit time rest. So I hope you're not as sleepy like me. So it's very interesting to be here and I feel honored to be invited to give this intro talk and be part of the group. So the story starts when I went to Weston as an external examiner for one of the PCCs. Stats department. That's where I met Pauline. That's where I met Pauline. And Pauline said, Oh, I'm organizing this kind of workshop. Are you interested in study stars? Nobody can refuse that offer. Especially my name, actually. My Chinese name, Xiu, means the sun. That's why I go by sunny. Because my brother's name is Galaxy. So my parents think that I should be the star in the galaxy. So he can protect me, I guess. So that's the story. I haven't done any research in this. Any research in this astronomy, this area, but I'm really interested to learn and to collaborate, hopefully, and with your guys. So, declaration. First thing I want to talk about is that by no means this review is comprehensive, okay? Even I probably just want to touch a little tiny bit of the big iceberg. And then the most graphs actually came from. the most graphs actually came from this textbook which I use for my static learning course and based on part of my stateful learning lecture notes and I also want to give credit to the presentations by Dr. Ib Chipman, Trevor Estee, and Nancy Reed, some of the famous statisticians. So will we talk about the big data, big data? So what exactly big data? So this is from Wikipedia and no, it's not very Not very restricted citations. So, from Wikipedia, big data is a broad term for data set so large or so complex that traditional data processing applications are inadequate. So, challenges include analysis, capture, data curation, search, sharing, storage, transfer, visualization, and information privacy. So, I still use this poster from The poster from IBM many years ago, I think a decade ago. So, IBM summarized the characteristics of big data into four ways. The first one is volume, especially this morning Colleen, I think, introduced the big data. That's a huge, more than I have ever thought of the big data duty. So and the other one is the velocity. So the data are generated so quickly, also need a process very quickly. Need a process very quickly. On what right is we mentioned, I think Pauline also mentioned that. So, not just like the traditional numerical values, not the data actually can be video, audio, social, medium, the social network data, right? And the veracity is that there are a lot of uncertainties inside data. So, in order to extract more accurate information from your data, so you have to handle uncertainty. There are no exists. So, today I basically just focus on two. Today, I basically just focus on two groups of aesthetic learning methods: supervised learning and unsupervised learning. I'm not going to talk about semi-supervised, reinforced learning, all of that. So, basically, these methods set up the foundation of statistical learning. They're very basic. They're not very advanced. I look at some of your titles. You already use a lot of deep learning methods, so I'm not going to touch that at all. I'll just give you a very, very basic. Gave you a very, very basic kind of review of this basic method. So, if you know all of this method, take it easy, have an easy ride with me. And then you're welcome to ask me questions during the lecture. So supervised learning, the difference between the two is that supervised learning has both X, which is a predictor, and Y, the response variable. And for unsupervised learning, they don't have the Y values, right? And for supervised learning, what we And for supervised learning, what we do is that we try to build a relationship between these predictors and the response variables. Then later on, we can use this relationship to predict response variable based on your predictors. So based on the type of variable of your response variable, based on the type variable of your response variable, if it's a continuous, then we call regression. Very general concept. And if it's a categorical, we call classification. We call classification. So, for any supervised learning method, I usually say it's more challenging than supervised learning because we don't have the actual piece of information that's a response variable. So, you try to identify the structures inside your X, the predictors. Sometimes, the most popular method is kind of like the clustering, right? You try to group objects into different groups. Or another one. Groups or another one is a dimension reduction. So you try to use the so project your data into a lower dimension. And that also, and later on, you actually can use supervised learning method on those dimension reduction data. So dimension reduction can serve as pre-data processing. Before I really go through those Before I really go through those statistical learning methods, I want to share the two famous quotes by the famous statisticians. First one is by George Box. Essentially, all models are wrong, but some are useful. So basically, we don't know the truth, but we try to figure out the truth. And we hope our models help us get closer to the truth. And the other one is the only way to find out what we are. The only way to find out what will happen when a complex system is disturbed is to disturb the system. So, not merely to observe it passively. So, encourage us to energetically passionate about what we're doing, to explore the data and uncover the patterns embedded inside. So, these are the kind of models that I'm going to talk about. The kind of models I'm going to talk about, K-nearest neighbors, generalized editing models, tree-based methods, including recursive processing, MPLAS ensemble method, including bagging, random forest posting, and support vector machine neural networks. And enterprise learning measures, I'm just going to quickly talk about PCA principal component analysis, k-means, and the hierarchical cluster analysis. Some of you say I use a functional PCAs, yeah, those are at the PCAs, yeah, those are advanced. I'm not going to talk about that, okay? The general framework is that we try to build a relationship between x and y. So we y equal a function of an input variable x, plus random arrows, right? So our job is to try to learn the relationship. So that's building the model. Most of the time we focus on this I function. Very, very few times we pay attention to the lowness in the statistical level. To the knownness in a stateful manner. So, K-neurous neighbor is one of the very intuitive classification methods. So, it's based on the assumption that we look like or behave like our neighbors, right? So, then our neighbors have a right to determine our response variables. So, here is a duty assumption. So, in the k nearest neighbors, k represents the number of neighbors or the neighbors. For neighbors or the neighborhood size. So K is also a tuning parameter to control the complexity of the model. So in this example, I hope it's clear. So the first one, K equals 1. So this dash, the blue line, is the true decision boundary. So separate the two classes. And this is a little bit more wiggly black lines is the decision boundary after we implement equal one. k equal 1. So you can see k equal 1 and this is a k equal 100. So two of them has a big difference, right? The black line. First one is very close to the truth but wiggly. So it means a high variance, low bias. The second one is very straight linear. It's really far away from the truth, right? This means a high bias, a low variance. So that's basically telling you that for most statistical learning method we're kind of trying to balance variance and the bias. To balance the variance in the bias. And the K is a control complexity of the model. So, Kn is being a very traditional machine learning method, is only good for small keys, especially less equal to 4. P represents number of predictors and for large points. And you may wonder why do I talk about this for large data? Because the KN is the very, very localized method. Very localized method actually give us some insights when we look at the big data. So, Kn is not good when the P isn't large because of the curves of dimensionality. We all know that when the dimension is really, really high, even your closest neighbors feel far away from it. And there's also many, many challenges related to the KNs. So you actually can modify that. For example, how do you measure the distance right between you and your neighbours? Between you and your neighbors. The traditional way is you can mean distance, but then when you have a different type of variables, how do you measure distance? Some use the Gau word distance. Also, for KN is once you decide a neighborhood, so all the neighbors actually contribute to the decision, final decision. Which is, to a certain extent, it doesn't make sense because the neighbor far issue should contribute less than the neighbor close by. So all of those things. By them. So, all of those things you can actually work on that. Next, I kind of move on to the generalized additive models. So, there is a strong assumption for this model is that when you vary in one variable, it doesn't depend on value of other variables. So, the first, this is a simple linear regression model, right? We write y as a linear combination. As a linear combination, so for y-intercept plus the coefficient of the predictor and plus the error term. And then for generalized linear model, we basically model each variables using this univariate function. These univariate functions can be very flexible. They can be continuous, they can be discrete, they can be piecewise constant, they can be polynomial, they can be smooth splines, all of them. Smooth splines, all of those models. So, this kind of model actually allows flexibility of a non-linearity, right? You use a non-linear function to model these variables. At the same time, you contain the linear structure of your model. And it's easier to interpret it. At the same time, also allow us to explore the local structure. So, another thing about this model is that it's easier to estimate. That it's easier to estimate the PV dependence of functions rather than the estimate of one big function. And the extension is that you can allow some low-order interactions within them. So next one, just quickly talk about the neural network. I didn't actually have the diagram, right? So usually when you learn the neural networks, you will have input layers, hidden layers, the output layers I didn't put there. Layers I didn't put there, but rather I put this kind of function. So function is the psi and phi are the kind of nonlinear function, but we look at a layer by layer, we can see that the inside layer is a linear combination of your input variables, right, x. And then you use the nonlinear function to transform that. Then you build a linear function, linear combination of those nonlinear transformations, and then you have outputs come out after you transform all those linear. After you transform all those linear combinations. So, simple way to say is that this neural network is actually a linear combination of a linear transformation, a linear combination of original variables. So, because the chips of those linear non-linear make the neural network become more powerful, and especially now the deep learnings. I know many of you use the deep learning method, so you probably know those. Nicole, so you probably know all of those. And decision trees. Some people think a decision tree is the most straightforward model, and it's very easy to use in practice, especially doctor. Doctor love this decision tree because it's battery. Yes or no, they go different branches. Decision tree is very simple. We basically try to recursively partition the explanatory variable space into the rectangulars. The rectangulars. So within each rectangular, we want to respond variable as homogeneous as possible. So this is just a very, very simple example. So on the left-hand side, this tree only has two split and three terminal nodes, and each terminal node corresponding to each area on the right-hand side. So this tree model is very straightforward, but at the same time, also, we have to consider which we are. Consider which variable we used to split and which one, right? So, tree model usually call the greedy algorithm. They only look at one step ahead. And so, whatever you get may not be the optimal solution for the result. However, on the other hand, the tree actually helps us to explore the local structure like the p does. So, decision tree definitely is easy to engage. Easy to interpret, flexible, detect the interactions of your predictors. At the same time, also perform model selection. Only the important variables will be selected to split your data. However, they are sensitive to noise. Not good at representing attitude models, which I mean, for example, if you use decision tree to figure y equals beta 0 plus beta 1 times x1 plus. times x1 plus and you will p times xp so that's a linear model so tree had a hard time to do that okay um and also as i mentioned they are allowed important variables dominate the each split so in order to overcome the limitations of the tree so we are feeding a sum of trees or our forest of trees so here just some denotations so this is So this is each individual trees. For example, you face 500 trees. And then for each tree, you're going to have one outcome, right? So at the end, the final result is aggregate all the predictions from those trees. So the two most popular ones is the random forest and the boosting. So random forest is actually really powerful is that Really powerful is that at each split random forest, we use a random set of predictors. We choose this method to try to select the variables that are not dominated, also decorrelate each individual trees, make them as different as possible. So they allow the trees to explore different parts of your data sets. On the other hand, each individual trees we look at them as weak classifiers. And so it turns out that after And so it turns out that after you have a set of weak classifiers, then you get aggregated results, you actually get better predictions. On the other hand, the boosting algorithm, in order to generate trees parallel using bootstrap samples, boosting algorithms actually fit trees sequentially. So basically, one tree is based on the previous tree, which is explore the space that previous tree. Explore the space that previous tree does not have a chance to explore. Okay, like life or information or residuals. So then you aggregate the result all the trees together. So the random forest and boosting actually become so powerful that they're among the state-of-art methods for supervised learning. So while you're gaining some prediction accuracy, on the other hand, you lose the interpretation. So you cannot. You cannot you get both. Sorry, and support vector machine is a two-class classification problem original, very simple idea. So basically you try to find a hyperplane that separates the two classes. And this hyperplane, so there are many hyperplanes between the two classes. So you choose the one that makes the margin as much, as big as possible. That's a column. As possible. That's called a maximum margin classifier. And however, sometimes you cannot have clear decision boundaries between the two classes if that happens. So we allow the little bit of misclassification and mistakes. And also we try to find out that decision boundary by projecting your data into the high-dimensional space. Hopefully, we can find the linear decision boundary between the two classes. Two classes. So, one thing is that we actually play the kernel trick. So, you probably, when you use the support vector machine, you try to use all kinds of linear kernels, polynomial kernels, or radio kernels, all of that, right? The trick is that try to avoid actually computing the high-dimensional mappings. And of course, one expense is that computation is very intensive. So, support. Um so support vector machine is one of the kernel masters. So then we move on. Do I have time hold on? About minutes? Oh yeah, go ahead. I'm on track. So next we kind of look at a little bit of unsupervised learning method. The first one is dimension reduction. So what we try to do is that we try to do dimension reduction, right? So opposite to support dimension. We try to project our X into the lower dimension. And without the Dimension and without the response variable involving right, so that's why it's called unsurprised learning. So, most popular ones are principal component analysis, which is we try to look for the first principle, second principle, which is the linear combination of the function, a vector of your input variables. And so, first, the principal components should have the maximum variance of your x. Of your X. So, PCA is a tool of visualization, also pre-data processing before you actually apply any supervised learning licensing tool. So, here is another interpretation for principal component analysis. So, if we just look at the two-dimensional data, so three-dimensional data, so the first two principal components. Two principal components actually kind of will try to find a hyperplane that closest to your observations. Visualization of that. On the right-hand side is that, so different colors represent different classes. Any questions? So clustering is also another very common and surprised learning method. It's a kind of a broad class method for The class of mysurfo discover subgroups in your data, right? And sometimes these groups are quite arbitrary. So I only talk about two very basic clustering analysis. First one's a hierarchical clustering analysis, the other one's a k-news. And there's others like a model-based clustering analysis as well. So for hierarchical cluster, the name actually tells us where exactly. Where it's architectural enough. So basically, you start for this one, we call bottom up. So you start from each individual observations, and then you calculate the distance between the between any pair of observations, and then you group the observations that close it to you, right? So then move up. And for hierarchical clustering analysis, we usually do not know the number of clusters until you see there's a kind of big gap in the dissimilarity measure. Big gap in the dissimilarity measurement. So I will cause here, and so you have two clear clusters. So there's many things involved in the cluster analysis. First is how do you measure the distance between any pair of observations? Another question is very natural: how do you actually measure the distance between two clusters? So that's what we call the linkage function. It's basically interface. It's basically intercluster dissimilarity. So, based on the different linkage functions like complete single average centroid, then your cluster actually forms quite differently. For example, for complete average centroid, they usually form very spherical clusters. But for the single linkage, you form the like string clusters. And k-means is also another very popular cluster. Another very popular cluster analysis method. So, here, just show you a basic idea that when you have exactly the same data, you start from different initial values, and you can have very different results. Just remember the color. So, k equals 3, the color coding actually doesn't matter, okay? Does not mean that this here is orange and they turn to different colors, and they tell you they're two different clusters. They're still the same. So, basically, They're still the same. So basically, when you do a k-means clustering analysis, for example, here we have three. So you basically randomly select three observations within your x-space. And you calculate the distance between those three survey centers. Then you calculate the distance between the observation to each center. Then you assign your observation to the closest center. Then you update the center, then you calculate distance again. So repeat the doing. Distance again, so repeated doing this process until your cluster center never changed. So with different initial values, we're going to choose different centers so that the clusters are actually quite different before you can see the colors. Sometimes the like this here, so there's green parts here, but sometimes that is part of the orange cluster. So this tells you that you don't get an optimal solution. get optimal solutions. What do you do user is you choose a you use the multiple initial values to choose the ones give you the smallest weaken cluster variations. And there's a lot of variations for the k-means as well. So k-model by secting k-means, x means k-means and the g-means. So there's many different extensions of the k-means. And while I'm just briefly talk about each individual these enterprise learning measures, that does not mean that you cannot combine them. There's many, many other things involved in statistical learning, right? For example, when you have a large data, sometimes we say our data is that if we use the tabular data, sometimes the P is much, much greater than N. Then in that situation, you don't get a unique solution, no matter where you fit. Will you fit? Okay, so we need to do the variable selection first. So there are many, many different ways to do variable selection. Traditional way linear regression, you use a stepwise, so forward or backward variable selection. And also you can use regular realization, so like a ridge muscle elastic night. And here I want to actually emphasize a little bit. So the regular realization, like a ridge and lussel, kind of introduce sparsity into the Sparsity into the predictor states, right? Sorry, the variables, the parameter space. But then which measure that you bring the sparsity into, so the support vector machine actually brings the sparsity into the variable space. So another thing is that how do you validate your model? That's another biggest topic since digital learning. Institute for learning. So you're not just using your data and to fit model and use that data to evaluate the model. Thank you. So you need to think about how do you kind of accurately estimate the validation of your model. So you need to, usually we do the training, test split, sometimes we do training validation test split. And then the common ones are K for the cross-validation. K4 cross-validation. So these are some of the reference books I used. And questions? Thank you. Thanks. So we have four minutes or so for questions. Yes, everyone understands. So simple, everyone. So simple. Everyone. Okay. Oh, you're first, I think. Okay. Thank you for the talk. That was a great overview. I am interested about what you were saying about random force. You ended the part saying that it's hard wi with respect to interpretability. Okay. And I had a question o about that. I haven't worked with that in course that much, but Work with random forests that much, but I thought that when the four splits, you kind of have an idea of like how the model is kind of so like how does the interpretability come in? So the reason I said hard to interpret it is because when we if we fit a single tree, so we need to consider the