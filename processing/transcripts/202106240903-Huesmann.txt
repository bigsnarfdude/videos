Okay, so I'm very happy to introduce Martin Hussman from the University of Minster, and he will talk about fluctuations in the optimal matching problem. Can Martin please start? Okay, thanks a lot. Thanks for setting up this conference. And it's nice to see this group of different people and applications here. Publications here. And thanks for giving me the possibility to present some of this recent work here. And so, what I'm going to talk about is a recent paper jointly with Michael Goodman, and it's about a very special problem, namely the optimal matching problem. And what we're looking at there is some kind of fluctuation result in a certain regime, which I call the mesoscopic regime. But I will start gently and introduce the or recall the optimal matching problem, which already featured in this conference. So what is this? This is just a very special optimal transfer problem. Namely, I'm looking at a transfer problem between a given measure mu and an empirical measure constructed from mu. So we take n IID copies XI, which are all distributed according to i which are all distributed according to according to mu we take the empirical measure and then we look at the optimal transport problem between mu n and mu with respect to the cost function piece power of the distance so this is a very say very easy problem and let's make it even even more more concrete and in for this talk we choose the kind of the model case in this problem where we concentrate on on the log On the law being just a uniform measure on the unit cube. So, this is a very classical optimal matching problem. And if you like pictures, so this is an example of the empirical measure. So, here you've just chosen a couple of points. And on the right-hand side, there's a picture of the optimal coupling. So, what you've seen, for example, in Donison's talk, I think on Tuesday, is that Is that in this case it's the optimal coupling for p equals to two? And what you do, you divide your space into polyhedral regions, and you just attach to each or you transport one of these regions to one of these points. And so, this you can produce very nice pictures and ask many questions about these pictures, but I will not do that today. But this problem is very classical and Is very classical, and I'd say it's an old problem. And there's an even though it's quite simple, maybe because it's so easy to sustain, there's a very wide interest in various communities. So for example, in the computer science community in the 80s, in a group around AITAI or in probability, Tamagawai has been working on this problem since the early 90s, and I think his last paper. 90s, and I think his last paper on it were maybe one or two years ago. A group around Ambrosio started to work on this in, I think, in 2016. So, and essentially based on an ansatz which was proposed by a physics group around Karachuga and Parisi. And this we've all just seen in Jonathan's talk, there are applications and statistics and many, many more applications. And I have no chance of, so this is just a So, this is just a very short and small list. And okay, so various people contributed to this problem. And also, I mean, this is just a very particular case of a whole family of problems. So you can easily change your problem and taking independent copies of this mu n. So this would be the bipartite problem, the bipartite matching problem. Or you can look at variance where you look at occupation measures, or you want to replace. You replace delta xi by Markov chains and ask of convergence to the equilibrium measure, or you can ask for the problem in atrophic optimal transport or in causal optimal transport. And there's a very big very big literature here. And so, what I'm interested in is, I want to understand this problem in kind of the most detailed way that is possible. And of course, this will not be. That is possible, and of course, this will not be achieved today. But I mean, as so, I want to understand it in any possible way that you can think of. So, the first thing that one immediately sees, and this is something that you see maybe in your first probability classes, that essentially as a consequence of the law of large numbers, of course, converges to mu almost surely. And kind of the most basic question that you can ask now is: if you measure this in the Wassachdian distance, how fast does it converge? How fast does it converge? And now, what you can guess if you look at this problem here, then if you throw n points into this unit cube, then kind of the typical distance between points would be something of order n to the power minus 1 over d. So, if you have to transport and you pay for transport distance to the power p, then what you should expect is something like n to the minus p over d as a cost. Over D as a cost. And in fact, so this is what you should expect to see. And this is true if you look in dimensions which are bigger than three. But if you look in dimensions which are smaller, then you see a different effect. And so in dimension two, what you see is that this cost behaves like log n over n. So essentially is the way that you expect, but with a log n correction. And in dimension one, you're a bit And in dimension one, you're a bit further off, so it's not one nn to the minus p, but there's a p over two. And this twiggle here means there's a, so you know that this cost is a constant above and below. So this is just the rate of this cost. So we can estimate the left-hand side from above and below by a constant with this rate. And now what we see here is that in dimension two, something interesting happens. And so interesting happens. And so dimension two is in this problem somewhat a critical dimension. And there's a very well, I like a very easy heuristic explanation on what happens in dimension two and I like to show you so to get a feeling of the challenges that arise in this problem. So let's just scale this problem. So we have end points and just scale it so that the typical point distance becomes of order, becomes order one. So we scale it to a cube of side length and to the point. Of side lengths n to the power one over d. And if you then look at, say, a cube of side length A inside this cube, then you can ask, okay, what are the number of points inside this cube? So would, well, because these points are id and uniform, you should expect a to the power d number of points, but then there are fluctuations. And I mean, the distribution there is essentially a Poisson distribution if n is very big. So the fluctuations are of order a to the d over 2. To the d over 2. And now, if you want to have a transport cost which really transports only like behaves like this power here, like n to the power p over d, then what you need to be able to do is you need to be able to compensate these fluctuations inside this cube with the mass which is in the neighborhood of this cube. Because you don't want to transport stuff too far out. Transport stuff too far out. So, but in a neighborhood around this cube, the Lebesgue mass in the neighborhood around this cube is proportional to the surface area of this cube, and the surface area is proportional to a to the d minus 1. So what you need to be able to do to get this nice behavior here is you need that a to the d minus 1 is bigger or equal than a to the d over 2 to be able to compensate the fluctuations. But now you see that if d is bigger or equal than 3, then this is. Bigger equal than three, then this is perfectly satisfied. If d is equal to one, you're completely lost, and d equals to two is exactly borderline. And you also see that this is some phenomenon which happens essentially on every scale. So we didn't use any property of A. If something is borderline happens on every scale, then the logarithm often pops up. So this is a very heuristic explanation of why there is a logarithm appearing here. Algorithm purity. Okay, so what we have here now, we have the kind of macroscopic behavior of our coupling in the sense that, okay, we know that the cost of this transport converges to zero, and we even know how fast it converges to zero. Now, what various people are interested in is to see now, can we get an even, I mean, in this situation, the measures are very explicit, can we get a finite statements for this problem? Can we? For this problem. Can we, for example, we know in this case that the left-hand side can be bounded from above and below by a constant times these quantities. So we can rescale the left-hand side by these quantities and ask, okay, if we now send n to infinity, does it actually converge? So we only know that if we rescale it by these quantities on the left-hand side, then it will be bounded. But the question is now, does it converge or not? And this is already quite a difficult question. So it's a kind of the first maybe the So, it's a kind of the first, maybe the first question which comes to your mind, and this is already not so easy to handle. Another question which is quite interesting is if I zoom into this picture, so if I do something like this, so can I get some kind of behavior of this optimal coupling? Can I understand this behavior not only on a macroscopic level, but maybe on a mesoscopic level, so on a level which is maybe something in between the macroscopic and microscopic level. Can you understand? Can you understand the coupling there on a more local level and not on a global average level, like in this cost? Or even maybe even more difficult question would be, is there something like a thermodynamic limit? So can I zoom into this picture so that the inter-point distance is of order one? And does this object now, if I just increase it, if I zoom in and increase the number of points and keep the inter-point distance of order one, does this object converge? Order one, does this object converge or does it not converge? Can I say something about the coupling? How does it behave? So these are the kind of the questions that I would like to answer. And now they are much too complicated so that I can answer all of them in this talk. But let's just start and see what has been known so far. And let's look at the easiest case, and this is a one-dimensional case, and let's look at the cost with P bigger than one. And we know that. Than one, and we know that then the optimal couplings are super explicit. So you just have a rearrangement of mass. And so now what happens here is now if you throw IID points into a unit cube, which is then just a unit interval, now you can just rename the points and order them according to their size. So you just look at this order tuple, and then you will subdivide your interval into equal pieces. interval into equal pieces and then the first point will be mapped to the first interval, the second to the second and the third to the third. So that's the optimal coupling. But now the advantage of this is that this optimal coupling is super explicit. It's not in an abstract way explicit that it's a great end of a convex function, which is also true, but it's explicit in the sense that you can really work with it. So now, so for example, if these are, if the x i's are iid uniformly distributed, then these order Distributed, then this order tuple, so each of these elements here has a better distribution. So, whatever that is, but this is a very explicit distribution, and you can calculate things. And in particular, so you can look at these different questions here. And if you look at the question of the rescale cost, if you rescale the expected cost, then you can calculate the constant. So, you can show the constant exists in a setting which. In a setting which is much more general than just this uniform setting. And you can derive very explicit results. And there's a very nice book by Brockoff and Lidou on this one-dimensional case. Now, this is a super nice book. And okay, so now if this is explicit, you can, and one can understand this first question. The question is, can one understand also the second question of the mesoscopic behavior? So if we slightly scale this up and slightly zoom into this. And slightly zoom into this. And there's a very nice result by Del Barrio, Gini, and Utzet from 2005. And what they do is they take the ice point, they subtract essentially the I over N, so essentially the average position where X3 is mapped to. And then they scale it. So they look at the distribution of this. Of this random variable, and the variance behaves like 1 over n or order 1 over n. So, if you in the CLT scaling, you should normalize it with the square root of n. So, 1 over the square root of n, and you divided by this, so then we'll be square root of n. And in fact, if you then not only look at this random variable here, but it's a whole process which I indicate here, we see i, then this whole process converges to a Brownian bridge. So, here you can explicitly. So, here you can explicitly calculate this fluctuation. And what is this fluctuation? This is a fluctuation of the displacement. So, this xi is essentially mapped to i over n. Okay, here maybe the whole interval, but you can very nicely can just think of this as being replaced by a point mass at i over n. And if you then look at the displacement of this object, then this is exactly xi minus i over n. You rescale it in a CAT scaling, and you look at the Fourier process, and then this converges to a Brownian bridge. Converges to a Brownian bridge. Okay, so this is kind of a very nice result on this mesoscopic behavior. And this is also what I mean with the mesoscopic behavior. You scale it up so that the point is not of order, is something between order one over n and order one. And now the question of the thermodynamic limit. Now, this, what one would need to do is we need to scale it up so that the distance between the points is of order one. So one would need to look at quantum. So, one would need to look at quantities which look like this. But if we already know that the square root of n of this quantity converges to a Gaussian, and then if you scale it up by another square root of n, then it's, well, this thing is bound to diverge. So this is very difficult. So it's just does not exist. But okay, I wrote a dot dot dot because you can do something. I mean, you can do some kind of renormalization. Some kind of renormalization to get some kind of limit if you want to. Okay, so this is a d equals to one, so they're very nice and explicit results, but all of them very much rely on this very explicit optimal coupling that you have here. Now, the question is, can we do something similar in higher dimensions? So, let me show you what has been done in dimensions bigger than or equal to two. And let us first start in the dimension two, well, strictly bigger than two, which is Well, strictly bigger than two, which is above the critical dimension. And now there's various people have looked at the question of this rescale cost. So if you rescale this cost, there have been in 2013, there have been two groups, Bart and Bodenauff and Derek Schotzon-Schottsted, who showed that in the regime where 2p less than D is less than D, this object actually, the limit exists. And only last year, And only last year it was Michael and Andario were able to show that this in fact this condition is not necessary. And there has not been so much done on the mesoscopic coverings, at least not that I'm aware of. For the thermodynamic limit, this has not been proved, but there's a very natural candidate, which is optimal coupling between the Lebesgue and Poisson, which I constructed in my PhD together with Theo Strom. In my PhD together with Theo Strom. But so far, it has not been well, it is not so easy to show that these optimal matchings actually converge in the thermodynamic limit to this object. So this is open. And now for the critical dimension, even less is known. The only thing which is known is for p equals to 2, if you rescale it with n log n, then this converges. Then this converges, and you have an explicit constant with one over four pi. This is a very beautiful result by Ambrosio, Stra, and Trivizan. And now I would like to single out this result and the result by Michael and Dario, because up to that stage, if you forget these results, essentially all of the other results use very clever explicit couplings, very clever atoch constructions. Constructions, very sophisticated ideas to come up with the estimates. Or in this case, here, maybe also some subeditivity enters a game, but which is just valid in this regime here. Now, what these two groups or these two papers really use is a completely new idea, which was brought into this, well, which essentially came up, Came up, came as part of the work of a group of physicists around Caracolo in Parisi. And this is a beautiful idea. And I'd like to show you this idea. So this is, and the idea is quite simple and very appealing. Okay, so the idea is now, if you have the optimal coupling between your empirical measure and your measure μ, we know by Brignier's theorem that we can. By Brignier's theorem, that we can write it as a gradient of a convex function. So it lives on the graph of the gradient of a convex function. Okay, so that's known. Now, if you forget that mu n is a discrete measure, you can do a change of variables and you obtain this Montchapier equation. And now if you remember that mu n converges to mu and say mu is a Lebesgue measure in our case, then we expect that We expect that is very close to one. So, what we should expect is that the transport map is essentially just a very small perturbation of the identity. Okay, so we make the answers that the gradient of psi is given as x plus the gradient of phi. And then the Hessian can be easily calculated. And then, because it's just a very small perturbation, it makes very much sense to just take a first-order Taylor expansion of the. Just take a first-order Taylor expansion of the determinant, which is just given by one of the trace. And now the trace of the Hessian is just the Laplace operator. So what you get now from this equation with the first order Taylor expansion is a new equation, which is just mu n equals one plus Laplace phi equals to one. But now you recall that mu n was essentially one, so you can forget this factor in front of the Laplacian of phi, and what you get. Laplacian of phi, and what you get is a beautiful equation. I mean, just a Poisson equation with right-hand side being one minus mu n. So just one minus mu n is just the difference of the two measures. And now what is what is quite appealing here is that, I mean, from, okay, this status is mathematically rigorous, but everything that you do from here on is maybe, well, one. One would not be happy if your students are doing this. But in the end, you come up with an equation which is completely fine. And if you forget how you derive this equation, you just end up here. And if you then recall what grad phi is, now grad phi is just a difference of the map with x, which is just a displacement of your coupling. And then the cost is just given with the Dirichlet energy of phi. And the idea. Phi. And the idea is that the series energy should be a very good approximation for at least for the cost. And this grad phi should also be, well, at least a good idea on how the displacement should look like. And now Caraculo and others, they use this ansatz to make several very explicit predictions about this problem. And yes. Okay, so phi phi is the convex potential. Am I correct? Or psi, yes, phi is a convex potential. Phi is a convex potential. Okay, all right, thanks. Yes. So, I mean, essentially, what you do here, you take grab psi is just really just a Brunium map, and then you just no, no, grab psi is a Brunier map, yeah. So, and then you just say that grab psi is a Brunium map, and then you just assume because mu n is so close to one. So close to one that the transport map should just be given as an identity plus a small perturbation. So, this is really the Bringing map here. All right, thank you. So, this is, I mean, this is just an ansat, but okay. And then the idea is that, okay, this undat should be quite good, good. And based on this undat, they make several very explicit predictions. Okay, and now. Okay, and now Ambrosio liked this idea very much. And together with Frederico and Dario, they worked very hard in a very beautiful paper. And they could show that at least on a macroscopic level, this is true. So this is a very good approximation. So it's true in the sense that it's a very good approximation. And what they do is they modify this measure with the heat kernel and then they Heat kernel, and then they show that, okay, then you can do all this program. And then in the end, they show that the error that they do is negligible for n tending to infinity. So in particular, and in the case in the second paper, they show, okay, if you take this here as an optimum map, so it's kind of the, yeah, so if you essentially take this as a push forward of the measure μ, then this is very close to μn as well. Close to mu n as well. So, this might also be another. So, at least, I mean, if you think of this as a modification with a heat kernel, this is very should be very close to the entropic semi-discrete problem. And here in these papers, they have very explicit estimates, which might be interesting for people who are interested in the suboptimal behavior. So, like in Jonathan's talk two days ago. Okay, and now in the second control. Okay, and now in the second contribution here, together with Michael and Felix, we show that this can be made explicit in a quantitative way, and it's true from macro down to micro scale. And I'll show you in maybe five minutes or 10 minutes in a very explicit result in this direction. Okay, so now what have we done? So essentially, is the main, well, this is kind of one of the main ingredients for us. And what we should Ingredients for us. And what we show is that for p equals to 2, so not for general p, just for p equals to 2, and for d equals to 2 and 3, on all mesoscopic scales, the displacement of the optimal matching converges to something that we call curve-free GFF. And if you don't have to, I mean, this is just a fancy name for some Gaussian field. So this displacement converges to some Gaussian field. Okay, so this is a statement. So, this is a statement that I would like you to just remember. So, this one and the slide before. So, these are maybe the nicest and easiest to remember things. And now, let me go into a bit of detail. And for this, I need to introduce a bit more notation. Okay, so recall we have IID points, uniform, and now I want to look at the mesoscopic scale. So, I need to introduce some parameters. So, I look at the scale diversion, so I look at the toros of side-length L, and then I look at my empiric. And then I look at my empirical measure and I want to play with intensity. So I have an additional parameter which is r, which is intensity of the number of points. And I normalize with one over R D such that mu of R L so this measure mu R L has exactly mass L to the D on my torus Q L so that I can couple this one with the Lebesgue measure on the torus and the optimal coupling will be called pi R L. Okay, so now I'm interested in the In the displacement. So I need to kind of catch this displacement, and I expect this displacement to convert to some quotient field. And in higher dimensions, this quotient field is quite irregular. So I think of this as a distribution. And now what I do, I do exactly the same thing as in the one-dimensional case. I take the displacement here, and then I have to scale it appropriately to the intensity. Properly to the density, and this is really what was before the square root of n is now just the square root of r to the d. So, this is really a CLT scanning. And now this is, if you want to have a bit more explicit what the ZRL is, now recall that this is essentially how our optimal coupling looks like. And then to each point Xi, the coupling associates one of these cells. So, for example, this point here gets this orange cell. Gets this orange cell and let's call this orange cell Ai and then we just average the displacement here over the cell and give this as a weight to our point mass in Xi. And this is precisely our distribution ZRL. And everything in a CLT scaling. Okay, so the important thing is the muRL is the intensity. So there's a R is the intensity, L is the size of my system. Size of my system and pi R L is the optimal coupling. ZRL is kind of an average version of the displacement. Okay, so let's the final ingredient is my Gaussian field. And for this, I just take W to be the white noise. And formally, and I don't want to be rigorous here, I think of my Gaussian field as a gradient of psi, and psi solves this equation here. Okay, so let's collect these ingredients. So let's collect these ingredients. So μRL is my empirical measure, πrl is the optimal coupling. Here's my Gaussian field, and this is a distribution. So the average displacement. And now the statement is that in dimension three, for any sequences R and L which diverge to infinity, this displacement converges weakly to this Gaussian field in some sobolev space. And now And now I claim that on all, so what I would like to convince you now is that this is a statement, which essentially is a statement that this displacement for the optimatching is, so this is on a mesoscopic level of the optimum matching. Okay, what do I mean here? Now, we've seen before, macro scale just means that we have here side length L fixed and say L equals to one, and we just increase the intensity, so we just throw more and more points inside. So, this would be kind of a macro picture. Macro picture. The opposite side would be the micro scale, so we keep the interpoint distance to be of order one, and then we throw more and more points, and then we just zoom in so that the interpoint distance is of order one. And now the meso scale is just a scale which is in between. We just send L and R jointly to infinity. But they do not depend on, it doesn't matter in which way or how fast they tend to infinity. Whatever you do, ZRL always converges to this quotient. ZRL always converges to this quotient field. Okay, so this is a statement for d equals to three. And now for d equals to two, things become much more complicated because there's this logarithmic behavior appearing there. So in dimension two, so this is our data. And before we had this graph psi, and now in dimension two, if you know, if you've seen something ongoing. If you've seen something on Gaussian fields or Gaussian free field before, then you now might know that in dimension two, Gaussian free field does not exist. But you can do something here. You can repair this by just looking at a renormalized version of it. So you essentially what you look at at the differences. So if you think of the discrete Gaussian free field, then you can pin it at zero. So which just means that you look at the differences of the Gaussian free field. And this is an object which exists on the full. Which exists on the full space. And this is now this object that I mean here. So you can think of this as: okay, you need to do some kind of renormalization. But if you have to do something on the level of the Gaussian field, then you also need to do something on the level of the average displacement. And for this, I would need to introduce a bit more of notation. So now let WRA be an approximation of white noise, which is just given as our empirical measure minus Lebesgue measure. Measure minus Lebesgue measure in the CAT scaling. And then we look at URL to be the QI periodic solution to our Poisson equation with the right-hand side being this approximation of white noise and of average zero. And then we take grant U1 to be the gradient of URL, which we average respect to a fixed smooth cutoff function. Martin, by the way, I have a quick question. What is QL? What is QL? A QL is a torous of side lengths. So QL is a toros of side length L. Okay, so we need to do some kind of renormalization here. And then here is our kind of added ingredients in dimension two. And now here's a result. So we So being in dimension, so it looks very similar to dimension three, but now we have to renormalize it with these two quantities here. Or we have to renormalize it here, and here we get this new measure. And then we also get that this renormalized displacement converges to this Gaussian field weakly in some Sohole space. And now there's an interesting feature here. So now when this is grad URL, This is grant URL, this WRL is just a very explicit object, just depends on this empirical measure. So one can explicitly calculate this grant U1. And one can ask, okay, how does this grant U1? So how big is this renormalization that we have to consider? And this renormalization, the law of this object, of this random shift that we have to subtract is very close to a Gaussian itself. And this Gaussian is a And this Gaussian is a Gaussian of mean zero, but the variance, and the mean zero is kind of clear because of the symmetry. But the variance is interesting. This is of order log L. So this is really, I mean, the error is much smaller than the variance. So this is a meaningful estimate. And what you see here, this log L is precisely the log L that you've seen before in the transportation cost estimate. So what one sees here is that this log L. What one sees here is that this log L that pops up there is really the main, so there's this one shift is the main contribution for the transportation cost estimator. So what happens there in this optimal coupling dimension too is there's one big shift and then it behaves essentially like an identity. But this shift is random and it moves around and it's close to a Gaussian. And it will diverge if L tends to infinity. Okay, so let me spend a few words on the proof of this. Because I think, okay, so I think it's quite nice. So we've seen before in this under by the physicists, what they did is essentially they said, okay, this Monsappe equation should be essentially. Chapter equation should be essentially replaced by this by this Poisson equation. But now, if you take this Poisson equation, if you do a CAT scaling here, then the right-hand side, because of the independence of this mu n of the different points in the mu n, this should converge to white noise. And then this grad u should essentially converge to the solution of this object here on the right-hand side, essentially really to our Gaussian field. Our Gaussian feed that we're considering. So, following this idea, this is exactly how the proof works. So, first, we show the kind of seemingly easy part, which means that we have to show that our approximation of white noise really converges in law to the Gaussian fields that we have here on the right-hand side. And then in the second step, In the second step, we need to show that the displacement is in fact very close in a quantitative way, very close to what you are L. And this part is a difficult part. And it in fact consists of two parts. And it's splitted in a completely deterministic estimate where you forget everything about stochastics. And you show estimates which are true on a pathways level. And then you check. And then you check using stochastic that the assumptions that you use there are in fact satisfied on an almost sure level for the problem that you're considering, and in our case for the optimum matching problem. And this deterministic estimate is essentially the last thing that I would like to show you. And this is his result. And this is, okay, so we need to slightly adapt this result to this matching problem. This matching problem, well, to this periodic problem of the torus. But essentially, this is a result jointly with Michael and Felix that is at the core of this. And this is the kind of the justification of this approach by Karachlo and others from a macro to a micro level. And this is the last thing I'd like to show you how this works. So just for notation, so this beta r is some can be. Beta r is some can be more general, but for us it will be log r in dimension two, and beta of r equals to one in dimension three, but you can also think of others here. And now the first thing is we have a measure mu. So I'll walk you through this. So we have a measure mu, and this is not the measure before, it's just a general measure mu. And for simplicity, let's assume that we are on a big ball of radius r bar, and this measure has the same mass as the Lebesgue measure on this ball. Ball. And now what we want is that on each ball of radius r, for each r between one and this capital, this macroscopic radius, so from a macro down to a micro scale, this mu is close to the Lebesgue measure in a quantitative way. So this replaces the assumption that mu is essentially one. So this mu n is essentially one. So in this approach by Karachlu, we assume mu n is essentially one. And we replace this in this Vassarian topology by saying mu is very close to By saying is very close to the Lebesgue measure in a quantitative way. Then we define phi, then we essentially do the same thing. We take the phi as a solution to our Poisson equation with the difference of the measures on the right-hand side. And there are Neumann boundary conditions because there is no node transport outside because of this assumption here. Okay, and then there's this explicit estimate here. Q is the optimal coupling between. Is the optimal coupling between our measure and the Lebesgue measure, so on the global scale? So the global optimal coupling, the y minus x is a displacement, and now this displacement in an average way, and we average with this cutoff function here, is close to this average version of the gradient. So close to a shift, which means just that y minus x is very close to, or the optimal coupling is close to the identity plus a shift. In well, in some kind of quantitative way, and this is, and then in the kind of the last step, well, having this result adapted to this periodic setting, what we need to do is we need to check that this assumption is satisfied for the matching problem, which is essentially the consequence of the nice result by Ambrosio-Strain-Trivesan. Strain Trivesan, which can be adapted here, and then one can put things together, carefully estimate, and that's essentially the proof. Okay, so I think it's a good time to stop. So thanks for your attention. Thank you very much, Martin. Thank you very much, Martin. From the audience, any questions for Martin? If I may ask. Sure. Thanks. Thank you, Martin. That's a very, very, very interesting talk. I have two questions. First of all, if you go to dimension three, the Gaussian field, say you Say you integrate it on a ball. What kind of so you get a Gaussian random variable, and how do you determine the variance, for instance, of that Gaussian random variable just to get a feeling about what the field is doing? So the variance is proportional to the L2 norm of the indicator function. Of the indicator function. So it's a bit more complicated than this, but you can really think of this as a variant of a Gaussian free field. So you should really think of this as a Gaussian free field. It's not a Gaussian free field because it's a vector valued, but it's really a Gaussian free field. And I mean, I could now write down the kind of form. Now, write down the kind of formulas that you would need to, and then, but I really don't want to do that because it doesn't help so much, I think. So, you should really think of this as a version of a Gaussian free field or as a kind of vector-valided version of a Gaussian free field. That's, I think, the right way to think about it. Okay, thanks. The other question was concerning the present slide, whether Present slide: whether you have also considered situations where one of well, you don't have really in the bag, but some other. Yeah, well, we didn't do that. So, so this is something that is kind of a pain. So, what we would like to do also is kind of something like the bipartisan matching, where one replaces mu and here, maybe another measure mu. Measure ν, but only half of the proof of this works for in the general case, and the other half does not, and relies very heavily on the invariance property of the back measure. So there's a kind of, so for the analytical expert, there's some kind of Campanato iteration in the background where one needs to shift this estimate. And there one really needs some kind of invariance or some kind of good control on the measure that if you shift it, then it looks. On the measure, that if you shift it, then it looks essentially like the measure again that you started with. So one could probably do something like density of the Lebesgue measure. If the density is nicely behaved, then I'm sure one can do something that it's fine. But the completely general measure is at the moment difficult, unfortunately. Thank you. Thank you. Martin, you have the paper. Martin, you have the papers on archive, right? Yes. Yes. So that's, yeah, both of them. Yeah. How do I search? Just searching by the names would give me the papers by the author name? I can also just send them to you. Okay, thank you, Artin. Okay, so and maybe a last comment here. It's only for D equals to three, but the D equals to three is really just our lack of, so this result here is really. So this result here is really the lack in the error estimate of the deterministic result. So there's no reason why this should not be true in dimension four and five. It's just that our estimate, I mean, maybe they, okay, no one expects this to fail in dimension higher than three. And at the moment, it's just our estimates, which are just too poor. If I understand this correctly, essentially that Gaussianness is simply coming from this basic fluctuation. Coming from this basic fluctuation of the empirical distribution from the things. So, yes, then it's just linearity of the Montjampère. Well, Montjam-Père is not linear, but I mean, sure. Sorry, I mean, yes, from the Poisson, yes. Yes, exactly. So, the first part is really just the linearity of the Poisson. And then you need to combine it with this error estimate of this linearization result. That the displacement is close to this Poisson equation. Basement is close to this Poisson equation. And there one needs this error estimate: how close is it? So that one needs an error should be much smaller than the object itself. And so that this is a reasonable estimate. And this fails in dimension four in higher. I see. So that's a problem. But exactly, this is you. You're completely right. So this is exactly where this quotient field pops up. So the first part is kind of the easy part. So this. Is kind of the easy part. So, this part here, right? So, these are the fluctuations, which gives you this Gaussian nature of the white noise, and then it's just the linearity of the Poisson equation. And then one needs to kind of put this into this machinery here. Right. Any other question? Sure. Okay, so if not, then let's thank the speaker again. Thank you, Martin.