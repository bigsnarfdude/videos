And the novelty from our approach, or the main feature it has, is that we want to do a spatial temporal analysis on our data set in this stationary setting. So mainly, if we think about an extreme meteorological event for rainfall, so let's say a big storm, then we can think that this storm will impact neighboring stations simultaneously. Simultaneously, so that storms typically have temporal and spatial coverage. And what we want to do is to aggregate those recordings that we have from this extreme event to enhance high quantity destination. So maybe just briefly add some slides about, in particular, the data set that we are studying. So if we look into this. If we look into detail at the scatter plot from pairs of stations at these three northeast locations, bedrock stations. Then we can see first from this plot that the rainfall amounts reach high intensity levels very similarly at all the stations. And moreover, if I focus on the extreme Moreover, if I focus on the extremes from these observations, so for instance, high values above a high quantile, then we can see that high painful amounts tend to occur simultaneously at these neighboring stations. So this can be illustrated by these points in black. And this is with respect, this was. This was with respect to the spatial coverage, but then if we look also at the temporal coverage, we can see that extreme rainfall amounts also have tend to last a little more than one day. And this is well illustrated by this empirical temporal extremogram, which is a function of the time lag. function of the time lag at the x x axis and at the y axis it tells us how likely it is to record an extreme event at time lag t given that there was one at time lag zero. So we can see heavy rainfall lasts a little more than one day and in this setting what we plan to do is To do is try to aggregate these information of the extreme event that have been recorded on nearby stations and also that have been recorded on, for instance, consecutive days of heavy rainfall to produce more accurate estimates of high quantities. Then, in the remaining of the talk, I will start by giving a brief introduction. Introduction to the topic and the methods that I would introduce. Then I will give the stable sample methods that will introduce this and I will further give some numerical experiments and I will conclude coming back to the case study of precipitation among synthesis. Then, to begin, let's set the notation. We will consider Notation: We will consider X to be time series, stationary time series, multivariate stationary time series. I will define, I will denote p norms by using this notation, such that for p equal to infinity, this will be simply this primum norm. And I will also denote consecutive observations from time a to time. Observations from time A to time B in this manner. Then a natural approach to model extremes both in space and time is the following. So let's say that we have n observations from my time series. Then I can look at these n observations as disjoint chunks of Disjoint chunks of windows of length D, temporal windows, in the following way. And then typically, in the heavy tail setting, we can model each of these windows of times of length b by imposing an assumption of regular variation. So, this is a very common assumption. And what does it mean? It means What does it mean? It means that these finite windows of time, so vectors of length, time length b, it can be modeled, extremes can be modeled as the product of two independent components. So one radial component, which is parallel distributed and has an index alpha here, and also an angular component which sort of Component, which sort of tell us the direction in which the extreme will take place in space or time. Then, from a practical point of view, how we can use this definition or this assumption of regular variation in practice, we will take now our n observations and look at these joint blocks and look at this. Look at this as a new sample to infer the properties from our model. So, in particular, we can fit to this sample, we can use the sample to infer this tail index alpha, and we can also use it to infer the direction of extremes modeled by this process Q that I'm writing here. However, an inference approach, this type of inference approach, This type of inference approach doesn't take into account that there might still be some dependence between blocks, and typically, inferential procedures can be perturbed by these dependencies between these joint blocks. So, instead, we borrow from the so-called blocks methods in stationary time series, where we let both the length of the block length go to infinity as we To infinity as we model higher and higher levels. And in this case, adding some mixing assumptions, it is more easily to assume that those disjoint blocks are not asymptotically independent. So this is nice for modeling reasons. And then we can also show a little bit more. We can also show that. More. We can also show that these blocks whose length is going to infinity also satisfy the decomposition that we had from regular variation, so that the extremes of these blocks can also be modeled as this product of two independent components, the radial part of distributed and the angular one carrying the shape of the extreme inner space and point. And then from this approach, coming back to our initial question. Coming back to our initial question about the inference of the tail of the vector X1, we now need a way to relay the tail of the marginal tail with the external features of these blocks. And basically, what we propose to do is to recover We cover the tail properties of X1 through the extreme behavior of alpha norms applied to these blocks. As we rely on this asymptotic relation that I am writing here, such that the gild coordinate from my vector, high levels of the tail of this vector can be Vector can be modeled through this equation, where mg are weights that I allocate to each coordinate that traces the spatial dependencies of the multivariate vector. And then maybe to explain why am I doing this is we can look what would have happened if I have chosen here not to take alpha detailing. To take alpha, the tail index of the process, but maybe some other P and aggregate recordings in this block with some average with respect to P. And then I can see that now my equation is modified by this constant Cp, where this constant CP now traces temporal memories of my series. So this is so let me say maybe So, let me say maybe a word on what this NG and CP are defined as. So, this NG relates the distribution of the J coordinate with the distribution with respect to the norm. And then this CP relates how the P norms of my series reach high levels compared with how an IH. With how an IID sequence, the inner of an IID sequence, reach high levels. So, in this way, it captures some information about the temporal memories of the process. In particular, if I take P equal to be the supremal norm, this is a very classical approach because it is related to the generalized extreme value distribution. Uh, generalized extreme value distributions, then this relation, this c, this constant c infinity, will equal the extremal index of the time series. And the advantage of taking p equal to alpha is that now c alpha equals to one, and the way we can interpret this relation is basically that the alpha norms of my series reach high levels at a constant rate. Levels at a constant rate, as in the IAD case, regardless of the temporal members. So the IAD sample behaves like the dependent sample behaves like an IAD sample. And this is a very nice feature. Then this is what we do. We want to take p equal to alpha and use alpha norms to model the To model the tail distribution of my marginal distribution. And the asymptotics that I gave later are based on large deviations of sounds, as the alpha norm is simply the sums of regularly varying increments of tail index one. And then, moreover, looking at these sums of regular Library increments. Liberine increments. Then we also built on central limit theorem, which basically states sufficient conditions for the existence of a recentering constant DN and a renormalizing constant AN, such that we also have that these sums well renormalized, its limit is it belongs to a parametric family, which is a family of stable distributions with unit stable parameters. With unit stable parameters. And then maybe we can see some analogy with our approach, a more classical approach taking P equal to infinity, in which I will compute blocks of maxima. But then this approach is tied together with the extremal index, as I previously mentioned. And then, just let me recall briefly what this extremal index means. It means that we It means that it is defined as a constant theta, such that assuming that the blux of maximum of an IID distribution has a limit G, which is a generalized extreme value distribution, then the series admits an extremal index theta if the blocks of maxima of my series now converges to a shrunken version of this G distribution and shrunken. And strengthened by a parameter theta that is called the extremal x. Then, typically, inference based on blocks of maxima requires an additional step, which is the estimation of theta in order to recover the properties of the marginal tail distribution. Then, in our approach, we are avoiding estimation. Within the estimation of this constant theta. Okay, so in practice, what do we do? So let's start with a stationary habitate sequence, St, and then we want to use this equivalence relation to extrapolate high quantities of each location. Issue location G. Then we start by considering a sample of n observations. We will again divide our sample into these joint blocks, so n blocks of length dn. And then in order to extrapolate from this expression, we need to give a model, we need to infer a model for. Infer a model for this probability of alpha norms being large and to model the distribution of the probability of these alpha norms being large. We recall that this is a probability on the sums of regularly varied increments. Then we will pursue it by fitting a stable distribution to this alpha normal. To these alpha normals. So, to sum up, how does the algorithm look like? It takes as an input an estimate of the day index and estimates of the weights that I am allocating to each coordinate and my sample. Then, in the first step, I will search for a suitable block length B such that when I take these sums of Take these sums of alpha powers. I want this sample to be nicely modeled with a stable distribution of unit stable parameter. And then for each location, I will use the relation I mentioned to extrapolate the high quantiles. And then I use parameter bootstrap to obtain confidence intervals on those high quantile estimations. And then my output are the D high quantiles estimates from the 50 years return level and confidence intervals. Notice that this is a multivariate approach in the following sense, in the sense that from my multivariate sample, I'm only fitting a stable distribution once. Distribution once in step one. And then I'm using this fit to extrapolate from all coordinates. However, I could have also applied each, but this algorithm component twice to each univary time series, and I would have also obtained a result. I will call this way a component-wise approach, and I will call this. Component-wise approach, and I will call this algorithm that I'm detailing here, where I fit only one, the stable distribution, the multivariate approach. Then to illustrate how this algorithm works in practice, I will simulate a thousand trajectories of length 4000. This is a typical length for a time series. Length for a time series, rainfall time series. I will simulate from both univariate and multivariate models, and they will all have a tail index equal to four. This is also a typical tail index for the rainfall time series. To estimate this tail index, I will use the Arnvias Hill type estimator in the HANA quarters. And I will compute confidence intervals for this high quantile, 99.98 quantile, which actually corresponds to the quantile for the 50 years return level of fall daily precipitation amounts. And to implement the algorithm, I will consider different samples. So I will consider some lengths equal to 30. So I will consider some lengths equal to 32, 64, and 128. But actually, for each of these fixed block lengths, I will only implement or give a return, the algorithm would only return an output when the stable distribution is a nice fit for my sample of alpha sums. And I will test this using the Using the ratio likelihood test. And then, as I said, I have both a univariate and multivariate samples. So, in the univariate setting, I will compare the stable sums approach with some more classical approaches as the blocks over maxima, as the blocks method, sorry, and the peaks over threshold method. And then for the multivariate models, I want to compare, I will also. I will also conduct an experiment to compare the component-wise estimator where for each univariate time series I fit a stable distribution against its multivariate version where for the multivariate time series I only fit once the stable distribution. So these are the results. Are the results for the univariate setting? Then I am showing here coverage probabilities. So I am computing how many times does my confident interval captures the right value that I want to estimate. And an accurate coverage should be around 0.95. And I am computing this on four different models. So the Burr model in an IID setting, the freshette models of IID, Freshette time series. And then I'm also doing this in two time-dependent models, which are the R-Max model for different tuning parameters. And then for the stable sums method, I said I would do this for different block lengths. And then the index that I am writing here in parentheses is the ratio of acceptance of the ratio likelihood. So it's the number of times I'm actually returning an output for this choice of blueprint. For this choice of block length, and what we can see for our method is that as the block length grows, the ratio of acceptance grows, but this comes to a cost because we can see that we might be estimated confidence intervals which are a little wide. But we can see that in both I am highlighting The best estimates, so the best choice of block length for each model, and we can see that for right block length, our algorithm is outperforming the block maxima and pixel threshold methods. Maybe what I must say is that I have done declustering to implement the pixel threshold method, and I have estimated the extremal index to implement these block multi methods. So it actually works. So, it actually works quite nice. And then, in the multivariate setting, so I'm simulating from a multivariate RMAX with noise, with the multivariate generalized, with noise from an IIID into value distribution that I am writing here. And its distribution is parameterized by a parameter tau, which measures the Tau, which measures the spatial dependence of extremes, such that this constant, the weights mg, can be written as one over d tau, such that when tau equals to one, I'm in the case of asymptotic independence, whereas when tau goes to zero, I'm in the case of asymptotic dependence of extremes. And then in this case, what we can see, if we look at the relative percentage change. At the relative percentage change, so it's how much I gained with respect using my multivariate approach. Then we can see that as tau gets closer to asymptotic independence, the multivariate approach is performing better than the univariate approach, in particular for the block lengths 32, 164. 30 to 164. And remember, for the univariate case, we saw that 64 was the optimal choice in this case. So we can see that this multivariate approach is working quite well. And then I have a little time, so I will go fast on these last slides. So we now implement our approach in the data set of the three weather stations in France. Weather stations in France, and here is the feed of the alpha sums with respect to the stable distribution. We can see that it works quite nice. And then last, these are our predictions. And so this is a function of return periods in years in x-axis against the return levels in millimeters per. In millimeters per cubic meter, in the y-axis, and the solid line is our prediction of return levels, and the dots, which are the observations, the highest quantities of each time series. And we can see that our prediction is covering really nice these highest quantities. Okay, so to conclude, we presented the staples log method and we can conclude that inference based on this L-alpha sons approach is robust to handle time dependencies as we implement it alike in independent and dependent time series. And I leave here with the bibliography of the work. And I'm And I'm happy to take any questions.