Currently at an instructor position at NYU's Courant Institute and the Center for Data Science. He will tell us today about information theoretic limits in Bayesian inference with Gaussian noise. Okay, so yes, thank you very much, Leah. Thank you very much to the organizer for letting me speaking at this nice summer school. Nice summer school. So, yeah, today I'm going to talk about information theoretic limits in the inference problem. And yeah, I wrote already a little outline. So first I'm going to give some definition and state a theorem. And then we are going to focus on two very simple toys models. That is the estimation of a sparse vector and the estimation of the rank one components of a Of a noisy tensor. So, feel free to interrupt me if you have a question or exit torch. And okay, so let's go. So, what do we consider? So, the general setting in this presentation is that we consider a signal vector that we are going to write x that will belong to Rn and this signal vector will be. And this signal vector will be drawn from some probability distribution that we write p of x. And we are going to assume that this probability distribution has a finite second moment, right? And okay, basically, x is the quantity that we want to infer. Is the quantity that we want to infer. But what we are going to observe is not x directly, is that we observe the vector y, which will be equal to some factor lambda times x plus z. That will be Gaussian noise. So z, this is a noise, and the components of z, sorry, are going to be taken IID. Taken IID standard normal random variable independently of X also. This factor square root of lambda will be what we will call a signal to noise ratio. And so the Y that you had on the left is what you observe. So given Y, your goal is to infer X, and you see. X. And you see that if lambda is very large, it will be very easy to infer X because the signal component of your observation will dominate. Whereas if lambda is very small, it might become trickier. All right, does that setting make sense? Okay, and there are many questions that you can ask in such a general statistical problem, but here we are going to focus on a very particular one. On a very particular one, which is the question that we ask: what is the value of what we called the minimal mean squared error that we define like this? MMSC. MMSC, depending on the signal to know the ratio lambda, is defined as the minimum over all estimator, that is all x hat that takes y input and outputs an estimate of x, so measurable function from Rn to Rn of the error in L2 sense of our estimator. Right. So we would like to compute this quantity. It's a very, very natural object of interest because it tells you basically what you can potentially hope for when recovering X. Meaning that if the MMSC is very large, if this minimal mean square error is very large, you cannot hope to recover X very well using the observation of Y, whereas if it's Whereas, if it's quite small, that means that potentially there exists an estimator that estimates x well. But of course, here you do not know if there is an efficient way to compute such estimators. It just tells you that there exists a function from Rn to Rn that estimates X well. Okay, so that's why we are talking about information theoretic limit, because it's just a notion of really information, but not of algorithmic hardness of. Algorithmic hardness of estimating X. All right. So if this makes sense, of course, you can say by Peter-Goren's theorem that this, the optimal estimator for estimating x given y in L2 sense is just the posterior mean of x given y. Okay, and so if you want to, for instance, plot the behavior. instance plot the behavior of this MMSA. What you expect, of course, is that as a function of lambda, you should start when lambda is equal to zero, you can not estimate better than the variance of x, and then possibly it goes to zero when lambda goes to infinity. But your goal will be to understand this curve a bit more precisely than that. Right? And so that's what we are going to try to do. So that's what we are going to try to do. And here we see that, okay, since the optimal estimator here is the posterior mean, it's very natural to investigate the posterior distribution of x given y. So that's what we are going to do now. So the posterior distribution of our signal given our observation is just going to be It's just going to be the probability of x being equal to little x given y. It will be proportional to something that is the prior that you have on your signal, this P of capital X, times a term induced by your observation. And because you have a Gaussian noise, it will take the following. It will take the following form: so root of lambda x, right? I recall that y is equal to square root of lambda x plus and this is a standard Gaussian. All right, and here if you expand the square that you have in your exponential, you can say again that this is proportional to exponential x square root of lambda. Square root of lambda x dot product with y minus lambda over 2 x squared. So here I expanded the square and I also removed the term that didn't depend on a little x because this term would be carried inside of this proportional symbol. All right. So that is our posterior distribution. Posterior distribution and what's inside the exponential, we will naturally call it the Hamiltonian and rewrite for it h of x and we'll see this as an energy or minus an energy using a statistical point of view. Right, and here once we have this notation, we can simply write what is our optimal estimator. Our optimal estimator. Our posterior mean is simply the mean under this posterior distribution, so this divided by the normalizing constant. This is the optimal estimator that achieves the NMSE. And here again, we are going to give a name to this normalizing condition. Going to give a name to this normalizing constant. So, this normalizing constant is usually called the partition function, and we are going to write it as z of lambda. All right, and so okay, that's just another notation, and the next notation that we use is that it's quite a pain of writing this all. This whole ratio. So we are going to use a shorthand notation for this, which is quite standard here. We are going to use a bracket where the brackets here denote expectation with respect to this probability distribution. Okay, so the little x that you see here is exactly the one that you have here. So, once that you have here, all right, okay, and um, okay, so before really studying this posterior distribution and this MMSE, we'll define the last object that we will need definition, and the most central that we'll need today is that we define the free energy. As the log normalizing constants, as so f of lambda, lambda is again the singleton ratio at the expectation of the log of this normalizing constant. All right, and here it turns out because we have we are studying a statistical problem, it turns out that it has a nice meaning in the sense that you can check. Meaning, in the sense that you can check that it is related to the mutual information between your signal and your observation. And so, what is the mutual information? It's just a measure of correlation. This is a Pullback divergence between the product distribution. The product distribution of x and y and the product of their marginals. Okay, so basically, this quantify how close and how much information Y convey over X. But okay, so studying this free energy is quite instrumental in statistical physics. Basically, that's the first thing you compute, and that also what we are going to do here. Also, what we are going to do here. And the reason for that is that we have a nice CRM called the IMMEC CRM. So that's what I'm going to talk now. I MMAC CRM. And this theorem was known for a long time as the As the brain identities in information theory, so it is but recently it has been developed developed by people like Gro Shamai Verdu and Rue, and these people call it rather the IMME CCRM. And the reason for that is that it relates the mutual information, so the I with the MMSC. So it tells you that the derivative of the mutual information, so basically the derivative of the free energy, is equal Of the free energy is equal to one half of the minimal military error. And because here we are not working with, we'll be rather working with the free energy, and we know from above that the free energy is just an affine transformation of the mutual information. Equivalently, it tells you that the derivative of your free energy is equal to one-half. G is equal to one-half this thing. So the right-hand side here is just the correlation of a sample from the posterior distribution, so a little x with the signal. And so this we are going to call it the overlap. And basically, it is equal to one minus the MMSC. All right, any questions so far? So, yeah, I'm not going to prove it completely because I think you probably seen that in the lecture of Jean-Christophe Moura at the beginning of the summer school, but I just show briefly from where it comes from. Briefly, from where it comes from. So we have f of lambda, which is again the expectation of the log of our normizing constant. So this. Here, I'm just going to write as a Hamiltonian properly. Expectation of square root of lambda x times z plus lambda time x minus lambda over two. Lambda over two. Okay, so this is our normalization constant. In fact, I just modify it a little bit from what we have before. I just replace the y that appeared in the Hamiltonian by its value, which is the square root of lambda x plus z. And here, if you compute, you see that whenever you compute the derivative of f without Of f with respect to lambda, this makes appear an expectation of the Gibbs distribution under the brackets. And so, what you get is something like this. So, x plus z plus minus one over two x squared, right? And okay, here I will not do it, but here what you have, you have But here, what you have, you have three pieces. The piece in the middle is basically what you want to have. That's what's in the statement of the formula. But there are two other pieces here and there. And what you do now is Gaussian integration by parts, plus then you use Bayes' rule. And if you do that, Where to end up is exactly what you want. You end up this. Okay, and because yes, the overlap is related to the MMSC and the free energy to the mutual information, this gives you also the first line of the theorem, which is totally equivalent. All right, so that's always the same thing. A standard thing in statistical physics. A standard thing in statistical physics, you compute your free energy and then you take derivative with respect to your parameters in order to have access to interesting information. So, here, we are not particularly interested in the particular value of the free energy, but we will rather try to differentiate it in order to have access to more interesting quantities as the MSI, for instance. All right. So now I'm done with the definition. So, I'm done with the definition, and we are going to move on and apply this kind of thing to some toys model and see what we get. And so, the first toy model that we'll get, I will call it sparse vector estimation. And you'll see it will be even a very toy model for sparse vector estimation because it will be the simplest things that you could imagine. So, what we are going to Could imagine. So, what we are going to do is we are going to take E1 to E to the D be the canonical basis of R D and we are going to take D of the form some power of n so. So, okay, it seems a little weird at the beginning, but in fact, we will see that it will be useful. Okay. And then what we take? We take some number sigma zero uniformly distributed between one and d, which is again equal to two to the n. And one And once we sampled a sigma zero like this, and we let our signal vector be x equal to e sigma zero. So x is nothing more than a vector of the canonical basis taken uniformly at random. Okay, that is your signal. And as you see, it's a very sparse vector, but on a very particular type, because it is equal to a vector of the canonical base. It is equal to a vector of the canonical basis. And once we have that, we observe again not x directly, but some vector y, which is x times some signal-to-noise ratio plus z. And here, the only difference with the setting I had before is that I had this n here that will be. And here that will be for scaling. And here's the again two to the sorry two to the n IID standard Gaussian. All right, so we are exactly in the same setting as before. And then you want to you like to recover X given Y. And here a first remark. And here, a first remark. You can argue that this sparse toy model is very stupid because we know that the maximum of the components of the noise, so for sigma equal to 1, 2 to the n of z sigma, is approximately equal to square root of square root of two log two times n because we have two to the n independent Gaussian. And so if we want to recover x, we simply say, oh, I'm going to look at the largest coordinate of y, and this will be my estimate for the corresponding vector of the canonical basis. Right? So we expect Expect that we can recover X if and only if lambda the signal-to-node ratio larger than two log two. So, yeah, in fact, my inference problem is completely stupid because here I basically know already the solution because I know the behavior of the maximum of independent. Behavior of the maximum of independent Gaussian. But we are just going to use our formalism of free energy and MMAC to see if we indeed get this result. And so what we do is we write our posterior distribution as before. Okay, so that's what we expect. So let's see if we can recover this. So the probability of sigma zero being equal to sigma given the observation rate. Being equal to sigma given the observation y, you can say that it's equal to one over the normalizing constant times your prior distribution on sigma zero, so just the uniform one, times the exponential of the same thing as we had before. So z dot this plus um Um minus lambda n over two right and so this is our Poisson distribution and so our free energy F n of sigma which is just now here I add an extra one over n factor for normalization purposes but apart from that Cn of the exponential of my Hamiltonian, so square root of n. Now I can write it as simply z of sigma plus lambda n one again because capital X and E sigma are a vector of the canonical basis minus lambda n over two. Lambda n over 2. All right, so here I just rewrite the free energy, and here you can do another remark saying that, okay, I basically know this free energy and know this Hamiltonian. Mark, why? Because this is very close close the random. The random energy model that you may have seen with Aukash, where the Hamiltonian is simply square root of lambda n times z sigma. Okay, so here we have the same term of the random energy model, but we have an extra term that is created by the signal that gives a bit more energy to the configuration sigma zero. All right, so you can see this. All right, so you can see this as a statistical analog of the random energy model. And in fact, we can study it. And what we'll prove is that this free energy that we had above converge as n goes to infinity either to zero if lambda is smaller than two log two or Or to lambda over two minus log two if now lambda is greater than this. Okay, that's what's that's what we will prove. And in fact, the proof would be much easier than the proof for the random energy model because, in fact, this new planted term will force only one configuration to take all the weight. Configuration to take all the weight of the Gibbs measure. But we will see this in the second. But just now the consequence of this of this, since f prime of lambda is equal to one half of one minus the MMSE. We deduce directly that the MMSE has this shape. That the MMSC has this shape as we expected. There is a critical value here of two log two, and the VC goes from one to zero. Okay, so this seems to work, of course, again, as Seems to work, of course, again, as the result is completely obvious. But how do we prove this convergence of the free energy proof? What we can say is that just using simple upper and lower bounds, we can say that this is equal to its definition. Okay, so that's my definition. And here, because I expect that only the configuration corresponding to the planted signal, so the configuration sigma zero will dominate, my guess is that this lower bound where I only take I only take the term corresponding to the planted term, so one to the n of exponential square root of lambda n will dominate the Gibbs measure. So I hope that this lower bound will be tight. So this is obtained just by considering the term altime alti ma zero. And in fact, what we have here is exactly what we have in our. Here is exactly what we have in our proposition saying that this is equal to lambda 2 over 2 minus block 2. Okay, so the lower one is very easy and the upper one is not much more difficult. We only use Jensen inequality to say that by concavity of the law. By concavity of the logarithm. This is smaller than our normalizing constants, but condition on the signal and the noise on the signal. And if you do that, you can actually compute all of this conditional expectation, and what you get. And what you get is that all of them for the configurations that are different from sigma zero will give you this term. And the configuration with sigma zero will give you that. Z sigma zero plus lambda n over two minus n log two. And log two, right? And here basically we are done because the first term here goes to one. Here, this term is of order square root of n, so it's negligible compared to the other terms in the exponential. So basically, the behavior will be dictated by this last quantity. And so you get. And so you get that at n goes to infinity, this either go to lambda 2 minus log 2 if lambda is greater than 2 log 2 and zero if lambda is smaller than two log two. And this is exactly the converse logarithm that I need. Exactly, is the converse lower mount that I need in my proposition. Okay, so here we have this sparse vector estimation, which is somehow a planted version, so a version of the random energy model with a signal. And here it is much easier to study because what we expect is when a lambda is large enough, only one configuration will dominate, whereas Configuration will dominate, whereas in the Renom energy model, you have a few configurations of top energy that will dominate your measure. So here, things are made much more simple by the signal term that we have inside of our Hamiltonian. All right, is there any questions so far? Okay, so now that we have we did the settle warm-up with this Warm-up with this quite simple example. We'll try to connect this simple example to maybe a more interesting model. That is a model of rank one tensor estimation. Okay, and here we are going to take a signal. We are going to take a signal uniformly distributed on the IPRQ minus one one to the n and what we are going to observe is observations are going to be y which we do again a signal-to-nose ratio with a scaling. show with a scaling n p minus one times x tensorize p times with itself plus some some noise z and so what does it mean exactly is that the entry of y that we are going to observe y will be a p tensor so we'll p indices y1 to yp This will be equal to square root of lambda divided by n p minus one times xy one xp where all of this again are I ID standard Gaussian random variable and this is just for This is just for scaling. Okay, so in particular, this model is a statistical problem where, for instance, when p equals two, you will like to recover rank one components in a noisy matrix, right? But when p increases, you have it to the tensor version of this thing, and that's what we have here. That's what we have here. Okay. And again, let's just write the, because we do always the same method here, let's just write the free energy. That will be again, 1 over n, the exponential, the log. Of now, we have something a bit more complicated as before, with sum over all possible configuration. Possible configuration, so we sum over x and the hypercube of the prior that we have an x, which is a uniform prior, times exponential h n p of x, where h n where our Hamiltonian is now something like this square root. Square root of lambda divided by n at the power p minus one. A first term that really corresponds, that is really the analog of the p-spin, that is actually the p-spin. ZY1YP P. So that is our presspin, but here we do not have a simple pace spin because we have a signal. Have a simple pace spin because we have a signal, and that's why we have also a signal term that takes this form x inner product of x with the signal normalized by n at the power p minus this factor. Okay, so here we have a very good idea. Okay, so here our rank went tensor estimation looks really like a P spin model plus an additional signal term that is here. And I think that probably Okosh told about that, but the P spin can be seen as, okay, sorry, the random energy model can be seen as the P goes to infinity limit of the P spin. And so here. Spin and so here we are going to use the same heuristic to compare this low-rank estimation problem with our sparse vector estimation problems that we saw in section two. And so we are going to do this very heuristically. So heuristically when T is large Is large. What we can say is that the signal term kind of simplifies a bit because we can say that it is equal to one if the signal is exactly equal to the planted configuration and zero otherwise. But here we But here we forget, we don't care about the configuration about x equal minus x in order to simplify things. So this signal term is basically when p is very large, either equal to 1 if you are equal to your signal, or it is equal to the power p of something between strictly between 0 and 1. And then you can say that when p is very large, it's approximately 0. Okay, so when p is large, the Hamiltonian will simplify. And also, what we can say about the random variables that are in our Hamiltonian. The question variable that we are going to write w of x that are one over pure. Pure two. Okay, the first of the P spin a part of the Hamiltonian times What we can say here is that they are approximately IID standard normal. Standard normal. And why do we do this crude approximation? It's because if we look at the covariance structure, the covariance between a Gaussian variable indexed by U and a Gaussian variable indexed by V, U and V being two vectors of the hypercube. This is equal to U dot product with V divided by N To the power p. And same heuristic as before, when p is very large. This is, and forgetting about the case when u can be equal to minus v, this equals either to one if u equal v and zero otherwise. All right. All right, so this is our very crude heuristic. So we expect we expect that for large P, the Hamiltonian of our order P Rank one estimate tensor estimation problem is approximately equal to square root of lambda. Of lambda n z of x minus lambda n over two and where this is iid i i d n zero one, because we expect that our Gaussian random variable uh behaves like uh as if they were independent for R G P. were independent for R p okay and here what you recognize is exactly the Hamiltonian that you had in the sparse very sparse vector estimation problem because here again you add two to the power p sorry two to the n configuration and the energy of one configuration here x is equal to some independent random variable here plus a term indicating A term indicating if you are equal to the signal or not. So here you have exactly the sparse vector estimation problem. Okay, so for Large P large, sorry this is the sparse vector. Vector problem. And this is analog, basically, exactly the same of saying in statistical physics, of saying that the random energy model is equal to the limit as p goes to plus infinity of the p spin. Okay, so while this was only heuristic, in fact, you can make this a bit more precise in the sense that proposition, you can say that the limit as p goes to infinity of the limit of your free energy F P n of lambda is equal to zero if lambda is below this two lockdown. Is below this two log two value and lambda divided by two minus log two otherwise. All right, so here this just makes this connection between our tensor and our sparse vector estimation. Tensor and our sparse vector estimation more concrete. And in fact, it's not difficult to prove. Basically, it follows the same line as the proof for the free energy of the sparse vector estimation. So we are not going to do it, but we are just going to discuss a little bit on the consequence. Consequences. Is that the picture that we have now for the this uh tensor estimation problem? Tensor estimation problem looks therefore like this. We have our lambda, we have our two log two here. So here we have the MMSE MMAC for basically p equal plus infinity, the sparse vector estimation. The sparse vector estimation. But what's a consequence of the proposition above tells us that the MMSC for the Lorentz one tensor estimation will approach as p goes to infinity the MMSE for the sparse vector estimation. So what we expect is okay, maybe for that we that is for let's say p equal to Is for let's say p equal to a low-rank matrix estimation problem. But as p will increase, we'll start to observe something like this. Let's say this is p equals 3. And in the end, if you take larger and larger value p, the messy of your sparse tensor estimation, sorry, tensor estimation will look like this. This will be. Tensor estimation will look like this. This will be functional for p equals 10. You will see something like that. And one interesting fact, in fact, is that you can see that you have always have a jump here for p greater or equal than 3, whereas for p equal to the matrix case, your MMEC goes continuously from 1 to smaller values. Okay, so let me just. Okay, so let me just conclude by two remarks. Two concluding remarks. So we have seen that this complex tensor estimation problem is in the k goes to infinity limit well approximated by this sparse vector estimation problem. But if you want to go a bit further and ask for a finite p. For finite p, let's say that you want to plot the curves that I draw here in colours on the graph above. For finite p, we would need to use techniques like the one discussed by Jean-Christophe in his lecture. In his lecture. The one presented by Jean-Christophe Morat using this Amistan Jacobi equations to compute the limit as n goes to plus infinity Right. And if you do that, you obtain a formula for the limits. And this formula will give you, when you differentiate it, it gives you exactly the curve that I sketched above. Okay, so this is the first remark. And the second remark is that, okay, this is now very vague and very realistic. But here we only talk about the best performance. The best performance you can have if you add any estimators that you like. But now, if you ask how hard it is computationally to estimate your signal, one heuristic you could say here is that the sparse vector estimation estimation. Estimation problem is sorry is computationally hard. Why? Because you need to check all of the two to the n entries entries and take the And take the maximum if you want to estimate your signal. And here, this is a lot of unwielding, but now if you say, okay, when p is very large, this tensor estimation problem is equivalent to this parse vector estimation, then you could believe that we could believe Believe that our tensor estimation problem is also hard for RP right, but okay, here this is uh just an Just an intuition that you could say here, but yeah, there is. I didn't claim to do anything rigorous about that. Okay, so I think that's all I wanted to say about that. So yes, thank you very much. And let me know if you have any question. Thank you, Leo. Let me actually unmute the participants so we can all thank you. Mute the participants, so we can all thank you together. So we have time for a few questions if