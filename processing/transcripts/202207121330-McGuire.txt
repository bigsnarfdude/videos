Great, thank you. So today I'm going to talk about a simplicial pooling layer, which is a learned way to pool simplicial complexes. And so for different types of input data, you'd want to use different types of machine learning architectures. For data that has natural 2D structure or 1D structure, this nice Euclidean data would be naturally suited for something like a CNN. However, a generalization of that is for graphs. Of that is for graphs where you'd want to use a graph neural network. And further, in the case that we are interested in, is the case of simplicial complexes. So, in this case, you have higher order structure, perhaps signaled on these higher dimensional P simplices. And in order to leverage this type of input data for machine learning, you'd want to use something like higher order message passing on these simplicies. And so, in particular, And so, in particular, what we're interested in is generalizing ideas for pooling on graphs for the case of simplicial complexes. So, there are various ways to pool graphs and various reasons you'd like to do this, primarily to reduce computational complexity. It can also speed up message passing between vertices that are far apart on a graph. It sort of requires less layers to actually pass messages between these vertices that are further away. That are further away. And there are similar reasons you'd want to do this for simple complexes, but it's even more important because of all of this higher-order structure that you're including, it can also become computationally expensive very quickly. And so the idea of collapsing down these higher dimensional simplicies in a learned way could be important in order to use higher dimensional information in your models. And in addition to all of the challenges that graphs face, That graphs face to learn pooling. Simplicial complexes have all the same challenges, with the additional challenges that learned representations have to respect the definition of a simplicial complex. So there is complex face and co-face relations that you have to maintain in order to ensure that your output is still a simple shell complex. So our goal here is to define a pooling layer for simple shell complexes and more generally data on with signals that have. With signals that are on higher-order structures. And these can be interleaved between any regular message-passing symplocial neural networks in order to reduce the size of these symplocial complexes. And it serves the purposes that I've outlined here that are the same as the graphs, with this additional challenge that we have to respect the face and co-face relations that are inherent with simple show complexes. So, a very quick review since I know that. So, a very quick review since I know that this has been discussed in various talks so far, but simplicial complexes are direct generalizations of graphs. So, in addition to the vertices and edges that we have, also have these larger collections of vertices that define the higher dimensional simplicies. So, we can have collections of three vertices that define a triangle, four vertices, a tetrahedra, and so on. And topologically, in terms of the algebra, we consider absolute. Of the algebra, we consider abstract simplicial complexes, which are just finite collections of these vertices. But they are geometrically realized as vertices, edges, triangles, and so on for higher dimensions. And it's also important to note that for this work and in general, we consider that these should be oriented simplicial complexes. And so that just means that we fix an orientation on all of our higher dimensional simplicies. So we have a fixed. So, we have a fixed direction orientation on all of the edges, and similarly, all of the faces. And so with these simplicial complexes, we can define vector spaces based on the different dimension simplices. So, for a given dimension, we denote chain groups as a finite-dimensional vector space, which is a basis of this chain group is just given by the p-dimensional simplicies. So, for example, So, for example, the zero simplicies are a basis for the C0 chain group. And elements of these vector spaces are just linear combinations of your P simplices. However, we also would like to define maps between these chain groups. And so, to do that, we use boundary maps. And this is just the boundary map on an oriented simplex, it's just this. Oriented simplex is just this alternating sign sum of the vertex that you're considering, where the arrow on the top indicates that that vertex is dropped out. And so this is better illustrated by an example. So the boundary map of E1 is just the two vertices that are on its boundary. And similarly, for the higher dimensions, you can also define a boundary map on faces and tetrahedra and so on. And so these are just linear combinations. And so these are just linear combinations of P simple Z's. And so we string these together and we have a chain complex between the different dimension chain groups with boundary maps between them. And this has an important property that the composition of two subsequent chain groups or two subsequent boundary maps is the zero map. And this is actually important for defining homology computations as we've seen in earlier talks. So, what we're actually interested in here for the purposes of considering signals on simple C's is we actually want to look at the co-boundary maps. So, this allows us to not operate directly on the P simplices themselves, but on signals on these simple Cs. And so this is just the dual space to the boundary maps, and we're operating now in the opposite directions. So we get a co-chain complex now with maps in the opposite direction. And elements of these co-chain. Elements of these co-chain groups are elements of the homomorphism group between the CP coach the CP chain group to the field. And so the important thing now is just that we're considering signals on simple C's instead of the simple C's themselves. So as I said, we want all of our boundary maps to be oriented, and this just means that we're And this just means that we're consistent with the fixed orientation of our input simplicial complex in the way we're keeping track of all of these incidence relations. And so the incidence relations are kept track of by ones and zeros, which just a normal standard incidence matrix would be for vertices and edges. This is also true for higher dimensions. We can define higher dimensional boundary maps and incidence matrices that keep track of all of this relationship with the orientation encoded as well by positive. Orientation encoded as well by positive and negative signs. And the reason that we'd like to do this is because we want to generalize the way we're keeping track of structure on a simplicial complex, similar to the way you'd keep track of structure for a graph. So normally we'd use adjacency matrices to represent a graph. And this tells us all of the vertices and the edges that connect them. So two vertices are upper adjacent if there exists an edge between them. This is our standard, normal way to define adjacency on graphs. Normal way to define adjacency on graphs. However, for simplicial complexes, there's additional ways that these simplicies can be related to one another. So the most natural generalization of graph adjacency is upper adjacent neighbor or simple Cs. So for any given simplex, these upper adjacent simple C's are just the set of simple C's of the same dimension, such that there exists a higher dimensional simplex that's Exists a higher dimensional simplex that they're both faces of. So in this example, you have the simplex edge labeled sigma here, and it has two upper adjacent simplices, which are the two edges that you can get to by crossing that higher dimensional triangle. So this actually corresponds to the up-down combinatorial Laplacian operator, which is just the discrete version of this operator, which is the composition of these two boundary maps. Maps. And in the opposite way, we can also consider neighbors that are adjacent via lower dimensional simplicies. So for the same edge sigma, it has four lower adjacent simplicies, which are just the set of simplicies of the same dimension, such that they share a common face. So the simplices, the edges that are shown in pink, are all edges that you can get to by crossing a lower dimensional simplex. So just cross. Lapse. So just cross a vertex to get to the next edge. And this actually corresponds to the down-up combinatorial Laplacian operator. So now using these two up, the lower adjacent and the upper adjacent neighbors, we can use this to think of our Hodge-Laplacian operator, which can be interpreted in terms of these nice algebraic properties from cohomology. And we can just use our co-boundary maps and our boundary maps in order to. maps in order to represent this operator. And so the first term we have our lower adjacency neighbors and second term is our upper adjacency neighbors. And you'll note that as was mentioned yesterday as well, when the dimension P is zero, this first term goes away because there is no B0 boundary matrix because there's no dimension lower than zero. And the second term gives us just our standard graphical caution. So, this Hajj Laplashik operator is actually how we facilitate message passing on simple complexes. So, for a standard graph, we know that we can do message passing using just the adjacency information. So, this first term uses the structure of the graph encoded in the adjacency matrix, and at n layers of our standard GNN, we get information up to n hops away from a single vertex. A single vertex. But if we'd like to use instead the input space of simplicial complexes, we can use the Hodgelaplacian operator in order to facilitate this message passing between these different types of neighbors. And so we can define this message passing simplicial neural network layer, which uses the Haas-Laplacian operator, L sub P, and also these various forms of the boundary operators in order to facilitate message passing between both the lower adjacent and the upper adjacent system. The lower adjacent and the upper adjacent simplices. And so now we have at n layers away, we don't just get information aggregated from the vertices that it neighbors, but also the higher dimensional simple Cs that perhaps have features on those as well. And so this version of this message passing neural network layer is equivalent to the message passing layer defined in this paper with different substitutions for the terms. Solutions for the terms. So now to get to the actual pooling, we'd like to recall that diff pool is a common way to do pooling on graphs, which is a hierarchical clustering scheme, which uses a soft partitioning of the vertices of your graph in order to learn a pooled representation of the graph. And so this consists of two separate GNNs, an embedding GNN, which outputs our An embedding GNN, which outputs our learned vertex embeddings, and a pooling GNN, which gives a learned vertex cluster assignment using the same inputs, but with a soft max. And so the input to this is the adjacency matrix, which tells us the encoded structure of the graph. And then using these two matrices, we can actually pool the graph using these S and Z matrices by applying the S to the left side of our Z matrix. To the left side of our Z matrix to give us our new vertex feature embeddings. And our new pool adjacency matrix is entirely determined by multiplying this cluster assignment matrix on either side of the adjacency. Okay, so what we'd like to do is we define nerve pool, which is a simplicial pooling layer, and it also generates these hierarchical representations of the simplicial complexes in a The simplicial complexes in a learned fashion. And we do this by starting with an initial vertex pooling on the underlying graph. And then we using this initial clustering, we then extend it deterministically to all of the higher dimensional simplices in a way that still respects the definition of a simplicial complex. And we do this first motivate it through the topological interpretation of this method, and then also explain the matrix implementation. Right, so starting with the topological motivation. So we can define the star of a simplex as all of the higher dimensional simplecies that have that vertex in its set. And so it can be defined for any dimension simplex, but we just care about vertices because we're starting with an initial cluster on the vertices. And so the star of a vertex V is this set of simple C's shown in blue, which are all of the higher. Which are all of the higher-dimensional syntheses such that this vertex is a base of it. And then, since assuming our clusters have multiple vertices in them, we then take the union of the stars of all of those vertices in a given cluster to expand this to cover the entire simplicial complex. And you'll note that the star of a vertex is not itself actually a simple shell complex because it's not closed under taking bases. Because it's not closed under taking phases. So now we have this extended cover that covers the entire simple show complex, and all of the higher-dimensional simplicies are included in one of these cover elements. And the set of this is a list of simplices, and that tells us which simplicies contribute to a single vertex in the pooled complex. So then, once we have the cover of our entire simple complex, we use a general. Entire simplicial complex, we use a general formula formulation, which is the nerve of a cover. So, this is a very general idea, but essentially, you have a cover of some space, and each of your individual cover elements becomes a vertex in your nerve construction. And then anytime there are n-way intersections of these distinct cover elements, you add an n minus one simplex in your nerve complex. So, for example, this first example up on the top, if you have a series of cover elements here, each Series of cover elements here. Each of them becomes a vertex in your nerve complex. And then we add edges where there's two-way intersections. We add triangles where there are three-way intersections, and so on. So we use this general nerve complex to actually extend our cover of our simple show complex into our new booled representation. So, just to summarize how this works, we start with a soft cluster assignment of A soft cluster assignment of all of our vertices. We then use the star of a simplex to extend this cover to include all of the syntheses in our complex. Then, using the nerve of a cover, we get our cooled representation of the complex. So, in practice, we actually do this via a series of matrix multiplications. So, starting with an initial vertex cluster assignment matrix, which just maps all the original. Matrix, which just maps all the original vertices in our complex to all the pooled vertices in our new complex. We want to start with that original matrix, which is this upper corner matrix here. And we want to extend this in a deterministic way to include all of the higher dimensional simplicies. So that involves filling out the rest of this matrix so we can map all of our simplicies in the original complex to our pooled complex. And so we do this via the two. And so we do this via the two different functions I have shown in two different color arrows. So first we map all the information down. So in order to cascade this information down, it's really just a series of keeping track of incidence relations. So we get ones and zeros depending on if the vertices of a given simplex had non-zero entries in our first cluster assignment matrix. But you'll note that this is sort of an odd. You'll note that this is sort of an on-off function, doesn't seem very differentiable. So, in order to actually train this model and to do back propagation, we've used a sort of continuous relaxation of this rule in order to train it. So then to extend the information to the right, we use this other function, which is just a series of element-wise products of columns of our first matrix. And that depends on the number of columns we're multiplying depends on. The number of columns we're multiplying depends on the dimension of the new pooled simplex we're talking about. And so we do all of the pairwise combinations of these columns. And if we get a column with all zeros, then we know that that simplex is not included in our pooled complex. So then in order to use all of this information to actually do the pooling, we use the diagonal sub-blocks that are highlighted in yellow. And so we're in as distance. And so, where in a stiff pool implementation we use the adjacency matrix as input, we now have the boundary matrix information to keep track of what is actually present in our symposium complex. And so instead of using the adjacency matrix, we have substituted in here a boundary matrix, and we're multiplying different dimensions of these cluster assignment matrix on either side of it in order to give a pooled boundary matrix. And similarly, to dip pool in the same way, we get the embedding. Get the embedding for features on Psymple C's by multiplying on the left by this cluster assignment matrix of the same dimension. Okay, so the pooled boundary matrices that we now have completely determine the new structure of our simplicial complex. And this can be interpreted in terms of these two pieces of the Hodgel-Possion operator that I had discussed previously: the upper adjacent neighbors and the lower adjacent neighbors, both of which we need. Neighbors, both of which we need to entirely determine the structure of this simple shell complex so that it could be passed down to subsequent layers. And so, to summarize how this works, we start with an input simple shell complex into our layer. We then have this two splits. So, on the right, we use input the boundary matrix and the features on zero and one simplices with a softmax in order to give us this vertex cluster. In order to give us this vertex cluster assignments. And on the left, in order to get all of our embedding matrices, we have a series of these message-passing simplicial neural networks. We would need P of them if you have P different dimensions in your complex. And so then once we have all of these, our new embeddings and our new extended cluster assignment matrices, we can use those in order to compute our new pooled embeddings and our new pooled boundary matrices. Cooled boundary matrices. And using those two, we have all the information to determine what is included in our pooled simplicial complex. Okay, so these two formulations that I've just described, both the topological interpretation and the actual matrix implementation, do give the equivalent simple show complex structure up to reweighting of these P simple C's. And it also And it also has a nice property that this pooling is guaranteed to be permutation invariant as long as the neural network, simplicial neural network model that you're using is permutation equivariant for any permutation matrices. And some other properties of this, by using only the diagonal sub-blocks of this cluster assignment matrix, we're ensuring that the collapsing of all Ensuring that the collapsing of all this information is only flowing down in one dimension at a time. And because we're learning this initial clustering of the vertices, we can actually collapse any collection of vertices. So in the nice pictures I've drawn here, it looks like, you know, the vertices you'd expect to be clustered together got put together. But this depends on training of the model and the loss choice of loss functions. So you're not actually guaranteed that the vertices that you... That the vertices that you think should be clustered together will be, or ones that topologically seem like they should be clustered together will be. And so, one of the things that we are, for future work, considering is perhaps adjusting these loss functions in order to include more of the topological information in the choice of the underlying vertex clustering. So, that's all I have, and I have open to questions. I have and I have open questions if anyone has any.