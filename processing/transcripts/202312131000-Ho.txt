So, today I will present our current theories on understanding the submax gating function in the Meetup Expert. So, this is based on the work with my student at UT Austin. He's a second-year PhD student and also my collaborator. And also, my collaborators in India. Okay, so in case that you don't know about Metropol Expert, then these models were introduced by my board of advisor, Michael Chotant and Hinton, let's say 40 years ago, mostly 30, 40 years ago. But recently, these models have been widely used in deep learning and machine learning. In deep learning and machine learning applications. And for example, in machine learning, they have been used in multitask learning, romanticization, and so on. And in natural language processing, people used in the language models. And also in the recent years, people also tried to attempt to use each of Esperant for latch multi-model models. So now let me try to go into some specific applications. To some specific applications to see why this area becomes like one of the most important topics nowadays. So, I guess anyone here is familiar with the language models, one of the key components of these models is about the scalabilities. So, usually, when people talk about these language models, they say that we have several billions of number parameters, but then the But then, the key components that we will use in these models is by integrating the visual expert into these language models, such that we able to somehow have several buildings of parameters in the language models. So, the idea is that let's say we have a deep neural networks with multiple layers, but then we basically Then we basically, when we have too many parameters, then it's become very hard to store all these parameters. And also, the inference and the training become very slow as well. The idea is that they try to replace some layers by a mutual expert. Let's say we have like a feed forward new networks in one layer, and we replace that feed forward neuron works by multiple. By multiple mixtures of feed-for-wording networks. If you see the figure over there and my pointer, you see that here we have four feet-for-wording networks. So traditionally, we only have one, but now we somehow consider a mixture of four based on considerable experts later. By this layer, let's say if you have like 10 billion parameters deep neural networks and directly implement that deep learning models can be expensive, we decompose that layers into 10 feet forward new networks. And let's say each of that one only have one billion. So in total, we still have total, we still have like 10 billion parameters. 10 billion parameters. But the idea is like we want to use sparse, mean like we only turn on one feed forward neuron works when we train the models. So it's effectively, we only have one billion, we only like even have like 10 billions, but we only train one billion parameters. And that idea was widely used by Google in 2017 when they actually introduced using the sparse widget expert layer in scaling up their mass. In scaling up their massive new networks. And then last year, Google also introduced using that idea to transform virtually safe e-learning models for language, communications, and so on. And a few days ago, we also have Michigan AI, where they also introduced 47 billion sparse language models, where they also consider using visual expert to somehow improve the performance, improve the scalabilities of their language. Scalabilities of their language models. So essentially, even though they have 47 billion parameters, but effectively, when you train the models, it's only 7 billion. So the popularities of these massive and lack language models to the increased popularities of understanding about which expert, like why these models, why these models are so efficient. Are so efficient and effective at improving the performance, at the same time, also improving the scalabilities of massive neural networks. We're going to go into some theoretical understanding of each of ESPAR after a few minutes. Now, I also want to mention another application, which are also very popular when people also try to use B-chip experts to improve. To improve the multi-task learning. Basically, we also have like multiple tasks and we try to model each task by a mixture of expert. And the idea is that we also try to share, like, let's say, expert among different tasks. In the sense that we hope that we can still have efficient way to train the multitask problems, but at the same time, we're also able to learn the strengths of multitasking. To learn the strengths of multitask. And so this idea is basically lead to the multi-gate expert, which is also very popular in all task learning nowadays. And finally, I also want to mention one particular application, a particular direction that I've looked at Austin and our collaborators are also focusing right now. It based on trying to build a pre-trained deep learning model for more tribe. Deep learning models for multi-model healthcare data. Basically, like for healthcare applications, we only have one type of modalities. Let's say now we have medical images, we have facts, we have time series, we have type of data. So basically we have like several different kinds of modalities. And in literature, usually if we have each type of modalities, then we have a particular pre-trained model for that. Let's say for images. Say for images, we have CNAM, and for text, we have transformer, and for like time series, we have like RNAMS, and so on. And the goal is that we want to have a universal pre-trained deep learning models for all the type of modalities. Basically, we hope that we're able to extract all the important features from different modalities in our pre-trained models. And so, in our ongoing work, we basically utilize bit of expert. Utilize bitch of expert to combine the type of modalities in the healthcare data applications. And our models also achieve step-by-step results for the multi-models task in the healthcare applications. So I just mentioned three typical applications of Meetup Expert from scaling up massive new networks to multi-task learning and to the multi-model. Learning and to the multi-model applications, but then the principles of these models still remain open, even though they are very popular in the recent years in machine learning communities. And from my perspective, there are four particular challenges with these models that still remain eye-opened. The first one is about heterogeneity. Let's say you have multiple experts. Each expert basically captures Expert basically capture one information about the image or one information about the text and so on. Then, how can we understand about the expert? How can we leverage the heterogeneities of the expert to improve the performance of the models? And the second question, also very important, is about model specification. Let's say, how can we know that the metric expert that we use in the massive neural networks able to Massive neural networks able to tell about the shape, about like the like, to tell about the underlying distribution, underlying like distribution of data. So what if like the meat of expert we use actually symbol to somehow like predict distribution of data? And the final two directions that also remain missing in literature is about how we can guarantee that the optimization method or The optimization method or the sampling method, all the algorithms that we use actually like scalable and stable. Can we guarantee that the let's say the actually stochastic grain descent that we use when we train these metrics but on the massive neural networks really can guarantee like the good local minima, right? Or can be absolutely stable and so on. So, surprisingly, even though Wikipedia expert widely used a bit machine learning. But wildly news, machine learning in the last few years, in like recent years, all of the questions I mentioned over there are still remained open. So for today's talk, we mostly focus on the heterogeneities of an SPAT. And when we talk about SPAT, we basically mention, we actually try to understand about the parameters. Because the parameter, we try to parametrize each aspect by separate parameters. By several parameters. So the goal is that if we're able to understand about the parameters, we're also able to understand about the behaviors of the SMOT and their heterogeneities. And one key component, one key things that basically underlies these massive new networks is that we always over-parametrized the digital expert. And we want to see the effect of over-parametrization on the behavior. Parameterization on the behaviors of parameters and experts when we use them in our training machine learning models. So, in the talk, we try to understand about, we try to provide some theory on the parameter aspect estimation under a popular Yao Shimito expert. If you have any questions, please stop me. So, now we're going to go to the backgrounds on To go to the backgrounds on which of experts and also the key components of our theory, the for noise-based geometries for understanding the parameters and the expert estimation. So far, there are actually a key distinction between like mitch of expert and the traditional mitch models. So we have like we only have like literature like last few years on using optimal transport. Transport for understanding the parameters in mutual models. However, it turns out that optimal transport is not sufficient to capture the parameters as per estimation in the mutual aspect. And I will try to show you why it's the case that optimal transport is not able to get us to the rate of parameters and how we can construct the for noise-based losses to also. Noise-based losses to also not only able to overcome the limitations of optimal transport, but also able to characterize precisely the behaviors of parameters of estimation in the mixer of expert. And the key points that I want to convey in the talk today is for the complex mixture model like Mietrop Espot, then the four-noise-based loss is natural and powerful for these models. And also, one thing. And also, one thing, when we also adapt the foreign noise-based losses for matrix models, we basically also recover a better local rate of parameters than optimal transport. So, let me try to give some background on the Gaussian expert. So, let's say we have a data, let's say Y1, Y here is the response variables, and X here is the covariate. So we have Y1, X1. So we have y1 x1 up to yn xn from rd to r. So it means that the y respond variables here in rd and the various oh sorry the y here is actually r and covariance in rd so i just basically get the wrong orders so the covariance here actually in rd and the respondent variables in r and they actually come from the conditional density function of the following of the form of the summations of index from one to k0s Of the index from one to k0 of the submax of the linear function beta one ix, but the beta is zero. And the submax function over there, basically just a weight, a gating function for the virtual expert. And then this submax gating function, we tempt the Gaussian distribution of y event the linear function of A i and B i and also. And BI, and also have a scale of sigma i. So, this basically is the simplest possible form of the mix of expert. In vertical applications, then the linear expert AI to export BI over there are usually replaced by new networks. But here, we only focus on the simple case, just that we get insight for theory. And the whole theory that we develop over there can also be modified. Can also be modified, also like translate to the new networks export function. And because in these models, the only thing that we know is the gauge distribution, I mean the FY given the S part. And there are several unknown parameters. The first one is the submax weight and biases from the submax function, submax leading function, right? Exponential of like Exponential of like beta 1i x plus beta 0i and we also have the aspect parameters a i b i for the the linear uh s for location and we also have the scales sigma i and finally we also don't know about number of aspect k0 so we see that we have several parameters to consider in these models and in the practical application we have ways more number of parameters here Weighs more number parameters here, but I mentioned we're trying to simplify the models just that we can gain insight into this problem. And there's one key things I want to mention here is that we formulate the parameters, the submac weight, biases, aspect parameters, scales, and number of aspect k0 into a single mixing measures. G, that I know over there. Where is also have K0. Where is also have K0 components, beta 1i, basically like the submax weight, AI, BI, sigma i come from the expert and the scale. And somehow we turn the bias of the submax into the weight of the mixing measures. And we have exponential of beta zero i. So this basically is used at the weight of the mixing measure. And one thing that we can see here that that mixing measure. We can see that that mixing measures is not necessarily to be a probability measures. We are the summations of the mixing weight of the exponential of beta 0it over there is not one in general, right? So it means that our mixing measures can be unbalanced, mean that the summations of the waste is not one. And that's also creates some challenges when we try to understand the behaviors of the missing. Behaviors of the missing measure tree. And you may ask the question: like, why we model, why we actually formulate the parameter in terms of that particular missing measure tree. Like, why we actually use exponential of the bias for the submax function as the weight for like the missing measures. And in principle, the idea is that the weight exponential of bias basically controls how well. Control how well, like even when you over-parametrize, like the visual expert, it's actually trying to make sure that the larger models can converge to the smaller one. And this exponential of the bias basically try to have the larger model to converge to the smaller models. And the key is also the key difference between the mid-passport to the mix of models. To the mixture models because for mixture models, we only have like the weights that independent of the covariate. But here's because we have the submax structures of the gaining function. And that's why now we need to take into account that. And we use the exponential of the bias from this sub-max function, a is the weight conviction measure. And from here, we already can see that if we use optimal transport. That if we use optimal transport to try to understand the behaviors of the mixing measures, that is no longer sufficient. We need to have other types of divergence to capture the behaviors of the mixing measure. And from our theory, from our proof, we also demonstrate that that way to formulate the parameters is also sufficient and necessary. I mean, that is basically like a very important way to be able to capture. To be able to capture the non-linearities of the submax function for the degrading function. Okay, so the goal is now that we try to estimate the unknown parameters, beta I0, beta I1, AI, BI, CPI I. Beta I0 and beta I1 are the expert are the scaling parameters. Gating parameters and AI, BI, sigma i's are the expert parameters. And we also don't know about the numbers of expert k0. So basically, like in the form of a mixing method gene here, everything is unknown. And we try to estimate all these things together. And we adopt the over specification settings, mean that we somehow choose very new. Choose very huge, we choose like basically a given number of experts. Let's say we only have three experts for the real models, but we use like 1,000 experts to fit that. And it's also like what people are doing in machine learning. They're trying to use a huge amount of experts to fit to actually in their massive neural networks. And so we assume that the choice of the number of experts that you use K here is most. K here is most later than the real number of Spot K0. And we also use like the MLE to somehow estimate the two parameters. Let's say we have G and hat, basically like the MLE, where we try to determine all the possible mixed measures with at most K aspart for the allot function. So with me that So, which means that we search over on the possible like possible models with most K asport, and we try to find the best parameters among these models somehow to maximize the function. And the next question is, if we use the emuli, G and hat over there, what can we say about the behavior of G and hat? The behavior of J hat, how well that one can approximate the true mixing measure G that captures all the parameters that we want to estimate. So before we talk about the convergence of the MLE to the two parameters, the first thing that we need to guarantee is the identifiability of the Gauss symmetric expert. So that results were established 30 years ago. 30 years ago, and he had to try to revisit these results. Especially that if we have a two-conditional necessary function at two mixing measures, G and G prime, where G prime here only have K prime numbers of aspect, then we can demonstrate that if the two conditional density functions are identical for most surely X and Y, then we can guarantee that the number of S was going to be identical. Identical and up to translation of the bias of the submax function and also of the weight of the back function that we're able to have the GPRAM to be identical to the two parameters. So, between that, the identifiability that we have for the Gaussian picture expert are up to translation of the bias and the weight parameter. Bias and the weight parameters of a submax function. So it means that we have some T1 and T2 over there. So we can find T1 and T2 such that we translate the bias and the weight parameters such that the G prime here identical to the translation of the two missing measures along the directions of the bias and the weight of the submax functions. And in practice, there's one way to overcome such identifier purity. Overcome such identifier purities means that we can define on the submax function by the first weight, by the first gating function of the SBAT function. And by doing that, we're able to eliminate the issues of translation in the identifier puberty. But the reason that I mentioned the T1, T2 here, because for the original training, I think you For the original training, other use in machine learning and only like an editing function by the first editing function. They basically keep the submax aiding function to be like in the original form. So that means that their parameters that estimate in their models are all identifiable up to translation. So the translation identifiers. So, the translation identifier I mean the identifier be up to translation also means that when we define a divergence between like the MOE and the two parameters, we need to actually take into account this translation. We're going to also somehow demonstrate that we have a very efficient way to do that. Now, we're going to go to the recipe for establishing the rate. For establishing the rate of parameters, like how can we get the rate of the emulator to the true parameters? So, one inside that even the models is very complex, but we can guarantee that if we estimate the conditional density function, then we still have the barometric rate, where H here stands for the heading distance. So, we mean that we basically still able to guarantee the barometric rate for the additional estimation. Additional estimation of even the MLE. And so it means that if we're able to establish the low values for the gap between the condition of the acid function between like the MLE and the true missing measure in terms of some divergence, DR, okay, something that we want to develop between like the MLE and the two missing measures. We are here, it is some power. R here, it just says some power, some orders, and r here is not like a single values, actually, it can be a vector, and most of the vector. We're going to actually try to, I'm going to actually try to, I'm going to introduce that R later. So basically, we try to find a way to develop that ER, right, the vertices, and also find that R, such that we can learn about the conditional gap in terms of the parameters gap. Parameters gap and the high-level ideas to establish just know about is based on using the expansion and the details of the expansion is that we try to do the expansion of the finished distribution of G and hat around the finished distribution at the QBC metric GG. And after some like combinations of coefficients based on the structures of Based on the structures of the bitch of S but, we able to find, we able to somehow guarantee that some coefficients are not going to vanish zero. And that allows us to get to the values of the power R of the divergence. So this is basically the key ideas that we're able to use the expansion to get that divergence and to get also the power R. But then let's go back to that one. So now what is the right choice of DR? What is the right choice of DR? Because what are the challenges that somehow refrain us to develop DR for a long time? So let me try to somehow recall new simultaneous. So we have MOLI, G and hat, and the two basic measure G. So both G and hat and G here are not probability measures. Okay, so it means that optimal transport cannot be used. Transport cannot be used to capture the difference between the MOE and two missing measures. It's worth saying. So it means that all the previous ideas we have for optimal transport need to be changed to some different choice of the versions. And then comparing to the traditional mid-chip models, there are also more complex interactions among parameters in mid-Sport. So let's say if we define So let's say if we define the function u of y, even x, even the export parameters a, b, and also like the weight of the submax beta one, and also sigma scale by the exponential of beta one triangle x. This exponential of beta one x basically comes from the submax function, a submax editing function, and fy given the linear aspect basically comes from the site. The estimate. Now we basically demonstrate that there are two hidden PDEs corresponding to the parameters 8, beta 1, a, b, sigma. The first one is when we take the second derivative of the u function respect to beta 1 and b, then it's linear dependent with the first odd derivative of u and a. So it means that the beta 1 and b here somehow interact with a. Here, somehow interact with A through that PDE. And a second one, also populous for the Gaussian distribution, is that the second-order derivative of the bias of the expert also interacts with the first-order derivative respect to this variance. So we have like two PDEs respect to the choice of the sub-magnetic function times the Edit function times the Aussian distribution of Spot. And this means that because of these complex interactions, when we go to the expansion, we're going to have a very complex interactions of these light derivatives. It's been that different parameters, let's say beta 1, A, B sigma, may have different behaviors, different rates of convergence. So it means that they basically have two challenges. The first one that The first one is that optimal transfer cannot be used. The second thing is that different parameters may have different behaviors, different rates. And our solution here is we preserve the challenges by developing the novel for noise causes among the missing measures. Okay, so here is our idea. Let's say that we have the red points at like the parameters for the MEL. Like the parameters from MLD, okay, and the blue triangles are the two parameters. So let's say, uh, in the first porno itself, basically, we have this, I mean, we have the first poronoid cell over there, and this is second poronoid self, and the third one, and so on. And so, in the first porno self, we have two parameters from the MLE, and the triangle here is basically the first parameters of the two missing measures. Two mixed measures. So the same thing for the second and third and so on sales. And for the foreigner cells, what I mean is that we try to find the parameters of the expert that are closest to the corresponding parameters of the two parameters. Let's say for the first parameters, we try to find all the annually parameters. All the annually parameters that are closest to that two parameters, comparing to like all the two parameters. And based on doing that, we're able to create multiple phono itself based on the two parameters and also based on the MOLE. So we see that if we look at that, like a single form, like from when we partition the parameter space into multiple foreign cells, and in each phone cells, we also. and in each form of cells we also have like a single two parameters sorry we have two parameters and we also have like um like the uh the parameters for the MLE and in some areas we only have one MLE parameters but then in some areas we have multiple MLE parameters and it suggests that if we only have one like prime one MLE parameters Prime one MLE parameters in for no itself, right? This being like we only have one way to match that MLE to that parameter from MLE to the true parameter, right? That's why in this case, it seems that for the foreigner itself that only have one parameter from MLE, which can still guarantee the parametric behavior. But then let's say if we have more than one MLE parameters in the fournoid cell, let's say in the fifth cell, we have three MLE parameters. Are three annual parameters. So we have like three ways for these parameters to get to the two parameters in the cell. And the convergence of D3 to that, like a single two parameters, may lead to like a slow convergence of the parameters. And to capture to capture like these behaviors, we introduce that like a formal loss function. Like a foreigner loss function, where we also define like the fast rate for the photonoid self with only one component, and also like slow rate based on some R depends on the kinetics of the photonoid self or for the photonoid self with more than one components. Then, based on definitions, we can demonstrate that we can lower bound the conditional distributions of a Distributions of two different mixing measures in terms of that foreign loss between the two mixing measures with the R bar order. The R bar here is the actual factors of R bar, the carrier analogies of phono cell one and up to like R bar, the carinalities of the phono cell K0. Where R bar M here is the smallest number such that the forward system equation that I have over there doesn't have any Over there doesn't have any non-trivial solution, and that system comes from our argument with the expansion, and also where we try to take into account the BDEs of the parameters. And we can demonstrate that when we only have like one, like we only have one extra parameters, then the R bar M is four, and when we have like M is two, then R by M is six. So I mean that when So it means that when the foreigner cell only have one component, only have like two components, then we have the race, we have like R by four, it's four. And when the foreigner cell has three components, then the R by M is six. And based on the conversion rate of estimation that we have earlier, we also translate to the rates log n by n, log n by n for the for loss function of the MLE to the two mixing measures under the To two mixed measures under the order R bar. And that result basically means that the rate of submax weight and SPAT bias are going to be n to the minus one divided by two of the kinalities of foreign cells, HA. And n to the minus one half for those four nodes cells would only have one component. So it means that for sub-max weight, and then if for the forenoid cells would have more than one component, then the rate. Would have more than one component, that the rate is going to be super slow. And the same story also for the expert weight and the variances. They also have a slow rate, but also like try faster than the one from the submax weight for those of four noise cells with more than one component. And finally, for the race of expert biases, then they are biometrics. So things like based on using the formal loss function, we're able to get a very precise Get a very precise behaviors of the rate for each parameters in the bitch of SPAT. But then it turns out that the order of bar we have over there is not tight. And in the current works that we try to extend one, we demonstrate that a sub-max way did have like a most faster rate and most one for those in the four-noise style with more than one component, more than one parameters. And it's only hold for the sub-macrast parameters. Hold for the sub-max raise parameters, but not for the sub-max parameters. But not for S-book parameters. So it means that the S-bot parameters still suffer from very slow rates of convergence. And we also develop comprehensive conditions on the SPAT function such that they have a much faster rate on estimating the export. Let's say if we have a sigmoid activation on the export function, then we're able to have. On the estimate function, then we're able to have like a much faster rate of estimating the parameters comparing to the linear estimate. Since I don't have much time left, I also try to wrap up my talk over there. So there are two major directions. The first one is the top case for MOA. So far, we only talk about the case when we use all the experts. But in practice, when they talk about massive deep learning models, they actually only consider like sub K values of. Of K values of the submac rate. It means that even you can see the like, let's say 1000 experts, but we only choose like two or three experts with the highest submac rate and we can truncate the rest. And that's also the ideas we will use for scaling up the massive debugging models. And recently, we also established a theory for the effect of using a top case spa MOE for the behaviors of the parameters and also expert. But then without any Also, expert. But then we're not able to understand how to choose the right choice of sparsity level and also its influence on the performance of deep learning models. And finally, our theory so far only for one layer of each of experts. Extending our theory to multiple layers, each of expert deep learning models is still very important and remain open. And that's all for my talks. Thanks everyone for listening. Listening.