To back it again to this amazing spot on earth and to the opportunity to give a talk. And indeed, I'll be talking about recent work on generalizations of the joint replenishment problem. And this is joint work with my PhD student, Guru. And it was done while he was actually an intern at MSR Bandalur last year. And it, yeah, let's get to it. So, the motivation for this problem really comes from an emerging trend within the design of optimization models for decision making. And that really is a sense that with the advent of really automated decision-making in a range of different settings, we want to. We want to actually inject a notion of fairness into how decision-making takes place. And this, you know, rears its ugly head in all kinds of different settings. And if one looks at algorithms that are not attuned to this aspect, one can see dramatically bad things happening. And this has been studied in a range of settings already. Clustering, I mean, in many senses, the work of stable matching. I mean, in many senses, the work of stable matching all goes back to one notion of fairness, and there are a range of different issues where that can happen. So let me start by then switching gears and just remind you or tell you for the first time what the joint replenishment problem is. It's a classical problem in maintaining inventory. So you have a discrete time horizon and you have a set of n commodities. Commodities. For each commodity, you have a set of demands occurring over time. So each demand is really a pair, a commodity type I and time T. So in this case, in the input that I've displayed, the shapes indicate which kind of commodity is impositioned on the timeline according to the time. And the nature of the problem is to stress the trade-off. Stress the trade-off between ordering just in time and not having to hold on inventory but being willing to incur repeated ordering costs versus holding off an ordering but then balancing that against the cost of holding extra inventory along the way. So that's the nature of the model and it's really a coordination model. If there were only one commodity we'd get back to something known as the lot sizing problem which is much more well behaved. Which is much more well-behaved. But really, though, it becomes a question of serving the demands. That we're going to think about having replenishment orders that are going to occur periodically throughout the time horizon. And for replenishment order, we first make the decision we're placing an order, and we're also saying which commodities am I going to order at any given point in time. Once I've made the decision, I'm not going to worry about. Once I've made the decision, I'm not going to worry about, in some sense, a capacitated version of this. I can order as much of, satisfy as many demands as possible. It's really the coordination of the joint ordering costs that are key to these problems. So, for example, as I look at that input and think about one possible solution, I might decide to just choose those seven points as my order points, or we'll sometimes call them just general orders, to distinguish from the fact that for each of those order points, The fact that for each of those order points, I also need to specify which item is there. There's something going wrong with the Zoom, but I don't know if anybody's on. I don't see anything on there. Okay. And it looks okay. Okay. So, for example, if you think about the triangle commodity, I've ordered it exactly to meet exactly at the point of meeting. Exactly at the point to meet that demand, and I'm not ordering it here simply because maybe that placed enough of an order to go forward so it's to meet the demand there, and I'm not adding that recurring cost for ordering that triangle. So they're going to be costs associated with both, I've made a general order decision and then for each item overall. So a little bit of notation that the join That the joint ordering cost it will denote by K0, just collectively over all of them. And for each of the commodities, the individual item orders I'll denote Ki for commodity I. And the goal is to serve typically for the joint replenishment problem. The goal is to serve all demand at minimum total cost. Is the question of the problem in this sort of traditional straight kind of set of? Straight kind of setup clear. And I'm going to think about capital D as being just a set of demand points. I'm not allowing multiplicity. I'm not allowing how much demand at a given point. All of those variants can be bundled in, but this is just notation of the. Now, in practice, life is not so simple. We don't actually meet, you know, if you think about suppliers, they don't actually meet all demand. All demand. Messages like this: sorry, we've got to stock out or we just cancel the order. And so you might think about, and this is sort of more traditionally been thought of in the concept of outliers, or we'll call them rejections, that a given demand might be rejected and actually not served. And of course, that might change the trade-offs. That as you sort of think about what the solution might be depicted, I'm now, you know, some. I'm now, you know, sort of saying, oh, well, forget those. And these are the orders that I actually do need to place. And if one thinks about optimization models, instinctively there are two pockets that you might dig into. You might dig into the pocket that says, well, we'll just charge for a rejection. And that just comes bundled into my cost. But another thing, which may be actually more palatable, is that I actually just have some, I'm willing to tolerate this. Have some, I'm willing to tolerate this many rejections at a given time, and we'll think about that as how we think about the planning process. Okay, so there, keep in mind that there are these two kinds of things. But I said this was about fairness, and indeed, here comes the fairness. And this is a really wonderful graphic from a paper of Swati Guptas recently, showing the correlation between which Between which market is experiencing those stockouts and which markets are not experiencing those stockouts so much. So, one might want to then add to your model some notion of fairness of when I'm going to reject your order, how much various communities suffer as a consequence of this lack of service. And so, one way that we can do this, and I'll boil this down, I say in a somewhat more general way first. Somewhat more general way first. Then you can imagine we have a set of C features, and each demand point has a weight in each feature, and maybe a rejection penalty. Maybe we're going to add both the constraints and there. We'll dig into both pockets simultaneously. And then we'll give a rejection weight, a bound on how much we can tolerate for each feature. So, for example, one special case of that is if for each For each demand point, we have a unit vector for the weight in each feature. That really means that we've identified a color class, a market segment that each demand point belongs to, and we're going to keep a bound on how many rejections we have restricted to each color class. And this is a kind of model which has been studied in exactly this setting as a model of fairness, particularly in clustering problems. But really, this is very apt for this. Really, this is very apt for this setting of inventory models. So, now, of course, the problem becomes a little bit more complicated. That in this case, I give a number of colors. And now, not only am I thinking about the rejections, but I'm thinking about a constraint with respect to a particular bound on how many rejections were getting coincidence. So, that's the problem I want to address. This may be too small font. Hopefully, those many of you have. Hopefully, many of you have better eyesight than I do. My apologies. I'm going to set up a standard integer programming formulation. This will help reinforce, if you didn't catch the model now, maybe I'll do it again. We have various kinds of key decisions. Let's start that. Where do I place my general orders? I'm going to have decision variables y. orders. I'm going to have decision variables y sub s for each order point s, that you know, one meaning I place in order. For each item type i, I'm going to have a corresponding variable y sub s of i, and that's going to be one, meaning that we're placing an order that that commodity is part of the general order at s. And then I'm going to have service constraints that say that if I think about a item demand point IT, that that's served from point. That that's served from point S, and that's going to be denoted by Xit. So now everything is pretty simple and very much routine. The objective function is to minimize the general ordering costs plus the item ordering costs plus the total holding costs overall. So H sub S I T is the cost of incurred for holding the item in inventory if it's placed at time S and serving a demand at time T. And serving in demand at time t for commodity i. And all I'm gonna assume about them is their monotone. That if I give you a super interval, that's gonna be at least as much as the summit. Constraints, every demand has to be satisfied. We actually have to serve every order, and the usual kind of facility location. And after all, this really is a kind of asymmetric facility location on the metric line in disguise that we're placing facilities that is orders. No orders, and so if a demand is served from S, well, then of course we actually must order that commodity. And if we've ordered a commodity at a given moment in time, we have to replace the general order. That's the full energy programming formulation, absolutely routine. Now, of course, we're going to have rejections, we're going to have outliers. So I'm going to introduce a variable RIT, which indicates for a given demand point IT, did I reject that order? And we do the And we do the very simple thing of just, well, either it's rejected or it's serviced, one of the two, and we add that into the constraint. And if I simply just have a rejection bound, then I have a constraint that way. And I can now indeed put a color-based rejection bound or a feature-based rejection bound and a penalty that captures into the more general constraints. All absolutely standard things. And so this is. So, this is just an anchor for what the problem is that we're talking about. At this moment, you're likely to start thinking, oh, he's going to do purely LP-based rounding, and that's more or less true, but not entirely true, because we'll discover there are some issues along the way. For the rest of this talk, I really want to focus on a simpler special case that gets rid of the mess so that I can get a few of the key salient algorithmic ideas out. There's a lot that I'm going to be hiding under the There's a lot that I'm going to be hiding under the hood. There's a lot that I'm going to be hiding under the hood algorithmically, but I want to at least get some of the key features about what works in this case to get a good bounce. And just imagine, in some sense, a feasibility point of view, that imagine that I'm not going to worry about holding costs per se, that I just want to say that they don't last forever, they're perishable. And so for each commodity, there's some time window. Some time window to meet a demand, I have to order it at most this much in advance. And so that really amounts to the holding cost either being zero as long as it comes within that interval, or it's infinite if it came too early. So that's going to be what I'm going to focus on. And indeed, for all the variants that I've talked about without colors, there are known results as well that can come from a variety of things with relatively good context. Variety of things with relatively good constants that have been obtained. That either the rejection penalty version or the rejection bound version, they're perfectly reasonable constants that they're. The title talked about improved, and indeed for the monochromatic case, the suite of tools that we're bringing to bear brings. That we're bringing to bear brings those constants that were four-ish down to a little bit sub-3. But that really, I mean, as much as that's a nice byproduct of what we wanted to accomplish, what we were really after was the sort of the multicolor thing to go after the fairness constraints, for which no constants were known, and for which there are good reasons, and I'll give some hints, that the standard techniques where we really needed to sort of bring to bear a broader array of techniques in order to get constant values. In order to get constant values. And as you can see, the constants that result are not so bad. And in fact, the coloring generalization doesn't weaken the constants, given the GARN set of tools. Putting this in context and giving sort of these token credit to some of the work that predates this on the various generalizations and the various kinds of coloring constraints. Just a quick slide. Just a quick slide there. I won't go through them in any detail, but just to give credit where credit is due, that this comes out of a swath of ongoing work that has developed many of the foundations that we built upon. So, if I go and consider the perishable case, in that the bad news hits us right ahead, even in the case of Even in the case of one commodity and replenishment, the LPs, which served us really well for the version without outliers, already have an unbounded integrality there for really stupid reasons. If I simply say that I can reject, Say that I can reject, you know, I have T demands, one at each point, and I can reject all but one of them. I can just smooth out what I'm actually serving across and end up with an optimal LP solution, which is a factor of T off from the optimal integral solution. So, life is bad if I just think about what this LP, even in this very, very simple version, could possibly do. Without rejections, and to give you a flavor of a key technique that comes back to the LP, there's a nice way of thinking about what the fractional solution buys you in a rounding technique, which actually will be a key to one of the main things that we'll talk about algorithmically. And imagine that if I think about, again, borrowing. Again, borrowing on the bad case that I just showed you with just one item type, that of course it's going to be if you only have one item type, the variable for when I order the one item is going to actually coincide with what the general order is. So I really only have these variables, y for ordering. And imagine that I just simply think about those fractions. Just simply think about those fractional variables and lay them out on a timeline. And think about that, in general, the LP in an amortized way pays for an order by having a sum of order points that sum to at least one. So if I think about these as being the fractional values, that I can think about when do I hit one, when do I hit two, and so forth, and placing a general order. And placing a general order at each of those points. And in many ways, for the start of LP rounding for the joint replenishment problem, straight off, this is the starting point of virtually all the LP rounding algorithms to think about it in this way. And that's great. And so that would mean that I place the orders just initially and so forth. Essentially, thinking about which of the fractional variables those integer points intersect and being sure that. intersect and being sure that I'm placing sufficient numbers. Now one might ask oneself well one other additional technique that gets used in sort of that toolkit is in fact that argument is very flexible with respect to a random shift of rather than thinking of it time zero when I hit zero and one and there, I can shift by a value delta. Shift by a value delta chosen uniformly at random. And then, as I think about the expected cost of that fractional value of y, as I think about that random shift uniformly between 0 and 1, it's a probability exactly equal to that fractional value of whether we actually choose that point as an order point. And so I get that the expected cost of the The orders exactly match the LP bound. And at this point, you should be really worried about that integrality gap because you kind of think that, well, if you think about the version with rejections, and indeed this example was set up to get you to think about rejections, and that's why they're actually less than one. I will, that random shift will That random shift will actually conform to the rejection bound in expectation. And in some sense, what we're seeing here is this strong tension between achieving the constraint in expectation versus achieving that constraint. It may achieve it in expectation, but none of the solutions actually come close to achieving it in actuality. But this is an LP-based approximation algorithm talk, and so how Algorithm talk, and so how are we going to get around that? And again, a sort of standard trick that we're going to let sort of rounding take out the base case, that we're going to think about separately, imagine the case where we don't make many orders, some large constant number of orders. If I only make a large constant number of orders, then I can do that option. I can do that optimally. It's expensive, you know. The polynomials that are involved, this is, you know, do not confuse this with practice. But we can do that optimally. And otherwise, I can guess sort of M order points, and I can guess the M most expensive individual item orders. I can fix them, and then I can worry about using the LP to. And then I can worry about using the LP to solve everything else. And that's going to be the framework in which we will be able to do that. But in fact, in sort of the next level of tool that will be brought to bear is that we're going to use that LPN, that same timeline trick, to actually split things into two separate instances, where we're going to have different control over the two instances and apply different algorithms to those two sub-instances that are going to separate out, partition the demands into two different coincident timelines. Coincident timelines that we're then going to build a solution by taking the sum of the two parts. So, this maybe gives a sort of more graphic view of what's going on in that algorithm, exactly as I said, that we're going to do this guessing that's going to strengthen RLP. We'll generate some what I'll call initial general orders that will allow us to partition, and then we'll merge the solutions back. So, let me talk about the splitting. So, the splitting, here comes that. So, the splitting, here comes that same picture again. That now I'm going to use the same general order point fractional values and say, well, every time I add up to one, the LP has paid for a general purpose order. So let's place an order there. Okay, so we have those initial orders. And now for every demand point, they come in. Demand point, they come in one of two flavors. They either come with the fact that they actually are feasibly served by the orders I just put in place, or they're not. So what I'm going to do is say I'm going to put in instance one the ones that could be feasibly served by this initial general orders. And, but of course, the key thing is I have these rejection bounds. Either model. Either monochromatic or color by color. What I'm going to view as sort of what it gets to have is its rejection bound is just what the LP gave for it. And so if I'm going to still maintain the rejection bounds for each of the two sub-instances, then collectively when I put things back together, I will still maintain satisfying the rejection bounds overall. So this gives me this partition. And one of the other Partition. And one of the other good things about the second input is that it nicely delineates the demand points that are partitioned, that are within, into sometimes batches, where the LP ordering is bounded by less than one every time, because it's, of course, that's how I created those initial general orders. Now, the next natural thing to think about, and I The next natural thing to think about, and I kept going back and forth over one or two this slide or that slide. So, this summing trick of thinking about the general orders, it's also natural to think about doing that for the item orders. That you would like the LP to pay for an item order by having the cumulative sum of a given item to reach a given value. The problem is that if I place the And if I place those general orders, those item orders, they're not going to coincide necessarily with the general orders, and we can't necessarily afford that. So one way of thinking about that is that as I think about the fractional solution, I'm going to think about, and sort of the right picture to view is the sort of overlapping hands. I know if I look at this overlap. At this overlapped fractional piece, that it's contributing perhaps to an order going this way or to an order going that way. But in any event, there must be a general order within one this way or within one that way. And if I just simply double that fractional solution and think about it going forward or going backwards, I'm hedging my bets. And so is to know that I can actually maintain a feasible solution for that instance problem. And so I have. And so I have an IP that I now no longer have. I mean, by having these general orders and thinking about those are being the general orders, I've done the usual decoupling that allows me to go from something replenishment style to something lot sizing style. However, I need to still keep the rejection costs and I need to bound things relative to the LP. So I need an LP rounding strategy kind of approach so as to maintain the cost there. And this we can actually do. The cost there. And this we can actually do by a kind of iterative rounding of just bounding the there's a bounded fractional content. So that gives some flavor of what happens on instance one. Instance two, and I don't have time for this, is going to rely on pipage rounding. I was going to do a review of pipage rounding, but I don't think there's time. I mean, the key idea, I'll say one sentence. One sentence. The key idea of pipage rounding, at least in the way we're imagining it, is that you have two fractional variables and you want to maintain some invariant. In this case, what we're going to be interested in doing is maintaining an invariant in terms of the overall cost. One is going to increase and the other is going to decrease, and it's going to be feasible to do it in either direction. And so you're going to pump one variable in one direction, the other variable in the direction, until you hit integrality. That's going to create. Until you hit integrality, that's going to create another integer variable and you're going to be able to iterate. We're going to do this in a more sophisticated way in that typically pipage rounding is done exactly as I said, this in terms of one variable up, one variable down. We're actually going to define sets of variables. So forget about this interloop here this entire. We're going to define sets of variables. Going to define sets of variables that are going to allow us to take each of these batches, which are decoupled, and impose a set of balance conditions that we can show that given the system of constraints, there exist solutions. And that's going to actually define two sets of variables which we can push in opposite directions and thereby maintain all of the Thereby, maintain all of the invariance that we need while gradually driving some of these variables to integer values. And I know that's just a hint of it. Again, a lot more complexity under the hood, but this allows us to do that. If you can't clean up enough, then you have to do some other cleanup that, but that gives you at least a hint of the flavor of what goes on for instance too. One thing, you know, sanity check. One thing, you know, sanity check, what we're doing is necessary. I mean, in some sense, that this guessing up front is exponential in the number of colors. So it only works for a bounded number of colors, but there's a good reason for it. Variable number of colors is set cover hard, so we can just get a mapping. You aren't going to do better than logs. One of the things that is nice to sort of reflect on is that. Is nice to sort of reflect on is that we developed this toolkit so as to be able to handle the harder generalization of the constrained multicolor, multi-feature way. But we did get improved constants for the things that were previously studied. It would be good to have some sense of where did that improvement come from. And one of the things that, in addition to everything else of sort of why things are nice overall, Of sort of why things are nice overall is among the details that I definitely kept under the hood is that demands that are rejected from instance one, we actually give a second shot in two. And this gives us more play in terms of the balancing between the performance guarantees overall. And just as we've seen time and time again in the facility location literature, that if you have these multiple competing That if you have these multiple competing pieces, you end up with this parameterized set of guarantees, and you have all these levers to balance to optimize your constants. And this just has given us that much more degree of control. So the main thing I just want to return to is that I do want to stress that this notion of thinking about the kinds of settings in which we really should care about fairness and trying to understand where fairness. Where fairness constraints should be incorporated into a model is something that, as algorithmicists, we should be thinking about and trying to understand that what kinds of techniques and there. And in some sense, at a very high level, you could say that this sort of guessing phase can be broadly interpreted as sort of a decoupling of a base set of requirements being met and then optimizing over that. Then optimizing over that. And that may be a takeaway message that one could take to a practitioner and say how you're thinking about trying to guarantee what kinds of metrics and trying to ensure that you're serving the public well. With that, I'm happy to take questions. Thanks. We can take one question. So, the standard joint replacement, what about these rejections? It can be thought as a. Rejections, it can be thought as aggregation on two levels, but then for multiple level trees, you still get constant factor approximations. I'm wondering, can you take this business of rejections also to the multi-level framework? Is it interesting? Is it open? Yeah, I would. My guess is yes, but I would want to be so bold so as to say that without harbor about it. Thanks. Thanks again. We'll get right on track at four.