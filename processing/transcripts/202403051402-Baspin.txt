And consider a state as a parameterized instance from the parameterized family where you have a hydrogen parameters and then if you can estimate the hand that proves in multiple states, then essentially you have the information and have the best thing. Yeah, but why? Because what we're interested in is the current scenario so that trace flow all will be. Right, so like we know I I don't go exactly that but if you do uh if there is the observable that is able to extend the parameter that you're interested in and if the error mitigation is one of enough to mitigate the error so that you can estimate value specific number shadow sample. If you have your channel sample, then you should have a high-estimate paradigm or uh parabolic construction factor. But you cannot make the parameter at all, so that that's the thing that's also interesting. Right, so I guess uh let's see, right, so like in the new process, the uh Right, so like in the new process the uh block measure, right? You expand your state how it expands. So you should start a mentality, then you essentially rip to the nation actually. So if you mean the addition tasks to test the graph, I think that's the company general. I guess you can give it something actually more general, but also faster than possibly. Yes, one in the morning. So those things are just a slightly different slide. Well, I'm going to have to make it a little bit confirmed to strip people out. Yeah, I'm so confident that I should take it. Okay, so let's start with the next talk of the afternoon. And it's going to be given by Johanni Foskir, who's going to talk about the layer codes. So, please. Hi, everybody. Thanks to the organizers for inviting me and organizing this really amazing workshop. Today I'm going to Much up. Today, I'm going to tell you about layer codes, which is a way to saturate the read-bounds in 3D. I'm going to explain what all this means. This was work done with Dominique Williamson when he was back at UCID and now he's moving to IBM. Let's get started. So, I want to talk a little bit about stabilizer guts. How do they work? So, imagine that you have a bunch of qubits, and one of these qubits is corrupted by another. One of these qubits is corrupted by an error. One thing you might want to do is you perform some measurements in your system. These measurements, we call them checks or stabilizers. And the hope is that at least one of your measurements will detect that this error happened. One thing that we usually require, and they're always the case here, is that all the measurements you perform, they have to communicate with each other. Great, there we go. And so these measurements are going to dictate the structure and the properties of the codes that you're using. Different measurements are going to give different geometric structures to these codes. So, for example, here you have the 2D toy code, which probably most of you know about. Here, you have like the Know about. Here you have like the logical Z operator. So a logical is an operator that is not detected by the measurements that you perform. And here you have the 3D Tor code. And you can see that the 3D Toro code has a different kind of logical operator than the 2D code. So a natural question is, to what extent do the geometry of these codes impact their structure or performances? An unfortunate fact is that local codes are pretty restricted. And to talk about that, I'm going to need to introduce a little bit of notation before. So when we talk about a code, oftentimes we're going to characterize it using three parameters. We're going to use n, the number of qubits in your system, k, the number of qubits that you're going to be able to protect with your code, and d is going to be the size. And d is going to be the size of the smallest error that you cannot detect. And once you have a code that has these parameters, oftentimes you write it in like these little square brackets. And a famous result in the theory of stabilizer codes is that when you have a code that lives in 3D, so like all your measurements are being performed on a 3D lattice, then you have a bound on the distance there that, like, And the distance there that scales as this function. And you have a trade-off between k and d that is the boundary by this quantity there. I'm going to give you a little bit of a landmark. So like a natural question is, okay, once we have these bounds, what is the best code that we can come up with? And it turns out that you can play around with these equations. Basically, the best code that you can achieve is a code where it has n qubits. Where it has n qubits and k scales as n to the one-third, and the distance scales as n to the two-thirds. Might be the first time in the two of these founds, so I'm going to illustrate them a little bit. This is a graph. The x-axis is log n of k, and the y-axis is log n of d. And the blue region there is the range of high terms that is supposed to be achievable according to these bounds. Is bound. And this point there corresponds to the optimal code. And the reason why this is the optimal code is that once you have this code here, you can kind of like slide up and down this look. So our main goal is to try to build a code that gets to this point. There has been many attempts. You can start with a toy code if you'd like, that achieves square root of n distance, which sits there. And distance, which sits there. Then there was the cubic code by HAAA, which sat a bit lower than that. And there's also the weld codes that were still not doing really better than the Tori code, really. And finally, very recently, a work by Port Noah got really, really close to these optimal codes. And our construction finally exactly achieved these variances. This is what a layer code looks like. And these codes. That's a specific case, right? Oh, yeah, so you can choose sizes where. Yeah, and you can do the same thing for the trigger, right? You can take it and break it down and have a quick number. I'm just taking these as reference points, and the idea is like And the idea is like once you have this point, you can't really go into this direction. You can increase k, so you can kind of like slide down as well. You slide down all the way up. But you can't really get to this region back. For example, once you have the toy code at this distance, there's no way for you to increase the distance rate. Like I can't use a bunch of toikrade blocks and get a higher relative distance. Why is the dist shouldn't the distance of the qubit code be ledger than the character code as well? No, because like the it's like the best so we don't know what the distance of the qubit code is. The best other bands we have have it sitting lower than these two. But I agree with Ben, can't you move the qubit code like right in a straight line? Because the dimension might fluctuate, right? The dimension might fluctuate, right? But if you pick a specific family, we'll have like constant or like better than zero rate, like it would be too big. Oh, yeah, yeah, true. You could probably like slide it up and down. Sorry, left and right, but yeah, you still could have like the target code has string like logic points, so it should be like m to the one fiddle that you wouldn't have. That should be m to the one fiddled plus at least epsilon. Uh No, it doesn't really achieve that. And like the toy code, if you lay it out in 2D, you still get squared of n, right? That's the 2D toy code. Yeah, but it's like in 3D we have access to like 2D codes by definition. Yeah, yeah, I know that. So the best you could do basically is starting from this point and slide all the way down to that point. But like you still wouldn't have access to this feature. Right. Yeah. Great, so these layer codes look like this. I kind of really like that we have like a nice way to represent them graphically. And as promised, they do achieve these parameters and they situate the 50 bounce in 3D. Now, what this constrictant is, actually technically, it is a way to spot, uh, to take uh Part to take codes that are not necessarily local to codes that are local. And so while we do that, we're going to lose some amount of distance and lose some amount of logical qubits compared to the number of physical qubits because of the BPC bounds. But we do that in a way that this loss of parameters is still controlled. And like the fact that this loss is controlled, then when we use this When we use this mapping with good sparse codes, sparse codes as input, we get in-output codes that have parameters that are good enough to achieve these bounds. What else? Yeah, so these pass codes, I haven't talked about what they are. We also call them LDPC codes. It's just a very nice class of codes. Like, are they LDPC calls? Yeah, yeah, yeah. Yeah, I mean, like, the way I use passcodes is like. Like, the way I use sparse codes is literally L APC. It's like, so a sparse code has. Oh, that's kind of like a semantic thing, like sparse means L A P C thing. Yeah, yeah, yeah. Again, I might just define it. So a sparse code is a code where all your measurements involve a constant number of qubits, and all of your qubits are involved in a constant number of measurements. Great. I'm now going to try and spend the most of the time. Try and spend most of this talk to explain how we build this mapping. And before explaining this construction in its generality, I'm going to focus on how we can build an equivalent of player codes, but on bits and in 2D. So I'm going to obtain classical codes in 2D instead of quantum codes in 3D. And hopefully that gives you a bit of a flavor for how the quantum construct. How the quantum construction works. The main inspiration for our construction, I do have to mention it, was from this paper. They also take sparse stabilizer codes, but map them to 3D local substant codes, which are different from stabilizer codes. The point is that this construction relies on a clock Hamiltonian, and I'm going to talk about how this Clock Hamiltonian works on this. Works on this. Great, how does it work? So the basic idea is you start with your input code and then you have a pi-take matrix for that code. I haven't explained what the pi-take matrix is, but you can simply think of it as a list of all the measurements you're going to perform. Then, because you're using a clock Hamiltonian, you need a circuit. Now, what is the circuit that we're going to consider? It is the circuit V. It is the circuit V. So the circuit V takes as input a bit string and a clean and slow register. And then it's going to output your bit string tensored with the list of the outcomes of your measurements. So on one hand I have h, which is the list of all the measurements I'm going to perform. And then I have a circuit that tells me, okay, that measurement measured this value, this measurement measured this value, and everything. Once I have this circuit, then I can. Once I have this cell code, then I can build my cloud culture and this will define my record. Great. It's going to be very useful to have a graphic representation of what this circuit looks like. So here I have a 1D circuit because this is the column of bits that I'm going to act on. This register, we can imagine that this is, for example, a data register, and this is going to be the Ansible register. So at time C is you. Register. So at time C0, I have my column, then I act on it with some set of gates. I end up in T1, then I act again with some set of gates, and on and on and on and on. Is this representation clear to everyone? Great. And you can imagine, okay, so if I input some kind of bit string and input, then all of these. Input, then like all these at all these intermediary steps, then my bits are going to take some values. Why do we care about that? Because like this allows us to define something that we call the history state. So a history state is simply the record of the state of the system for every time step. So instead of having my 1D circuit that I evolve over time, That I evolve over time, then I end up with a 2D array of bits where I have recorded the value of the bits at every single time step. Good? Does that make sense? Have a solid line space? Yes. The solid line is like when I don't apply an operation. So that's why if you start with this bit in the state one at this time step, then you're going to always end up in the same. Always end up in the same state after that. Okay? No questions so far? Great. And this will allow us to define the code that we're going to obtain. The code that we're going to obtain is going to be defined on this 2D array of bits. And we're going to have checks, so measurements, that are going to impose that my code. My curt states are precisely the history states of this circuit. So, like I started from a 1D circuit that was evolving over time, and I end up with a 2D code such that it allowed states. So like the allowed states are when I perform all my measurements and they don't notice any violations or errors, then these states have to be history states of my circuit. State of my second. Yeah. I was just checking. So the history state is a classical computation history, right? Yeah, yeah. So I'm still in the classical world. Yeah. All of this is just bits. No two bits yet. Yes, like a history state is literally going to be a record of CON1s. That's good. Computation history. Sorry. Like computation history state. Yeah, absolutely. How do I do that? So there's a fairly So there's a fairly simple way to derive the checks that I'm going to perform from the individual gates. And it's actually used, it's kind of cool, it's also used in the Kukleman theorem. If some of you knew of it, it's nice that there's this connection between the two fields. Anyway, to give you a more concrete example, imagine that I have a swap gate between these two bits. These two bits. What kind of check would this correspond to? So if I have a swipe gate, then the value of this bit A here has to be the same as the value of this bit D here. And to enforce that, I simply have to impose a ZZ measurement, because the ZZ measurement is going to be a parity measurement. It's going to measure whether these two bits have the same value. And then you can do the same thing for the bits C. thing for the bits C and B. I have a ZZ measurement between the two and like if my ZZ measurement doesn't notice any violation it's because these two bits are in the same state. Is that? That makes sense to everybody? Good? Yeah, good. Great. Now what is like a more concrete example? So imagine that I have an input code and I'm going to ignore three. I'm going to ignore a few details, but I have this bit A here, this bit C here, and I have like a bunch of qubits in the middle. But the point is that I'm doing this as a measurement between the two. What would the circuit look like? The circuit would essentially consist of swab gates so that I can bring this bit here all the way down there close to my bit C. And then once this is done, I can perform a ZZ measurement bit number two. Far so good. And this is the kind of code that it would correspond to. This is the kind of code that it would correspond to. You essentially have a repetition code that is going to bring the value of A here all the way down there. And once you have brought this value all the way down, you can impose a ZLIT check between these two repetition codes. And that would give you the code that you're looking for. I don't know if everyone has heard of a protection code. Our partition code is simply like a line of bits with a bunch of ZZ checks between them. Is that okay with everybody? Great, and if you stay at it long enough, you can realize that you can actually break down these long-range checks without ever needing a circuit properly. For example, if I have this long-range ZZ check, Long-range ZZ check between two bits that are far away, I can simply build a repetition code between these two. And this is going to enforce that this bit has the same value as this bit within the code space. Now, if I have a three-body check, I can do the exact same thing. I'm going to have a string of refresh encode here that is going to bring the value of this bit here all the way down there. Then I have a check there that mirrors. Check that that like mirrors this check here. And then I can carry the value of this bit all the way down there to make sure that the value of this bit is the same as this bit. And I can generalize this to KB checks. Great, and now this is kind of really nice because I have described to you a method to take an arbitrary non-local code and map it into Add 28 code. And to summarize the construction a bit, so I have my input code has m checks. Here in this example, I have three. I'm going to map each bit into an m longer fiction code. So here I've mapped each bit into a question code of length three. And using the recipe that I've given before, I can take each of these checks and break them down into Break them down into checks that are local in 2D. And when I do that with the rut code, this gives me a construction that saturates the classical equivalent of the WPD bounds in 2D. Is that okay? You're looking confused. Oh, yeah. So, what you do is you take the paragraph matrix, and then you take a circuit decomposition off of that and turn the circuit. Yeah. And then on the right-hand side, you basically have the COPELAB type computation history of the server implementation of your checkpoint program. Thanks. Yeah, that's a really good way to think about it. And so when I do that, I'm going to add a substantial number of bits, but so that thing has some overhead, but also I have preserved some of these parameters. It can guarantee you that, for example, k is relatively large. K is relatively large and the disk sensor so remains relatively large. And so when I do that, if I pick the right input code, then this like cooklevin type mapping is going to preserve these parameters well enough that I can saturate the classical BPT balance into E. And like this result in and of itself is interesting enough because since the BBT b uh the BPT papers came out and they also stated the classical BPT bounds. The classical Bibita bound, we didn't know how to saturate these bounds. So it's quite nice that this kind of like relatively simple construction is in a way the best thing you can do. What are the bounds exactly? So in 2D it's like k times squared of d is less than order n. Which corresponds to like n qubits distance of order n and j of scaling as And k is scaling as square root of n. Great. Now the natural question is: how do I map this to qubits? Because the hope is kind of like, if this works well for bits, maybe this would also work very well for qubits, right? And by analyzing the construction that I've given you before, I can kind of like try and guess what the quantum equivalent would be. What the quantum equivalent would be? I mean, so when you go from classical to quantum, I mean, what, I mean, are you using like some sort of product construction, or what's the. No, no, okay. So like what I mean is literally, I have this construction that saturates the PPD bound for classical codes. Now what would be, can we like get inspired from this construction to build something quantum that would also saturate these bounds? So like I wish. These bounds. What I'm a wish is in the quantum. Yeah, exactly. Yeah, it's more like an NZAT. Great, so we started, so like the classical light codes, we started with an input code that has m checks, and this was pretty important. And in the quantum case, you're going to have, I'm going to assume a certain kind of code where I only have checks that are either measured. That are either measurements of Z-type Harley operators or X-type Harley operators. So, like, the measurements that I perform are either just product of Z's or product of X's. Good? And we call that CSS code in the literature. Great, so this is probably going to be relevant information. The second thing is what we did was we mapped each bit of What we did was we mapped each bit of the input code to an M-long repetition code. And what is the quantum equivalent of repetition code? It turns out to be, I mean, like, you can make a good argument that the equivalent is a surface code. And accordingly, we're going to have a surface code of dimension mx time mz. Times time. Great. Now the problem is like, Great, now the problem is like how do we break down these long-range checks? If I have to implement checks between two surface codes, how do I implement them in a local way in 3D? And the answer is going to be like a fairly well-known methods. Use a name that is surgery and something to like try and describe. So this is the surface code as a really quick. The surface code as a really quick recap. Ben told us a lot about it, so I'm going to be quick. We're going to consider the surface code where you have rough boundaries at the top, at the bottom, and you have smooth boundaries on the left and the right. The consequences of that is like my x-logical operator is going to go from left to right, and my z-logical operator is going to go from top to bottom. Go from top to bottom. Because we're going to deal with 3D codes, I'm going to use a certain convention to represent the surface code in 3D. This is the surface code in 2D, and we're going to draw it that way. I'm going to abstract away the rough boundaries, just because it's a nice slide to draw. But they're still there. So the logic holes still have the same structure. It just looks a little bit different. Great. So we can go back to our original question. Go back to our original question. So, in the classical case, we had a check between two bits. We mapped each of the bits to a repetition code, and then the longer check between the two was essentially another repetition code. How do I do that with two surface codes? The answer is that's separately. And the way it works is you have your two surface code patches, and you add another surface code joining the Code joining these two patches. Here is the thing in red. And then you're going to couple these patches at the boundary there using some very specific measurements, like some modified checks. And this will allow you to measure the product of z here and z there, and this will implement essentially the operation that you want. I'm going to like. If you want, I'm going to have a few more diagrams to explain a bit more. So, what does I do? What does that do? I start with two uncoupled surface codes, right? And so each of these have independent X logical operators, right? When I map them to something with a lattice surgery patch, then the product of these two Z operators now belongs to the stabilizers. Now it belongs to the stabilizers. So, like, now this belongs to the set of values that it can measure. And because it is in the stabilizers, then this X-type logical operator is no longer in the logicals, simply because this X is going to anti-commute with this Z and it becomes an issue because. Becomes an issue because logicals have to commute, right? So now what we've done is we've shown that this x was removed from the set of logical operators. And instead, it's been extended to a much longer, much larger operator that has to span the two service code patches. So now you can verify that this new X-type operator does. X type operator does connect with the new stabilizers. Does that make sense? Yeah, good, great. Great. So we've seen how it works to implement checks between different service-grid patches. One issue is that if I have to implement a ZZ check between two surface-bit patches, Check between two service-bit patches and an XX check, so like I would end up with a different kind of lattice surgery patch. These two patches do not commute, and that is a problem because we're assuming that we've got stabilizer codes, and for stabilizer codes, everything has to commute, and so we have the problem at these junctions where things aren't quite right. The other question is: when we need to have a check acting on several sub-script patches, it's not Several sub-scope patches. It's not quite clear what that would look like. The lattice scoping method was very much developed and thought about to act on just two patches, so we need a way to generalize it to a larger number of patches. The solution to generalizing light surgery and solving these questions is going to use tools that are called topological defects. What are these? So, before talking about what they are, I'm going to About what they are, I'm going to say a little bit more about the surface code. Surface code is quite well known for the fact that in syndrome can be divided into equivalent classes. And there are syndromes that can be created and some that cannot. So, for example, I can create this syndrome. So, each ball corresponds to a Z-type stabilizer being triggered. And so, I can see that like. And so I can say that if I have this string operator in the middle of my surface code, then it's going to trigger these two measurements, and I'm going to end up with this pair of triggered measurements. The thing I cannot do, though, is this kind of triggering, because when I have a string operator, which is essentially the only A string operator, which is essentially the only kind of logical operator that is allowed in the surface code, I can't have only one check that is triggered. I always need two checks to be triggered. And this is going to be extremely important. Did I lose anyone here? Does that make sense? They're like, syndromes of this stuff is going to have to end the body. Is that good? Is that good? Yeah. Okay. Now we can bend those roles with defects. So defects are regions of the surface codes where the measurements that you perform are different from the measurements that you perform in the middle of the surface codes. In particular, the rough boundary is a really good example of a defect because now I can actually create this kind of pattern. I can actually have my string operator coming out from the top. Coming out from the top, ending it in the middle, and having a single measurement being triggered. Why is this so important? It's important because this will directly dictate the structure of the logicals. When I have a pair of both boundaries, this will directly allow me to have this z logical and same thing for the x logical. So if understanding the defects that I have allows me to some extent to understand the structure of the logical. To understand the structure of the logicals that I end up with. Another thing that's quite nice about defects is that once I understand how they work, I can understand how to obtain the measurements that I need. So when I have a rough boundary, I can have these two this excitation pattern, and then I can connect these two. And then I can connect these two because syndromes are mobile with the surface code. And I end up with this closed loop. This closed loop doesn't trigger any measurement by definition. And so at that point, it has to be in the stabilizer group. And so that tells me that this up roto is in the stabilizer group. Great. We're going to need a little bit more notation to generalize this construction. We're going to We're going to introduce two defects, two types of defects. And I want you to think of this as like this being an equivalent of the X check and this being an equivalent of the Z check. So in the X check, we can allow for this pair of expectations, this pair as well, and this 3, which makes sense because an X check would allow for this logical. So for this one. So for this, like an X type check would not detect a pair of Z operators. It would also not detect this pair of Z operators. And also, it induces this triplet of X operators by the mere fact of being an X type check. And similarly, you have a Z type check. The Z type check is simply a three Is simply a three z's and this is going to allow for these two operators to never be detected and these three operators to never be detected. That makes sense. Great, and now I can see that this is exactly what happens with the junctions at the vattis surgery patches. So if I have this string approach. So if I have this string operator, this z-type string operator here starting from this patch and going up in that, then what I have is essentially this pattern. I can also have a string operator starting from this side, ending up on this side, and I end up with this pattern. And also I have this string operator, and I end up with this pattern. Okay, so this allows me to characterize the Characterize the defects by these intersections. Does that make sense? Great. And so you can also look at the whole lattice server patch where you can see that if I have this string operator that starts here and goes across this patch and ends up with that, then the pattern of excitation that ends up is. Excitation that end up is exactly the thing that you obtain with the combination of the defect that I've mentioned before. That's the logical operator to code. Yeah, almost, right? There's like the keys into the banning. Yeah. Yeah, that's exactly the point of this diagram. Like, when I have, when I combine these defects, which are like, say, like, this line here is a defect, this line here is a defect, because it's not the usual rules inside the surface code, this can. Inside the surface scope, this can be made to correspond with this diagram, with the rules that I defined previously. And you can see, yeah, absolutely, that this tells you exactly what happens in the logical structure of this thing. And this is why this notation is quite useful. It's because it tells you what happens in your code. Are the boundaries oriented the same? So the red thing? Is that top edge the same value type as the top of the black coat? Oh, yeah. I can't remember what your convention was, but it's all rough on the top. Yes, like the zands go on. Yeah, yeah, yeah, yeah. Okay, good. Great, and so I can verify as well that for an exylical, um, the pattern of excitation that I observe also corresponds to this combination of diagram. Combination of diagrams. Great, now I can attack the first question I had, which was: if I have a K-boded check between, so like if I have a check that involves several surface code patches, how do I break it down into something that is local? And we've seen that in the classical case, I can take a check acting on the k bits and break it down into this like this. Down into this, like this series of checks. And I can do the exact same thing with these defects. And once I look at this kind of decomposition in strings of defects always possible? Is this kind of decomposition in strings of defects, is that always possible, or is that an artifact of that example? Oh, no, yeah, it's. I mean, like, so you can see that, like, yeah, once you recombine this tensor, essentially, you recover like. Like tensor, essentially, you recover like the same things that. Great, and so what does it look like though? So we've seen that this we've seen that each of these corresponds to a lattice patch, right, between two surface group patches. And so what this ends up looking like is just I have all of my surface group patches, and then I have like a series. And then I have like a sequence of lattice surgery patches in red that connects all of them. And so like this string of lattice surgery patches in red corresponds to this string there. Great, and as I've mentioned before, understanding the topological defect that you are using allows you to derive Are using allows you to derive quite easily the actual measurement steps you have to perform in order to realize your defects. And right. Well, I want to see this. Time does. What? So on the those SLX, what wise? Oh, the red link just means it's on your joining it's a star up right here. Yeah, yeah, yeah. Okay, yeah. Right here. Yeah, yeah, yeah. Oh, yeah. Yeah, yeah, yeah. I get it. Great. So, the other problem we had is that if we had an X-type surgery patch and a Z-type surgery patch, then they don't exactly permute. And the way we're going to resolve that is we're going to have to link up these surgery patches. We're going to have to modify the measurements in a way that's going to make them commute. The way we're going to The way we're going to couple these patches is a little bit abstract at first. So you might be able to agree with it. But imagine that you have a code on these qubits, a quantum code on these qubits. And this code might have two checks. You have an excite check there and a z-type check there. These two commute and so are alive on some number of qubits. And we're going to decide that That so the lattice, the surface grid patches associated with like these two qubits highlighted in green are going to be modified in a certain way. And same thing for these two. The pattern of like curling is defined a bit more in detail in our paper, but it's not that insightful. So for now, I'll just assume that amongst some of the qubits. Some of the qubits in the system, we're going to have some kind of coupling between the surface-based patches. And what does it look like? So this is like a more concrete example. An input, it's like the input code here is the 422 code. It has two checks, the ZZZ check and the XZZ check. The kind of coupling that we're going to do is between these two qubits and these two qubits. And what does that look like? We're going to end up with Does that look like? We're going to end up with a line defect between these two surface code patches corresponding to the associated qubits, and we're also going to have a defect associated to these two qubits. Now, what are the terms that we need here? Like, sorry, what is the, what do these defects do? We can use our diagrammatic notation to Diagrammatic notation to describe what they do. So like when there's the green line, it's going to modify the condensation rules locally. And when there's no green line, it's kind of just like if you had two surgery patches that were simply not interacting, but just like go past each other. And once again, you can derive what the What the exact measurement terms are. This is the good stuff, but you'll last through 10 seconds flat. You want to start out? Yeah. Any more techniques for the challenge in the area? Yeah, so it might be good to have a little bit of. Yeah, so it might be good to have a little bit of a recap and take an explicit example. So I'm going to check as input the shell scope. I took this table from one of David Pullen's paper, the one on the stabilizer formula for operator quantum air correction. And the question is: okay, so if we take this code as input, what do we end up with in terms of layer code? In terms of layer code. And the first step is going to be that each of these, each of the qubits is going to be. So you want to now get the layer code from the Schultz code, right? Yeah, exactly. So each of the qubits of Schultz code is going to be mapped to its own surface code. Then I'm going to tackle the Z-type checks. Each of these checks is going to be mapped to a certain lattice surgery patch. And then the same thing. And then the same thing with the x-hat checks. And because these span multiple qubits, we're going to use the breakdown rule that we explained before. And so it ends up looking something like this. You also have to do the coupling between the checks, and eventually you end up with also these little regions with modified measurements. And this is essentially the And this is essentially the end result that you get. Now I'm going to talk a little bit about the properties of these codes and how we can derive them. And I'm going to introduce a little bit more instructions. So as a reference point, the parameters of the input code I'm going to write them with lowercase letters. And the parameters of the output code I'm going to write them with uppercase letters. So when I Start with an infra code with these parameters. The number of physical qubit that I end up with, essentially, I have like n surface code patches. Each of these patches is mz large and mx high, mx tall, I guess. And so the number of physical qubits I end up requiring in the end scales as this quantity, and because like we can take. And because you can take codes where mx and mz are linear and n, then you end up with this kind of scaling for the output layer code. Then the second question is a bit more tricky. You want to know how many logical qubits you've preserved. And to do that, it's a business to count the number of logicals that you have in your code. And it ends up being quite easy. Ends up being quite easy. I can actually take the logic calls that I had in my input code and map them to my logical code, to my layer code. So, here, for example, this is the layer code for a simple repetition code. So, I have three qubits. I have a ZZ shake between these two and a ZZ shake between these two. And you can see that the X type logical operator kind of looks a lot like the logical operator of the Logical operator of the regression code because it acts on the three surface codes the same way that the x-type logical operator of the regression code would just be a product of x on the three qubits. And the only thing you have to watch out for is that you kind of need to do some kind of stitching on these large surgical hatches, but it's quite straightforward. So essentially, you get, okay, I shouldn't have like the theta here, you get exactly. The theta here, you get exactly k is equal to k. And so the expression of the distance is a bit slightly complicated. But to give you an integration, what you have basically is that because you can, because like the logical operators of the new code basically correspond to the logical operators of the input code, they only have to be long enough to span Long enough to span the service code, you end up with a distance that's simply the distance of the original code times the width of the service codes that you're using. That's why you end up with a distance that scales like this. And as I've explained before, usually these quantities are linear in n. And so you end up with a distance that scales as n to the square when you use a good sparse curve. Sparse. A good sparse code, sparse code as input. Great. So when you like look at everything and you input a good code, then you end up with these parameters as comics. Yeah, great. So there are a bunch of cool open questions. So are these codes quantum memories signed by? We don't know. Dominate things that no? I have no idea. Into that error, I have no idea. Memory, self-worthy memory is not. Oh, yeah, yeah, yeah, yeah. Because it also has, like, I forgot to mention, it has like an optimal energy value as well. Okay. Yes, like, it's linear in the dimension. The linear, yeah. Yeah, exactly. But the problem is like, it kind of still has the problems of the surface code, right? So you kind of like. So you kind of like, it kind of looks like you might be plagued by the same problems that you had with the surface code. Can we get a combinatorial formalism? So in the aggregation community, we like a lot of product constructions and things that are a bit more formal, maybe. So it would be really nice to make this construction fit into the more usual framework. The more usual framework that we use. It would be cool to know whether this can be used as a complex theoretic gadget where I can take a parallel Hamilton and break it down into something that's local while preserving some kind of property that I like. Also, the way we're obtaining the boundary terms and the terms of the defects, I kind of oversold how easy it is. It's actually like you're gonna have to go through it through quick force and like. It through quick force and like, yeah, check that everything works out. It's a bit tricky. It's a bit tricky. Like, you kind of have to brute force in a little bit to some extent to make sure that the checks you end up do satisfy the defect that you want. Like the same way that if you wanted to like derive out of the vacuum the boundary terms, like the rough boundary on the surface code, it would take you like a few tries before you get it right. Huge five before you get it right. Great, we don't know how to generalize those to higher dimensions, which is unfortunate because the BPT bounds also extend to higher dimensions. So how to do that, it would be really cool to know. And finally, we leave the question of how to do logical operations and decoding on these codes. This is open. I just want to do that. Great, and that's it. Does that construction preserve the robustness of the code? And by that I mean for all errors smaller than a certain weight, then the syndrome of the error will be proportional, the weight of the syndrome will be proportional to the weight of the error. And so you could have that in good code. But then is it also true in delayed code? No, right, because precisely because we're using centuries. Precisely because we're using such a thing. Yeah, so you can have a big arrow in the middle of something. Yeah, just a big screen. But it still deserves it in the way, in the sense that at some point, if you want to build a logical operator, you do have to start going into the defect network. And then you'll necessarily, this is why you have the enterprise, because going through these networks and building the actual And building the actual entire operator does incur a certain cost and syndrome. It's more like you have like robustness divided by a polynomial. Yeah, so like for small n of ads, you might not get anything. Yeah. Is it obvious to you why the 3D Why the 3D PPT bucket should follow? I mean, why they should follow the 3D BPT bucket? Because building a two-dimensional grid into a 3D volume. It's not obvious why it should follow a three-dimensional BPT bound rather than something between the two of them. A three-dimensional BPT bound was really built thinking of a three-dimensional lattice. This is not a three-dimensional lattice. This is like structured. Absolutely. So I think it's more indicative of how bad quantum codes are, like local quantum codes are. Because yeah, indeed what we're doing is like we're taking like a 2D construction and popping it out into 4D, which might be useful to explain actually. There's a gap between Explain actually, there's a gap between the BPD bounds for classical codes, or like even search stem codes, and the BPD bound for search stem codes. And I think this is the way that I explain it to myself. It's because you can think of most stabilizer codes as CXS codes, right? And then it kind of makes sense because you kind of need 2D to embed the logicals of one type, and then 2D to embed the logicals of the type. And then 2D term methodological is of the other type, and then you can kind of like sew both of them together, right? Which, like, if you do that well enough, then you get a 3D object. So I think it's more indicative to like, yeah, like quantum codes are just really bad at maintaining, like local quantum codes are bad at maintaining quantum information. Then if you could take it to the next level, get it to 4D. Yeah. Very, very thin. Very, very thin license, 40. Yeah. That's 40 behavior in 40 total starts looking so so bad. Yeah, there's like, because an interesting question is like, we used surface codes as our building blocks. There's no reason why you would need that. So like, what I mean is, you could probably use like the color code as well as a building block. Code as well as a building block, and then like it's just that your defects would have to change a little bit, but you would essentially get the same construction. So you can probably take as building blocks before the toy code and maybe get something like a bit different. Um I refer it's not self-profit. That's why that's why I guess why something about your question. I mean I think it's for it's mainly the arguments for which it's key and then I think we had to rely on And then I think you can kind of rely on the fact that you've got fast code. So like if you want to watch one alley on something like try and run across the watch operator that I don't most of the time agree is just one alliance. You should be able to issue the fact that you can sell these feet without these technologies. Yeah, I think the only thing you'll see this is like I think you can think of saying the subway think of this face as like difficult space high arrow. And yeah, it kind of makes sense that you kind of look like the original code, like those code a base high arrow base and the base constants. So if I massage over The lower most commonly just trying to capacity and normally put the beginnings. Yeah, I'm sure that's a good idea. I think that would probably have to be better. But why would the novel field on stabilizer subversion modify? It's about translation invariants. Ah, it's it's really good I'm very good at Oh, that's right. Yeah, those are very infrequent because he's building LGBT code on it. So the logical is just going to spend that many on main EC story. So it's just going to hit branches every now and again, but it's going to try and wither off a few times, but to get super. Off a few times, but to get to this LDTC, you know, that those are very infrequent and they're constant valence you too fit so it's called yeah exactly no no no that was yet why don't you go take those um yeah because we don't even care about stable anyway go take out radio states and someone and and and that's what we have we get this thing of like the distance is divided The distance is divided by the weight of the distance. The distance is divided by the weight of the distance. Yeah, it's like it's proportional to the angle. Half the distance is proportional to weight of the weight to be stabilized. It's stabilized by the state. So, what do you mean by the weight? Yeah, like the average. Oh, okay. I can get that. I'll just put it on it. Yeah, okay, so it feels it. But it'll be something I'll. Well, wait, no, no, okay. So the distance is okay, but yeah, well, I would still say that would be better. So I mean, like, the the codes that I come up with in that paper are like and check your email. You know, good code and check. So you would still have it. Again, I don't think this is anything possible. Yeah, okay, so if it's what's it so the distance is the distance of the obstacle, right? Yeah, okay, so however, multiply that. Yeah, yeah. Actually, it may be a matter of fact. I will use it real quick. I would believe that you have a chance to be harsh query. I would want to use the grant as many tests as the X components have. Yeah, but if you've got a way of localizing like that doesn't matter but like this was gonna be like yeah this is also I sorry I don't want to bang on about it but I'm sure like your graph if you put this one with the But the you can you put the torrent well well, so first of all, like I would have like torrent code in 2D to the to the yeah, I mean you said this, right? K is equal to L. So then you would have, you know, this would be like one-third on the, that was the longest on the logical qubits. And the distance, you wouldn't use any distance because 3D printer would have, so this guy would still be like one third. And I'm pretty And I'm pretty sure there are instances of the Ha code which have a number of logical qubits that's this many. And because it has no string operators, it's at least an epsilon bigger than this cache. So I don't understand why the qubit code is not... Okay, for like here, so like, that would be the qubit code, and that would be the target code. Why isn't it? Well, sorry, when I say the target code, I mean a stack of 2D codes, because you don't lose distance by just having a stack of 2D. And then you're here somewhere, right? But if you're here somewhere, right? Yeah, so like I think like I don't get so I think like if you extend this graphic, you would end up with your graphic. Extend this graphic. Like you could like as Mike said you could have like a code that has like a higher rate and then like you could maybe end up with the whole region of the toy code with like but the distance should be an epsilon bigger than this one. Because the toe code has this unsquared of n, right? Huh? The toy code has this unsquared of n, right? The toy code has this one squared event, right? No, these are all 3D codes. Oh, okay. Yeah. Sorry. And so if you look at the case where you have a bunch of separated tarot codes, I see. I see. Sorry. This is a. That's really a 2D tarot code. So I was. The picture I have was these little 3D codes. I should have said like Suffice code. Yeah. I would. I would, in this diagram, I would make this 2D 2D torrent code, and maybe I would put the 3D torrent code there or something. Sorry, that's what I'd interpret. That's what I thought. Okay, I understand what you were saying then. Yeah, I would think encamping those homological product codes of Hastings and Browdy would be more likely to be self-correcting than the codes you actually have in this product. take like if we could take like the the input checks and like maybe add some redundancy you know like because we have like a lot of like vacuums like like empty space right like if you look at this you have like there's a bunch of space that's left right so maybe you could like recombine the checks and like add some redundancy in a way that then you get self-correction by having the similar effect as you would having the similar effect as you would with the high weight LDC code. Because you attach a branch out often. Choose a generating set for my stabilizer that's like high weight but will have like single shire regression or something like that. They still have some robustness. So you can always do that I think. Then you also need quite a large number of stabilizers. Then you also need quite a large number of stabilizers. I think it's not enough. So, if you want to actually make these checks local, I think you would need more stats than the width of the code. I think, I'm not sure. That thing I just said was a bold guess. But you should interpret as a, I don't know what I would have said. But I think if you want a single shot, you need a lot more redundancy than order in with a 2D code. It is my guess. I haven't listed anything. I haven't yeah no I I don't know I can't remember like adversarial noise I think they probably wanted to play I'm very frustrated with the fact that we have to rely on some skirts it's a bit weird You guys just have the box. It's a totally different thing on the on this top. On the other hand, the lateral part. No, that's the background. No, not the python is more. I'm going to check it out. But yeah, there's like some pictures. And actually, it's like this kind of plot construction is a lot closer to banana picket type. Because if you type them in, you don't get it. And then you don't get the number of interface that scale has nice, right? You just get mixed with the scale. I never associated those construction with codes. I know there was paper processing about these things. I never associated codes, so I don't really know. I'm just representing my recognition term. Time ramps represent whatever they're just kind of like topic things. And then I'll just like convert the timers. Yeah, it's like usually it's yeah, it's like more complexity stuff, but um yeah, there's like this cool fact that regardless of how normal your code is, it can always be like um like a circuit that is local and that has like a polygon that sets. And like the second thing, I think it's it is it's extractable. So like even though in terms of like have access to the container that's why you seem like It's still not as good enough to convert the circuit to a five-person thing. And that's why I'm making fun of this because the space tables are not eventually there, because eventually we don't rely on the last one. But then it's a step. No, no, it's more like a conceptual step rather than a stands. Like how the idea came to eventually in the paper we don't use psychics at all yeah so what if what do I need this with? Because if I just do doms version of the talk I'm just working with other sort of codes to the how they like segment like parallel control more trying to explain it why they're here. More contracts for you, uh, well, the same thing is there's so that you're saying there's an alternative way to construct the system itself. Like to think about building ways. And it's also like we're also building It's also very useful to think about the segment version, to think about how to do it. Short time cheaper. Turn that back into a solar pattern. Like that would be like a specific point, like that extra point for official portraits code. Like this is a second that's really neat. And then to take that segue, you might have to um To like at least in the baseline time of data, what they do is like there's a method to like a curve that is like a set of code. And the problem is that like this is a part of it. And the idea is like, instead of doing the second thing, you can like think of it in terms of taking this long extra checks and breaking them down. And once you break them down, And once we break them down, I need to go to a recover notation. Got this extra step in front of each one of these 100 people.