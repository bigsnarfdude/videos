So, what did we do yesterday, and what have we done so far today? Okay, so previously, let's kind of take stock. Previously, we talked about share processes and got finite-dimensional distribution convergence, and then we constructed the area. And then we constructed the Airy Line Ensemble. And the Aerialine Ensemble is this sort of beautiful continuum object, I think it's beautiful, that has this nice invariance property that if I look at the law of restricted some region, so you can kind of think of the indices as one variable and then the horizontal as another. If I take a little Other, if I take a little rectangle in that index space, and you can, of course, deal with more general shapes, then the law here is given by exactly the conditional measure of a Brownian bridge condition to avoid above and below. So the law itself is a random variable, which is a function or measurable with respect to everything external. So it's a random variable. And it has this non- And it has this non-intersecting Brownian Gibbs property. And so, what I want to explain today is that there exists other line ensembles that have other Gibbs properties. Now, in the discrete setting, we already saw an example of this. But I'll talk about another continuum, another Brownian Gibbs line ensemble that doesn't have such a strict restriction on the ordering of paths. So here, if I If I took a sample path between and it and it touched, then that would lead to a rejection. And what I'm going to do today is I'm going to talk about a line ensemble that has a soft version of a Brownian gift property. Of course, you can imagine trying to define arbitrary such things. But this is going to be related to what's called the KTZ line ensemble. Actually, let me mention one. Let me mention one thing. When you're dealing with lattice models, like easing models, stuff like that, or tiling models, constructing infinite volume Gibbs measures is not always easy. It often takes a lot of work. And in fact, that's what we did in the last lecture. We constructed an infinite volume Gibbs measure, the Airy line ensemble. So you shouldn't think that just, you know, if I specify a Gibbs property, if not all. Property, it's not always clear, it's not obvious, it certainly is not something that you know just falls out immediately. It's not obvious that there exists an infinite volume version of that, that there exists an infinite volume measure which admits that. So, this is you know, that is even one of the challenges. So, what I'll do today is I'll talk about such an infinite volume Gibbs measure, a Gibbsian line ensemble that has a softer version of the Baron-Gibbs property. I'm not going to Property. I'm not going to give you the construction of it. And the construction of it goes through some slightly more sophisticated methods than what I was describing for the SHUR processes. And if you're interested, I'd be happy to fill in, you know, give you some references, give you some more explanation. Okay, so we're going to construct this, or we're going to talk about the KTZ line ensemble, and then we're going to use it to correlations. Relations of the KTZ equation. Okay, so that's the plan for today. So let me start by, and of course I'll tell you what this soft brown gibbs property is. So let me start by defining the KPZ line ensemble. And unlike the Aerial line ensemble, the Aerial line ensemble doesn't have, you know, I could. You know, I can define it in terms of finite dimensional distributions, but as a stochastic process, I can't really define it as a function of some IID noise. It turns out that the KPZ line ensemble, which is somehow a progenitor, which in some limit should go to the Aerialine Ensemble, actually admits a description in terms of IID noise, in particular space-time-white noise. Particular space-time white noise. This is like the fact that the KPZ fixed point and the KPZ equation, you know, one is a function of white noise and the other is not, for those who've heard about this. So let me define the KTZ line ensemble and it's going to be a function of space-time white noise. And so if you're unfamiliar with space-time white noise, I'll just give you an impressionistic notion of what it is. It's not going to be terribly important, it will just kind of set a definition for an object, and that'll tell you properties of the object. Okay, so let x C T X be Gaussian space-time white noise. So there'll be a little bit of stochastic PDEs, but very, very little. Very, very little. Nothing hard. Nothing hard. So, what is space-time white noise? If you've never thought about it, well, you know, what is white noise? One-dimensional white noise, the derivative of Brownian motion. It has a covariance, which says that if I integrate the white noise, which recovers Brownian increments, that the covariance of the integral is just, you know, between two different intervals, is the area of the overlap. And so, space-time white noise is defined by the same property. If I have two regions, A and B. Two regions, A and B, then the covariance of the integral of the space-time white noise over A with that of the integral over B is just the area of the overlap. So it's a pretty easy object. It's not a function. It's a generalized function and has negative regularity. Now, I'm going to define something that's related to directed polymers. And I haven't really talked about this, but But okay, so let me. The first thing is just not at all related. It's just, I'm going to say that I'll call Z sub 0 TX identically equal to 1. So this is just a base case. And then for K bigger than or equal to 1, we're going to define, as you could imagine, ZK TX in the following manner. So let me give a picture and then I'll give a I'll give you a formula. So, we're going to imagine the following. So, we have time and space. And then we have space-time point noise, which, okay, of course, it's not a Poisson-Point process, but I'm just imagining it's something like a Poisson-Point process. Of course, it's some sort of weak limit of that. Now I have Now I have a starting point, and let's say an ending point, which is Tx. The starting point is 0, 0. And I'm going to imagine that I have K non-intersecting Brownian bridges. Okay, K is 3 in this case. And what I'm going to do is I'm going to define my Z. Z to basically be the partition function in the language of directed polymers of this polymer, you know, multipath polymer ensemble. So the first thing I'm going to do is just multiply by the heat kernel to the kth power, and that takes into account the fact that I'm dealing with Brownian bridges. So if there was no noise, this would just be, Z would just be exactly equal to this. And then I'm going to look at the expected value over my Value over my bridges, so let me call them b1 through bk, so over the b's. And again, this is a measure on b1 through bk, which is given by k non-intersecting Brownian bridges, all of which start and end, start at 0, 0 and end at tx. Now, of course, there's a little bit of work needed to even define that because if they Needed to even define that because if they start and end next to each other or at the same points, then how can you condition them on not intersecting? Well, you do that through a limiting procedure. You condition on not intersecting with them spaced epsilon apart. You take epsilon to zero and you can argue, you can show that that limit exists. Okay, so that's nice, it's not a trivial calculation, but it's a nice calculation. It could be an exercise, but okay, I haven't written it. But okay, I haven't written it. And then what you do is you look at the exponential, and I put these dots in, and I'll explain what they mean in just a little. In some sense, this will be an impressionistic definition anyway, but I'll explain how to make complete rigorous sense of it. The exponential of the sum of the integral of the white noises along the k path. So j equals 1 to k of the integral between time 0 to time t of the white noise. Of the white noise integrated over the time parameter say s and sampled at the location bj. Yes. Okay, so this is something that's called a continuum partition function, and it takes work to make sense of. So why is this hard to make sense of? Well, the noise is not a continuous function. Noise is not a continuous function, so it's hard to integrate along a random path through that. So, what you could imagine is smoothing the noise. So, a natural thing to do is to replace C by C epsilon, which is given by, say, C convolved with in space, a little nullifier, delta epsilon. So, something that smooths things out, you know, of width epsilon and height, say one over epsilon. one over epsilon. So you smooth in space, that's the easiest thing to do, the noise. Then you can define what's everything here. And the dots, what that means is that it says that, so when I write this, this is like this wick exponential. So we can take this integral of the smooth noise object and then actually subtract, I think it's something like the Think it's something like the convolution of the noise kernel with itself and evaluated at zero. So there's some sort of regularization, so times t over two, which makes this actually a martingale as epsilon. And this thing, so that the dots means doing this. So that's what the exponential dot thing means. means. And as epsilon goes to zero, this has a limit. And this was something that was argued by Bertini and Ken Craney in the 1990s in the case k equals one, but the same sort of argument should go through. Okay, but putting aside the technicalities, this is some functional of white noise has this description. I'm actually not going to really use much about this description. I'll use a property that is proved really independent of this description. But I still wanted to define something, you know, and have things well defined. Define something, you know, have things well defined. From the ZK, what we're going to do is we're going to define a line ensemble. Before I do that, let me mention a connection between Z1 and some objects that you may have heard of. So if I call Z, just dropping the index, if I call Z Z1, then this solves something that's called the stochastic heat equation. In fact, what's written can be understood as a What's written can be understood as a Feynman path representation for the stochastic heat equation. So the stochastic heat equation is the stochastic PDE is the heat equation plus multiplicative white noise times z. And that's why it, you know, it's a potential, multiplicative potential. That's why it admits a Feynman cache representation, and that's what's described up here. Now, if I take the h to be the log of z. log of z, or say the log of z1, then this solves something that's called the KPZ equation, which is a very well-studied model for continuum random growth. So it's a Hamilton-Jacobi equation with additive white noise. So this is really supposed to model, so the stochastic heat equation is a model that relates to directed polymers. Directed polymers. It actually also relates to things like branching/slash dyeing, random walks in random environment. It describes the sort of density evolution in a random environment of, say, a population of algae or a population of disease that's spreading and dying out or growing at random rates. Random rates. KPZ equation is a very important model for stochastic interface growth. Okay, and so the object that we're going to be interested in really is this top, is the Z1 or the H1, you know, the top thing. But it will be useful for me to work with general K. General k and the reason is that there is going to be this hidden Gibbs property. So the KPZ equation doesn't look like it has anything to related to Gibbs properties a priori. But it turns out it does. So let me define the KTZ line ensemble. So for k bigger than equal to one and t fixed, so I'll think of t as some fixed number. Let's define Let's define a KPZ. And in fact, for each key, there's a different line ensemble. A natural question is, you know, is there a stochastic TD that describes the evolution of the Z2, Z3, so on and so forth? And there's work towards this. I'm not sure that it's been totally resolved, but there's work of O'Connell and Warren and subsequent work. And Warren and subsequent work trying to address that. And I think the answer should be yes, but I don't think it's been fully resolved. So the line ensemble will index by t and then we have the curves k indexed by k and we have a spatial variable x and this will be defined as the logarithm of the ratio of zk tx divided by zk minus 1. by zk minus 1 over tx. In particular, h t1 of x is exactly the KTZ equations. And there's actually a particular initial data that's called narrow wedge. I'm not going to really get into the details of what that is. It has to do with what happens as time goes to zero. So the picture for the KPZ, so this defines for me now for each. So this defines for me now for each k, right, so I'll look at the collection of curves, k equals one to infinity. And what I'll get is something that looks like this. And I'll tell you why, again, there's a parabolic behavior. Now, there's nothing a priori that says that the curves need to be ordered. And in fact, they're not necessarily ordered. So this I'll call H1. I'll drop this super. This I'll call H1. I'll drop that super T, H2, H3, H4. They're not necessarily ordered. The reason why they're parabolic is pretty easy to see. So in fact, if I take my H's and I add a parabola X squared to them, it's possible in the normalization I've done, maybe it's X squared over two. But anyway, if I add a parabola back to them, If I add a parabola back to them, I claim that this is stationary. So the collection. So for each K, this is a stationary process in X. This is actually kind of a cute way to see it. If we go back to the definition of ZK, first of all, the parabola has to do with the heat kernel, so there's a heat kernel. So it's a heat kernel, you know, the Gaussian, that sort of thing. I guess there should be a two in the way I've normalized it. So probably down here, I should also have it over two. So if I go back. So, if I go back to the definition in terms of this white noise, if I were to shift my endpoint somewhere else, well, the Brownian bridges will, if I have K independent Brownian, K non-intersecting Brownian bridges and I just shift them all by an affine shift, I actually get back the law of K Brownian bridges, but just with different starting and ending points. On the other hand, if I do. On the other hand, if I do the same affine shift to the white noise, you know, everything gets sheared. White noise, you can show, you can argue, make sense of, is also invariant under that shift. And so what you get must be stationary and up to removing the parapola, which comes from taking the logarithm of the ratios of the p's. Okay, so that the stationarity is actually pretty easy. The important point is that there is a hidden. Is that there is a hidden symmetry or a hidden invariance hidden property? And this is why going to this going to this enhanced structure. So rather than just caring about the KPZ equation, I look at the bigger structure and I try to embed the KPZ equation. And I try to embed the KPC equation into this bigger ensemble. And the reason why I do that is that I now can show that I have a Brownian Gibbs property. So for T fixed, so for any T fixed, my KPZ line ensemble is lagging. Lagging has what I'll call a soft brownie in property. And what does that mean? Well, Well, um Okay, so let me you know what I'm gonna transfer it. This computer is starting to lag, so actually I'm gonna change over and go from pre-made slides at this point. I'm going to switch to a different different computer. So we'll screen share this one. So I'll do the rest off the slides just because somehow the computer is starting to lag on me. Okay. Can people see that okay? Looks good. Yes. So the idea is that we are looking at our line ensemble that I was just drawing, and if we look I was just drawing. And if we look at the any, let's say, k1 less than or equal to k2, and any a between and any a less than b, what we can do is we can look at the law of the line ensemble between curves k1 and k2, or you know, including k1 and k2, between times a and b. And the claim is that the law of that, given the external sigma field, is only dependent on the boundary data, which is to say, The boundary data, which is to say the values at A and B of the curves between K1 and K2, and the K minus, K1 minus first curve on the interval, and the K2 plus first curve on the interval AB. Moreover, the law is given or really invariant under the following resampling procedure. Okay, so what you do is the following. So let's say, you know, here in my picture. Let's say, you know, here in my picture, K1 and K2 are just both equal to two. And so, and A and B are the left and the right endpoints. And so, what I do is I start by drawing a uniform 0, 1 random variable. And now I sample the k2 minus k1 plus 1 intermediate Brownian bridges, in this case, just one Brownian bridge. I sample it according to the law of non-intersecting Brownian bridges, just non-intersecting with them. Just non-intersecting with themselves. They don't care about the curve above or below. Then what I do is I come up with a sort of energy, a sort of cost associated to that curve. And the way that the cost is calculated is I look between each consecutive curve, including the top, you know, the curve that was the bounding curve and the curve that's the bottom bounding curve. And so I look between the k minus first curve and And okay, actually, what's written here is a little confusing, but what I do is I look between consecutive curves, between each consecutive pair of curves, and I compute an interaction energy, which I call h. And so I take this function h of the distance between the two consecutive curves, and then I integrate that over the region from A to B. And I sum that over all curves, and that gives me an. Over all curves, and that gives me an energy of my configuration. That I then turn into a sort of a Gibbs weight by exponentiating it to its minus negative of that. For the KPZ line ensemble, this H, this Hamiltonian, is actually simply itself e to the minus X. And this, the claim is, is that this is the Gibbs property that the KPZ line ensemble satisfies. It's not obvious that it satisfies any Gibbs property. It's not obvious that it satisfies any Gibbs property, but it does satisfy this property with this particular formulation. Okay, sorry, so I do this and I accept the first bridge or the set of bridges for which this energy penalization, this Gibbs factor, exceeds my random variable u. So, this is a little, you know, let's kind of parse this a little bit. So, what happens? So, what happens? So, here I've written down a slightly more general version of this, which involves this e to the minus rx as my energy. If I take r to infinity, then the function e to the minus rx, what does it look like? It's either zero if x is positive, or it's infinity if x is negative, right? So it's either zero or infinity. So it's either zero or infinity. So what it means is that depending on the order between the curves, consecutive curves, consecutively labeled curves, I either, in the r equals infinity case, I either get absolutely no energy or I get an infinite amount of energy. And the condition between the two is if the curves are in order, I get no energy. And if they're out of order, I get infinite energy. And then what I'm doing is I'm exponentiating the negative of the energy. Negative of the energy. So basically, if ever the curves when r is infinity go out of order, I get this right-hand side is zero. And of course, zero will never be bigger than a uniform random variable. So I'll just keep doing this until eventually I get a case where the right-hand side gives me zero energy. In other words, it equals one. So this is this this is um this is what the gibbs property is in the r equals infinity and so r equals infinity really recovers the non-intersecting brownian gibbs property and the way you should think about the finite r version is that this becomes a penalization it's the sort of potential penalization when the curves move out of the correct order correct relative to their index they get a nearest neighbor penalization which is of an exponential Penalization, which is of an exponential form, integrated over the amount of time that they're out of order. And that penalizes the energy, which then makes it less and less likely. Another way of interpreting this right-hand side is that this actually gives you a Roder-Nicodem derivative. Equivalent to this is the following statement, which is that if I look at the law of the KPZ line ensemble, given the boundary condition inside of this region, it is given by a Roderon. Region, it is given by a rather nicodem derivative with respect to, say, the non-intersecting K-Brownian bridges that would connect the endpoints, which is exactly proportional to the right-hand side of this formula. So this is really the Rodenica derivative. I need to normalize it by this sort of expectation value of this relative to the condition, you know, the conditional expectation relative to the external sigma algebra. Okay, so. Okay, so here's a nice conjecture, which actually now seems a little bit more reasonable if you've never thought about this. This gives some reason associated to it. So if you take the KPZ line ensemble and you rescale it, now I claim that you should rescale transversally like key to the two-thirds, and in the vertical direction, like key to the one-third. And by vertical direction, I mean, you know, each. Vertical direction, I mean, you know, each curve is in the t to the two-thirds range, the fluctuations of it will grow like t to the one-third. So you rescale the values by t to the one-third. Then the conjecture is that as t goes to infinity, and maybe there's some constant factor of two or something, forgot, but this will converge to the Airy line ensemble minus a parabola, which actually has this H infinity Brownian Gibbs property. Now, why would one believe this? Well, One believe this? Well, what we know to be true, what we proved about a decade ago, is that if we look at just the top curve and we look at the one-point distribution under this t to the one-third scaling, it will converge to the one-point distribution of the Aeri line ensemble. In other words, this Tracy-Whidem GV distribution. So we know the one-point distribution of the top curve converges, and we that actually using the Gibbs. Using the Gibbs property, using the same type of arguments, a little bit more complicated as I gave in the previous lecture, you can extract tightness of the entire rescaled KPZ line ensemble from that. But we don't know how to prove uniqueness of the limit. But the conjecture is that there is a unique limit and it is the Airy line ensemble. And at the end, I'll actually kind of explain that there is a sort of uniqueness characterization result that would essentially. That would essentially give this. But this is kind of a reasonable conjecture from that. And so, like I said, what we know from this conjecture is the one-point convergence of the top curve. So, if you look, and of course, by stationarity, you can look at different locations, but if you look at the top curve, it turns out that there is a centering that needs to be accounted for. That will fluctuate like t to the one-third and converge to this law of the A10. This law of the A10. Okay, so this is what this combined with the Gibbs property gives us and a belief of kind of uniqueness of these infinite volume Gibbs measures will give us this conjecture. I don't know how to prove it. So what I want to do now is I want to show how this Gibbs, how this KPZ line ensemble is useful. Before I get to that, is it clear? Before I get to that, is it clear what is the Gibbs property for the KTZ line ensemble? Again, it's like the Brownian Gibbs property. It's a soft version. It penalizes you for going out of order rather than just conditioning on non-intersection with the curve, you know, the curve that bounds you above and below. So this is a soft version. So actually, an interaction between the curves, you know, if I have Between the curves, you know, if I have k1 to k2, those are also interacting with themselves in this soft manner. I should, I think, I wasn't clear about that when I said that. Okay, but you can kind of imagine that it's some generalization of the structure that's present in the area line ensemble minus the parabola. So now what I want to do is I want to show you how you can use this gives line ensemble property, this kind of hidden property to extract nice information. Of hidden property to extract nice information about the KPZ equation that couldn't really be extracted by other means, at least so far. Okay, so why might we care about the KPZ equation? Well, it's supposed to model lots of things, in particular growing interfaces. And this is like a picture of a little bacterial colony that's growing over time in an outward direction. It's not growing perfectly. You know, there's randomness in there. There's randomness in there. And what you see, and of course, this is just a drawing, is that there are little bumps, and the bumps kind of propagate in time. And eventually, the bumps will die out. And here's a region where there was no bump, but then a bump evolves. And a natural question is to understand how does the KPZ, how does KPZ growth, in particular modeled by the KPZ equation, how does this decorrelate in time? And what I And what I really mean by this is how does this decorrelate in a long time? So the K-PZ equation is believed to be a kind of a universal model for randomly growing interfaces, but the universality is supposed to be that its long time behavior is supposed to represent something universal. So what I'm really interested in is kind of looking after, you know, some. Is kind of looking after some reasonable amount of time, call epsilon a sort of scaling parameter. I'll look after a time which is of order epsilon to the minus three, but then I'll look in that scale at a few different times, t1, t2, t3, and I'll see how that behaves. So that's what these little pictures are. These are different times. And the belief is that the correlation should decay in this epsilon to the minus three scale. With a so as I vary t in that scale, I'll see some non-trivial correlation structure. And then I also, if I vary, and I'm not really going to do this, but if I vary transversally like epsilon to minus two, it'll decorrelate in that scale. And the scale of the fluctuations itself should be of order epsilon to the minus one. And so, really, the two-thirds and the one-third that we saw earlier in the non-intertime. In the non-intersecting geometric random walks, is present here as well. So I have a two-thirds coming from space and time, and the one-third from fluctuations and the time exponent. So the belief is that actually the KPC equation should kind of have an interesting scaling limit when I introduce this epsilon and I rescale it in this 3, 2, 1 manner. So epsilon to minus 3 times a time parameter, epsilon to minus 2 times a space parameter, I need to center it. Two times a space parameter, I need to center it and then I scale the whole thing by epsilon to the one. And this I call h epsilon. This is this sort of renormalized KPC equation that's supposed to be had universally as epsilon goes to zero. And like I said, what we know is that for any fixed t and x, we know one point convergence to the Tracy-Wittem GUE or some rescaled version of that. Another thing that we know that I just indicated is that if we fix t, in which case then what we're looking at is really a rescaled Then, what we're looking at is really a rescaled version of the top curve of the KPZ line ensemble. Then, using the Brownian-Gibbs property, the soft version, this H1 Brownian-Gibbs property, we can prove spatial tightness as epsilon goes to zero and that the limit points are absolutely continuous with respect to Brownian bridge. And that's something that we did with Alan Hammond. And that uses just the Gibbs property and the one-point tightness. But what I want to talk about is that what about the temporal behavior? What about the temporal behavior? What if I look at the correlation at two different times, let's say along such a ray? How does it behave? What happens when the times are close together, and what happens as the times spread apart? And this is something that people actually study in experiments. And they observe certain scaling exponents. And the question is, do those conform with what the theory is predicting? So I want to give you that. Okay. I think that's, yeah. So here's for those who forgot what is the correlation of two random variables. It's a number between zero, minus one, and one. It's given in this form, center them, take the product of their centered expectation, or the expectation of their products, and divide by the square root of the variances. And what the theorem that we proved, and this is with Alan and Promi, is that if we look at the KPZ, this sort of rescaled. KPZ, this sort of rescaled, you know, this sort of the real model for KPZ growth. If we look at the time one and then a time which is one plus beta at position zero, I guess I dropped the zero here. There should be a comma zero in the second term. So I look kind of a long array in my picture and I look how much, you know, what's the correlation between the height fluctuations at this time one, which really is time one. 1, which really is time 1 times epsilon to the minus 3, but and then at time 1 plus beta, which is really 1 plus beta times epsilon to the minus 1 to the minus 3, how correlated are those random variables? Of course, you imagine that as beta goes to 0, the correlation should go to 1. And as beta goes to infinity, you would imagine it goes to 0. Now, it doesn't need to. It could always be 0. You know, it could be hidden. You know, it could behave weirdly as epsilon goes to zero if I kind of got the scaling wrong, right? If I actually had scaled things with a different scaling exponent in time and I scaled, you know, I didn't do this three, two, one scaling, I might actually miss the transition between correlated and uncorrelated. And so what this is really proving is that that three exponent for time is the correct exponent. And it's moreover refining it and describing the correlation structure around that. And so what we prove is that. Around that. And so, what we prove is that as time basically is for all small enough epsilon, the correlation behaves like this. As epsilon goes to infinity, as my points move apart, as I kind of look after a really, you know, at one time and then after a really long time, I grow the process much further, then the correlation is decaying like that time difference to the minus one-third power. On the other hand, if I take the two times together, then it will be times together, then it will behave like one minus the time difference to the two-thirds power. Moreover, if I take any subsequential limits, we can show tightness of this H epsilon object as a space-time process, we can show that you essentially have Holder one-half trajectories in space and one-third trajectories in time. And there's just one point to be made about this, which is that if you look and you do stochastic PDEs, Stochastic PDEs, if you study the stochastic heat equation or the KPC equation in finite time, small time, you find that the holder continuity in space is a half minus and in time is one quarter minus. So there is a different holder exponent for the short-time behavior of the KPC equation and the long-time rescaled behavior. There's a sort of effective change in the scale of continuity that's arising from this limit. So, what this really tells you is. And so, what this really tells you is that the short-time KPZ, the equation, the stuff that's accessible from stochastic PDE methods alone, is really, in a sense, a different beast than the long-time limit. And the long time limit is an object that's very important in integral probability. It's called the KPZ fixed point. And for the KPZ equation, it's only conjectural that you have convergence to the KPZ fixed point. There are other models for which you can prove and actually construct the KPZ fixed point, and I won't go into that. The KPC fixed point, and I won't go into that. So, the caricature is that the correlation looks like this: it goes from zero to one with these sort of exponents two-thirds and minus one-third, in describing that transition. So what I'd like to do is I'd like to substantiate this theorem, and I'm only going to address the remote correlation case, but I'll give you a sense of the tools that go into it. And there are really three ideas in the proof, and once I'll explain this. I'll explain these briefly and then I think I'll try to come to a close pretty soon because I see that we're almost at two hours. There are three ideas that are used in the proof. So the first idea is to show, you know, when you're dealing with the correlation, you need to deal with two different times. And a priori, there's no relation between the two times of the, you know, this two joint distributions and the KPZ line ensemble. And that's something I'd like to use. And that's what I list as the second. Use and that's what I list as the second ingredient. So, what I'm going to do is I'll first describe a variational formula that represents the two-time distribution in terms of two independent solutions to the KPZ equation. And because of that, I can represent then the two-time distribution in terms of two KPZ line ensembles. And I can use the line ensembles to study the variational formula. And the third thing I need, which I'm not going to go into, is one-point tail-downs. And we already saw that when we do these. And we already saw that when we do these sort of regularity arguments using the Gibbs property, we need tail downs to kind of regular, you know, to control the endpoint, the deterministic locations and how stuff behaves. So let me describe to you the first part and then how does the KPZ line ensemble play into this. So the picture will do it, you know, will do for us. So remember, let's just take k equals one. So we had this representation for We have this representation for the stochastic heat equation, or for the Z1, as a single Brownian path moving through space-time, integrating spacetime white noise, exponentiate it, and then take the average over all such Brownian paths. That was the definition. We could even do that not just starting at 0, 0, but starting at any location Sx, for instance. And that's what we call this field ZSX TY. Now, I claim that. Now, I claim that there's the following sort of semi-group or Chapman-Kormogorov property for this field, which is that if I take the field between 0, 0 and Sx, and then between Sx and Ty, and I integrate over the intermediate location x, and this could hold true for any s between 0 and t, that what I get is the field that just pastes together the two endpoints. Now, this is kind of clear when you look at the finding paths. Clear when you look at the finding paths representation. All you're doing is you're figuring out where you went through and you're integrating over that. And then you're using the fact that the integral is additive. I can split it up. The space-time white noise is independent on the two different time intervals 0 to s and s to t. And so I get independence of these two representations. Okay, so this is a fairly simple thing to say. Of course, if there was no noise here, this would just be the semi-group property of Group property of the heat equation solution of the heat kernel. But that same holds true for the stochastic heat equation. And these are independent. Now, what does that tell me is that if I'm interested in the joint distribution of, say, the value at zero at time s and the value at zero at time t, I can represent that in terms of two independent copies of the stochastic heat equation, one which is based on the equation. Heat equation, one which is basically running from time zero to time s, and the other which is actually run backwards from time t to time s. And that's because space-time white noise and everything, the Brownian transition probability is reversible. It's invariant under flipping around the order of time. So if I do that, what I find is the following representation. So if I look at, and here I'm just kind of putting in all of the scalings that I was interested in. Scalings that I was interested in. If I look at the law of, I'll go to logarithms now, right? So logarithms, things become kind of additive in some way. If I look at the law of my h at time one and position zero, and my h at time one plus beta and position zero, then that's the same as the joint distribution of h at time one position zero, and then a composition I'll call supsuit. I'll call soups of epsilon between the process, which is H1X, and an independent process I'll call H tilde beta minus X. So the X is going to be varying here. The 1 and the beta are fixed. So what this H tilde is, is it's really coming from this second independent solution, the stochastic heat equation, working on the independent noise, and it's really just a rescaling of. Rescaling of in law my H, the H1, which is kind of the object that I'm going to think of as the canonical version of this rescaled KTZ line ensemble. Now, when I do this, I need to do a scaling of space. You know, there's some one-third, two-thirds scaling. So I do a scaling of space by beta the minus two-thirds, and I scale the output by beta to the one-third. So this is basically what I'm doing is. Basically, what I'm doing is I'm taking the supremo, this epsilon version of it, which I'll describe in a moment, and I'm putting together two independent solutions or top curves of the KPZ line ensemble, but one of them I'm scaling in a diffusive manner with respect to beta, with this one-third, two-thirds scaling. Now, the soup epsilon is basically a soft version of taking the soup of a function. It's the you know the law. The you know, the logarithm of the integral of the exponential. This is, if you look at what happens as epsilon goes to zero, this exactly converges to the soup of the function f. So, for all purposes, I'm just going to imagine replacing this soup epsilon by just the soup. And let's just think about this. And what I'll actually think about doing is replacing, I'll explain the argument just with the H epsilon replaced by the top curve of the Airy line ensemble minus parabola, and the same thing for the H tilde, but just Same thing for the H tilde, but just with the different scaling, just beta to the one-third, beta to the two-thirds scaling. So the whole problem kind of boils down to understanding. There should be a plus sign inside the soup. Right? Yes. Sorry, yeah, yeah. There should be a plus sign in the foo, yes. Not a comma, plus. Not a comma, plus. Yeah. And so what it really boils down to is that I imagine that I have one curve that's, let's just kind of go back to the KD, to the Airy minus parabola minus one. So I have the top curve, and I'm assuming the parabola is like x squared. And then I have the other parabola, you know, the other, and it's just a rescaling of an independent top curve of. Curve of Aerialine ensemble, but with this beta to the minus two-third and beta to the one-third. So, if beta is large, what's happening is it's actually the top, you know, this second independent curve is actually getting wider and wider and wider. And what I'm interested in understanding is the joint distribution of the value at zero of this bottom curve and the value of this at that maximizer, which is actually equivalent to the minimum distance between the To the minimum distance between the way I've drawn it, the minimum distance between these two curves of the sum. And what I'll argue is the following, which is that, and this is true kind of in a uniform way in epsilon for this picture here, which is that the location of the maximizer stays tight as epsilon goes to zero. It doesn't start going away with epsilon. It doesn't scale away with epsilon. It doesn't scale away with. Epsilon. It doesn't scale away with beta either. It really just stays tight right around zero. And moreover, the increments, so if I can show that the maximizer is very close, is pretty close to zero, order one away from zero, I also want to argue that the increments, the difference between the value at zero and the value at the maximizer is not too different. And if I can do that, then I can actually control the Then I can actually control the correlation. And so that's what I'm doing here. If I'm looking at the correlation between the bottom curve at zero and the sum of the bottom and the top curve rescaled at the maximizing point, which is essentially, if I replace the soup epsilon by the actual soup, which is the value that I'm actually after, then I can rewrite this as the value. The value at zero of the L curve plus the value at zero of the L tilde curve times beta to the one-third plus two errors, where the errors are the distance, you know, the difference between L of X star and L of zero. And then this E tilde error is the distance for the L tilde, but with this beta the one-third, beta to the two-thirds scale in this diffusive scale. And so, what I basically need to argue is that. So what I basically need to argue is that x star e and e tilde all remain order one as epsilon and beta go to epsilon goes to zero and beta goes to infinity. And if I do that, then I will get the correlation behavior because I can basically forget about the errors. And if I forget about the errors, then I'm looking at the correlation of L with itself and then something independent. And so the correlation, the covariance term, is just going to be the variance. Is just going to be the variance of L at zero. And on the denominator, I'll have the square root of the variance of L at zero. And then the variance of beta to the one-third L tilde at zero. And this beta to the one-third, everything else stays order one. And the beta to the one-third comes out, and that gives me that my beta to the minus the third decay. So everything really boils down to controlling small-scale oscillations, and that's the last thing I'll explain how to do. And that's the last thing I'll explain how to do: how to control small cell oscillations. So, let's forget about the K-PZ equation, forget about the K-PZ line ensemble. Let's go back to the Airy minus parabola, our friend from the past lecture. And what I want to argue is the following result. So, you have your area minus parabola, so it looks kind of parabolic. And I look on a small interval between zero and eta. 0 and eta. And of course, I can shift that around, but that's between 0 and eta. And I want to ask: what's the probability, the soup of the difference between the value at 0 and the value somewhere, sorry, this should be L of X, not L of eta. The value between L of 0 and L of X, the soup on the interval 0 to eta, what's the probability that that exceeds eta to the one half times s? Now, eta to the half half of the half Now, eta to the one-half is natural, right? If you think about, forget about Airy Line Entumble, think about a Brownian motion. For a small interval, what's the probability that it goes up high? Well, the right scaling is eta to the one-half, and then you kind of look in that scale, and of course, for a Brownian motion, you see a Gaussian decay as s goes to infinity. And what I claim here is the same holds true perfection. Holds true, provided that I also assume that the Airy line ensemble at zero and at two and at eta and at minus two plus eta, so at four deterministic times, is not, say, larger than s in magnitude. So, what I basically need to do is I need to pay a small cost to control the values at specific times, that at deterministic times, that I can do using test. Times, that I can do using tailbounds. Once I do that, then I can argue that the probability of these sort of extreme jumps, it's not quite a no-big max, but it's kind of a no-big change, that these extreme jumps on small intervals has Gaussian type decay. So the proof of this, it splits into two things. One has to do with a quick, you know, a large fall. So what's the probability that the value So, what's the probability that the value decreases quickly in this interval to below s times eta to the one-half? And what's the probability it increases? So, we'll call the, we'll first talk about the fall event, the probability that it goes down very quickly. Well, the good news here is that we know that the value at zero is controllable. It's between minus s and s. And we know that the value at two is also controllable. And so the problem, or for that matter, at eta, but let's say at two. But let's say F2. So the probability that a fall occurs that you really drop down, well, remember there's this second curve of the Airy Line ensemble that's kind of propping up the first curve. But if we just drop that, we remove the second curve, the top curve will drop, but it will drop at most to being a Brownian bridge. And a Brownian bridge between some reasonable height and some reasonable height on an interval of 0, 2, reasonable meaning between minus s and s, the slope of that is. And s. The slope of that is at most, you know, 2s, or no, it's just, I guess, the slope is at most s. And if you look at a Brownian bridge that has slope s, you can still argue that on a small zero to eta interval, the probability that you decrease by more than s to the s eta to the one-half is still exponentially Gaussianly small. And that boils down to the fact that the slope is like eta, but eta but the scaling of the the change we're interested in is like eta to the one half and for eta small eta is much smaller than eta to the one half so eta to the one half is still a really big change even relative to the slope that we're looking at okay so this rules out you know this shows that the probability of a quick fall is is Gaussianly small so that's what's described here What's described here? Now, what about a rise, a quick rise? Well, if we go back to the line ensemble, unfortunately, monotonicity doesn't help us, right? Because the second curve of the line ensemble actually has the tendency to push the first curve up. So you could think that the second curve actually might induce a large rise. But in a sense, that's the same issue that occurs for the no big max lemma that we spoke about, right? No big max. That we spoke about, right? No big max ruled out the probability that something got really big at a random time. Now, of course, monotonicity says that the curve beneath you might push you up, but no big max gave us a way to say that, well, if that happens, then it actually implies something bad at a deterministic time. And so the same argument goes through here. What we do is we say, well, let's take a rant, a stopping time, the first time between zero and eta that we exceed a We exceed a change of s times eta to the one-half. If that occurs, then the path that remains between that time and two is bounded below by a Brownian bridge, and the Brownian bridge is 50% likely to be above the slope, you know, above the linear interpolation at time eta. And of course, that by again, the smallness of eta to eta versus eta to the one-half, that thing, and because the bounds. That thing, and because the bounds are, you know, the starting and the ending points are not too sloped themselves, that thing becomes an unlikely event, which means that eta, that the existence of the stopping time chi itself was unlikely. And so what we're using here is the strong Gibbs property on the interval chi to 2, along with, in fact, the fact that we can show that a change between zero and time eta by, say, eta by say s eta to the one-half over two, which is still a big change, that that's unlikely. And to do that one, we use the fall argument. We already showed that if we look in the other direction from eta to zero, that would correspond to a large fall. And the same argument works in that direction. Okay, so that is the argument that gives us the control over all of these errors that we needed to control to prove the correlation. To control to prove the correlation behavior. So we see that, you know, by having this structure, the Gibbsian line ensemble, we were able to make arguments work that kind of married together probabilities, probabilistic tools, and integral tools. And so in the last minute or two, I just want to mention three directions or three questions. The one direction, which I hope to cover, but you know, is a little too much for these lectures, is about the origins of these. The origins of these integrable line ensembles. I gave you the origins of the Airy line ensemble. I haven't explained to you the construction of the KPZ line ensemble. Where does this H Brownian Gibbs property come from? And in fact, I haven't given you the full context. So it turns out that there's a number of very interesting discrete line ensembles that have to do with generalizations of the KPZ equation, which are called stochastic vertex models. And these have to do Vertex models. And these have to do with generalizations themselves of Schur processes where you replace Sure polynomials by things called spin-hall little bit or spin-Q-Wittaker processes. And so this is a theory that's not fully developed. There's some work on it, but there's a lot to be done in that direction. And it's a useful thing. It gives you tools in all of these places. In a sense, it gives you a tool whereby, even if you don't know how to take asymptotics of a lot of Know how to take asymptotics of a lot of the formulas. If you can use this, it gives you kind of properties of the limit, even though you don't necessarily have all of the structure that survives from the integral. Another question has to do with other initial data. So it turns out the variational type formula allows you to access other initial data, one-point distributions, to express them in terms of variational formulas. There's that's kind of one-point distribution. There's very nice work. One-point distribution. There's very nice work of Alan Hammond in the context of Brownian last passage percolation and the Airy line ensemble or Dyson-Browning motion line ensemble on the patchwork quilt, which relates to other types of initial data. There's work of Duverne, Arthmann, and Birag, where they actually show something which is very nice, which is that a very important object in constructing the KPC fixed point called the Airy sheet, which is really like the limit of this. Which is really like the limit of this Z S X T Y, this sort of sheet that goes from point to point, that the limit of that exists and is actually constructed from the Airy line ensemble. So in a sense, the Airy line ensemble, all of these line ensembles, end up being really the central object in world probability and describing everything. And to couple, you know, so given that, given the importance of that, it's very natural to ask about uniqueness. And there's a conjecture which you can find written in the paper of Alan and myself on the Airy Line Ensemble, which says that the Airy Line ensemble is in a sense unique. It's uniquely characterized by the fact that if you subtract a parabola, you have the Brownian-Gibbs property, the non-intersecting Brownian-Gibbs property, that it's stationary and that it is ergodic. And of course, there's And that it is ergodic. And of course, there's also some sort of shift invariance that you need to mod out by. But the belief is that this is somehow conjectured, you know, conjectured uniqueness. Now, if you can prove such a thing, it would be nice because then it gives you a way to prove that things go to the Aerial line ensemble without needing to study formulas by just by kind of proving that properties converge, like for the KPZ line ensemble. Let me mention a really nice result in this direction recently of. Recently, of Konstantin Matetsky and Yevgeny Dmitrov, which proves a different uniqueness characterization. It proves that if you have any Brownian Gibbs non-intersecting Brownian Gibbs line ensemble, the law of the top curve determines the entire line ensemble's law. So the boundary data just given by the top curve completely determines the entire line ensemble. In particular, if you have two line ensemble ensemble ensemble If you have two line ensembles whose top curves are both the Airy2 process minus parabola, that implies that they're both actually the Airy line ensemble. And that could be useful because if you're trying to prove, say, the KPZ line ensemble goes to the Airy line ensemble, but you can only prove that the top curves converge, you could probably turn this into some sort of effective theorem that would imply the entire line ensemble converges. Okay, so our message was give. So, our message was: Gipsian line ensembles are interesting and they're useful. And I tried to explain this in the sense of non-touching random locks, insure processes, the ARIE line ensemble, and then I gave you the KPZ line ensemble and the application of it. Okay, so sorry that all of this got compressed, but I think it is very important that we really recognize and pay attention to what's going on in the world today. And I think it's great that tomorrow. And I think it's great that tomorrow is the day for that. So I'll stop here and be happy to take any questions.