Good morning, everyone. Thanks, the organizing committee, for inviting so that I have a chance to share my research with all of you. I'm going to talk about matrix completion with model-free weighting. I will begin with a brief introduction to the problem of matrix completions in case some of you are not familiar with this problem. The goal of matrix completion is to complete and deloyge a high-dimensional matrix from its parcel and possibly launch the observation. So, it has many related applications in areas such as In areas such as bioinformatics, collaborative filterings, and even quantum computing. So, a very popular famous example of matrix competition problem is LaFix Prize problem. Back in 2006, LaFix offered a grand prize of 1 billion US dollars to the winner of a competition. In this competition, the goal is to predict or complete missing entries of a big rating matrix. In this matrix, In this matrix, the rows correspond to users and columns correspond to movies. So this matrix has around like 500,000 rows and 20,000 columns. So of course, I think most people do not watch all the movies in ethics. And even someone watch all of them, he or she might not rate all of them. So there's an interesting study here about how many days you have to. How many times how many days you have to spend in order to watch all the content newly added in 2017? So, as expected, their observation rate is very low and there's a very high lump of high missing rate. So, in this data set, there are around only 1% of observed entry out of all the entries in this matrix. So, in fact, like for quite a number of For quite a number of matrix completion problems, the observed rate is pretty low. Without any structural assumption, it's virtually impossible to recover this target matrix. A very common assumption is low rankness, which lead to rank minimization, and the corresponding optimization is very challenging. It's a very challenging long-commerce optimization problem. In the last decade, there has been a few breakthrough. Breakthrough. Some seminal works actually show that a perfect recovery of a lower matrix can be achieved by a convest optimization. In other words, this problem can be solved efficiently in practice. Most of these earlier results are based on noiseless setting, where the entries are observed without any error. As for noise images completion, some earlier works focus on arbitrary loise. And there are more recent work. And there are more recent work from the statistical communities that based on random error model. And my works are mostly based on these settings. So as for the missing structures, most work with theoretical guarantees is developed under a uniform missing structures. So in this structure, every entry is assumed to be observed with the same probability. In a number of data sets, this is not a realistic assumption. This is not a realistic assumption. And expressed, for example, in this data set I'm going to demonstrate later on with our method, which is called Yahoo Webscope data set. We have some empirical evidence that it is not uniform. So for data time constraints, I'm not going to talk about the details. So existing work under non-uniformity is relatively sparse. I will focus on our class of work that aims for active adjustment. That aims for active adjustment via a model of missiness. So, this kind of work tries to improve the estimation by modeling the missing structures and actively adjust for the non-uniformity. So, basically, like every single entry has observation probability and together you have an observation probability matrix. So, this method tries to model this observation probability matrix, and many of the models can be viewed as a special low-rank missing. Low-rank missing structures. So, common model is a product sampling model where the rows and the columns are chosen independently according to some marginal distributions. And this leads to a rank one observation probability matrix. So, and we have a recent work that tried to generalize to a general low-rank structure. And I'm also going to talk about this work briefly so as to motivate our latest work. Our latest work on model-free rating. Sorry. So here is the setup, which is indeed not complicated. So we have a target matrix, A star, and then we have a contaminated version, Y. And then we don't observe all the entries. So we use this Tij to indicate whether the IJ entry is observed. So collectively, we use this notation to represent. We use this notation to represent the observation indicator matrix, and these tij are assumed to be independent below only random variables with the corresponding observation probability denoted by pi ij. So collectively, you collect all this pi ij into matrix, then that forms the observation probability matrix. So this is the simplest setup of matrix completions. So here, essentially, our data are just the observed entries. Are just the observed entries. We don't have any extra like row or column attributes. In fact, like, well, I call it simple, but in fact, like without those extra attributes, it's actually harder to model the observation probability matrix. So I'm going to use some matrix notations. So I will use a few matrix norms. So the first one is New Knorm, which is a convex realization. Which is a convex realization of rank. So that's useful for low-end matrix estimation. And then we have the for Bitness norm, spectral norm, NGY's mass norm. Those are common norms. So and we have a less common one here, which is called mass lump. So this norm is not common, it's not that commonly occurred in other domain. This is defined this way. But this lump is also a common norm that you might see in nature's. A common norm that you might see in nature's completion literature. I'm not going to talk about the details of this norm, but for the understanding of this topic, essentially, this norm is also a convex realization of rank and can be used as an alternative to alternative to nuclear long for low-rank matrix estimation. So, we and I have not, and here's the notation for Fubinus inner products. Venus inner product, Hadama inner product, and also I lead annotation for NGOs power. So in this area to estimate A star, our very common framework is empirical risk minimization. So the typical target risk takes this form, which kind of treats all the entries equally. So under uniform sampling, that means when all pi ij are the same. When all pi ij are the same. A typical empirical risk that's adopted in the many estimator is the uniform empirical risk, which take this form. Essentially, it's just a scale sum of the errors on the observed entry. So this is unbiased for pi times r. And for minimization purpose, you can ignore this constant multiplier pi. So the empirical The empirical waste in minimizer take this form here. Of course, we minimize empirical with and we also have a class here which is called a hypothesis class. That's the model, basically a model for the true A star. So if you're interested in low-rand matrix estimations, then you will pick a set of matrix, a set of low-rand matrix. Matrix sets of lower end matrix as the hypothesis class. But this leads to highly challenging long commerce optimizations. So a popular alternative is a nuclear long ball. So and another alternative is mass lon ball. So if you so by method of Grangians, you can also translate this constraint optimization into this penalized form. So, and of course, like as I said, like uniformity is not uniform, some uniform business is not that realistic for some application. So, a common strategy is to is to use the weighted empirical width to solve that long uniformity problem to actively adjust for long uniformity. So, electrochoice is inverse probability as the probability is unknown. Yes, the probability is unknown, so most methods with IPW insert the estimated probability based on certain models. There are two related questions here in order to use the IPW. First, how to model the high-dimensional matrix pi. So not standard like we don't have extra covariance here. We don't have extra attributes. So there are not too many models for high-dimensional probability matrix. Dimensional property matrix. So, second, what is the effect of the estimation error of pi to the estimations of a star? Because a star is our target. So, when so in our previous work, okay, we extend the random missing structures to a low-wind generalized linear modeling of pi. So, I'm going to talk about this work only briefly to MODI. only briefly to moderate our latest work on model free rating. So basically in this work, we just essentially use the generalized linear model idea, right, to link the probability with another parameters through link functions. And then we want to actually impose a low-end modeling on this matrix M however like the probability is the observation rate could Is the observation rate could be very slow, it could be very low. So, um, so, so, so, you, so, so, typically, if you try to vectorize, uh, put a low put a nuclear low vectorization, let's say, on M, then it will vectorize it to, it will actually shrink it to zero. Well, if you use logistic length functions, then that corresponds to shrinking the property to 0.5, which is not good. So, that's why we need to, that's why we separate the evats. So, a bit those details. So, a bit those details are not very, very important. So, but when so we have the model, and then we well, as soon as we try to look into the method, we found some challenges with the IPW approach for matrix completion problems. So, in matrix completions, due to the high missing rate, so researchers are generally interested in settings with diminishing probability of observations. Of observations. So, in other words, the probabilities are not uniformly found away from zero. So, this causes like two problems. First of all, like for the true probability matrix, there exists small observation probabilities and that could lead to extreme weights. So, IPW could be unstable. And second, as for the estimations of the probability, if you adopt a more general or more flexible high-dimensional probability matrix estimation like what we have. The matrix estimation, like what we have here, like a low-end estimation. There is a possibility that this diminishing probability setting can lead to, can worsen your estimation. In fact, it could change the rate of, it could change the convergency rate. So that causes another issue. So in this work, we kind of like solve this problem by imposing some extra, imposing a re-establishment. Imposing a re-estimation to re-estimation procedure with some constraint. And the constraint has to be chosen aggressively. With quite a bit of work, we can actually show that like this estimator can achieve optimal rate, near optimal rate. But the constraint level has it's very tricky to select in practice and it's also very difficult to analyze theoretically. So one observation that I got. One observation that I got from this work is that applications and analysis of conceptually simple IPW could be quite long-trivial in a ratio-computation settings due to the high dimensionality of the probability matrix and also the diminishing probability. So this is the motivations for our work on model V-weighting. So in this work, we target as a method that does not require explicit modeling and estimation of the probability matrix. Probability matrix. And second, we do not construct weight through inversion. So we're trying to break away from the RPW framework. So this is how we achieve that. So we call we have this weighted empirical risk, right? So and we and if and for motivation purpose, let's try to assume that the observational error of the entries are zero. Of the entries are zero, a lowest setting. In the lowest setting, this degenerate the weight amplitude degenerated into this form, and the corresponding fully observed ampigues take this form on the right-hand side. So, we try to find weight that make the left and the right-hand side similar. And in other words, we try to find weights that balance the left and the right-hand side. So, if that can be achieved, then Achieved, then we can use like the weighted empirical width as if we were using the fully observed empirical width. This sounds pretty ambitious and aggressive. We actually don't, when we formulate this, actually we don't know whether it works. So, well, of course, we go on. So here, if you put the, if you kind of look at the difference between the left and the right hand side and take absolute value. And take absolute value, we can define a term like this balancing error. So, the goal is to try to minimize the balancing error, right? Use the weights to find weights that minimize the balancing error. However, like because we're going to use it in empirical risk minimization framework, so we actually don't know what A, which A we want the balance. We want to make the balancing error small, right? So, we have to. Right, so we have to actually balance, we actually have to find ways that have the balancing power over a class of A. So that's, of course, is related to the hypothesis class of A star. And if you look at difference, you can define a class called D, which is induced by the hypothesis class of A star. So this soup, okay, is we call it a uniform balancing error. And we want to balance. Error and we want to find weights that minimize this uniform balancing error. So, um, so of course, like immediately you see that actually we have to pick this uh this hypothesis class, right? So, and see, and there are two considerations in picking this hypothesis class, D. First, from the modeling perspective, well, since D is induced by the hypothesis class of A, and A, a typical And A, a typical assumption for A is low rank or possibly low rank. So it makes sense to try to use a mass lump ball or Lucy lump ball as a hypothesis class of A. So and the current and this essentially just induce a hypothesis class of D for delta, which take this form. It's just the corresponding long ball. If you choose like mass long ball, then this will be then the then D will be also mass long. then the then D will also mess on board with a bigger with a bigger radius so the second consideration is computational so if you look at this here you find that like we try to minimize we try to find the ways that minimize the uniform balancing error right so actually we try actually the corresponding optimization is a mean mass problem so there are like two optimization one nest within the other one so if the inner one that's this means that means If the inner one, that's this means that means this soup has no closed form expressions, it could be quite difficult to actually perform this optimization to find the weight. So well, like then a very lateral step would be, well, try to actually check whether the mass lon ball or the Luke-lon ball does lead to a closon expressions for the inner optimizations. But unfortunately, we cannot find the closing expressions. Find the closing expressions for them. So, well, it actually takes us a while. And then we, luckily, we're able to actually find, we're actually able to develop a set of inequality that lead to a relaxation. So this sets of inequality takes weird form, okay, and especially left-hand side. This is not a very common target, but basically it's constructed based on here. This is C, this is B, right? We have C. C, this is B, right? We have C, how the motor border of B, and the inner part of B. So basically, that is it, that is the left-hand side. So, and and these sets of inequalities are tight in general. So, if you apply them to our balancing error, then you will find this upper bound. And what's interesting is that if you take a maximum ball, for example, like the uniform balancing. For example, like the uniform balancing error, we play the soup of taking the soup of delta and then over with respect to, let's say, the maximum ball, then we find out that the maximum, the right-hand side will be degenerated to just simply a scale version of a constant multiplied by, let's say, this spectral law. Okay, so this actually leads to a PD. So this actually leads to a PD leads optimizations or the choice of W. Basically, we want to find the ways that minimize this TW minus the spectrum of TW minus J. So well, if you go back and then look at my motivations, I'm motivating all these things through assuming the observational error is zero, right? So when the observational error and the observational error is not zero, Observational error is not zero. You look at the decomposition, you can try to look at some decompositions, and you will find out that the weight will become, so there is a term which is related to the Hadama product between the weight and also the error. So in fact, like to control that term, we have to control the variability of the weight. So that's why. W of the weight. So that's why, like, we have this constraint, which can be translated to vectorizations on the variability of W. So, and this is our proposed weight. So, and that's, and this and the corresponding optimization is convex. Actually, you can conduct this optimization through a simple error fund like BFGS. So, and we were, of course, like this task is this weight is, I mean, This weight is, I mean, this is a pretty ambitious task, right? We try to find weights that try to have this balance power over like a mass long ball. So we look in, we do have some theoretical results that essentially give us a sympathetic upper bound for the balancing, for the uniform balancing error. Okay, with respect to this W hat. So that's our estimate. That's our balancing weight. So we provide a band at this point, like this the right-hand side, this band, the specific barn is not very important to most of you, I guess, because well, this band will have to be propagated to like our estimation of A, to see whether this bind is good enough, to see whether control of the balancing is good enough for the estimation of A. So, but we do have a result, okay. uh but we do have a result okay that uh tell us like uh what is uh tell us like uh that control the balancing error so um so uh as for the estimation of a star uh there is uh so uh of course like uh we already picked the hypothesis class for a which is the maximum ball and as for and here we also incorporate an additional new kiel long microization so uh that's motivated So that's motivated by two points. One is that you can see in the middle, so we have this inequality. In the middle, we have this term, right? That actually involves both the maximum and the nuclear long. Might be imposing nuclear long recognization that leads to tighter control. Okay, so and second, this combinations of mass lon and nuclear long is And nuclear long is not new. That has been used in some previous estimators for matrix completions. So, and that shows that this actually achieves stable, this achieved better results. So, that's another motivation for using this hybrid constraint regularizations. So, well, our mode, our innovation of this verse is mostly this empirical, this weighted empirical risk with this local weight. for weights. So this corresponding optimization is CommS. So we can use ADMM average forms to get A head. As if you have a very large scale problem, that means if the A is PD big, it's PD big in dimensions, then we also develop a long convest version through a long convest formulation. That's a bit that's scale. That's a bit that's scale, that's the corresponding alpha is more scalable into the size of the matrix. So we have uh so as so we conduct some direct analysis for our estimator. So one major challenge for the analysis is the estimated nature of the weights. So since the waste are estimated, so the waste are kind of like dependent, okay, possibly dependent, okay, across like different entries. Across like different entries. So if you put it into like the weighted employee risk estimator, then we no longer have some of independent, we don't have this sum of independent random variable structure. So this form a contrast to the uniform empirical width, which is basically a sum of independent terms. So this complicated analysis. So and in our analysis, we also study in our analysis we also study two asymptotic regimes so uh relate regarding the missing probability regarding the observation probability um so the so here pi l represents the minimum uh observation probability pi u represents the maximum observation probability uh across pi ij so the first setting is what we call an asymptotically homogeneous setting so in these settings these two pi l and pi u These two, pi L and pi U, have the same order. So this setting does accommodate long uniformity, does accommodate long uniformity because we just require the order to be the same. So the probability could be different. So of course, this conditions requires that all the observation probability are of the same order. So and this is actually most existing works on non-uniformity are soon decepting. So I could Desetting. So, arguably, this is the simplest level of heterogeneity. So, and there is some empirical evidence that, like, this might not be a good asymptotic model. Because, like, for example, in the web scope data set, we have some empirical evidence that shows that there exists highly varying observation probabilities. So, there are some piergy which is very large, there are some piergy which is very small. So, and as And the second setting is as in particular hideogeneous. That means that you allow the observation probability to have different order, different diminishing order. So, okay, we have assumptions on the error structures, which is subgausian. So, and then we arrive at this result, which actually provide a long as important error bound for them on the matrix recovery. So, there are a lot of details here. So, there are a lot of details here. Basically, you don't have to pay much attention to them because I'm going to explain them on a high level. So, this so, but you can see that the guarantee is on this distance D. So, and this distance is defined in this way, which actually measures the uniform error. So, you can see that all entries are cheated equally. So, this is contrast. So this contrasts to the common guarantee in the form of, in terms of uniform error, which take this form. So this form looks complicated, but essentially the entries are kind of weighted by the true probability, true observation probability. So in other words, like this measure pays less. Place less less emphasis on entries that are less likely to be observed. So, since it focuses more on those entries with higher observation probability, that means with more information. So, you can easier to obtain a band on this measure, and the band could be stronger sometimes. So, in the asymptotically homogeneous setting, these two have the same order. So, if you're interested in order, that does not matter that much which one you consider. But in asymptotical hideogeneous setting, they are not the same. So, especially for the non-uniform error, entries with probability of order smaller than pi u might be ignored within this measure. So, due to the time constraint, I'm not going to actually spend a lot of time on the RC protocol. lot of times on the on the asymptotic homogeneous setting so uh simply speaking simply put our our result is comparable uh to existing error bond um so in this setting so as for the asymptotical homogeneous heterogeneous setting um so uh first of all this setting is rarely studied uh and we're going to focus on like the scaling of the upper bond to the to pi u and pi l that means we focus High U and high L. That means we focus more on how the error band relays or scales with the observation probability. And that's actually the interesting aspect of this hetiogeneous setting. So the only work that I know, okay, that actually studied this before our work on the balancing weight is our pile work on nowhere missing structure. And in that work, And in that work, we provide an extension of existing upper bounds to this setting. And the upper bound scales with pi with pi with the inverse of pi L times the square root of pi u. So, and in this work, we also perform interesting theoretical experiments. Actually, we assume that we know the true probabilities and we try to improve the scale rate. And we can actually improve it significantly to this, to the square roots of, to the inverse of the square root of. The square roots of to the inverse of the square roots of pi L. So, in the asymptotical homogeneous setting, pi u and pi l are of the same order. So, these two are the same are the same in terms of order. So, but in the heterogeneous gene setting, so this is significantly, this could be significantly better than this, because this actually introduces all the differences. So, after that work, it is actually unclear to us whether there exists an estimator that can achieve. Existing estimator that can achieve this scaling without using the true without the knowledge of the true probability. And interestingly, if you look at the theorem two, you'll find out that this our new result actually provide a positive answer to these questions. So essentially, our weighting estimator can achieve this scaling. So, and in our work, we also show a little minimum lower bound. Uh, minimized lower bound that essentially shows that this scaling cannot be improved in general. So we apply, so of course, we have done some simulations which actually show that the estimator performed well. Due to time consumer, I'm not going to talk about them. So here now I focus, I mostly focus on the two on two wheel data. Okay, so the first one is The first one is the coast shopping data set. So, this data set is a smaller data set. So, there are like two subsets that come with the data set. The first one is a training set. The second one is a test set. The training set consists of self-selected ratings. So, this self-selection process could introduce long-uniformity. We don't know. So, and the test set is constructed in a way that like it tries to maintain. That like it tries to maintain uniformity. So it's actually uniformist HRG consists of uniformist letter ratings. So we compare a few different methods in our work. The first one is balancing, is our proposal, the balancing rating. So the second one is a very popular method called soft impute. So this solve imputes. So this method involved it used a uniform and pickle width and combined with a nuclear long recognization. And the next one use mass recognizations. And the next one use a hybrid of mass form nuclear long recognization. And there's also another alternative which also assumes uniform missing structure and adopt nuclear long recognization. So the last two So the last two try to adjust for non-uniformity. So the first one assume random missing structure. The second one is our prior work on no random missing structure. So and here gamma represents the constraint level which I've come in before. This level is not easy to select in practice. So we have so we So we look into, so we compare the method like through three metrics. One, the first one is the root mean square error on the evaluation set. The second one is the mean absolute value or value, means absolute error on the evaluation set. And the last one is the rank. It's estimated rank. So here are the results. I highlighted the best two. So basically, if you look at here, so Here. So ours performed the best in terms of like roomman square error and MAE. So there are some improvement. What's interesting is that the rank is also smaller than the second best. So this is actually quite interesting because it achieves smaller error with a more compact model. So in other words, it's actually the method. Actually, the method how the proposed method is able to summarize or to capture the signal in a more efficient way. So, um, and the next data set that we have tried is a bigger one called Yahoo Web Scope data set. In this data set, we have this in this data set. Sorry, so this data set is from Yahoo and it consists of a bigger data. Consists of, this is a bigger data set. The training and the tests are formulated in a similar way. So here are the results. So we have, if you look at the results, I also highlight the best two. So at the very beginning, we essentially just look at like compare our method with the existing method and also some version of LR. That's our power work. But if we look at this, we actually found But when we look at this, we actually found that our work is actually performed pretty well. But we actually also noticed that our pile work on nowhere missing has a tendency to produce better results if we can't really change the constraint level. So here we try a bunch of constraint levels. So and then just show the corresponding evaluation error. So of course, like this, it's not, of course, like it's this constraint level here. Uh, this constraint level here now is not chosen by the data. So, but it's good, it's good as a reference. So, the bad, so the second best is for the second best result is from the error is from our power work with this constraint level. So, this is indeed actually performing, this indeed actually performs better than balancing rates. So, in terms of MAE, it has a comparable MSE. Has a comparable MSD, but if you look at the RAN, it's actually much bigger. We don't know whether it's needed to whether we can actually form something in between that have both smaller wang and at the same time has a small error. But that's what we got here. So if you look at like the exist other method, there are a few methods that have good results. Okay. So here's CZFLT. Okay, so here's CZFL T N W. So, and interestingly, they also have smaller rank. So, since a very common observation is that a model could, a method could be able to actually produce a little better prediction performance if you allow it to have a higher rank estimate. So, one interesting question is whether this This inferior result is due to like the rain estimate. So we look into this estimator, we vary the tuning parameter, and then we produce a solution path. And in this solution path, there are higher rank solutions, there are low-rank solutions. And we look at the corresponding evaluation error. And what's interesting is that we found that, like, this tuning parameter is already tuned very well. very well so uh basically like you basically you cannot find basically basically we cannot find another so another solution in the path that actually produce a better evaluation error so in other words these models uh they just this small this method they basically do not have the ability to capture more signal in this data set uh essentially to utilize a high-rank solutions for prediction purpose so um So that's pretty much all I have. Like this work, this presentation is based on two works. So the first work is on the low-van missing mechanism. The second work is on the model fee weighting. So these two works, we have due to two different teams. There are some overlap authors. The union of the collaborators are here. So Jaiyi is my current student. Is my current student at AM. She's going to graduate this summer, joining UT Dallas as an assistant professor. And Saojun is my previous student. John is supervised by Professor Songzi Chen and also Professor Dan Nettleton at Iowa State University. And he's currently at Fudan University. And these two works also benefit a lot from the senior collaborators, Professor Gary Chen from U of Washington and Professor Song Xi Chen from Peking U. Washington and Professor Song Zicheng from Peking University. Last, I want to acknowledge support from NSF and also TSS AM University High Performance Research Computing. So that's all for my talk.