If you have not heard enough of Gaussian state spoke today, and if you thought S plus IJ was just written on the blackboard for the last time, it will not. What I'm talking to you about today is an introduction to quantum Gaussian states. So, what will be the sort of focal point of the talk? But let me remind you what a classical normal random variable is. Is. There are many equivalent definitions. For starter, one usually is given this one where a vector is normal distributed if every linear combination of its entries is again normal distributed, even though it's not written on the slide here. And well, this is maybe a good definition, but the way one usually really sees normal multivariate distributions are Distributions are by diagonalizing them. So, in some sense, a vector is normal if you can rewrite it using just a normal standard distribution, one-dimensional normal standard distribution, which are independent of each other. So you start with a vector of independent and identically distributed normal random variables, and you apply a linear transformation and you sum with the mean, and what you get is all. And what you get is sort of all possible normal multivariate variables. Sort of means that in general, you could also have something a bit more degenerate where there is no full Gaussian, but you have just a delta in some coordinate. But this is the starting point. A Gaussian variable satisfies this condition and is characterized by the fact that you can diagonalize it into independent. The first one-dimensional distribution, um, normal variables. And what we're going to do, I hope, in this 20-minute talk is to bring you along the analogy of quantum Gaussian states to get to this point. Quantum Gaussian states are defined in a similar way, not by this property, but instead through the characteristic function, which is another equivalent. function, which is another equivalent way of defining normal multivariate random variable, classical normal multivariate random variable. And eventually we will see that actually even quantum Gaussian states can be sort of diagonalized into independent, simple normal quantum Gaussian states. So which are usually normal like standard random variables, but also the generate ones. So this is the random So, this is the arriving point and analogy of this proposition for the Gaussian states. First of all, for the second time, I would like to talk to you about the setting. This is not actually the focal point of the talk. I know that some of you are not really acquainted with these definitions. I have no time to go into the nitty-gritty of how they are defined. So, I will point to you. So I will point you to some reference, but for now, let me simply say we have a Hilbert space, which is the Bose-Fock space of random articles that Federico introduced really well before. I just need the fact that inside this space, there are exponential vectors, which are indexed by an element of C D, and of which we can compute the inner product with this simple formula. The inner product of these two. This simple formula. The inner product of these two exponential vectors is actually the exponentiation of the scalar product of the indexes. They are dense, so we can work with them and hopefully extend to the whole space just with some topological magic. By operators, again, really important, lots of really nice properties. What we just need is that we can define them on exponential vectors with this expression. It's just a constant. Expression, just a constant multiplying another exponential vector, which just adds the argument of the valid operator to the argument of the exponential vector. It so appears that they satisfy a really, they are bounded operators, first of all, they satisfy this nice formula, and they satisfy the vile form of the CCR. So, in some sense, they are really important in the physical meaning, from which we can get all sorts of From which we can get all sorts of things. And also, they generate the whole algebra of boundary operators. So, again, we just need to look at value operators to hopefully extend all the results to the whole algebra, because valid operators generate all of B of H. So, this is a starting point. Moving on from this, since we will need some of these four Gaussian states, starting with biooperators, we can put a real We can put a real parameter inside the vial operator, and this generates a continuous semi-group. So it has a generator. We write it like this. And with this generator, we can define all sorts of other operators, which are really important in physics. But this is a sort of a cheat sheet in order to get really quickly into how do we use them. And I agree, as you see. Them. And as you've seen, are creation and inhibition operators, so they're important physics for some part. The momentum and position operator, again, really important, but for now, these are just some formulas to get to where we need to. They satisfy all kinds of accommodation relation. And maybe the most important part of this slide is that these important operators that appear in these formulas also appear in the expression for. Also appear in the expression for the vile operator. And this would be the really starting point because in the last one, last expression, we have e to the minus i, a constant which I will disregard with no shame, and a linear combination of p and q. So if one just looks at the at the from a distance, this is exactly what one gets when computing a characteristic function. A characteristic function in classical probability, which is e to the minus i scar product of some variable with a random variable. And here the random variable is supposed to be the vector of p's and q's. And the value on which we are evaluating the characteristic function is the real and imaginary part of z. So this is actually the expression of a characteristic function if p's and q's were just the name for some random variable, classical random variable. Random variable, classical random variable. And actually, this is the starting point. I will gloss off this one, starting point for defining the quantum characteristic function. Since the value operators are so similar to the expression we need to put inside an expectation to obtain a characteristic classical characteristic function, we define the quanto-characteristic function via the expectation that we saw in the previous talk against density matrices. Density matrices, the density matrix, with this quantity, which should be the quantity needed to compute a characteristic function. And well, one nice property that this new characteristic function, well, this new property holds is that this map is injective. So a density matrix is uniquely identified by its characteristic function. So in some sense, one can. So, in some sense, one can speak of the characteristic function to speak of the density matrix itself. The idea, just the idea, is that it should be equal to zero, the expectation of this difference of the two density matrix is against value operators. And as I said, value operators generate the whole algebra. So if it is zero against value operators, we can hope it is zero on the whole algebra. Is zero on the whole algebra, but it is. Just a comment for now, the association is just injective because we don't know what properties should satisfy a characteristic function, a function to be the quantum characteristic function of a state. So can we put any function here? No. So the association is not subjective because we don't know the codomain at this point. We get to that. At this point, we get to that. For now, we have seen many times what is the definition of a Gaussian state. We say, we ask for the characteristic function of being one, which is precisely the expression of a characteristic function of a normal classical multivariate random value. Omega is the mean, as is the covariance matrix. So, this is a starting point. Quantum characteristic function analogous to the classical characteristic function. So, we take the expression for the classical characteristic function. Take the expression for the classical characteristic function of a normal random variable to define a gamma. Examples. What are the examples? Well, let's start with dimension one, which actually are really important since I've spoiled you that we will get to the dagonization property of Gaussian state. So actually dimension one are all the Gaussian states we need to speak also of multimodal Gaussian state. Multimodal Gaussian state. The first example is the one where, well, the mean is zero just for simplicity's sake, and the covariance matrix is just the identity. And this results into the projection on the vacuum. I will not dwell too much into the proof, but the first point is to exploit the fact that we can compute a trace not with the basis, which is the way one usually does it, but Usually does it, but by integrating against all possible, let's say, scalar product against exponential vectors. This is a formula that one can show. I'll give some reference later. And starting with this, since vial operator sends exponential vectors into other exponential vectors, since this is a projection on exponential vectors, and again, scalar products of exponential vectors are exponential of their Exponential of their argument. Well, all this quantity inside the integral actually turns out to be x of a complex number. So this is a Gaussian integral. And you can compute it and obtain exactly the fact that this quantity is the characteristic function of a normal random variable. So, apart from the computation, which takes some time, you arrive here. Yes. Yes. Yes. Yes. That is the case of E0, E0, W is it. Simply E0, W is it E0. Yeah, also. You know W is it. Yeah. Yeah. I wanted to do this because it's easy on your computation and is the method you use to compute the second one. But yeah, actually computing this first trace is far easier than what I show you because you simply put easier on the other part. I put E0 on the other part. But this very argument also works for computing the fact that this is the Gaussian state associated with a covariance matrix, which is not identity, but is a multiple of the identity. Okay, so thanks for the comment. I should have pointed out myself, but the idea is that the computation is a bit longer, but goes through in the same way, actually. So let me point. Let me point out that if the various matrix is the identity, you get a sort of degenerate state, is a pure state, is like a delta Dirac as we just saw. And if it's not, if the identity is a multiple bigger than one of the identity, the coordinate matrix, then you obtain a state which is a gibbed state, one could say. And these are all the things we need. So the first, the next part is why do we need to take sigma bigger than one or c or at least sigma equal to one? Well, this is the analogy, the converse part to what functions can we choose for a characteristic function of a quantum state? Well, the answer is, I'll come to the definition a bit later, is this theorem. A function g is the quantum characteristic function of a state if and only if, and this is the results. And only if, and this is the result similar to what would Bochner's theorem, which states we need positive definiteness. Here we have some continuity conditions and the fact that this kernel must be positive definite. So the real part is that we have a complete characterization of when a function is the characteristic function of a quantum state. And we can use this. And we can use this. Actually, this is what you use to get to s plus ij. You take one of these functions where s and omega now are arbitrary, and you ask, when does this function satisfy quantum Bochner's theorem? Well, turns out the answer is precisely when s plus ij is greater or equal than zero. And since Piju mentioned earlier, I will also I will also give some insight on, well, really quickly, the yes. So all I can ask describes the Gaussian state. And actually, as Franco told you this morning, both of them give you insight on the moments of position and momentum. Of position and momentum in the Gaussian state. So it is a Gaussian state, and S is the covariance matrix of the state since its entries are the variance and the covariance of possible momenta and position. This is what is written in the slide. If you take, in the case where the mean is zero, if you take trace of rho against momenta, momenta, you get one of the entries of S, provided you compute. Entries of S, provided you compute the right constant in the computation. But what I really wanted to show is what is the relation with the Eisenberg identity principle. In the dimension D equals one, you take a matrix S, which should be a real union operator, transformed into a two by two matrix. F plus Aj now has this expression. And if you consider, for And if you consider the necessary condition for it to be positive, which is determinant positive, you exactly get S11 times S22 greater or equal than this quantity. And S11 is the variance of the position. S22 is the variance of the momentum. And therefore, you sort of get an analogous to what was written on the slides in the previous talk, not on the Blackboard. Not on the blackboard. Okay, so this is something that we expect. Now, for the last part, I will show that we can actually diagonalize Gaussian states. So we have seen that a Gaussian state, what is the definition of a Gaussian state? We have seen that for it to be a Gaussian state, s plus ij should be greater or equal than zero. And in particular, this implies that s itself is positive. We've seen examples of one-dimensional Gaussian states. Now let's go back to these. Let's start with a generic S covariance operator. So S plus Ij is greater or equal than zero. Williamson's theorem states that we can diagonalize this matrix. And we can do it with a symplatic matrix. This is important because we want, even though I didn't get into the very details of this affirmation. Into the very details of this affirmation, but we want to preserve the CCRs. We want to preserve the commutation rules of our operators, which can be done as Federico showed actually, by taking a symplatic matrix. And for it to be a symplatic, we can simply look at this algebraic conditions if you want. So now that we can diagonalize the covariance operator, which is one The covirus operator, which is what one usually does in the classical case. Next step is: can we implement this diagonalization onto the space? Can we really act on the environment state in the state in order to diagonalize it? Well, yes, also Federico showed this. You can take a unitary operator acting, therefore, on the Inber space, not on the, let's say, argument of the Fox space, not on C D. Argument of the Fox space, not on CD, but on the state space, but on the whole FOX space. We're changing value operators into another value operators, plugging into, well, showing us how we can n can arise inside the argument of the value operator. And as a nice aside, we can change any given state by this unitary transformation. Transformation into one which has a diagonal covariance operator. And I finished it. It's just the slide. The next one is the last one. So we can diagonalize the matrix, we can implement it on the Fox space. And implementing this transformation on the Fox space, one can diagonalize the covariance matrix of the state. And the last result is that we can rewrite this as a product. Write this as a product of one modal Gaussian state. Namely, starting with, of course, with zero mean Gaussian states for simplicity, we can diagonalize the matrix. This should be sigma. The entries of this matrix can be split into two parts. One which, well, one which are equal to two. One which are equal to one from D to M, and then all the others, this plus one should be a suffix. The other one are actually strictly bigger than one. And in this way, one can rewrite any Gaussian state into the product of Dirac whenever the eigenvalue actually is equal to one. Eigenvalue actually is equal to one, as one does when the covariance matrix is just the identity. You get the projection on the vacuum. And on the other components, whenever the eigenvalue is strictly bigger than one, then you obtain a gate state, actually. Well, a product of the gate state. And so actually, State. And so, actually, we have recollected the theorem proposition on normal multivariate classical random variables where you can diagonalize them. So, this is what I want you to do. Let me just point out some references. The first one is pretty much on Fox bases. The second one covers both Fox Paces and VILO operators, CCRs, and so on and so forth. Instead, for So forth. Instead, for Gaussian states, the 2010 article, as you can see, part of the article also has a great impact in my slides. What is a Gaussian state is a great introductory topic to Gaussian states themselves. And the last result actually comes from this 2013 paper, all the symmetry group of Gaussian states. And that's it. I finished. Questions? Can I go back to the injectivity of the map from row to row head? Yeah. So since you have the Bohemus theorem in this quantum setting, can you now give an answer to the Exactly. Yeah. So if you take a function that, so to say, this exists a state which is a quantum function. You're welcome. So original state doesn't fit positive. Just like inverse to be a function. Yeah. 