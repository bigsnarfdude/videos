This is something I did recently with my former PhD student, Heng Fang Wang. So I'm going to start with some introduction and then introduce the problem and then present the proposal, which is a weight smoothing, and then show some application and similar study. Okay, so Okay, so this is a motivating example that I actually consulted for government in Korea. So the survey that I consulted on was Korean Workplace Panel Surveys. So it is a survey of workplaces, companies, and they are interested in feeding a regression model from the sample. And the regression model is very simple. So the y-variable. So the y variable is a local sales by per person and x is size of company, which is number of companies, employees in the company, and x2 is type of the company. So in this case, these x1, x2 variables are always observed, but the sales data is not available. Available of external. So they have to ask about their sales, right? So why is the subject too missing this? Interesting part of this project was that in addition to the study variable and auxiliary variable, the survey company collected a paradata variable Z regarding the respondents for reaction. The respondents for reaction. So Z equal to one, if it's a friendly, so they react friendly. C equal to two, moderate response. Z equals three, negative response. So not surprisingly, Z equals three is a negative response. That means that the response rate is significantly low for the units with C equals three. So in this data set, the response rate for Z equals one and Z equals two is about 70%, while Z equals three, we got only 45% of the response rate. So obviously, this G variable paradata is a strong predictor, strong predictor for the respaze mechanism, right? Mechanism, right? But it's not really a good predictor for Y because C is the attitude of the interviewee, while Y is a characteristic of the company. So there's no reason to believe that Z can be correlated with Y. In fact, we feel a regression. That we feel a regression and the regression coefficient for C in the regression model is not significant. So they are not basically dependent. I mean, they are independent. So in this case, the question is, so the question that the client asked me was, should we include Z into the non-response adjustment weighting or not? So that is the question. Or not? So that is the question. And, you know, because he is a strong predictor for the response mechanism, the initial reaction would be that we should include. But what if this G is totally irrelevant to Y? Will that still matter? So that is the question that I like to answer. Question that I like to answer. So I'm gonna give an abstract. Now I'm gonna formulate the interest of notation and concept. So X and Y is a vector of random variables satisfying certain estimating integral equation. And so u is often called the estimating function, and parameter of interest is a set. parameter of interest is set up set annull. And I'm going to use red color to denote unknown. Whatever unknown is red. So here, set an al is unknown. So that's why I put I used the red color to denote it. And then if you look at this equation carefully, actually there's a some uh it's an integration with respect to some distribution function, right? And so this distribution function is also this distribution function is also unknown but it depends on seta so so so p is completely unspecified other than the restriction in in this integral equation so this is a semi semi-parametric model in the sense that there are infinitely many p's satisfying equation one for given setup but but we are only interested But we are only interested in settlement, right? We are not really interested in p, we are only interested in zeta. So the model space is a function of depends. So given seta, right? Given seta, the model of p, so the distribution of p satisfying this equation is a depends on, so we can write it border space as a function of setup. Okay. Okay. And then the queue-like divergence of P with respect to Q is known to be this form, right? So we're going to use this one to apply the information projection theory. So basically, we are interested in finding P star that minimize the Kulberg library divergence among this model space. Among this model space, among p in the model space for gibbon setup, where p hat is the empirical distribution in the sample. So that's just the point mass in the sample. Then using the definition of the quiet line of divergence, you can write it this way. And then notice that p hat has a positive point mass only on the mass. Positive point mass only on the sample point. So for non-sample part, this one will be zero. I mean, infinity, right? So, because the denominator will be zero, so that the whole thing will be infinity. So that means in order to avoid the infinity case, the solution should put zero zero point zero mass for any point. For any point with the p-hat equal to zero. So for outside the sample, it also gives us zero weight. So that means we can formulate the problem as just minimizing this point mass, right? Because we now can write it as a no discrete distribution type because of this. Because of this property. And then this discrete distribution should satisfy the constraint. The first constraint is the normalizing constraint. And the second constraint is the constraint in because we are only careful. So this constraint implies this, right? So the PI should depend on setup through this many equation. So that So that problem has a solution, and that is founded by Shenak, proposed by Chenak in 2007 and also statistics. So it is actually a two-step estimation. So each step, basically given p hat, we minimize divergence among the class. Among the class, right? Among this class. And the solution is this form. So the p hat pi star is the projection solution to this optimization problem where this p star satisfies the integral equity, I mean the estimating function equality. After that, SATA is unknown. So, to estimate the setter, we treat it as a density function and then maximize the log likelihood in the sample, right? So, that is actually this form. I mean, as can be viewed as a minimizer of this form. So, step one, p-hat is given on the second term, while in step two, p. While the in step two, p-hat is in the in the first term. Okay, so so I'm gonna give the graphical illustration of this concept. So each step is evaluating the evaluating right the callback libeler. Okay. So so each step So each step is basically minimize the Kulberg library diagons among P in P seta. So for fixed seta, for fixed seta, you can think of all possible p's. So this is a space. In this space, you minimize the projection, right? You find the p star, which The pista, which which is this form, itiform sorry, okay, and then this space is a space of a p-star setas. So this is over seta space, while this is key p space, right? So the EL step. So the ER step is empirical like a step is projection. So data and this is the border space. So your border space previously was general, but now you have an easy step to define the parametric form, compute the parametric form, and then apply the Apply the empirical likelihood, basically maximum likelihood estimation to obtain set ahead. Okay. So the first step, each step, is actually a modeling step that is use the information projection to obtain a dual expression of the model. And the dual model is an exponential tilt inform. And second step is an expansion step. So the EM. So, the EL step is an estimation step, and that is among this class, Buddha class, you now have a maximum like the estimation. So, that is the base, very brief introduction of the information projection. Okay, now the problem is that for this IID case, if the problem is IID, this is the solution. Is IID, this is the solution. I mean, this is the way to go using to solve this information to apply the information projection. Now, the problem is that we don't have IID. We have a non-property sample. So then we can view it as a two-phase sample structure. So phase one, we have a finite population generated from the semi-parametric model. From the semi-parametric model. And then from the finite population, we obtain a sample with the unknown sampling mechanism and observer X and Y in the sample. Okay, so that is a kind of challenging problem. So to estimate the, to solve the problem, we assume that X is observable throughout the final population, and we can do something. Something. So this is essentially a missing data setup where the sampling mechanism corresponds to the response mechanism. Now I'm going to introduce another concept, a tensor-to-ratio function. So I'm going to use okay. So what is I'm not sure whether I introduced data, sorry. Introduce delta. Sorry. So, okay, I should have introduced delta first. So, so delta equal to one and zero, if i equal to in the sample, otherwise, okay. So that means our final population p, i mean final population u has a two component delta equal to Delta equals one component. And so that is basically the partition, right? You can see we are basically partitioning the original population into two parts, delta equal to one part and delta equal to zero part. So the approach I'm going to take is to consider the conditional distribution given delta. So So pk is a property of x and y conditional on delta equal to k, but the k can be either 0 and 1, right? So okay, here I actually introduce delta. And as we assume that, you know, this has a density function, so that we can consider this a ratio of two densities, we call density ratio function. So this is So, this is actually a function of x and y. And an interesting part is that using the density ratio function, the property of an event B at P zero. So, P0 is the delta equal to zero case, right? So, this population. This one has a P0 and this one as P1. And so, And so the distribution at P0 can be expressed as an integral integration evaluated at P1. Okay. And so you have a tensor-to-ratio function. Okay. And then using this density ratio. And then using this density ratio function, R, I'm going to rewrite the model space. So the original model space we are interested in is this model space, right? So this is the model for P. Now, using the density ratio function R, X, and Y, you can decompose into two parts, right? into two parts right for theta equal to peak delta equal to one case population delta equal to zero population so you can separate out and then uh for delta equal to zero you using this equality right using this one using this one you can write it you can re-express this integration to this integration To disintegration and combine. So that is the that is this expression. So now interesting part is that you have some function. Excuse me. Excuse me, some function that we can call propensity rate function eventually. So the border assumption can be expressed as in terms of P1 now. And so P1 can be written as something like this. So you can see that it is actually one-to-one correspondence. Actually, one-to-one correspondence. Okay. So I got a question about the uniqueness. So I believe that the solution, the projection is not unique, but the P space, but the setup space, it is unique under some conditions. But that so that is my answer. Maybe there's somewhat fundamental issues, but that's my best answer. Okay, so using that L1, so basically now we now probably change the problem into finding P. The problem into finding p to finding p1 so so because p1 hat is available to us always, so p1 head is just you know empirical distribution among p1 space, right? So p1 head is always available, and so I can use apply if I know density ratio function. No density ratio function, I can basically it's kind of changing the coordinate, I mean, change the variable type. So we just have a one-to-one correspondence between the two distribution. So basically we can find, apply the same information projection to find a P1 star and then use the ETL argument. Okay. So if R is known, so basically R is known, then the problem leads to finding the maximizer of this quebe-libler entropy function subject to the constraint, to constraint. So if the dimension of a set is equal to the rank of the Seta is equal to the rank of the estimating function, then it is just identified, and equation 4 does not contain any extra information. In this case, condition 4 can be safely ignored in the optimization for P. And so in this case, for the just identify the situation, we have an explicit solution. So that is the weighted is making equation with the weight function w. So that has been called. has been called proposed wave function. So now I'm going to call it this one as a proposed wave function, which is essentially one of the response probability function. Okay. Propositive score wave function is used to estimate parameters from the sample weight, sample with selection bias. So this is not really new, but we just use the tensor-to-ratio interpretation. The tensor de ratio interpretation to apply the information projection idea. And now, two problems. The first one is R XY is unknown in general. And then secondly, even if R X Y is unknown, it does not necessarily lead to efficient estimation for SATA. Okay, so that is basically the introduction of the basic problem. And now I'm going to propose. Propose our proposed method, explain the proposed method. So to avoid any issues on model identifiability, we consider missing a random assumption about Rubin, which is this assumption. Although this assumption is not necessarily realistic assumption, this is, I will just use that for explain the concept in this. Playing the concept in this presentation. So on the missing random density ratio function, which is this one, is actually you have a missing random cancels. So your density ratio function itself is a function of x only. So that means your propense function is also a function of x only. And zero, and so this is usually known. So that's why I use. So that's why I use a blue color, a blue color, red color is unknown. So, way smoothing idea is very simple. So, the idea is if you know Rx, right, rather than using this original propensitive weight, we may use the projection. They use the projection, projection of the proposed weight. And what space do we project? Well, so the projection we are using is kind of weird because we are projecting on the estimating function. So I'm going to explain soon what we really mean by this one. But the concept here is that if we, so this is. So, this is subscript to one means that conditional on the table to one. So, the basic concept is that this you had SPS settle is nothing but conditional expectation of a seta given given sorry. Sorry. Keep on U and so Kibon U and Delta. One. Delta. So conceptually, this is conditional expectation. So we can use the Lao of Laborization argument to show that this is simple spinning function should have a small Spinning function should have a smaller variance, is unbiased, and smaller variance. But the question is: how to compute this conditional expectation in general, right? So first, because w is a function of x only, so because function w is a function of x only, condition on y doesn't play any role. Doesn't play any role. And so, so we it is actually equivalent to condition. So, we have this equality where it's conditional on uxy is equivalent to conditional on u bar, which is this guy. Conditional expectation of uxy given x. And then the catch is that suppose this u bar This u bar is lies in the linear space, function space, in this function space. So suppose we know this function space. So this P1, 2, PL are basically the basis functions. Okay. And then in this case, conditional on u by is equivalent to Conditional on u bar is equivalent to conditional on h right and now now because h doesn't depend so this one doesn't depend on setter right doesn't depend on seta so it's so you you you the dependence on setter seta disappears using this trick if you can sell find this space six so Six. So, so basically, we are able to. So, basically, here originally equation five was this one, but now we can safely change it. Equation five is now safely changed to conditional on H. Okay. And still, it's not really clear how to do projection, even if you know this space. So, that's really. So that's where the information projection plays. So we're gonna, the catch is that we use F0 and F1 to distribution and apply the Kulberg-Leibler divergence and the constraint. And so because F0 is a density, we know that it should satisfy. We know that it should satisfy the normalization. And there should be a constraint. And those constraint that we want to projecting on is this constraint. And this constraint is the basis functions in H. Because we are interested in projection on H, right? The space H. And so that means the projection. The projection should satisfy the woman conditions in the basis functions of H, which is Bx. Okay. So if you use that, basically we treat F1 is known and F0 is unknown. And so apply the projection. So then these So, apply the projection. So, then this one is a kind of Lagrange multiplier ULO equation. So, we have a solution. And the solution is this one. So, that is information protection solution. And that information protection solution actually can be discussed as a so if you divide it by So if you divide it by, right, divided by F1, then this becomes a ratio, density ratio. And so, and then this is exponential function. So you take the logo, logo density ratio function, you get this one. So this equation 11, so that is actually model. Remember that the information projection has a two step. The first step is the model. The first step is the model. So the information projection finds the best model in some sense, satisfying the constraint and minimizing some divergence measure. That is basically it. So use that. You can obtain the best model in the sense of minimizing this queen library. And then second step is the estimate. And then, second step is the estimation step. The estimation step, you need to use the empirical likelihood optimization. And then the solution actually is this one. So I skipped some details, but in the end, end of the day, the parameter estimation can be obtained by solving this calibration equation. Calibration equation. So that is actually the kind of procedure to obtain the smooth propensity summary weighting using projection. So you may use this one. So this is the actually final propensity weight. And then this propensity weight satisfies has an exponential form. Exponential form has a dysfunctional form, and the parameter is obtained by this calibration equation. Okay, so for example, so how much time do I have? About 10 minutes? I guess. Yes, you have about 10 minutes. Yeah, thank you. So then this is the smoothie the propensity. This is the smoothie the propensity score estimator for the population mean. And you can, if you subtract that difference, right, then you get this one. So this one is a model. And then if model belongs to this space, then the calibration, because of calibration, this part is controlled, this part becomes zero, and this part. Becomes zero, and this part is the only one that left, and that is so. This is a function of x only. So, on in the missing random, this should be a zero expectation. Okay. And you can have a very several different interesting interpretation. So, the smoothie the PSC stimulus can be. Can be actually, you can write it this way. So, this is the purpose to weight estimator, but you can add and subtract the mean path, right? And this mean part is linear in your basis function, and you don't have to know the parameter. And so, so basically, you can write it after some algebra, algebraically equivalent to this way. So, you have some kind of imputation. Some kind of imputation form, and then you have some residual. So if you choose beta to make second term zero, so if you choose beta to like say quantum zero, that means your proposed co-estimator is actually equal to imputation estimator because you choose particular petah to satisfy this zero. zero so so basically this means that uh the left side is the weighting but the right side is the imputation so uh so the final calibration weight does not directly use the regression model for imputation but it implements regression imputation indirectly so that's kind of an interesting part and the theory is and here's some theory the theory is uh relatively strong Is relatively straightforward. So you have some nonlinear noises parameter and you need to linearize. But here we assume the finite dimensional basis function. So then theory should be easy. Okay. But I have a couple of comment. So basically, you should be able to show some kind of synthetic domelo. Some kind of synthetic normality with some variance in mean part, and then you have a conditional variance and then inflation. And so in the end, this one achieves the symptomic, very famous semi-parametric variance law bound of James Lobins at all. And a second part is also interesting. If you can find H and R. If you can find H in order such that the mean function belongs to this space, H leaders space, in this case, we can make variance of 3D, the variance term, smaller and obtain more efficient PSD instimator using the basis functions in H null only. So increasing the dimension of H may lose efficiency. Of h may lose efficiency. So actually, there's a good bias and variance trade-off going on here because if you increase space, function space H, then you are likely to capture the mean part, right? Control the mean part. So you get unbiasedness. But it creates noise. You have inflation, weight inflation. So you increase the variance. So in the end, you may need to. So, in the end, you may need to use the penalizing technique. Another point is that the proposed PS weighting method can be described as a calibration weighting problem, minimizing this one, subject to calibration constraint. So, end of the day, it's just the calibration using this particular calibration function. Heimer method uses slightly different. Similar method uses slightly different function, but it's quite a well-used one and sometimes called entropy balancing a method. Okay, and back to motivating example. So remember the motivating example, the Korean data example. So I'm going to explain the answer. So basically the outcome model is gamma equal to Is is gamma equal to zero, so that we only need x to the propensity rating. So, so the true response model, even though the true response model is this, the conditional expectation of y x and z does not depends on z. So the summary ps weight should be a function of x only. So it's better not to use z in construct the ps weight. Okay, so that is the basic idea. So, that is the basic idea. I can briefly introduce the extension. So, the idea actually can be extended to multiply missing data. So, suppose you have a non-monotone missing pattern like this. Basically, we still, with this data structure, we still have this estimating function. And we see to construct an estimating function using all available. function using all available information right so we have a s1 so we have a s1 s1 s2 s3 so partition of the sample based on missing pattern and so s1 we observe everything in s2 s2 we observe y1 and y3 so we can consider uh computing the conditional expectation of this uh given the observed data Uh, keep in the observed data as three, we observe y1 and y2, something like that. So, as four, we observe y1. So, that is the basically the setup. But this one would involve the conditional computing, the conditional expectation and involves some modeling. So, so instead of using a model for conditional distribution, we can use a density ratio function such that Such that this density ratio gives the approximate this condition. So, the idea that I introduced previously, the information idea can be naturally used to develop the professional weight for this problem. So, I'm going to skip the details, but the basic, but the bottom line is that for each line is that for each missing pattern so so for each so we we want compute the density ratio function so this is a density ratio of the f t given f1 okay and and so this one has a disparate model using eye projection and then Projection, and then parameter estimation can be done using a calibration. And in the end, you can just use this propenscal weight that basically reflects all the density ratios. So, this is the final weights for PSD estimation can be used for basically this. So basically this weight will implement the conditional expectation. Sorry to interrupt to keep the talk in time. Could you wrap up in five minutes? Yeah, okay. I will skip the simulation. So basically, bottom line is we perform very well. So here's a take-home message. So inserted ratio estimation is the key component for premises score weighting. Component for process score weighting, which is this form. And C is known. R star is a projection. And to obtain the R star, we use information projection, which we end up getting. So we first find the function space, and then the information projection gives this as a solution. And then model parameter can be used as calibration equation, which means this also one or nothing. Is also one of the things. So, increasing the dimension of H may lose efficiency, and penalization technique can be used. And I have a couple of research topics, future topics. Thank you very much.