Okay, I'll start. Hello, everybody. So, yeah, I will not repeat my handout, my title and abstract because you've seen it and maybe you have it. Also, my total handout. I hope you're all copy. There are more on the desk over there. People online, you can find my handout. You can find my handout at this URL, http droorbn.net/slash/oa22, or you can, well, you, I suppose, you can scan this. There is, this URL is defined to be omega epsilon beta, and you can find a paper on this subject at omega epsilon beta slash apai. And let me start. Oh, sorry, I can remove this. Remove this. So let me start. So we seek strong, fast, and homomorphic, not entangled invariants. So let me first explain these words. So strong is kind of clear, which means having a small kernel, whatever imprecisely, but you know what it means. Fast means computable also for very large. Also, for very large knots, so this means that it's best if it's polynomial time computable, so that you could compute it on the 48-crossing Gomp-Sharlman-Thomson knot, which is a potential counterexample to the Ribbon equal slice conjecture, or on this knot that was considered by Lisa Picirillo, which I think is 56 crossings, or perhaps on even bigger knots. Even bigger knots and homomorphic means the following. And I'm slightly imprecise. And if you want to see a more precise, everything can be made more precise, but I'm not going to do it right now. So I want it to extend to a tangle invariant. So I want good invariants to extend to be tangle invariants. And I want them to be well behaved under tangle operation. Behaved under tangle operations. So, what operations do you have on tangles? Well, you can compose them by putting them side by side and connecting some of the strands. Or you have the doubling operation. So, if you have a tango, you can point at one of the strands and say, let's double it. And then it becomes two parallel copies. And I want invariance with the property that if you know. With the property that if you know the invariant of this, you can compute the invariant of that. And also, if you know the invariant of the two components, you can compute the invariant of their composition. So briefly, why? Because basically, once you have these operations on tangles, there are classes of knots that can be defined algebraically using these operations. Using these operations. So, for example, there is a theorem saying that a ribonaut, sorry, a knot K is ribon if and only if, but forget the theorem, you can take this as the definition of a ribonaut. And people are interested in ribonots, so that becomes an interesting definition. So a knot is ribbon if there exists a two-n component tangle T. Entangle T with skeleton as below with this skeleton really means that it has two n strands, n of them go from bottom to top, and n of them go from top to top in this order. And with the property that tau of the tangle is the knot, where tau of a tangle of the tangle is the thing you get by doubling all the vertical strands. All the vertical strands and composing with caps and caps below and above, as shown, and such that, and where delta of the tangle is the untangle, where delta means delete all the vertical strands. So I want T, so a knot is ribbon if there exists T like that, but the inside can be knotted, so that if you remove the vertical strands, you get. Remove the vertical strands, you get a picture like this, which really is unknotted, and such that if you double some of the vertical bands and add caps and caps, you get your original knot. The point of this is that this is a completely algebraic definition of Ribo knots, given the operations we have. So, if you have an invariant that respects these operations, it stands to It stands to say something about ribbon knots. Maybe it will give you a condition for when a knot is not ribbon, and then maybe you will be happy about it. And I could tell you more about it, and you can find more about it by going to omega epsilon beta slash AKT, but I will not do it right now because I want to do other things. Okay? So, well, I want to tell you about the invariant I want to talk about today. Okay. Talk about today. Okay, so you know, last time I this is essentially a repeat of a talk I gave not so long ago in the Jones Memorial Conference, and that reminded me of Jones. So one time I was, of von Jones, one time I was walking with him in the Berkeley campus, and I was telling him about some grand theory that I have, about I had about the Chern-Simons theory, something about interpreting it as a collection of compatible. As a collection of compatible push-forward measures, something like that. And he said, Can you turn it into formulas? And I couldn't really. And he said, well, formulas stay and interpretations change with time. So, and he was right because basically all these funny interpretations. All these funny interpretations got completely lost or completely uninteresting now, but formulas tend to stay. So I'll start with formulas. Okay? No interpretation, no intuition. Here is how you compute the invariant. So you draw an end crossing not as on the right. So first of all, you draw it as a long knot, starting from the bottom, ending at the top. The top, you make all the crossings, you put all the crossings such that they are pointing up, that both strands are pointing up at the crossings. And of course, you can do it by just rotating around the crossings. And you label it with two sets of integers. One is just a running index. So the edges are labeled one, two, three, four, five, six. Four, five, six. And if you have an n crossing knot, it means that it will run up to 2n plus 1. And the second labeling is, so you see, each edge in this picture begins and ends pointing up. So it makes sense to count its rotation number. And the rotation numbers here are 0, 0, 0, 0, 0, and 0. 0, 0, and 0, but there is only one interesting rotation. So phi 4, the rotation of edge number 4, is negative 1. So that's the input. This is how you set up your things. Then you make a matrix as follows. So you start from the 2n plus 1 by 2n plus 1 identity matrix I, and then you add to it a 2 by 2. Add to it a two by two block for each crossing. So each crossing is really three integers: a sine s for whether it is positive or negative, and the index of the upper strand. So here it's I, and here also it's I, because I index, I just decide that I is the name of the upper strand, and the index J of the lower strand, and then I add. And then I add to the matrix a block like this. So at row i and row j, and column i plus one and column j plus one, I add the entries written here. And then I let uppercase G be the inverse of the matrix A that you thus got. So for example, for the trefoil knot, which I wrote over here, Which I wrote over here. You start with the identity matrix. These are the ones on the diagonal. And then for each of the crossings, you add a two by two block. So I color coded it. I think in the printout, it comes out better than on the projection screen. But I color coded it. So the yellow crossing is a positive crossing with i and j equal three and six. So in row. So, in rows three and six, and columns four and seven, you get a copy of the matrix written here. So, that's that defines the matrix A. And then you invert it, call the result the green function, because it plays a role similar to the green function elsewhere in mathematics and physics. And here is what it looks like. Okay, that's just the result by stupid computation. Stupid computation. Okay? Inverting a matrix, by the way, is a cheap operation. So everything I've done so far is cheap mathematically, computationally. Now, a note is that the Alexander polynomial of the not is given by essentially the determinant of A with some power of T renormalizing it to make it balance, to make it properly normalized. It properly normalized the power of t. Well, maybe I will not even go through what it is, it's related to the total rotation number and the rise. And the determinant of A is, of course, the typical numerator that you see in the inverse of A. So these are all, the numerators are always the Alexander polynomial. Okay? And if you are a classical topologist, you should. Topologist, you should be saying this is boring and yawn. Would anybody say this is boring and yawn for me? Thank you. Thank you. Good. Indeed, the two by two blocks that I've added for each crossing was essentially the Bureau representation. And the result, the matrix. And the result, the matrix you get, is a presentation of the Alexander module of the knot as computed using Fox calculus from the Wiertinger presentation. And then the inverse of the matrix, so the matrix G, the green function, is not quite but closely related to the Blanche-filled pairing of the knot. And if you look at these pictures of these fine gentlemen, they're all Gentlemen, they're all black and white, which kind of tells you how old they are. Okay, so this is ancient history. But let me continue with the formulas. So now for any crossing C, define uppercase R1 of C. So a crossing is a triple of integers, the sine S and the indexes of the upper strand and the lower strand. So define R1 of C to be S. one of C to be s times the following expression in the entries of the green function and I have nothing to say about this expression except that it works and that it's quadratic so it's just quadratic in the entries of the G and then define row one of the node to be well the sum over all crossings of R1 of the crossings Crossings of R1 of the crossing minus some corrections term, which is really the rotation number, a sum over all edges of the rotation number of the edge, multiplied by something which is even lower order, so linear order in the green function. And then so in our example for the trephoil note, by just direct brainless computation, you find that row one is the expression written here. And I assume. Here, and I assert that row one is a not invariant, and I will show you a proof later. But before that, if you are a classical topologist, you should be saying now, you know, you should be completely changing your attitude. So you should be saying, what the hell is going on here? This is something that's not done in topology. You never, I mean, maybe you look at the inverse of the Alexander matrix of the. Of the Alexander matrix, of the matrix presenting the Alexander module, or of a matrix presenting the Alexander module, but you'll never look at quadratic expressions in that matrix. You're never going to form this expression. You should be you should have no clue what is going on. Okay, yes. Oh, oh, oh, delta was the Alexander polynomial. And remember, everything was quadratic in the green function, and the green function has the Green function, and the green function has denominator the Alexander polynomial. So I multiply by the Alexander polynomial squared just to kill the denominators. It's not essential, it's minor. Okay, thank you. Okay, so you see, I took Jones's advice to heart and generalize it. So I no longer even believe myself when I write formulas. I only start believing myself when I write computer. Myself, when I write computer programs, and the computer programs run. So, before telling you anything about cars and interchanges and all that nonsense, here are the programs. So, you can find these programs at this URL that I gave you at AP. And the first thing is some preliminaries. So, I'm loading some library functions. With no content whatsoever. The only content is just the list of knots and a routine that computes rotation numbers. So, rotation numbers are easy to say, but given an actual knot, figuring out the rotation number of each edge takes a little bit of computational effort. That's what rod.m does. Sorry, computational programming effort. The computational effort is trivial. It runs in linear time. That's nothing. Runs in linear time, that's nothing. So I load these programs, these preliminaries, and here is the program that implements everything that I said so far. So first of all, here is the definition of R1, and you can compare it with whatever I wrote on the, with whatever I had on the board before. It's identical. Then the program rho of k does the following. First of all, given the note k, it computes its Not k, it computes its list of crossings and its rotation numbers. This is done by this library function. Then it computes the length of the list of crossing, calls it n, that's the number of crossings. Then it lets A be the identity matrix of size 2n plus 1 by 2n plus 1. And then for each triple Sij in the list of crossings, it takes this matrix I in row I and J and this matrix A. Column, sorry, this matrix A in rows I and J and columns I plus one and J plus one, and adds to it a Bureau block, a two by two Bureau blocks. It computes delta by exactly the formula that I said, the essential part is that it's the determinant of A. It inverts the matrix A and it computes row one by taking a sum over all crossings of R1 of that crossing and then the normalization term. And then the normalization term. And it outputs the Alexander polynomial, delta squared times rho one. Again, the delta squared is just to kill the denominators, where you replace every symbolic gij by the actual value of the green function matrix at, you know, so every g alpha beta becomes uppercase g of alpha beta, where uppercase G is the inverse of the matrix A. Is the inverse of the matrix A. So the program is completely straightforward and it's just a few lines. And computationally speaking, the steps that take time are computing the determinant of a matrix and inverting a matrix. And it takes very little time because these operations can be done very quickly with Gaussian elimination or more sophisticated algorithms. Sophisticated algorithms, the computation of rho one is actually nothing because it's just a sum of quadratics. It's a linear time computation. So this invariant is as hard to compute as the Alexander polynomial, essentially. Let's see a few examples. So first of all, I am computing row of k for all notes with up to six. For all notes with up to six crossings, and here it is. So for five, two, the Alexander polynomial is this, and row one is that. And I have nothing to say except these are the results. Maybe I should only say that I take the pair Alexander polynomial and row one together because they're essentially a part of the same thing. Okay, row one is somehow an extension of the Alexander polynomial. Next, so here is the Gomp, Sharlmann, Thompson not with labels as I'm supposed to label it. And here it is in some symbolic way. And I run the program and after 164 seconds, it spits out the answer. This is the Alexander polynomial. It factors because it's a slice knot. And this is row one. And all I have to say is that. And all I have to say is that it runs. Okay. And by the way, the timing. So nowadays, almost always my computer runs some large processes in the background. So the timing is the timing given that my CPU is tired. If I run it on a fresh CPU after turning on my computer without waiting for it to heat up, it's like a third. Well, it's about 60 seconds. So it's really efficient. And finally, it is strong. So I let the computer print the number of knots with between 3 and 12 crossings, and then I let it compute how many different values rho of k attains on these knots, and how many different values Homphley plus Khovanov attain on these notes. And the answer are the top. These notes? And the answer are: the total number of notes is 2977. The number of values rho attains is this. The number of values Homphly and Khovanov together attain is that. So rho1 or rho, which is the Alexander polynomial, and rho1 are stronger than homeflippiti and hovanov, at least numerically. Okay? Any questions, any comments? Any questions, any comments so far? Okay, so let's start interpreting it. So there is a lovely discussion of traffic on knots. And I think it starts with Vaughan Jones, and then there is a continuation by Lin, Tian, and Wang. And it goes as follows. So basically, you think of the knot as The not as roads, and cars can drive on these roads, always following the orientation. And the rule is that if a car goes under a crossing, it comes out on the other side unchanged. But if it goes over a crossing and the crossing is a positive crossing, then it comes out. Then it comes out unchanged with probability t, which means that with probability one minus t it falls down. So if it starts here, it comes out with probability t on the upper strand, but with probability one minus t, it falls down to the lower strand, like this picture here. And we are thinking of this as like, as if t is a is. As if t is close to one, and one minus t is close to zero. So most cars pass through, but occasionally a car falls down. Now, if you came on the lower strand, you come out with probability one here and probability zero here. And for under crossings, it's well, if you, sorry, for negative crossings, if you start at the lower strand, you At the lower strand, you come out at the lower strand with probability one and on the upper strand with probability zero. And if you start on the upper strand, you come out with probability on the upper strand with probability t inverse, which is also close to one, and drop down to the lower strand with the complementary probability. And you may complain that these probabilities are not numbers between zero and one. I don't care. I mean probabilities in some algebraic sense. In some algebraic sense, meaning I add them and multiply them like you add and multiply probabilities, but I actually never care about the fact that there are numbers between zero and one. Okay? So you'll see in a moment that it's irrelevant. Maybe I should say, what are these bars here? So, you know, when you drive on the road, occasionally, On the road, occasionally you pass these black rubber things that count the traffic. These are traffic counters. So here's a picture of a traffic counter. It's a device to the side of the road that counts how many cars go by. So really what I wrote here is: so if the car enters here, a traffic counter placed here would measure that many cars. Okay? Claim. Oh, oh, sorry. Here's just another lovely picture made by Roland and his kids. Never mind. So claim, the green function, so the inverse of this matrix A that I constructed is the reading of a traffic counter. So G alphabeta is the reading of a traffic counter placed. Placed at beta, assuming that you've injected car traffic in position alpha, on edge alpha, with the convention that if alpha is equal to beta, the counter is always later than the projection point. So on strand alpha, if the car is inject if the cars are injected here, you read the traffic later, at the later point. Later, at the later point. So, here is an example. So, let's compute the green function of a kink. Okay, so if I inject the cars here, a traffic counter placed here will measure one. Okay? Now, what about? Now, what about a traffic counter measured here? Sorry, placed place. Sorry, what about a traffic counter placed here? So a car injected here will go under with no change. So you'll have one continuing. And then with probability one minus t, it will drop down. So the counter here will measure one plus one. Measure one plus one minus t, but then it will continue and it may drop again. So it will also measure with probability one minus t squared another car, another count. And then, so basically the counter placed here will measure the sum over p, where p is greater than or equal to zero. P is really the number of times you went around of one minus t to the power p. Minus t to the power p and this is a geometric series and the sum is t inverse okay now suppose I inject the car here oh sorry sorry sorry and then if I place the counter here it will just measure one because all cars eventually come out now if I inject the cars here they The cars here, the counter here will measure zero. The counter here is exactly as before. It just doesn't matter if the car was entering here or here. So the counter here will measure T inverse. And likewise, here it's as before, it measures one. And finally, if I inject the cars here, then this counter will never see traffic. This counter will never see traffic. And this counter... Never see traffic, and this counter will see a traffic of one. So, overall, the green matrix, the green function, is will have one t inverse one on the first row, inverse one on the second row, and so on. Okay? Are you okay? Good. Am I okay? Yes. So, theorem. Oh, here. Oh, here. Theorem. Oh, sorry. I have to prove the theorem, right? I gave an example, but now I have to prove it. So basically, both sides of the theorem, so the traffic measurement and the green function, namely the inverse of the matrix A, satisfy some equations. So near across Equations. So near a crossing C with sine S and with upper strand I and lower strand J, the green function and given any placement beta of the traffic counter, the green function satisfies gi beta is equal to the Kronecker delta of I and beta plus T to the power S Gi plus one and beta and so on. Sorry, plus another. Sorry, plus another. Basically, what I'm telling you here is that, so ignore the green function, ignore the Kronecker delta for a minute. What I'm telling you is that if you fix the placement of the traffic counter, then if I inject the cars. If I inject the cars at I, so if I inject the cars here, then that's the same as injecting a car with probability t up after the crossing and with probability one minus t up after the crossing on the lower strand. And that's exactly what the traffic loss. And that's exactly what the traffic laws were. Okay? And then the delta function has to do with the fact that if the counter is precisely, so if beta, the placement of the counter, is precisely on strand I, so if the counter is here, then there is actually a difference between injecting at I or injecting above the crossing, because the counter Because the counter, because if you inject at I, you count one more car than if you inject here or here, right? So traffic, the traffic matrix satisfies this rule, but this rule is also the fact that A times the green matrix is the identity, which is the fact that the green was the inverse of A. The green was the inverse of A. But if we're at it, so I'll call this the G rules. And if we're at it, we can notice that if A G is equal to the identity, then G A is also equal to the identity. And then you can read rules out of it. And you get some further G rules which tell you not how to move the injection point of the car, but how to move the traffic counters. Traffic counters. So, no matter where you inject the car at alpha, no matter what alpha is, putting a traffic counter on edge I is equivalent to whatever that says. Okay? Good. So now let me prove the invariance of row one. So first of all, let's start with the hardest. So let's prove invariance under Raidemized. Invariance under Reidemeister 3. So really, I'm repeating the invariance of the Burau representation. So here are the two sides of Reidemeister 3. Suppose I'm injecting black cars on the lower left. Then here I will have a car with probability T. Here I will have a car with probability one minus T. with probability one minus t. If I continue, I'll have a car here with probability t squared. Here I drop down with probability one minus t. So the probability of seeing a car here is t. So the car counter here will measure t times one minus t. And on the other side of the Riedermeister move, so I inject the cars here, they come out with probability t to here and with probability one minus t to here. This t becomes t squared. This t becomes t squared over here. Now it drops down with probability one minus t. So I'll get here traffic at the amount of t times one minus t. And down here, I will get t times one minus t coming from the right and one minus t squared if I drop a second time. And the sum of these two things is equal to that, which is a part of the proof of Riedermeister 3. And you can complete the rest. Meister 3, and you can complete the rest yourself. Basically, injecting traffic here is even easier, and here it's even easier yet. Okay? So, the moral is that overall traffic patterns are unaffected by doing a Rydermeister 3 move. Right? If you do a Rydermeister 3 somewhere on the note, then unless you're inside the zone. You're inside the zone, the area where the Rydermeister 3 occurred. Where so, so like the quantities that are related to the inside are changed, but the overall traffic patterns are not changed because no matter where the car enters, it will come out with the same probabilities on the upside, no matter which side of the Reidemeister 3 you're looking at. So the green. So the green function is unchanged provided that alpha and beta are away from the move. And so the only contribution in the sum that I considered when defining row one, the only contributions that might change is the contribution coming from having R1 here, here, and here on the left, or Here on the left, or here, here, and here on the right. Okay? So let's look at this. So the left-hand side is R1 at J and K. So this is this crossing, plus R1 at I and K plus, so this is the second crossing and so on. And so on. The second, the right-hand side is another sum of three R1s, and that will give you entries of, so if you compute this sum and this sum, it will give you entries of the, oops, sorry, what have I done? This. It will give you entries of the G matrix at various points inside the zone of the R3. Of the R3. But I had the G rules, which allow me to push injection points and traffic counters up. So here are the G rules implemented as computer code. And then I apply to the left-hand side the G rules that apply to the left-hand side. I apply to the right-hand side the G rules that apply to the right-hand side. I punch simply. I punch simplify left-hand side is equal to right-hand side into the computer, and the computer says that it's true. So we have invariance under Reidemeister 3. You could have done it by hand, but the reason I'm delegating it to a computer is to emphasize that it's completely mechanical. It's really brainless. Okay. Likewise, we can check Ridomeister 1. So for Ridomeister 1, again, So for Rydermeister 1, again, the traffic outside the zone of the Rydermeister 1 is not affected by doing the move. So I only have to check the contribution coming from that zone itself. There is one crossing and one rotation number. So one crossing and one rotation number. And here I can simply use the values of the traffic matrix that I've already computed. So. So I let the computer compute it, but again, I could have done it by hand. And the computer tells me that the answer is this. And this I actually did by hand to prove to you that I can do things by hand when I really, really need to. So you compute this, I did by hand, it comes out to be zero, which is the contribution from the other side of the Reidemeister move. So we have invariance under Reidemeister one, and invariance under the other Reidemeister moves is proven similarly. Okay? Okay, so wearing my topology hat, a green knot, an orange knot, a red, a red knot, I have completely no clue where this is coming from. And in fact, I hope you can tell me. I really have no clue. So the formula for R1, some simple-minded quadratic thing in the entries of the inverse of the Alexander matrix, I have no idea. Alexander matrix, I have no idea what it stands for. Even why should you look for anything like that? I have no idea. Wearing my quantum algebra hat. So I only have seven minutes left, so I'll wear this quantum algebra hat really, really quickly. The first thing I notice is that there is a Heisenberg algebra hiding in the picture. Hiding in the picture. Because basically, cars and traffic counters behave like the Heisenberg algebra. So the commutator of cars, P is for Porsche and X for counter. So the commutator of the two is one, right? If you place a car before the counter, the measurement is one. Counter, the measurement is one, but if you place the counter before the car, the measurement is zero. So I spy a Heisenberg algebra. And now you can ask yourself, where is it coming from? And this is the story that I will not tell you. It comes from a lovely, not quite deformation, but contraction of SL2, which can be quantized in much the same way as SL2, then a certain SL2, then a certain limit can be taken, then you can look at Verma modules. It's a long and complicated story, and at the end of the story, you recover the formulas that I gave you before that basically anybody in high school could understand. So, but you also learn that there is more. So, basically, this this expansion that I used here in powers of this parameter epsilon that I never explained can be continued and instead of row one you can get a row two and row three and so on and furthermore instead of using SL2 as the initial as the seed I could have used any other semi-simple algebra so so so row one is not alone alone there are more invariants More invariants like it. And I should say that this whole thing has a history. It's very similar to work by Rosansky and Overbay, and in general, to the loop expansion of the Kontsevich integral and for the Color-Jones polynomial. The main difference is that it can be understood by high school students. Okay? Now, if all of this If all of this reads like insanity to you, then it should. Because all of this, I mean all the quantum algebra nonsense, reads like insanity to you, then it should. Because row 1 is super simple and we should be able to find a topology home for it, something that topologists could understand. So, homework. And so, homework, find that topology home. And also, well, I didn't tell you how, but the quantum algebra nonsense explains that row one is homomorphic. So make these properties explicit at the level of row one and then use them to do topology. And just to indicate that you can use them to do topologies, let me say that. Let me say that, well, rho one is kind of a friend of the Alexander polynomial. You take the same matrix and do something slightly different, different to it, okay? So the Alexander polynomial gives a genus bound. So if you know that the Alexander polynomial is of degree n, the genus is going to be at most n. In a similar way, row one gives a genus bound, and it is sometimes better. Is sometimes better than the genus bound that you get from the Alexander polynomial. Now, you could say the problem of determining genus is solved, but this is happening in polynomial time. So this is happening, you could in principle run it for extremely large nodes. And then my question, a further homework question is: how much does this further this friendship extend? Is there a A Fox Milner-like criterion for 01. And I think I want to end by not showing you the last page of the handout, which is a similar implementation of row D for D greater than one. So, first of all, the theorem becomes a bit harder. Here's the A bit harder. Here's the baby version, and here's the full version. So it's harder to state, but it's still polynomial time to compute. And in fact, row two is still, the summation in it is still dominated by inverting the Alexander, by inverting the Alexander matrix. So row two is in. matrix. So row two is in fact no harder to compute than row one and then the Alexander polynomial. After row three it becomes harder. And then here is an implementation. First of all, some pre-computed polynomials that are necessary for everything to run and part one of the program, part two of the program and some output. And I didn't really mean you to read this. I just wanted to show that it exists. And I have an extra minute which I will not use. So any questions or comments? Maybe we can do Chris the Miles because that was well a moment ago. I still haven't learned how to do it. Hello, excellent. Okay, so questions for draw. So, have you done a computer? So, have you done a computer search for other polynomials in the coefficients of the green function matrix that give you not invariance? Sorry. Have I done... Oh, no, I don't need to repeat this. Oh, good. The answer is slightly more complicated. More complicated. So of all the things I'm trying to do, I've done, I'm trying to remember which exactly did I do. So first of all, First of all, the unknowns are okay. The R1 or what comes from quantum algebra is polynomials in variables p and x, the same as the generators of the Of the Heisenberg algebra. And then they become polynomials in the green function, in the entries of the green function, by some procedure, some differentiation procedure. So you exponentiate the green function with some formal variables and then differentiate it with respect to the The uh p's and x's. I'm being very rough, okay? So, um, so quantum algebra gives you something more specific than a polynomials in the G alphabeta, namely, it gives you polynomials in the p's and x's, and out of this, by some mechanical procedure, you can get polynomials in the g alpha. You can get polynomials in the G alphabetas. I have searched for polynomials in the p's and x's, and in fact, this is the result of the search. So, quantum algebra tells you that such polynomials will exist, but finding them, it's actually easier rather than running through the whole lengthy procedure, it's actually easier to just search for them and find them. Them. But there might be more which are polynomials in the G's that do not come from this procedure from the P's and the X's, from polynomials in the P's and the X's. I don't know how much, how many there are. That's one point. The other point is that quantum algebra predicts that Well, that there will be many variables examples. So instead of T, which is a single probability, the way I presented it, there should be invariants that depend on T1 and T2, or T1 and T2 and T3, where the number of variables is the rank of the semi-simply algebra you started from, and I haven't searched for these at all. Okay. At all, okay, though it will happen sooner or later. I think there's a question from Claudius, but actually, just a quick follow-up since we're on this subject. I think you just answered this, but I don't think I understood. Did you start it from the quantum algebra and went looking for this, or did you want something computable and then realized that you were getting at the quantum algebra interpretation? Second? The history is longer. The history is longer, so uh, you know, uh, uh, like the fact that I want uh strong, fast, homomorphic, uh, nothing, so strong and fast everybody knows, or everybody wants, but the homomorphic property, which I haven't talked about, so this is something I started looking for in 2007, maybe or eight, so a long time ago. Or eight, so a long time ago. And okay, so and I looked in many, many, many different directions and eventually evolved and evolved and evolved into this quantum algebra business, which is actually directly related to earlier work by Orzansky and Overbey. So then I had. Then I had, we had, Roland and I had, yeah, I don't know where it is, but Roland and I had very good quantum algebra formulas for strong, fast, and homomorphic not invariants. And I've given talks about these formulas before, and they're on the web, they're online. In fact, I think they're on the handout as references. References. And then at some point, we realize that we can simplify it to high school level, at least in the case of the simplest of those variants. And that's what I talked about. I don't know if I answered your question. No, that's absolutely. Thank you. That's great. So the history is long. Great history less, if I did a great summary of what. Yeah. Okay, Claudius, go ahead and okay. Yeah, thanks. Thanks for the talk. I've also got a question about this homomorphic property. So can you? So, can you do you get any relations between different angles? So, do you get like a skein relation type thing? Gee, I don't want skein relations. You know, I'd almost say that if I had skein relation, I'd throw skein relations, I'd throw them away because skein relations actually don't do much. Actually, don't do much to topology often. Now I'm saying, now I'm being too radical. I'll reverse that, but definitely they tend to give you exponential time computations, right? If you want to compute the Alexander polynomial with the scan relation of the Alexander polynomial, so the Alexander of over minus the Alexander of under is Z times the Alexander of Smooth. The Alexander of a smoothing. So, first of all, I'm not aware that it gives you much in topology. And secondly, if you wanted to compute, it would take an exponential amount of time. Yeah, but just because the Alexander prenemial satisfies this gain relation doesn't mean you have to use it to compute computations. I'm just asking: does your invariant satisfy? Have you been looking for any relations there? So it's not mine, it's really about the unscan over pay. But does it satisfy a scan relation? Not that I know. On the other hand, there is a reasonably simple extension of it to tangles, which I have not shown. And you exactly know how to compose. Exactly, I know how to compose tangles or even better, stitch the ends of strands in a tangle. But anyway, you exactly know how it behaves under operations of tangles, and so you actually can compute it kind of by divide and conquer, by like adding crossings one at a time and making your tangle bigger and bigger until it. Your triangle bigger and bigger until it reproduces the knot you want it. So, so what is the okay? Maybe I'm getting out of time. What does the invariant look like for, say, just a single crossing? I mean, it's probably more than just a single polynomial. Yeah. What happens when you then compare it with the other crossing? And yeah, okay, it's a it's a pair of things for a single crossing, it's a pair of things, namely. Namely, somehow the transfer matrix. So a certain matrix. Sorry, you know what? Sorry, for a single, the answer for a single crossing is not better to answer what's the invariant of a tangle. A single crossing is an instance of a tangle. Okay, so the invariant of a tangle will be Of a tangle will be a pair of things: the Alexander information and the new information. The Alexander information is basically a matrix, roughly the Birau representation, except generalized to tangles. So basically, a matrix that tells you if you put in a car, so where is So, where is it? It's essentially essentially this matrix. So, if you put in a car in any of the inputs, what will a counter read at any of the outputs? That's the Alexander information. And the additional information is that you put But you put cars and counters, or a pair of cars and a pair of counters as dictated by uppercase R1. And then you use the traffic rules to push them to the top. So as I said, cars and counters are really the peak. Cars and counters are really the P and X of a Heisenberg algebra. So you really get an element of degree four, so two P's and two X's in the Heisenberg algebra raised to the power of the number of strands. So the information is a transfer matrix and some specific element of the Heisenberg algebra. Of the Heisenberg algebra, of a tensor product of Heisenberg algebras, but that element is of degree four. So, in fact, tensor products of Heisenberg algebras are infinite dimensional, and I don't want to talk about, you know, they're not polynomial time in any way, but I'm looking at elements of a bounded degree, and such elements are polynomially described. Polynomially described. I don't know if I've answered, but I don't know if I've answered your question, but I'm done with my answer. Okay, thanks. Okay, I have a three-part question. I'm going to start very low and then go high. First is: in your example for the trefoil, it's up. Trefoil. It's a positive right-handed trefoil. Row one is some alternating version of quantum two cubed, like more like T minus T inverse cubed. Is that for a good reason or this? Well, more like on the first page under formulas in our factored. Yeah. So I. Backdoor. Yeah. So if it's for a good reason, I don't know what it is. Okay. I mean, it's a bit like asking, is the Alexander polynomial, like interpret the Alexander polynomial. So the Alexander polynomial of 6, 3 is 1 minus 3t plus 5t squared, blah, blah, blah, blah. I don't know what it means. It just, it is what it is. Is what it is, but it has properties. So, for example, sometimes it factors and then it may tell you something about the knot. Yeah. So, the second part of the question, which maybe is nullified by the first one, is do you have a sense of what this invariant is on Taurus knots, or if there's another class of knots where you think it's particularly simple to describe or positive? Okay. Okay. What can I say about Tall Snows? So I can't say anything right now, except in principle, it should be computable in closed form. So I haven't done this computation, but it should be computable. Uh, but it should be computable in closed form for that specific class of knots, TORUS knots. Right. Uh, what can I say about other classes? I don't know. I just think that we could you can probably easily come up with a conjecture if you just did a bunch of two-strand tours, right? And then looked at their oh, yeah, and and I can easily go up to you know, Taurus notes with 100 crossings, so you'll have enough. With 100 crossings, so you'll have enough data to look at. Uh, but but I haven't done that, we haven't done that. Okay, I asked part three, or are we out of time? Yeah, no. Okay, so I was looking, so the randomizer one proof looks like it's very understandable with the trapping interpretation. Um, is there some way of introducing maybe a different variable? Or so if I wanted to think about, say, transverse not and I wanted to break. A transverse knot, and I wanted to break the symmetry from right-meister one. How would I keep track of that using this interpretation? I'm not sure. I'm not sure if I know anything about that, but So, you know, let me say that, well, sorry, I have to skip somewhere right here. So the formula really looks like a base formula, which gives you this is for row D in general, but it's General, but it specializes to row one. So the formula for row one is also the same, right? But it looks like a base formula, which is written here, whatever the symbols mean. And the point is that the base formula is an invariant of not of knots. And a bnot is a note where you Where you allow only a partial subset of the Rydermeister moves. So basically, you allow only the Ridomeister moves that are acyclic. So the ones that can occur in braids. Okay? And then you make it an invariant of full knots. Full notes by adding some corrections and making things a little bit more complicated. Computationally, it's the same. So you make things a little bit more complicated by adding corrections for rotation numbers and things. So conceivably you could take the same not invariant and correct it differently to get To get knots, I suppose. So the correction from the data is more than just no, the correction from not to knots has it's roughly the same as in quantum algebra, as in standard quantum invariance, where you have to make a correction for caps and caps. So here it's written in terms of rotation numbers, but it's essentially. Of rotation numbers, but it's essentially the same. Thank you. Now, is it explored? No, it's not explored. Do you want to explore it? I want to talk to you later, but that answers all my questions for now. Okay. Okay, well, I'm just briefly because I don't have. Can you take a moment, maybe? I don't know if the story is out. I don't know if the story is aspirational or hopeful, but the connection that you'd like to see between slice ribbon and using there. So a priori, so okay, the invariant is a neighbor of Alexander and it seems to share properties with Alexander. So though some of the things I don't So, some of the things I don't, I'm speaking from because I know how to prove them, and some are experimental. But, you know, it's palindromic, it's like the Alexander polynomial, it has a genus bound, which I half understand, but not fully understand. And it seems to have, well, and it's homomorphic, which means that it has a It has a ribon knot formula. So if you have a ribon presentation of a ribon knot, you should be able to compute row one of it directly. Okay. And that, of course, makes you hopeful that you will be able to find a non-ribon condition, right? The Alexander polynomial. Right, the Alexander polynomial has a non-ribon condition, namely, if the Alexander polynomial doesn't factor as it should, then it's not ribbon. So optimistically, you will get a non-ribon condition for row one. Whether or not you will actually get one is hard work that's to be done in the future. Whether or not this non-ribon condition will be used. Condition will be useful is well, will take a lot of luck for it to be useful as we're exploring. Thank you. Okay, I think, yeah, let's thank Georgia and I think wait, what's in ten minutes?