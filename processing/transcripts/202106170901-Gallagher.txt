Probably. So take it away. Thanks, Chase. So, yeah, let me just jump right in. So, this talk is sort of the culmination of me working with a bunch of people and also having exploring differential programming and practice with Robin Cockett and one of his summer students. And I got to, we were been playing with this language called. To we were playing with this language called Julia, and then I was also getting into playing with this other language which is similar called Pyro. And both of these languages do differential programming, but they also have probabilistic stuff stacked on top of it. And I wanted to understand, could we understand? Yeah, I wanted to see if we could understand how all of that fits together within the framework of differential or tangent categories. And it all fits together, but maybe it's a little long. So we'll see if we can get. A little long, so we'll see if we can get through all of the stuff today. But move forward, slides. Dial shout. Yeah, there we go. I wanted to start off with a little puzzle about probabilities. So this is from Tversky and Kahneman. And I'm just going to give it to you. And I want you to go ahead and maybe throw out in the chat what you think. So Linda is a 31-year-old, very, very bright, very outspoken, single lady. She did her PhD. Single lady. She did her PhD in behavioral neuroscience. And while she was studying, she was deeply concerned with issues like discrimination, social justice, and she was always at the climate change marches and protests. So here's the question, which is more likely? That she is the lead researcher of a neuroscience research team? Or she is the lead researcher at a neuroscience research team who in her free time contributes to anti-discrimination programs. What does everyone think? What does everyone think? One or two. You can throw it in the chat or unmute and say. Two, two, two, two, two. Oh, wow. I got a lot of twos. Great. Everyone fell for it. So here's the point. From the probabilistic point of view, the probability that A and B is always less than the probability of A. The probability of A. So, this is what's called the conjunction fallacy, or sometimes it's wrapped up into what's called the causality fallacy. You think that the fact that she did all these wonderful things means that it's going to cause her to do wonderful things later. But that's actually, probabilistically speaking, that's a mistake. And this was picked up on by these psychologists, Zversky and Kahneman, who won a Nobel Prize in Economics for showing based on For showing, based on this puzzle and in puzzles like it, that humans are basically completely irrational. Okay, the idea though, yeah, Linda can't be just a researcher. You're supposed to be outraged when you hear that story. Okay, so the main point here is that there's this bias that humans have that they rank sort of causal information or input-output type relationships as being more important. Relationships as being more important than statistical or probabilistic relationships. And the Nobel Prize was partly on identifying evolutionary structures and stuff responsible for this. I don't want to make this a talk on psychology or anything, but I think this is super neat. One of the main ideas, though, is that if you give that problem to someone again and you rephrase it, like there's 100 people that are all like Linda, how many of them end up here? How many of them end up here or there? Almost no one gets it wrong. So 85% of the population typically gets it wrong if you just ask the question. But when you ask it the other way with explicit quantitative aspects to it, no one gets it wrong, or almost no one gets it wrong. So the idea here is that if you weave probabilistic thinking into your sort of process, you improve the reliability of the conclusions you make. Conclusions you make. Is that all clear so far? Cool. So, yeah, probably being probably correct is more important than being correct. That's the moral of the story. Okay, but here's another problem. There's two treatments for kidneys. This is also a famous one from a medical journal. Two treatments for kidney stones. One is treatment A and the other is treatment B. Treatment A is 78% effective, while treatment B. 8% effective, while treatment B is 83% effective. And this difference is known to be statistically significant between the two treatments. So it's a real difference in those effectiveness rates. What do you think in the chat is the better treatment? B. B. B. B. B. Awesome. However, Awesome. However, there's this thing that happens, and it happened in this study, where when you break this down into, there's like another factor, which is the size of the kidney stone. And it turns out that while B is overall more effective, treatment A is more effective on small stones and is more effective on large stones. So treatment A is better. Treatment A is better. And the fact that this can happen when you're doing probabilistic reasoning is what's often called Simpson's paradox. So it's this thing where you've made a conclusion based on pure probabilistic thinking that gets swapped around once you realize the other variables, that there's another variable that causes a sort of sign change between the two things. So this happens all of the time. And it's unfortunate that, I mean, like, Unfortunate that, I mean, like, most of the arguments I see on the internet are like Simpson's paradox, like in Reddit forums and stuff. It's like all Simpson's paradox. Okay. So there's like an apparent cure to Simpson's paradox where if you're really careful and you set up your probabilistic problem as an explicit Bayesian network, and you've got sort of like nodes in that network. Of like nodes in that network that correspond to what's it called when you take something apart? Like, yeah, when you do that to some variables, then you can actually algorithmically, decidedly test for whether Simpson's paradox will occur or not. And this is what some people are calling. So, before we were saying causality is more important than statistics, and this is causing a lot of people to say, yes, and it should be. Yes, and it should be. So, there's like even a meta-paradox between those two problems. And all of these little issues show up not just in sort of natural neural networks or brains or humans in our reasoning, but they show up in different forms in neural networks, summarized by the fact that if you're doing sort of differential programming or machine learning kind of things, creating artificial neural networks instead of Artificial neural networks instead of teaching babies to think or whatever. Then, combining both sort of statistical or probabilistic reasoning together with causal or just straight differential programming leads to much more robust solutions and optimizations that don't fall into sort of overfitting. So, that's a huge benefit to this is that these stochastic probabilistic networks, they don't fall victim to sort of Networks, they don't fall victim to sort of overfitting, or they're not skewed heavily by outliers or anything. And then, also, like in the core, in the core underneath of them, basic algorithms in both differential programming and probabilistic programming use each other. So, to do differential programming at scale, you're talking you've got inputs that are like 10 billion, you know, r to the 10 billion sometimes for some of the computer vision ones. Or vision ones. So you, and then you've got a training set, which is probably, I don't know, how many training data do you need to train a car not to hit cats? I mean, a lot, like lots and lots. So you've got to use a little bit of stochasticity or deer or other cars or people with yellow jackets. You've got to use some amount of stochasticness to sort of say, I'm not going to look at all of the things in my inputs. This I'm going to look at some R to the N, which is a lot smaller than 10 billion. Which is a lot smaller than 10 billion. And I'm just going to gradient descent along this sort of subspace. And also, yeah, you also want to sort of not look at every training item at every pass in your algorithm. So they kind of really do fundamentally help each other. And even the probabilistic ones, there's this technique they use to approximate any probability distribution with this. Distribution with this special what they call a window. And the way that works, the way they get that trained is actually using the gradient descent. So they start off with this sort of approximation of like a windowy arbitrary probability density, and then they sort of train it to the one you're seeing in practice by gradient descent. So, excuse me. Excuse me. Basically, the point being, Basically, the point being probabilistic and differential programming are almost made for each other. And they're really much better together than they are apart. Okay, so what I want to do today, so when I was thinking about, okay, how do you set up, you know, a probabilistic extension to a differential programming language is the idea. So we're going to start just like Julia and just like Pyro do, they start with this core of a differential programming language. Of a differential programming language. In Julia's case, it's this flux or zygote language. In Pyro, it's PyTorch. So they start with these core differential languages and then they put probabilistic machinery on top of that. So we're going to try to do the same thing. However, the first problem we run into is that we know, we sort of already know, it's already been out there how to model differential programming sort of first order, no higher order functions using these kind of categorical tangenty. Kind of categorical tangenty Cartesian differential structures we've been talking about. But doing it with, you know, with higher order functions is sort of, you have to be, it's a little bit more subtle. So we're calling it a dill pickle for differential linear logic pickle. No? No? JS, come on. I thought you would, Christine laughed. Okay, good. Yeah, and then what we're going to do is we're going to pick up. And then, what we're going to do is we're going to pick up this big piece of technology that's been developed in, like, I don't know, a lot of papers by Bart Jacobs relating what are called effect algebras to probability. So, we're going to actually take this effect algebra approach to probabilistic programming and like smash it into the differential setting somehow. But like I said, we have to solve this. We have to show how to solve this other problem of higher order stuff first. I want to mention that there is. I want to mention that there's a really smooth and really easy way to do all of this that was given. There's a sort of purely locally presentable way to do this. There's a paper by Staten and Vakar where they look at a theory, like an essentially algebraic theory for omega quasi-Borel spaces. And you can take models of this into any locally presentable category, including any locally presentable tangent category, and get a Presentable tangent category and get a model of this stochastic probabilistic programming language that they are working with, which is quite nice. And it works super well in this setting and generalizes nicely. But a lot of times in the real world, I guess people want to view probability as like an effect and compute it monadically, like as in a clise category or with some kind of do syntax or something. So you want to kind of pretend that you're not probabilistic until you need it and then do it in this sort of clise. And then do it in this sort of Kleisley category kind of way. So I thought it would be really helpful to see if we can do it this way. Okay, so what I want to do, so what I'm going to try to do at first is walk you through, and we won't have time to do, probably, yeah, definitely, won't have time to get into all the details of how everything fits together, but I wanted to show you how to get a model of a differential programming language with higher order functions that also has. Order functions that also has recursion. And the reason for this is that there's a little bit of subtlety if you go from just differential first-order programming to differential programming with sort of higher order functions. There's a very technical subtlety that gets in the way and that you have to be kind of careful with. And then we'll extend this to the probabilistic setting using these effect algebras. So that's sort of the goal. Any questions so far? Let's see the chat. Yeah, cars hit cats. Okay. Oh, wait, what did I do? Okay, so this is the main sort of starting point. So we'll see how this language works. This is called a simple differential programming language. It was defined by Abadi and Plotkin. And it's sort of the minimal first order kind of differential programming language. You've got essentially. Essentially, recursive functions, like down here, you've got recursive functions, and then you've got this partial or this reverse derivative. So, they use in a lot of times in machine learning, you want to use the gradient because you've got a function from r into r. And you can compute the gradient of such a function sort of by walking through the graph of that function, sort of all of the basic operations it does in a single. Basic operations it does in a single pass. Whereas, if you do, and that corresponds to taking a reverse kind of derivative. And whereas if you do the forward one, you've got to take each partial derivative. And so you've got to compute, you've got to walk through the graph of the function, say, n times. So they like these, what are called reverse derivatives in practice. And then, yeah, they have recursion and sums and so on. So that's the starting point language. That's the starting point language. And once you start having recursion in a language, you immediately need some kind of partiality in order to recode, like in bare minimum to encode non-termination. And I don't know if everyone here is familiar with restriction categories, but we're going to use restriction categories as our sort of categorical abstraction for partiality. For partiality. If you've not seen them before, you should go check them out. They're super nice, convenient ways to work with categories of partial maps. What do I say here? Ah, yeah, yeah, yeah. Okay, this is all great. However, for like a lot of the purposes that we've got for today, we don't really need to dive really hard into the theory of restriction categories or this kind of partiality that we're going to use. We just need to know. Going to use. We just need to know the idea. And the main idea is that, you know, for each map F, there is an abstract domain of definition called F bar, which is you think of as providing the domain of definition of F. And the main important idea here is that there is a subcategory of total maps. So these are the maps that aren't partial in a technical sense. And the other thing we'll need is that. Is that restriction categories are always partially order enriched, where we say a function f is less than g if f is less defined than g, but otherwise they're the same. So if f is defined, then g is defined and they're equal. The idea of the partial order enrichment there. And then we need the notion of a Cartesian restriction category. This is just a restriction category that has a monoidal structure on it. A monoidal structure on it that behaves like a partial product. In particular, if you have a Cartesian restriction category, then its category of total maps is actually a Cartesian category or has finite products. And I think that should be, like I said, if you want to dive into these things, you really should. There's some links at the bottom to look at. They're fun. All right. And the main example we're going to use today are these are what are called categories of partial maps. Are what are called categories of partial maps? So, a partial map in a category is just a span where you've got a monic or a sub-object of it. So, a partial map from A to B is a sub-object of A and then a normal map from A dash to B. And then, you know, you've got this kind of situation where if you've got a class of monics in your category that's close to pullback composition and isomorphism, then you can make this new category called the partial map category. Partial map category, which is always a restriction category. So, this is kind of a recipe for making partial map categories out of total categories that we will need. Okay, so with that in mind, then the very first thing we need to talk about is what is this reverse derivative we did in that simple differential language? So, it follows if you've remember the definition of a differential. remember the definition of a differential of a Cartesian differential category from earlier this week it's very similar so for but it's it's kind of backwards it's linearly backwards so if you remember earlier we had that for every map from a to b there's a map from a cross a to b which you think of as sort of the jacobian of f at the point in this direction and what we've done here is you take the jacobian of f at a point and then you take its transpose so instead of a map from a cross a to b you get a map from From A cross A to B, you get a map from A cross B to A, where you've sort of transposed that matrix there. And this, it's just so a reverse differential restriction category is one of these Cartesian restriction categories where every object is a commutative monoid canonically, and you've got this operation that sends maps to their reverse derivatives. And it satisfies axioms very similar to the Cartesian differential categories. Categories, but what you would get if you transposed all of those axioms. And this can be made formal. So these sort of reverse things are the same things as the forward ones. So reverse differential restriction categories are exactly differential restriction categories where you can sort of daggerize, you can flip maps back and forth. So if you've got a map from A cross B to C, you can get a map from A cross C to B. And if you're linear in this B. If and if you're linear in this B, then you're linear in this C, and then the operation is an involution. So it's like a dagger, but where you're holding one of the objects fixed in context. Does that make sense? More or less. Okay, cool. So that's the idea of the reverse derivative. They encode our gradients. However, when we went through, and this is going to be important for what we're going to do and how we're going to get the To do and how we're going to get the closed stuff to work. When we went through the proof that these restriction differential categories model, what are they called? The simple differential language, we didn't actually need all the axioms. In fact, we don't need this. If you go through the list, there's nine axioms. We don't require the second, sixth, or seventh axiom. And so we called these things without those three axioms basic. Axioms basic reverse differential categories. And there's a corresponding notion of a basic forward or normal Cartesian differential restriction category. And again, you drop the, if you look at the literature, you're dropping the second, sixth, and seventh axioms. If you want to know, these are the axioms, the second and sixth are the axioms that say that the derivative is linear in its sort of vector argument, and the seventh is the one. And the seventh is the one that says that there's a symmetry between higher partial derivatives. So we don't need those, so we can throw them out, and we do in a sense. So the main theorem, and I did not state it correctly here, but it's actually that any basic reverse differential restriction category with joins provides a model for the operational semantics of that simperential? What? What? Simple differential programming language. So I can point you to literature on that if you want to see a full proof. But the main idea is that recursion is interpreted into these joins. So these joins, remember that restriction categories are partial order enriched. And so you can talk about joins of maps in a partial order. And so you have to have all joins of what are called compatible families. And that includes joins of directed families. Joins of directed families. So, in particular, these join things are DCPO enriched. And so, you can interpret the join like you always do into any DCPO enriched category, or sorry, the recursion like you do into any DCPO enriched category. And then the reverse derivative in the language is sent to the reverse derivative and the category. And, like I said, we don't need those other axioms. Okay. Okay, so that's how we do it for the sort of simple first order differential programming language. Are there any questions about like what's going on, the structures being used here, and so on? I want to make sure. I can't see everyone's face at the same time, but I'll scroll. No. Okay, everyone looks pretty calm. Pretty calm. All right, so the next step is: so, if that was simple DPL, then maybe there's going to one day be an advanced DPL, which I don't want to claim. So, we're going to call this thing intermediate DPL. And what we're going to do is just add higher order functions to the language. Real simple. So, we've got everything that we had before, you know, if-then-else statements, differential statements, recursion statements, but now we've got the ability to do. But now we've got the ability to do eval, so a function here applied to an argument, and we can create lambda terms, the lambda x dot t. And also, I wanted to note here that before we had this R, and this is the very first subtlety you run into, there are some models of Cartesian closed categories that have a reverse derivative in them. However, they're kind of hard and Of hard and subtle to get, and we don't know of any way to get a model that's closed as a restriction category. Excuse me. So, in order to do the sort of higher order stuff with recursion and partiality, you need to sort of, you need to be closed as a restriction category in a sense. So, we don't have any models of that. So, we dropped it and we're just moving back to the normal forward derivative here for the moment. Here for the moment. So that's one thing to keep in mind: we're no longer using the reverse thing, and there is kind of an open problem as to whether it's possible to mix the reverse derivative with recursion and higher order at the same time. So if someone figures that out, that'd be super awesome, whether it's possible or not. All right, next. So, how do we get, yeah, how do we get to the semantics of this thing? Well, Of this thing. Well, wow, this is a lot of words. Here's the main point. The main point is that we've got these lambda terms here, and these are higher order functions, but they're sort of higher order partial functions. And when you think about the partial functions even into the terminal object, that's going to be something like the space of open sets, right? The partial maps into the terminal topological space is exactly. Topological space is exactly the space of open sets of the original thing and so forth. So when you're looking at partial maps into a terminal object, you get this thing that doesn't look like a vector space. And the metaphor is that differential categories are kind of abstract categories of smooth maps between vector spaces. But these objects which will have to exist kind of don't fit into that framework. Into that framework. They do, however, fit into this more general situation of tangent categories, which have been being talked about quite a bit. So these are kind of more manifold-y kinds of things than sort of vector space-y kind of things. However, for technical reasons, we're going to work today with something called a prototangent category, which I'll tell you in a second what it is. I'll tell you in a second what it is. It's a generalization of a tangent category that allows us to get around a small technical issue with doing this in tangent categories directly. There is a way to do it in tangent categories directly, but it's a lot more work and definitely has to require some subtle thinking. Okay, but the main point is we didn't use all the axioms of a differential category to model the language, so we don't need all Model the language. So we don't need all the axioms of a tangent category. That's the main idea. So if you remember back to Garner's talk or his paper on the embedding theorem for tangent categories, he showed that tangent categories are exactly these categories with an action by V algebras that preserve where the action preserves certain limits or connected limits of V algebras. Um so so we can we can use that to define and the the axioms that we dropped this the second sixth and seventh axioms that we dropped correspond exactly to saying ah actually we don't need to preserve those limits all we need is that that action part so so so when we when we move this up to the tangent category setting we'll drop that limit preservation so a prototangent category is then tangent category is then a category that has an action by V by V algebras but know nothing about preserving any limits. And then it's Cartesian, it's a Cartesian prototangent structure if every one of those actions preserves products. So you've got these general abstract tangent functors, T sub U, one for each V algebra, and all of those tangent functors have to preserve products. But in general, there's nothing. But in general, there's nothing about preserving limits of V algebras. So, in particular, you know, T of M doesn't have like local addition. There's no plus map from T sub 2 of M to T of M and things like that because those limits aren't preserved. T sub 2 isn't a pullback and you don't have that map leaving it. Yeah, so that's what. So that's what we have to do. Okay, now, so that's the idea of a prototangent category. And the next thing we need to do comes, the next thing we're going to do, it's actually kind of a neat little idea that came from a paper by Kachin-Reyes called Manifolds in Formal Differential Geometry. So they had, in the setting of synthetic differential geometry, it's supposed to be like an abstract. Abstract generalized category of smooth spaces, but you might wonder: what are manifolds in this thing? And in sort of normal topological settings, you construct manifolds by gluing together a bunch of sub-objects of Rns. And so you might want to know what kinds of sub-objects could you glue together in synthetic differential geometry to make manifolds. And you don't want to just take arbitrary subsets of Rns, you want to take sort of nice. Subsets of RNs. You want to take sort of nice sub-objects. And the definition that they came up with was what's called a formally etal sub-object. So this is a sub-object A of B such that the naturality square is a pullback. So in particular, it's like a local diffeomorphism as well. So that's this notion of a formally a tal subobject. And these are quite important. These are quite important. So, remember back in a few slides ago, we said that the main way we're going to come up with restriction categories is by creating a partial map category out of something in some monics. It turns out that if we want to, say, take that partial map category and have it have a tangent structure on it, or something like a tangent structure, like a proto-restriction tangent structure. It actually is required that all of the monics in our class. That all of the monics in our class M have to be formally atal. So it's actually a requirement. This is a hard requirement here, which is really neat that it came from this other perspective of how do we do manifolds. It gives us the right notion of a class of monics so that we can put a prototangent structure on the partial map category. And there's a little lemma you can prove here that if you have a displace, so a displace system is just. So, a display system is just a class of maps that's closed to pullback. And here we're saying that if we've got a display system of formal et harmonics, which is closed to composition and isomorphism, then the partial map category always gets that action. And if that prototangent category was actually a tangent category or a Cartesian tangent category, then the partial map category. Tangent category, then the partial map categories are tangent restriction and Cartesian tangent restriction categories as defined by Jeff and Robin in their paper. So this notion of matches up, like this notion of these actions that you get matches up exactly with what they should in the case that it was a tangent restriction or a tangent category. But it's a little bit more general again. So, this condition, I hope it's oh man, I'm gonna not even have close to enough time. Cool. Awesome. So the main point here is that there's this notion of being coherently closed. And the notion of being coherently closed is that you've always got for each of these generalized tangent functors or each of these actions by Or each of these actions by V algebras, T sub U, there's always this cotensorial strength from sort of the tangent bundle of A implies B to A implies the tangent bundle on B. And the point of being coherently closed is that this map is an isomorphism. Whoops. So, right. And this is. Right, and this is important because we want to talk about closed restriction categories. So, closed restriction categories are like having restriction categories whose underlying category is Cartesian closed, whose total map category is Cartesian closed, and that have classifiers for partial maps. So, we need to have a coherently closed tangent category so that our total map category is Cartesian closed. And then we say. And then we say that it's formally atal classified if there's a monad P with the property that partial maps from A to B are the same thing as total maps from A to this P of B thing. And that means that we have a formal atal subobject classifier in the prototangent category, P of one. So it's like a formal atal subobject classifier is just like a normal sub-object classifier, except maps into it. You know, maps into it are only going to be giving you formal eta monics, so not all monics, but just the formal eta ones. Yeah. And then finally, the main theorem here is that if you have one of these coherently closed, formally atau classified prototangent categories, then the partial map category is a model of the operational semantics, so that intermediate DPL language. So this is So, this is sort of the main result here: is that we can abstractly classify, characterize what a model of this DPL is using these prototangent structures with these, you know, this list of adjectives. And I have to admit that these adjectives, while they might seem like, they're actually every single one of them is sort of forced as you go through. Like, if you were to have a model, you would have to have all of them. A model, you would have to have all of these things, all of these properties, so there's no way around it. Okay, and then the final little bit here is that hey, guess what? Models exist. So the way you get a model of this is to start with a well-adapted model of synthetic differential geometry that has what's called the amazing right adjoint property or wham.5 in Cox's book. It says that the tangent functor is both a left and a right adjoint. Functor is both a left and a right adjoint. So you know in synthetic differential geometry, it's always a right adjoint. But this is saying it's a left adjoint as well. And that means, in particular, it preserves co limits. And that's important in showing, because the join can be characterized as a certain kind of co-limit. So to get joins on that partial map category, you need this kind of co-limit preservation. Preservation so that you can get the join of formally atal monics to be formally atal again, so that we have unions of formally atal subobjects. Okay, great. So that is that's the end of part one. So that was a very high level, whirlwindy kind of tour through the fact that we can use things coming from this tangent structure point of view to get at models of differential programming. So I'll stop. Differential programming. So I'll stop briefly here. Are there any questions on what we're doing, how we're doing it, or why we're doing it this way, or anything like that? Don't see the chat lighting up. Cool. So let's go to the next part. So, this part here, like I said, we probably won't have time to finish, but if you want, I'm available and love to talk about these things. So just find me. About these things, so just find me. So, the main idea is coming from the PhD thesis of this person named Meng, who I found out about this from a paper of Rory, but he did this little abstract trick where what you want to do to get a probability, like you want to do probabilistic programming. Well, one way to do it, like we were saying, is to get a probability monad on your category and then do your probabilistic programming as or stochastic programming. As or stochastic programming, as programming in the Kleisley category of that probability monad. And he presented this really neat way to generate probability monads from this simple data of sort of you've got this category of sets or compact Hausdorff spaces. And then what you do is you look at sort of the convex objects there. So convex sets or convex spaces. And these turn out to always be. And these turn out to always be categories of algebras of a certain theory with values in the base category. So this convex spaces is a category of algebras valued in set, and this is a category of algebras valued in compact Hausdorff spaces. And for like sort of various simple reasons, there's a free one. These things, they're co-complete enough so that I can always form the free thing. I can always form the free thing, and which means that there's the underlying functor here has a left adjunct. And that D that you get is always the probability monad. So these probability monads can be viewed abstractly as sort of free objects that exist with respect to the category of algebras of a specific theory in a category. In a category. And the category, the theory is really like matrices whose rows sum to one. So it's not a very, it's like the theory of affine spaces or something. And that's what he used. So he used this sort of general setting to show how to take a normal programming language and then extend it to sort of stochastic setting. And the idea here is that you might think of set or compact Hausdorff spaces as your base programming language. Bayes programming language, and then your Kleisli category is going to be the free the free algebras of this thing, so that you can you can do the stochastic programming in there. I don't. And we have a question. Yeah. All I can tell you about that is that Bart Jacobs does have some work relating these symmetric C-star algebras to some of these settings. Like he does. Settings, I like he does use something like the dual of one is the other, or something like that, in some places. So that does come up, but I'm not exactly sure the exact connection there. Okay, thanks. Could you please maybe at the end give me the link of his thesis, please? Thank you. Oh, it's at the bottom here. If you click that blue thing, yeah, you can. It's actually available on NLAB if you go to contact spaces and scroll down. Context spaces and scroll down, you can find Meng's thesis. It's actually just stored there in the in-lab. Okay, great. Thank you. Yeah, thank you very much. Another question. There's another question. Yeah, that's also related, Jeff. So there's a paper by Bart Jacobs called Convexity, Duality, and Some Other Word. He's written a lot of papers on this. I can't keep them all straight. I'm sorry. This, I can't keep them all straight. I'm sorry. He's written like 30 years, so I mean, it has to be a million or something. Um, he's really crushing this area, but he, yeah, there's a paper like convexity, duality, and something, and they do make a direct connection to the ultrafilter monad. Yeah. Thank you. Yeah. That's also, by the way, related to the condensed mathematics that Peter Schultz is tracing out as well. Oh, neat. Cool, cool, cool. Wait, how did I get here? Oh, yeah. Okay, so this is neat. So, Rory has this paper where he generalized this thing that Meng did, and he talks about the category of affine spaces or convex spaces with respect to an ordered ring, I guess, in any Cartesian closed category. And again, you get this adjunction as long as you have enough co-limits. Have enough co-limits, you get this adjunction where you can construct the free thing, and you get this adjunction between C and the affine or convex spaces in C with, you know, as long as you start by, you know, taking that theory of some ordered ring or maybe just, yeah, like an ordered ring. What's neat about this? So, this is neat because you can generalize that setting to say, oh, there's always a distribution monad that comes. There's always a distribution monad that kind of works, and I can always interpret my probabilistic extension to my language in this kind of setting. Yeah, so the question now is, is that good enough to just throw that at a tangent category and say, like, or a prototangent category and say, hey, just do this thing. Boom. It turns out, next slide, that a lot of times, That a lot of times in practice, like there's various companies, there's this. Do I have a link to them? Yeah, there's this programming language called the Hakuru, the Hakuru language, which does probabilistic programming. And it's for their applications that they have in mind, it's really important that they're not just having a probability monad, but they also have this other monad called the expectation monad. So you want the ability to not only see To not only sample probabilities, but you want the ability to say, hey, what's the expected value of this distribution that I've been developing? And the way to do that, this has been developed like super kind of abstractly in some papers by Jacobs as well. So the way he does it is you start with this adjunction that gives you the probability monad. And then there's always this other adjunction between whatever that convexy kind of. Between whatever that convexy kind of category is and the category of what he calls effect modules. And if you compose this adjunction all the way around, what you get is exactly the expectation monad. So if you compose the D followed by this monad here, sorry, by this adjoint followed by that thing, you get the expectation monad for the finite or sorry, discrete probability distributions. And if you do it for the And if you do it for the for what are called Banach effect modules, then you get the expectation monad with respect to sort of continuous radon probability measures. Okay, I am realizing I only have one minute left, right? Yeah. Okay, so what I'm going to do then is I'm going to carefully Skip way ahead. So, look at this. Boom! We have a little, we have a nice little lemma here. So, if we start with a okay, so here's what's cool. Let me just tell you what's cool is that the notion of an effect algebra can be characterized completely in terms of a partial algebraic theory. So, an effect algebra is a. An effect algebra is essentially a partial commutative monoid with something called a complementation operator that satisfies some axioms. And this can be described entirely as the algebras of a so well actually as product preserving functors from a special Cartesian restriction category into the base Cartesian restriction. Into the base Cartesian restriction category, where your maps, though, are total at every maps are natural transformations, but that are total at every component. So this is like a morphism of effect algebras is, what is it? It's like a total, it's like a total function of the underlying sets that interacts well with the partial operations. And the main point that I want to get across is that effect algebras are pretty much generalizations. Pretty much generalizations of the unit interval. Because you've got zero and you've got this complement operation, zero complemented, you think of as one. And you can show that these are always ordered in a certain way so that you get something like an interval from zero to one and so on. And so you can think of these effect algebras as abstract unit intervals. And we can take, again, you can take probabilities into them. Probabilities into them, and they are generated as algebraic, like models of an algebraic theory. So we can do the trick that Rory does in several of his papers, where we then say, okay, if our category is Cartesian closed, then I can treat that effect algebra itself as a theory enriched in my Cartesian closed category, which is self-enriched. And then you can look at models of that theory into yourself. And it turns out. And it turns out that if you do this, this category here, if you do that sort of partial product preserving functor from this affine theory into x, that is exactly convex spaces. And this is exactly effect modules. And the details are in the slides, so you'll have them and you can look at them, or they'll be written up sometime. And yeah, this is the sort of the main point. And that the next main point is that we had to generalize this a teeny bit. We had to generalize this a teeny bit. Effect algebras usually require the so you always get this canonical ordering on it. And the way they're defined is that it forces that to be a partial ordering. But we need it to be a pre-ordering to get it to work for synthetic differential geometry. And I'll just state that here, the main little result of this is that if we start with a model of SD. If we start with a model of SDG that is a well-adapted model, has the amazing right adjoint, and has something else called the order. The order axiom? It's basically a partial order on R that interacts well with infinitesimals. Then you can always do the same thing. So you get this distribution monad here, where you look at, yeah, you get this distribution monad here, and you get the. Here, and you get the expectation one out going the full way around the circle. And the effect algebra that you get in this model is the unit interval 0, 1. And what's nice about it is that it's actually a microlinear object. So the unit interval is somehow an open, which makes everything sort of work together with the tangent structure kind of nicely. Yeah, and there's a lot of future work to be done. And there's a lot of future work to be done, but I'll stop there and take questions. All right, let's first unmute ourselves and thank Jonathan for the talk. We have time for maybe one or two quick questions. Peter? Hi, Jonathan. Somewhere in the middle, you talked about models and models you Talked about models and models you got out of SDG. I was wondering if you try to build a model out of the syntax. Ah, yeah, like a model out of the syntax. So with the syntax that I was using earlier, it was for a programming language. So I don't know, there wasn't an equational axiomatization of that. Equational axiomatization of that. It was more like there's an operational semantics for it. The equational theory of sort of partial, what would you call it, partial differential lambda calculi, I don't think has been completely worked out yet. So I don't, so no, I haven't tried to work out an equational model. And I don't know what that would look like. And nor do I know what, like the other. Know what, like the other side to that is if you start with, say, an untyped model of the differential lambda calculus. I also don't know what it looks like if you do the sort of partial equivalence relation thing there. So there's there's two things that really haven't been done kind of related there. Does that answer your question? Unfortunately, it's a negative answer. Yeah, thank you. All right, the next man I saw was Jeff. Just quickly, do you have any thoughts? Just quickly, do you have any thoughts how this could relate to Tamar Hard's talk on Tuesday? Because he had sort of a probability setting where he had something that looked like a tangent category but didn't have addition. Yeah, I really want to know. Because so what I'll say here is that there is a notion of a cockle, what probably should be called a cochlear pre-effect module. So, you know how in SDG, a cochlevier module is A cocklevier module is an R module with the property that the map from V to the tangent space at zero of V is an isomorphism. One way to state it. And excuse me, for effect modules, what you get is that the map from V to T zero of V is formally atal. And if you work through this, what you get is a partial isomorphism from T of V to V cross V. From T of V to V cross V. Instead of like a differential object where you get T of V to V cross V, you get a partial ISO from T of V to V cross V. And it has a total inverse. And you can write this down. I started doing this where you write it down in a differential bundly kind of way, where you change things that need to be changed as you go. And you do get some kind of, you know, derivative there. Like it's something like a derivative, but it's not. And so I. It's not. And so I don't know if that matches up with what Tomal was talking about. And if it does, how it matches up. But there is definitely, like I said, for future work here, there's definitely something to work out there, which is a direct axiomatization of these KL pre-effect modules. Gordon? Yeah, hi. That was a great talk. Thanks. But the model you get is. But the model you get is very abstract in some way, very complex. It'd be nice to have a simpler model. I mean, like what, for example, what distributions do you get? I mean, with finite convex things, you're only going to get the finite distributions. We need some kind of limiting thing to get other ones. Or suppose I take some higher order function, what's this derivative like? No, concrete questions. Yeah, that's a great question. And it's a good point that this model coming from synthetic differential geometry is. differential geometry is really abstract. There should be a model of this in what are called Frolicher spaces or maybe diffeological spaces. But I haven't really worked through all the details there and how it works. So yeah, all I'll say for now is that that should be done and I need to do that. That should be done. And I need to do that, where I go through like a much more concrete model where you can see the sets and play with things and see what it looks like. Right. Absolutely. Yeah. All right. Last question from Robin. Who's muted? Yeah, just a quick question. So, I mean, you started off by talking about these proto-tangent categories. These proto-tangent categories, but they sort of haven't played a role in your latter parts of the work. So I just wondered, can you sort of explain to me why you went there? Ah, yeah, absolutely. So the reason I went there was that it's unknown in general whether a formally atop Whether a formally atal sub-object classifier is microlinear. And we need a sort of proto-thing to sort of have a formally atal sub-object classifier living somewhere. And so that's why we went with these proto-tangent categories. And I'll just say that if you have a model of synthetic differential geometry E, it is always a proto-tangent category. prototangent category right every every object in sdg has a a uh what is it a tangent functor but just not every object is microlinear um so you can play the same game with prototangent categories like you do in sdg where you look you know you keep looking back at what are the microlinear objects what are the microlinear objects but i didn't do that here i just sort of dismissed microlinearity because it's not necessary to get models of these languages To get models of these languages and kind of went with just the prototangent structure to develop things. I do think there is at least one model that we know of.