Thank you so much. This is a paper with co-author with Ametas Di Martino and Guillaume Rasapiro. So our paper starts with identifying a difference between how NLP and Computer Vision do different tokenization for their transformer-based models. So for example, if you look at NLP, look at NLP, they do sort of a semantic tokenization. Basically, there's a lot of different tokenizers, but most of them, like when we tokenize, like each tokens are interpretable and humanized, so they're more semantic. But in comparison, if you look at computer vision, they divide the image into equal size patch without considering any semantic meaning. So if we look at an NLP, if we like divide the sentence into four equal characters, then it will be hard for us to understand. Then it will be hard for us to understand what each token means because they're a combination of two different words. So, similar to that, if you look at computer vision, like some of the patches over here, it has a head of a female, and it has a painting, and it has a wall and window. So it's very hard to say what this individual patch means. So basically, our paper is about applying segmentation model to first divide the image into sort of more. Image into sort of more semantic parts and use each semantic part as a token. So I'll go over like Vision Transformer very quickly. So Vision Transformer first resize to equal size usually like 224 by 224, and then divide the patch into equal size patch, and then linearly project them through use of linear layers or convolution layers, and add again, like partition embeddings. Partition embeddings, usually they use like sinusoid function so that the encoder or the transformer can know that whether this patch is from the left top corner or like bottom right corner. And then like put it into transformer and then do a lot of different tasks. So what we're a little bit different is that we've given the image, we first apply segmentation model and grab the segments. So each part has only one like object or one semantic parts. And then we Semantic parts, and then we would resize them to equal size patch. And then, just like original VIT, we'll linearly project them and add a position embedding. But what is a little bit different now is that we have a separate trainable neural network that takes in a X and Y bounding box coordinates for each segment and the pixel size. And then we will put that into a neural network and get the embedding and we will add that as a partition embedding. So, what is very interesting is that, like, So what is very interesting is that like, if you think about like NLP, the word embedding for like, for example, I am riding horse and I am feeding horse, the embedding for the horse changes more of a transportation when we have like riding in front. But if you're like feeding horse, the token embedding will mean more of a like pet. Similar to the image now, let's say we have two segments. One is human. We have two segments: one is human and one is horse. If the human is on the top of the horse by the bounding box, then the horse embedding will have more of sort of a transportation meaning. And then if they're on the side, it will be more of a pet meaning. So basically, we sort of enable the like our new model so that they could do very similar things as what has been done in NLP. And then after segmenting everything out, we still have a sort of remaining. We still have a sort of remaining pixels. We'll resize them and we'll resize them as an equal size patch again here. And then we'll assign negative one to the bounding box so that the model can know this is from the background pixels. And then the rest will be the same as origin of the IT. How do you handle the the segmentation of the IT? Taking the segmentation part, the output of a segmentation network is not actually. Oh, right, right. So we will just resize them and then we'll put like the background as a black. It might be if we apply a convolution, then they'll like look at the shape. So we thought it was okay, but maybe it's not. And I guess the question is, is this a semantic segmentation network there for every week respond to the object or is it just a handy provider? Oh, it is like from the model. Like I made anything model? Oh yeah, definitely. But we just put the image in the segmentation model, and then they'll give us about like 196 or like 200 segments, and then we'll use that as a segment. I'm not sure I'm answering. But maybe that default users would be. Maybe the default users are with issues. Oh, yeah. Okay. Oh, yeah. But I mean, suppose, for example, that one of the segments was all the background. Right. Then you would, then one of the tokens is now the whole image. Right. With all of the part foregrounds that's black. Oh, yeah. That's the favorite. Oh, yeah, yeah. That that's okay by you. Oh yeah, yeah, like so they basically like based on some of the data sets, like if we do like object-centric data sets, then they tend to have less number of token because like there's less number of segments. But we still show like in the experiments that we do much better. It could be due to like compression or like having sparse like input. But like I thought it was very interesting, but we do have an improvement still. And for pre-training, all we pre-trained. For pre-training, we pre-trained on the Place DSS, which are 1.8 million scenario images. Basically, nowadays they train on ImageNet, but we chose PlayStation because ImageNet is sort of an object-centric data set, which are like from the wild image, they do copying on one object. So basically, if you train on ImageNet, we felt that we were trying to approximate like what dictionary does. But instead, what we really wanted to do is sort of learn a contextual information. Do is sort of learn a contextual information. So that's why we chose like places a scenary where there are a lot of different objects and segments so that we could sort of learn the contextual information. And then we fine-tune and evaluate on 14 diverse data sets. One of the advantage we first saw was like we first trained on like the core of the places and we wanted to see whether we were more efficient in training with low number of data. And we were able to Number of data. And we were able to get a good improvement compared to like origin VIT to our S VIT. And one of our theory is like we expect that since we use segmentation model, we sort of incorporate semantic prior knowledge. So basically, we really narrow the prediction search space for tokens. For example, if you look at token METH, from if you apply NLP on like non-semantic tokenization, like what is the word? Like, what is the word? Like, first you come up with, like, is it a word from method or methanol? But if you look at like closely, it's more of like time death. So it's even hard for the humans to know like whether this token is from two words or three words. It would be much difficult for the model without any language knowledge, meaning it needs more data. So it goes the same with the computer vision. If you look at the orange hat guy, this guy could come out and chat guy this guy could come out in three different patch six different patch based on his location but while we use segmentation model it will be like always consistent with only one token for him and second also like we really reduce the number of tokens required as input as you have discussed so basically this is some of the table we could see like in general even non-object centric data sets has like about less than um like half of 196 total Like half of 196 tokens, which are the usual input number for the computer vision models. And we could actually see like much decrease in all geocentric data sets. And basically from this, like we might compress the information well, that's why we might have much improvement. But so this is the results from the improvement. So basically, we train on this, place this data set, and fine-tune on all other data. And fine-tune on all other data sets and evaluate it. And on non-object-centric data sets, we were a superior in all the data sets. And for the object-centric data sets, I think they usually call this as like out-of-distribution generalization because you are pre-training on sort of a scenary image, but you're like finding and evaluating sort of object-centric data sets. And among six out of eight, we were superior in like six out of eight data sets. We are in like six out of eight data sets. And especially if you look at aircraft, cars, flower, and pet, we were increasing by a lot in maximum of 30% in accuracy. And that could be due to like semantic scale invariance of our model. So what is semantic scale invariance? Again, when we're given this two image, one is non-object-centric image and object-centric image on the female. And let's say we're really focused on the female parts. Female parts. If you do non-semantic tokenization, the patches for the female changes drastically just because the relative size of the female changes. So the model will not notice that they're the same person. But in our cases, it stays consistent because we segment and resize them to equal size. But we all like not only consistent, but we still do give information about how the relative. Uh, information about how the relative size has changed because we again we have a position embedding that takes in information of the bounding box and the pixel size. So, this is, I think, what's very interesting because now we sort of combine CNN's inductive bias because we also have like translation invariance while capturing the global dependency and contextual information of transformers. So, we're sort of grabbing both advantages and for robot. And for robustness on natural distributionship, so basically this is looking at like we fine-tune on ImageNet and evaluate on a variant of ImageNet. So ImageNet V2 will be the data sets that has been collected after a decade. ImageNet A will be a very zero where a lot of vision model tends to fail. ImageNet R and ImageNet S is like a label from the ImageNet, but it should be more of like drawing and sketches. Be more of like drawing and sketches. And as you can see, our model outperforms the origin of VIT. Even if they do worse on like the ImageNet, they do well on like ImageNet A and R and S. And one of the reasons we kind of noticed is that from looking at the graph camera of the VIT, the VIT tends to focus on background if the object of interest are very small compared to the background. But if you look at our But if you look at our interpretability, we tend to focus on the object of interest, even if they're small. And I think this is intuitive because Vision Transformer tends to give equal weights on the pixel level, while we give equal weights on the token level. So basically, if the objects of interest are very small between two images, and if background is very large, then if you look at like difference in the background versus the object of interest. The object of interest, then absolute value will be much bigger in the background because the background has more pixels. But now, like in our cases, what we do is we resize the background and object of interest into same size. So we're actually looking at like a normalized difference. I think that's why our model were able to focus on the small object of interest. And I think this should be intuitive because in NLP, we don't give a more attention on the search. A more attention on a certain word just because it has more characters. Because the NLP tends to give equal weights on the word level, also. And we also made some improvement on augmentation. So currently, augmentation has been applied in like a whole image. For example, horizontal flip, we'll flip the image and we'll get one image. But now how we could prove is we could actually apply a horizontal flip in each token. A horizontal flip in each token level, meaning if there's n number of segments or tokens, we could have two to the power of n potential combination from each augmentation. So we could actually apply crop and resize color zitter. So basically on any augment, like almost a lot of augmentation model, we could increase their diversity. And second, we also could augment on the position embedding space. For example, let's say. For example, let's say there's a data set where like relative distance among segments are very important, like classifying a group of force, two group of force, or three group of force. And also, like also considering some problems where if there's two people on the same line, based on the angle of the camera, like one person could come out very big and the distance could look very shorter. What we could generalize is that we could add like sort of a Gaussian noise on the X and Y. Ab Gaussian noise on the X and Y bounding box in the pixel side. So, in our knowledge, there hasn't been any augmentation tools to generalize into those problems. So, I think we are the first one to sort of generalize. And lastly, is interpretability. So, basically, if you look at a lot of grad cam based interpretability models, it's very hard to interpret because, for example, like Because for example, if you want to describe this, it would be very hard, like saying it is looking at the bottom part of the rock arc, and then a little bit on the medium part or the top part they're not looking at. So basically, if we have a two-person and I'm the one who's explaining and another one has to highlight, it would be very hard to exactly highlight what they did. So in that sense, I don't think it's very interpretable. But in our cases, it becomes more. But in our cases, it becomes more interpretable because again, we use like segmentation model to segment, divide the image into more interpretable parts. So I think in that case, we are more interpretable. And this has been a sort of a discussion in other papers. For example, one of the paper claims that disentangling representation into a few explanatory variables using unsupervised learning is challenging without strong inductive bias. So basically, if you look at GregCam. If you look at GregCam, GregCam is trying to disentangle sort of the variables to become explainable, but they don't have any strong inductive bias of understanding how humans view image as an interpretable part. So basically, if you look at GregCam, like in order to have good examples, we usually have to apply like hundreds image and go over them and choose some of them, choose some image that makes sense. But so that's why it's not very consistent. But now our approach becomes more consistent because we first disentangle the input to explainable patch using segmentation model. So segmentation model in this case will be more strong inductive bias. And because this has been explicitly trained to sort of divide the image into explainable parts. Thank you. Thank you. Questions? So, great talk. I really enjoyed that. I think it makes a lot of sense. The one part that is not super clear to me is when you do the segmentation. Clear to me is when you do the segmentation and then you resize the stuff, that makes sense. But is the actual segmentation network gonna be able to adapt to different scales? Like, how does that be able to identify things in the different scales? I think from where I understand there's different models, but like the big models tend to like resize all the images into 10,024, 10,024, and then put it into their model. And then put it into their model. And then, after like grabbing the mask or the segmentation, I think they resize them so that it matches the original image. Thanks. Okay, so if I have two pictures and one picture, there's a small person, and then another picture there's a large person. Is the segmentation actually going to be able to identify the both of those people? I think it's really based on what kind of model you use. If you use good model, I think they are able to identify. But if you don't use a good model, I do see some, I use some very small model and it doesn't do well. It only detects a few segments. So yeah. Thank you. In the all times of computer vision, they would be different architecture to the There would be different architectures to detect a 30-pixel person versus 100-pixel. But today, the architectures that are multi-scale are able to deal with reasonably large changes of scales for objects. When the objects are just too small or very distant from the camera, then you would see those failures. But what I presume was happened here is that if those regions are not if those regions are not segmented and the downstream task doesn't depend on that person in particular, then you're okay. But if the task was, say, counting how many people, then you're okay. I guess you were next, right? My question was already. I'm curious about maybe some of the implementations. Maybe some of the implementations of this. So, one is how do you compute the resize segmented added batches? I'm also curious if there are any throughput issues associated with like defining SAM or something. Oh, so you mean like throughput? Throughput, yeah, yeah. The blobs like crawl kinds of different. Oh, yeah. So if you have used it in special techniques, you've done so. So, of course, like the computation is very huge. So, during training, what we did was we go through once on whole image. And then during epoch, we like, I'm not sure I'm answering your question, but we grab all the segments separately and resize them to 16 by 16. And then from image of 224 to 24 and like white space. To 24 and like white space, we will like put one segment into like top left and then top right, and we will have like separate image for each like image. So it will be like one segment, one segment, one segment, one segment. So yeah. So after that, we don't have to go through segmentation model during training. But still during inference, we have to go through segmentation. So it's computationally not efficient. Yeah. All right, thank you very much. Thank you.