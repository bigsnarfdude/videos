Gonna talk about one of my favorite topics, uh contractual positions. So please please. Okay, thanks a lot. And yeah, thanks to Daniel and all the other organizers also for organizing the workshop and asking me to talk here. And okay, so kind of compared to the other talks that we have here today, kind of now we come to something completely different because my background is much more on the information theory side. So I was kind of thinking what to speak about at the workshop that has to do with like really fundamental limitations. And it turns out that many here actually work on fundamental limitations using information theoretic tools also. And what I want to talk about is mostly this toolbox of contraction coefficients that helps you in your proofs hopefully to really attack these fundamental limitations. Really attacked these fundamental limitations on quantum computing applications. And sorry, as kind of any good information theory talk, we need to first discuss a lot of definitions and different quantities. And I think the most commonly used information theory quantity is relative entropy. So you have the definition here. And I'm going to flash a couple of more. And I'm going to flash a couple of more definitions now. But for those of you that are not in information theory or don't have the background, I'll afterwards give you just two takeaways from all these quantities, and you don't need to remember the quantities itself. So the relative entropy is this commonly used quantity that probably everybody knows. But then it's also a special case of much wider families of divergences. Okay, so divergences, I'll say a bit more what I actually mean with a divergence, but What actually means with the divergence, but the relative entropy is an example of those. And for example, another quantity that we could think about are the Rainy divergences. These are a larger family of divergences that have the particular property that if I take this, that I have this additional constant alpha, and if I take the limit of alpha to one, then I get my relative entropy back. So it's somehow a larger family with nice properties that also measures somehow. Properties that also measure some of the distance on or between two quantum states. And I give you here two different rainy divergences. And I can tell you there are lots more in the literature, but these are the two commonly used ones. And if I want to be even more general, so I don't just want to generalize based on a parameter that I introduce, I can do something even more crazy and introduce a function. Crazy and introduce a function. So now I allow an almost arbitrary function. And usually I want this function to be convex and have like a few other properties, but very like basic properties. So convexity is the main thing. And that gives me an even wider class of divergences. And these are, again, two examples how I can define these F-divergences. And kind of, again, if we choose a particular instance, Instance, which is the function x log x, then this actually just reduces to the relative entropy that you're probably familiar with. And now I said you don't actually need to remember any of these definitions. There are only two takeaways that you should have from this, which is one, there's an almost unlimited amount of these divergences. I can commute these states around, and it will be a different divergence. I can have. And it will be a different divergence. I can have parameters and functions. There's like tons of them. Okay, that's takeaway number one. And takeaway number two is that even if I have two divergences that have the same classical or that implement the same classical divergence, then by non-commutativity, I can still get different quantum generalizations of the same classical divergence. Okay? So, and I've not just the And not just the range of how to generalize them is large, but also even within the setting I can have many of them. And we kind of get back to this idea later. And besides these divergences, what we can also think about, instead of having a distance or a distinguishability measure, many more, we can also think of measures of correlation. So we can kind of transfer these distance. Can have transferred these distance measures into correlation measures. And the most prominent example here is the mutual information, which is based on the definition of a divergence. And we'll get back to the meaning or the relevance of this quantity a bit more later. And now all these quantities, they find a lot of applications. I mean, kind of most naturally in information theoretic settings, where you have some structures that you want to. Structures that you want to just kind of, for example, typical results are like the relative entropy is the optimal quantity when you want to look at asymptotic distinguishability of quantum states or channels. Then the relative entropy will always pop up. But also when you have, for example, optimization problems, you need some objective function or something, like something that you actually want to optimize. And typically, you can use functions like the relative entropy to look at to optimize. Not yet to optimize. Or lastly, kind of if you have, for example, network structures or any kind of structures with a lot of channels or different connections, you might be interested, for example, what is the correlation between certain points in your networks, like the input and output layers of neural network or something. So that can give you information stereotypic bounce on the quality of some tasks. Of some task or something that you want to do on this network. And now let's kind of come to what kind of why do we call these functions divergence or why do they all or what do they all have in common? Which is next slide. Oh, okay. No, not yet. Okay. Which is they all obey a form of data processing. That means I take these divergences. That means I take these divergences, and if I apply a channel to both of the states in my divergence, the quantity is always going to get smaller. Probably everyone has used this at some point if they had divergences in their paper. And that's super useful, but often it's not enough to know that the quantity always goes down. Like this is useful if you just want to get rid of a channel in your proof or something. But often you actually want to have a guarantee that it goes down by at least. Guarantee that it goes down by at least a certain amount. Okay, and that's like typically that's super useful if you have like noise channels or something in your structure, and you want to say, if I have too many of these noise channels, then the quantity is going to be so small that I can't distinguish the output states anymore. It's like a typical example. So we need something stronger than the simple data processing inequality. And for that, we introduce this additional constant that can depend on different things, always a channel, and sometimes also. Things are always a channel, and sometimes also one of the states. And this has to hold now, for example, for every state row. And it gives us a stronger version of the state of processing inequality, like specific to a particular channel. Okay, and these constants, simply by rearranging this equation, right, you can. You can assign kind of a formula to these constants. And as I write it here, we have, if this has to be true for every row, because it's a strong data processing inequality, or SDPI constant. And if I want something even more that is true for every row and every sigma, so we don't have the sigma here as a condition, then we optimize also by sigma and we call this the contraction coefficient. Okay? So the contraction coefficient. So, the contraction provision kind of tells us by how far it's at least going to go down in this data processing process somehow. And yeah, I briefly want to discuss one special case because it's going to come up a lot in the following slides, which is we can choose a particular instance of a divergence, which is the trace distance or the one norm. And for the one norm, we call this the trace distance control. The one-norm, because it's a trace distance contraction coefficient. Based on some older work by Woskel, this actually has a very simple formulation because we know that this is achieved on orthogonal fear states. So you can kind of simplify this to optimizing only of orthogonal fear states. Okay, but for now, it's just a side remark. And I hope I kind of motivated two things. That one, we have a ton of these contraction coefficients now that are two, potentially all interesting. Potentially all interesting because if we want to measure anything in terms of like a divergence between like input and output state, or two states that we want to see how close they become after a lot of quantum channels, et cetera, then these quantities are kind of exactly what we want to evaluate or what we want to look at. And maybe as one comment, if you want an additional motivation, kind of what we need these contractions for, Alex is going to talk about some stuff tomorrow where Stuff tomorrow, where also these contraction coefficients are kind of used as one of the tools in the proof. So now let me give you kind of an outline what I want to all discuss in terms of these contraction coefficients. And I have like four chapters. Some of them will be very short. First, I want to go back to this connection between the relative entropy and the mutual information and kind of talk about the contraction coefficients. And kind of talk about the contraction coefficients of these quantities. Then I will talk about F divergences and what we can learn from the relationships between the contraction coefficient of different F divergences. Then I'll talk a bit about upper bounds on these constants. And finally, if I have time, I'll talk briefly about an application. So the first chapter, relative entropy with mutual information, comes from a paper with a complete. Comes from a paper with Hercambrus and Daniel. And I'm just gonna, okay, there's a lot of stuff in this paper, but I really want to talk just about one aspect, which is about the two quantities that we introduced. Now, we have already seen for the relative entropy, I can define a contraction coefficient, but I can do the same thing also for the mutual information. So, just kind of, okay, the relative entropy has obeys data processing, so you can see the mutual information. So, you can see the mutual information does as well as long as we stay in one of the two halves here, kind of of the state. If we either apply a channel on the A system or on the B system, we still have data processing for the mutual information. And one way we can define a contraction coefficient for this quantity is that we take the mutual information here, in particular, of a classical quantum state. So, if I write like an x here, I mean a classical system. system and otherwise we define kind of the straightforward contraction coefficient so we apply a channel to one half of this or to one of the systems sorry and this is also like some quantity between zero and one that tells us how much this mutual information is contracted okay and I'm not going to show a lot of proofs or anything so I strange results kind of to just give you this toolbox that I promised like Give you this toolbox that I promised, like connections between all these different quantities. And the first result kind of that I want to talk about is that these two contraction coefficients are actually always the same. And I put the citation here because many, I mean this result kind of proved in our paper, but many of the tools actually come from the study of like these partial orders, which was initiated by Schoenwater Nabel. By Schoenbertanabel. And I'll talk about the partial orders a bit more in a bit. Yeah, so these two contraction coefficients are always the same. And this actually also holds true if we replace this relative entropy by many of the F divergences. And that's useful. You can see that maybe useful in many different settings. For example, if you want to bound the relative entropy, you can, for some reason, that might be difficult. For some reason, that might be difficult, so you can just bound the contraction coefficient for the mutual information instead, and you know you're bounding kind of the same thing, right? And you can see, for example, the relative entropy can even be infinite in some settings, so this might be potentially quite difficult, or you might run in some complications, but the mutual information often behaves much nicer and is like nicely bounded. And so, I'm not saying this optimization is easy by any means, but it's maybe often nicer to work with it. Nicer to work with it than to talk about the relative entropy. Okay. And I'm very confused by the sub mi, there is this. There's something that doesn't make sense? Oh, no, probably. So it's a supremum of our classical quantum states. What is x? And from where to where does n go? n goes from a to b. So I have. Okay, from A to B. So I have rho x A and then the mutual information x A and then I apply my channel to the second system A and then I get the mutual information X B. And yeah. And then I briefly want to talk about a second observation that kind of follows from that. And for that, I'm going exactly to these tools from this paper here by Vatanaba, namely these partial orders. Namely, these partial orders. So, let me talk about just one of these partial orders that he introduced, which is the less noisy partial order. And you'll see in a second why this is relevant. So a channel is called less noisy than another channel, exactly if the mutual information of that channel is larger than the mutual information of the other channel. And so exactly the same setup that we had on the last slide. So you take a classical quantum state xA and you put Quantum state xA, and you put the A system through the channel. That's what I call the mutual information of that channel. And of course, now you can already see that the contraction coefficient for a channel that is less noisy is always going to be larger than the contraction coefficient for the other channel. Back to the state, or does this? With respect to all states. So this has to hold for all. States. So this has to hold for all classical quantum states for xA. Yeah, so you can already tell that this partial order implies something for the contraction coefficients. But I actually want to talk about an even more specific case. Let's look at a particular channel. Let's look at the Razor channel. The Razor channel is very common example where you, with high probability, you just keep your state. probability you just keep your state as it is and with an erasure probability epsilon you map it to some erasure symbol which is orthogonal to your input space. Okay so you have a d-dimensional input and you map it to a d plus one-dimensional output. Well if you have probability epsilon you kind of end up in this extra dimension kind of. And for this channel it's actually super easy to calculate this mutual information for kind of like any input state. For kind of like any input state. And so if the eraser channel is less noisy than the channel n, then its mutual information is larger than the mutual information of n. And this mutual information I can calculate quite easily. So you get just 1 minus n and the mutual information of the input state. So you can immediately tell that I can just upper bound the contraction coefficient of n by dividing by this mutual information. So, this contraction coefficient is upper bounded by 1 minus n, always. I mean, as long as this partial order here holds. And both of these sides are very well behaved. So I can kind of strengthen this or like conclude from this that the contraction coefficient of the mutual information is actually equal to 1 minus another constant. And this constant is just kind of the largest epsilon. Comp the largest epsilon such that the razor channel is always less noisy than the channel n. So these contraction coefficients, if I, for example, just look at mutual information and relative entropy, they're not just always the same, they're also fully determined kind of by this partial order with respect to the razor channel. Okay. Yeah, like I said here, unfortunately, this partial order is less noisy. I mean, still mutual information for all input states. Mutual information for all input states is not always easy to check. I'm also sure Daniel will be wondering now why I'm kind of talking about one of the most, I don't know, simple aspects of the paper that I mentioned. But later I come back to this, and then it becomes clear why I'm actually interested in this particular constant. But before we talk about this, I want to talk a bit more about the contraction for the relative entropy or for FDA. For the relative entropy or for F-divergences in general, because then we can look at a much larger class of these contraction coefficients. And for that, I'm going to move for now to a different paper, which came out last year, this joint work with Marco Toma Michel here. And the paper itself is about F-divergence in general, but I'm going to mostly focus about the contraction coefficient part. The contraction coefficient part here. And for that, let me briefly get back the definitions of the F divergences. And okay, they all reduce to the same classical quantity. But in fact, contraction coefficients for these F divergences have actually been investigated at many points in the literature. And it turns out that when you look at classical F divergences, they have a very nice structure. There's a lot we know about these contraction coefficients. A lot we know about these contraction coefficients, and we'll see later what these results are. But if we have, for example, we take these two popular definitions of quantum f-divergences, things become a lot more complicated. And now you could wonder, okay, we have these nice results in the classical setting, and not so nice results, or they are nice, but they're really complicated in the quantum setting. What can we do? Can we somehow get something? Can we do? Can we somehow get something that simplifies this whole picture? And the approach is kind of the natural one. If we're not happy with anything that comes out from the quantities that we have, let us just define a new quantity that hopefully behaves nice and gives us the results that we want. And that's exactly what Marko and I did in this paper. And as you can see, okay, there's And as you can see, okay, there's you can think of how do I get from a classical divergence to a quantum divergence. There's usually like kind of a structure. Okay, you can think the q, I make it a sigma, and the p divided by q, I just need to figure out some quantum version of that. So that's, for example, what's done here. And the second definition basically does the same thing. So you kind of go from this probability distribution definition somehow in a natural way to an operator definition, then you check if it's an. To an operator definition, then you check if that's a nice quantity. And in some way, that's what we did as well. But of course, you could put different versions of the division here or something, but people have tried that, and these are kind of the ones that turned out to be useful. So we kind of went a different route, and that comes from a paper by Sassen and Berdou. So very recently, actually, I think that's well okay, not that recent, but like twenty sixteen, they wrote a paper about the classical F divergence. About the classical F divergence, and they showed that this quantity obeys a very nice integral representation. So I can write any such or any F divergence, classical F divergence, as an integral over like the second derivative of the function f and yet another divergence, the so-called hockey stick divergence. And I just put the definition of this hockey stick divergence here. But maybe as a comment, just it's kind of a generalization of the trace distance. Of the trace distance. So if this parameter gamma here is one, this is just the trace distance. And okay, typically the question is why is this quantity called the hockey stick divergence? And that's simply because if you plot it, it looks like a hockey stick. And the second observation is that this hockey stick diversion also found some applications in quantum information theory. So we actually had a couple of papers. So there's a Couple of papers. So there's, well, I think Basi and others had a paper where they used a quantum generalization of this hockey stick divergence in information theory. And then also quite recently, actually, Daniel Cambuse and I get a paper where we use this quantum version of this hockey stick divergence to look at a differential privacy setting. So now we learned all. I learned kind of about this integral representation. About this integral representation. Now, the natural question is: well, we can take our recipe and just exchange probability distributions by quantum states. So, if I take just the classical hockey stick divergence and replace it by a quantum hockey stick divergence, this is actually just a meaningful quantum F divergence. Okay? And we kind of need to check: did I just find an integral representation? Just find an integral representation for one of the known quantities, or this is maybe even like some weird thing that we shouldn't have done, and this is a completely useless quantity. So you start doing all kinds of checks, and the first one is, does it have the properties of a good F-divergence? And it turns out basically everything that we want, it obeys. So you have things like data processing and the quantum hockey stick divergence obviously generalizes the classical hockey stick divergence. The classical hockey stick divergence. So, also our F divergences kind of generalizes the classical F divergence. So, we kind of get a valid new definition of an F divergence. And now the question is, does it reproduce just any of the known quantities? So, you start checking on those ones. And of course, the standard one that you really want to recover is the relative entropy. And in fact, luckily, a lot of the Luckily, a lot of the hard work here was already done just before we came up with this idea by Peter Frenkel, who had just in a, I would say, quite technical paper proven a different integral representation of the relative entropy. And it turns out if you just spend like some time with this quantity, you actually just recover our definition of the f-divergence again for the function x log x. That's kind of exactly what you want. Of exactly what you want. So, so far, everything works out. We have a nice new representation of an F-divergence, but it could also just be that we just recover one of the F-divergence that we knew already anyway, which I said don't behave very nicely. So, we need to check some more quantities. And the next step is, of course, to check for rainy divergences. And the first problem is now rainy divergences are not actually F divergences. Actually, f divergences. But we can define a particular function, particularly that has this x to the power alpha that defines us a different divergence, which is called the Hellinger divergence. We can notice here h of alpha, or h alpha. And if I put this in a logarithm together with some constants, then actually this gives me exactly rainy divergence. And then you can investigate this mostly, I mean, in different ways, and it turns out that this rainy divergence. And it turns out that this rainy divergence is actually none of the rainy divergences that we know. And that should kind of tell us that this F divergence that we defined is a different quantity than the known F divergences as well. And then there's some kind of more troubles that you run into with this rainy divergence, which in particular is one property that we're missing is additivity. So this quantity, you can actually check this numerically also, is not. Can actually check this numerically, also, it's not additive. Okay, which is usually something that you want from every rainy divergence that you might use. And there's a lot of steps now, but I'm just going to kind of jump to the conclusion to kind of argue that while this is by itself maybe not the rainy divergence that you're looking for, it actually is closely related to those that you already know. So I said this quantity is not additive, which means I can regularize it. So I just take this. I can regularize it, so I just take this quantity and evaluate it on n copies of the channel and divide by 1 over n and take the limit n to infinity. So we call this a regularized version of this quantity. And it actually just reduces to the already known rainy divergences. So it's actually closely connected to the quantities that we are usually interested in. But that's not really the main part. I kind of wanted to make clear that this is really. Make clear that this is really a different quantity than the ones that we are usually looking at. So there's some hope that also for this contraction coefficients, this quantity behaves nicer than the ones that we've been looking at previously. What next? Ah yeah, okay. And before I move on to the contraction coefficients again, I want to talk about one more divergence in some sense. Which kind of comes out of this? So, did you just say that there's no way to get the rainy, the normal rainy divergences at from these hockey-stick divergences? Or is it just that the way that you try it doesn't work? Well, at least this integral representation doesn't have the rainy divergence aspect. The Rainy divergence as a special case, right? You need to take the detour over these Hellinger divergences and then put them. Oh, let me see. Okay, there are two points, right? To go from an F divergence to a Rainy divergence, you need to do this kind of playing around with the Hellinger divergence, putting in the logarithm, and so on. But then, if you want to recover the original Rainy divergences, you need to also do the regularization. I don't. I don't well, I don't know. I can, this maybe goes a bit too far. But I mean, of course, you can write these as, like, for example, the paths is like via Nussbaum-Scholar distributions or something. You can write this also as like this integral representation. But yeah, these ones by themselves are not part of this definition of the F-diversions. You really need to regularize to get them. Okay, so I wanted to talk about these. Okay, so I wanted to talk about these chi-square divergences. And this might look a bit difficult to pass, but what we essentially want to do is we want to take, okay, usually we have like rho and sigma here. Now we leave sigma in the second argument and we kind of disturb rho in the direction of sigma in some way. We have this convex combination here and I want to take the second derivative in lambda and then evaluate it at the point lambda equals zero. Point lambda equals zero. Okay, this seems very arbitrary, but it turns out, at least classically, that's exactly how people define the chi-square divergence. And it turns out that this gives you one fixed quantity, kind of. And this fixed quantity, we call it chi-squared divergence for kind of literature reasons, because that's how it's usually called. But it actually turns out to be exactly equal to the H2 divergence. So this Hellinger divergence. H2 divergence. So this Hellinger divergence I defined on the last slide for alpha equals 2, it's exactly this divergence. And yeah, so you kind of, it doesn't, the most important point here is kind of it doesn't matter which function f you put here, this divergence on the right hand side is always the same up to like a small prefactor, oh yeah, like a prefactor that depends on the function f. Okay, and this kind of just Okay, and this kind of just to define this quantity or this divergence. But I kind of have two remarks about this quantity. And the first one, what's really remarkable here is what I basically just said is that it doesn't matter which function f we put here, we always get the same second derivative. And if we look, for example, the usual f divergence, Pet's F divergence, that's not the case. Not the case, the chi-square equivalent or something that we would get here on the right-hand side actually depends on this function f. So maybe don't try to pass this definition here because it doesn't really matter for the rest of the talk. But I just want to say that it's really a unique, or as far as we know, unique thing to this new definition of the F-divergence that the right-hand side gives us one definition of the chi-square and not a huge family of definitions. Okay, and I put this paper here if you. Again, I put this paper here if you want to learn more about this particular relationship, and we have one of the authors here also. So, yeah. This paper is really a great resource if you want to learn about the contraction of the pets and also these different chi-square quantities if you start with the pets definition. I'm just gonna show you kind of the results based on our definition. I can promise you this version is much more complicated. This version is much more complicated, okay? And the second kind of observation I wanted to point out is so yeah, so this holds for every function f. And of course, for some functions f, we actually have a nice way of looking at this left-hand side. Okay, so for example, you can choose this simply to be the relative entropy, which for x log x is like one valid choice of this left-hand side. Choice of this left-hand side. And the second derivative of the relative entropy, we just know. So actually, we can get kind of like an operator version of this quantity, which I don't know. It sounds a bit weird because it's still an integral, but I call this an operator version because it's not anymore like an integral over other divergences, but it's really more like something, yeah, it kind of has like this operator form. has like this operator form like the rain the other rainy divergences for example and what you can think of is this this if you look at these are my little this part the second part here like rho sandwiched by these inverse terms together with the integral that's actually just the derivative of the logarithm or in some way it represents like this rho divided by sigma that we have when In other ways. It just does it in like an integral representation form. So this chi-square, we can find this nice representation. And it turns out that this particular quantity, of course, it's also a special case of the results discussed in this first paper, but also by itself it was investigated in a couple of points in the literature, also in the context of contraction coefficients. And I point as one nice reference. Point as one like nice reference to this paper here by Lee Gaal and Caprice Woosch, who looked at this quantity not as a chi-squared, but like as a quantity on its own. Okay, so now I've talked a lot about tons of divergences and so on, so I hope I didn't just, I don't know, confuse everyone with way too many definitions. But now I want to take all these puzzle pieces and kind of put them back together, okay? And put them back together. So you see what are the connections between these different quantities. And the first result that I take you straight from the paper is for basically every function f, the contraction coefficients for these f divergences is upper bounded by the contraction coefficient for the trace distance. Okay, which is in some way particularly nice because I discussed in the very beginning that the contraction coefficient for the trace distance Thanks to the result, Beroskay has this kind of nice form that is much easier to work with than the other quantities. And it's not so clear if anything like this holds for the other definitions of F-divergences. And surprisingly I think we didn't even know this for the relative entropy special case. And I think I probably have time. I wanted to briefly show the proof. I wanted to briefly show the proof of this inequality because it kind of shows the advantage of this integral representation for the F divergence. And it's actually very simple. So we'll just hit here at the bottom. So I write down the F divergence for the channel outputs. And what is really nice here is that the states somehow sit in this divergence, and the function f sits outside in this very independent. f sits outside and is very independent, like these two things seem to not interact that much, actually, which is usually the problem if you try to prove anything for f divergences. So what you do is you kind of just use the contraction coefficient for these hockey stick divergences and you get an upper bound. Now you can't take them already out of the integral because they still depend on gamma, but you can upper bound them and that's the second thing that I have one proofie separately. That I won't prove you separately, but you can find this in the paper with Daniel and Campus on the differential privacy. You can actually upper bound this always with the contraction coefficient for the trace distance. Okay, and now it's kind of trivial. You just take these out of the integral and you get exactly what you need to prove that the contraction coefficient is upper bounded by the strace distance contraction coefficient. So things that were kind of unknown or very hard to prove before. Known or very hard to prove before become like really simple proofs if you look at this new definition of the F divergence. So let me show you a couple of other results that we get for these contraction coefficients. And the first one is I didn't introduce this K square divergence for no reason. So besides its nice structure, it also actually gives us a kind of universal lower bound with all these contraction coefficients. bound on all these contraction coefficients. So this chi-square contraction coefficient is always lower bound on the contraction coefficients for all F divergences with like some mild regularity conditions. And we can kind of push this even a step further by saying that, okay, ignore this first line, but we can show that this is an upper bound also in some other contraction coefficient, which together with some integral representation of operator convex functions tells us. Operator convex functions tells us that if the function f is operator convex, actually all these contraction coefficients are the same. So they are equal to the chi-square contraction coefficient, also naturally among each other the same. And that's kind of remarkable result that we knew this in the classical setting, but not for any of the quantum generalizations of the F-divergence. And what it kind of in practice, what you should take away from this is. Practice: What you should take away from this is that, for example, this k-squared divergence can, in many cases, be much easier to work with than the relative entropy. And if you end up with a proof where you have the contraction coefficient of the relative entropy and maybe this trace distance bound is not strong enough, or you really don't want to lose anything in the derivation, you can, for example, just go to the chi-square contraction coefficient and work with that one, and it will give you a bound on the center. A bound on essentially the same quantity. Or you can choose any other operator convex f more or less and just work with that quantity in your proof. So that's kind of a very brief summary of some of the results from the paper that relate or connect to these contraction coefficients. And now I briefly want to talk about the upper bounds and contraction coefficients. What was the What was the L C lambda? Oh, yeah, I didn't define that. It's a different contraction coefficient. So if you, to prove the statement, you need to do, you need the operator representation, or sorry, the integral representation of operator convex functions. And it will give you a divergence that is like this divergence. And then you upper bound this divergence by the contraction coefficient, and then you get this bound. It's just. Then you get this part. It's just, yeah, I shouldn't have got this on the slide. It's just a technical tool. I can explain it later in more detail if you'd like. But the crucial part is just that from this result, it follows that for all F divergences, we get the same contraction coefficient. But basic question. So basically many of these divergences are very closely related, right? There's like polynomial relation. Does the same thing happen on them? Anything happening on? Yeah, to some extent, yes. So it's a bit... I mean, with this divergent, like this new definition, we can actually translate a lot of the results from the classical setting. If you choose other definitions, it's a bit more complicated. But I mean, you still get a lot of inequalities between the divergent systems, and they're still closely related to that. How hard is it to compute these? Like like let's say I just want to compute this contraction coefficient for the chi-squared or trace channel. How hard is that computation? That's very hard. Yeah, so okay, that's actually a very good question at this point because up to this point what I kind of talk about, you should probably see it a lot as theoretical tools. As theoretical tools. Like I said, you can replace the relative entropy contraction coefficient by that of the chi-squared or any other quantity, and you get very good results. And even theoretically, these chi-squared, they're much nicer to work with often than the relative entropy. But in fact, they're all still difficult to compute, okay? Like very difficult. In some settings, this contraction coefficient trace systems can be a bit easier to work with, even numerically. Even numerically, but even for that one, it's not clear how to efficiently compute, okay? Even through you have this kind of easier way of writing it down. And that's exactly where the next chapter kind of comes in, because what you usually want is like upper bounds on these contraction coefficients. And particularly kind of to decide if that's really smaller or strictly smaller than one or not. And so, briefly, going to talk about Briefly going to talk about, well, I guess, upcoming work or some results on upper bounds on these contraction coefficients. And for that, I go back to exactly what I discussed in the first chapter of the talk. Okay, so you remember from the first chapter of the talk that the contraction coefficient of the mutual information is just one minus some constant, and this constant was the Constant was the largest epsilon, such that the razor channel is less noisy than the channel n. And as I said, at that point, this less noisy partial order might still be difficult to check. So let's kind of do the obvious thing and replace this less noisy partial order by a different partial order. And I think the most famous kind of partial order, at least in information theory, is this degradability partial order. Theory is this degradability partial order. Now I call a channel, or I can degrade the channel F E, or here the razor channel, but any channel you put into a channel N, if there exists another quantum channel D, such that if I apply it to this razor channel, you get back the other channel. That's a partial order in the set of channels. And where do you want to go? In particular, just by thinking of data processing. Just by thinking of data processing, right? If a channel can be degraded into a channel N, it's also less noisy than a channel N. That's not a very difficult observation, so it's data processing because you can kind of just get rid of this channel D with data processing of the mutual information. And now I can use this partial order to kind of get a counterpart to this beta quantity. And I define this, okay. I define this, okay, alpha n, as the largest epsilon such that the razor channel can be degraded into the channel n. So far, not very interesting, but because you have this implication, this implies that you always have an upper bound on the mutual information contraction coefficient. Okay, and of course you have learned earlier that the mutual information contraction coefficient is equal to that for the relative entropy. To that for the relative entropy. And the relative entropy, again, in the second section, we learned this is basically equal to the contraction coefficient for any F divergence of operator context F. Okay? And now we give this alpha a name. We call this a quantum Dublin coefficient, which obviously the name comes from the fact that there's already a classical Dublin coefficient that you can write in exactly this way, also. And okay, we have this upper bound. And okay, we have this upper bound with a partial order that we're maybe more fond of. And so we want to look at this quantity a bit further. And okay, we have this upper bound. Put this here again. Okay, basically what I said. This implies also an upper bound on all the F divergence contraction coefficients. But we can also show something stronger. It's even upper bound on the strace distance contraction coefficient. Okay, and I should actually say, okay, didn't put the reference, but this is closely connected to results that you can find already in Michel Wolf's lecture notes. But it requires some rewriting, basically. But they can think of it as a very simple proof just by using data processing, kind of, of the definition of this quantity, which is if you take the trace distance of the one-norm, you know this channel n is just equal. This channel N, it's just D to a degraded version of the eraser channel. Then you apply data processing, so you remove this D, and you can evaluate the quantity for the trace distance. So you get a very simple proof why this Durbling coefficient gives an upper bound and every contraction coefficient essentially. And yeah, okay. So I promise kind of that. I promise kind of that we can actually make use of this upper bound for calculating this quantity. And that's actually not very difficult because if you just look at the structure of this order or this partial order, you can just kind of write out this quantity. And by realizing that this channel D kind of can be separated in a part that acts on the d-dimensional input space here and a kind of orthogonal input space. Input space, you can rewrite this quantity as something in a different way. And instead of looking for just the largest constant, now you're looking for the largest constant C such exists, the state sigma. The kind of means that C sigma is always smaller than the channel N. Okay, and when I say smaller, I mean here with respect to like the completely complete positivity order. Complete positivity order. And yeah. This does one thing. For one, it kind of establishes this relation as what is called the Durable mineralization condition. So this again is like, I don't know, maybe somebody worked on Markov chains and so on. This is like a very commonly used tool in the classical exploration of Markov chains, this mineralization autumn. Um yeah. But okay, it just kind of gives a name to the quantities that we that we put here. But by doing some more rewriting, we can actually give this a very simple form. And we see almost immediately that this quantity here is actually an SDP, so semi-definite program. And if you look at this form, I think those people are kind of familiar with information here at quantities. Familiar with information derivative quantities, are probably not very surprised with this because this actually looks very much like a max relative entropy. Okay, if you're familiar with that, otherwise, it's also fine. But the point is we can write this quantity as a semi-definite program, which means we can now actually efficiently compute these coefficients. Okay, and I'll just show you a picture because pictures are nice. How about this STP doesn't tense your eyes, right? Yeah, I think it doesn't. No, I think it does actually. So it is found tense right? I think so. I mean, even if it doesn't, it's very nice. I have some notes on that. We can check that later. I think it does temporarize that. Okay, and then without much meaning to show, okay, you can calculate this quantity now efficiently, just numerically. For example, here's just a generalized MQT damping channel. And the point of this is kind of just to say that even for like these kind of channels, it's actually kind of hard to, for example, compute the contraction coefficient of the relative entropy or even of the trace distance. I think trace distance. Of the trace distance. I think trace distance is still possible. You need to use something like symmetry arguments and so on. But at least numerically, you need to do extra work before you can efficiently compute the contraction coefficient. And this alpha quantity, the Dirbland coefficient, now really gives you like an efficiently computable alpha bound. Okay, so if you're not satisfied with just the theoretical proofs and going through the quantities, then you can also put like plots. You can also put like plots and numbers to it, replicate. Yeah, it's a very good question, actually. So a lot of my problem is that you can't calculate the one. So for the ones where you know what the contraction coefficient is, like the depolarizing channel or something, they're equal. I don't know, I should, yeah. I think they're quite close. I think they're quite close. I think a bit of an open question, which I have right now, which is probably to look at, is if this is strictly larger than zero every time the contraction coefficient is strictly smaller than one, I think that would be the most interesting observation. I think it might be not the case. I think there's a separation. But in general, I think it's a very good part. Fraction propositions. And then I think maybe like a few minutes, five minutes or so left. So I'm just going to briefly talk about an application. But I should warn you, by application, I don't mean I want to talk you through like a whole practical setting and so on. I just want to give you one application that uh or one you know or like uh a structure of a problem where it makes sense to apply these contraction coefficients as a proof to it, okay? Coefficients as a proof to it. And this will, this is like a very small snippet from upcoming work with Hautchung and Cambrus. And I want to talk about sample complexity. In particular, I want to talk about sample complexity of hypothesis testing. So hypothesis testing is a problem where I get a black box, and in the box is either the state rho or the state sigma. I have access to many copies of this box, and I want to find out which. This box, and I want to find out which state is actually in the box. I can do measurements, also joint measurements. And what I usually want is I want to find the probability of error in the task of getting or identifying the right state. And typically you look at the decay of error or something as a function of n. But something else you can look at is a sample complexity, meaning you want to find the smallest n such that this probability of error is smaller than a constant. probability of error is smaller than a constant data. That makes sense as a nice application of something. And it's actually not that difficult to prove that the sample complexity is generally given by one of the divergences that I introduced earlier, which is this h1, so the Hellinger divergence with alpha fixed to one-half. This is already okay, I need to do a little bit of proof, a little bit of work. Bit of proof, a little bit of work to find this. More natural is maybe you can also express this. I've used this kind of weight notation, right? So you can express this in terms of many kinds of divergences. You can even express this just as minus log fidelity or something. In terms of order, they all, in the end, they all behave the same. And in this work, what we particularly look at is that we want to look at the We want to look at the central complexity under a certain restriction. And in this particular case, it's going to be local differential privacy. Also, the exact definition kind of, or the exact restriction doesn't kind of matter for the argument that I'm trying to make you. But the sample complexity under this restriction is simply the minimum sample complexity if you put your You put your state through the channel, or through some channel, and you take the minimum of all these channels in your restriction class. So, for example, all apps non-locally differential private channels. And what you want is now you want to find achievable and converse bounds on this sample complexity under restriction. And I'm just going to discuss one possibility of proving a converse pool. Of proving a converse pool, which, of course, you can just look at the contraction coefficient of this divergence that appears in the definition of the standard sample complexity for this problem. And this contraction coefficient is going to tell you exactly kind of how much you lose in the worst case kind of by applying this channel n to your divergence. Divergence. And for example, we can just use this upper bound that we get from one of the earlier slides. And instead of looking at the contraction coefficient for this kind of annoying function, we just look at the contraction coefficient for the trace distance. And turns out this point is actually much easier to evaluate. And it's kind of, okay, like this is an example where you can, by exchanging the contraction coefficients, you get a nice converse. Contraction coefficients, you get a nice converse bound on, like an easy converse bound on an application. And as common, maybe we've also seen earlier that these contraction coefficients are actually often the same. So you can also, for example, go to the relative entropy. And this is true now in our case. And classically, people use this to get an even better bound on this entropy. Even better bound on this application. But I kind of leave that as no problem to continue with this quantity because while now with our new tools we can go to this quantity, turns out that we don't have yet the tools to evaluate this quantity. Just kind of saying that there's a lot of room for improvement also still in these groups. And anyway, the kind of the final result is that you have the The sample complexity under a restriction, which is given by this quantity with the channel n. You kind of just bound the contraction coefficient for the trace distance, which is easy. And then you get a lower bound on the sample complexity in terms of the original quantity. And that is kind of just one of many applications how you can use these contraction coefficients in practice to bound for information or Information or computational problems and give them a converse once. Like I mentioned earlier, you see different applications of these contraction coefficients tomorrow in Alex's talk. And yeah, I guess I'm at the end of the time also with that. And just have a brief summary. So, what kind of did we learn? Which is one, these contraction coefficients are kind of the tool of choice if you want to look at the worst case behavior. Look at the worst-case behavior of noise and some kind of like computational structure. At least, if your alternative tool was just data processing. And in many cases, these things just reduce. So instead of relative entropy, you can look at mutual information or other quantities, and that's basically just all the same. You can further simplify it by using kind of the right definition of the F-divergence. Again, more and more quantities just have the same. More and more quantities just have the same contraction coefficient, the same behavior under data processing. You get kind of nice upper bounds that are like the trace-distant one, which is simple to work with theoretically, or the Drbling coefficient, which is even nice to work with numerically. And yeah, okay, you get that. And so, kind of, whenever you kind of now have these proofs, I hope you kind of remember this toolbox that I talked about. And when you have That I talked about, and when you have a contraction coefficient, instead of kind of being annoyed why they are so hard to work with, you can use one of these tools to bound them in a way that's useful for your setting. And then at the very end, I just have a few open problems. That should appear on a later slide. That are kind of okay, they're not necessarily my open problems, but I would encourage people to work on this. I would encourage people to work on this. So maybe you wondered earlier why this mutual information contraction coefficient has a classical part in it, and we don't just put like a fully quantum state. And in fact, you can of course define this with a fully quantum supremum kind of over input and yeah, input and reference quantum, but we do not know if that gives the same quantity. Okay, and I think that's quite an important problem for a lot of technical uh things. For a lot of technical things. And at least classically, we can even go from the mutual information to a different quantity. I don't define now, but for the people who know, this is a conditional mutual information. And if this conditioning system is classical, actually, again, it's just the same as all the other ones. But we don't know if the same is true in the quantum setting. Again, that's not really my open problem. And that's not really my open problem, it's just an open problem that has been around for quite a long time. And yeah, this would have a lot of implications, for example, if we talk about Markov chains and they have a close relationship to these conditional mutual informations. That's like an interesting open problem. Sorry. The second one is if these quantities tensorize. So the contraction coefficients do not tensorize, but this STPI constant that I introduced in the Constants that I introduced in the very beginning, they might tensorize. At least they do classically. And we simply don't know if they do in the quantum setting. So I think this is kind of the second big open problem that is still left in this contraction business. So it's written here, right? But I should say, okay. Generalization means that if I have products in both of the arguments, you just get the maximum of the individualized pattern constants. And whether this is an equality in the Whether this is an equality in the quantum setting is also unknown. And the final one is just kind of my own curiosity, which is, I discussed a lot the degradability condition between an eraser channel and a channel N. And then classically, people were like, oh, this was useful and very interesting. So they looked at replacing this eraser channel with symmetric channels. Which kind of classically is a natural choice, and the main paper here is a paper I cite on the right. But something I can explain maybe in the coffee break or something to people that are interested. I have arguments why it's much more interesting when you look at the symmetric channel to look at the other direction where you degrade n into a symmetric channel. Or in the quantum setting, maybe you want to look at a depolarizing channel. At a depolarizing channel. They would have a lot of application to kind of understand when exactly you can degrade the channel N into a depolarizing channel. And I figured that might be something where people that are here have maybe an insight into. Just haven't thought about this too hard. So maybe somebody just knows what the solution to this is. And yeah, that's actually everything I want to talk about. So thanks for your attention. So thanks for your attention. All right, do you have any questions? Also with Tanya, we have some, let's say, hibernating project on complexion coefficients for metrology. And the idea there was to like, if you, for example, have an evolution under the set rotation, then the defasing change. Then the defacing channel, which does not contract according to this notion, is still very bad for the signal. So the question was: can you kind of define these contraction coefficients with a sense of direction in mind, if you have any faults on that? I mean we can talk about it. I guess you would need to define it with like a restricted input set for your supremum or something. Yeah, but you haven't thought about it in particular. But you haven't thought about it in particular. No, I haven't. Particularly, it's not quite as popular. Wondering, several times you mentioned the integral representation of the F divergences. Is this the like one-to-one mapping between a factor class and after all many F-divergences? Okay, most likely when I said integral representation I meant the Presentation, and then the this kind of take a long time. This, the one that we define, the F divergence, is kind of based on an integral representation, right? So I just kind of referred to this definition of the F divergence. Yeah, I think when they're operated monitors. Yeah, I think when they're operator monitors models, they can be a little bit more. Okay, yeah, that was maybe the other thing I was referring to. So if you have a function f and you know the operator convex, you just have the separate integer representation how you can express this function f in terms of like other functions. That helps you for a lot of these proof techniques. Can you do you have a balance between the usual alpha range? Between the usual alpha Rani relative entropies and the alpha augustic, or you just know the limits? Yeah, we have. So is it like below or above? Well, I mean, there are lots of different bounds. For example, you look at alpha larger than one, the new ones are actually the smallest ones. They're even smaller than the sandwiched one, which is exactly where this non-additivity comes from. There's like tons of inequalities between them. These partial orders, I mean, it would also make sense to maybe compose with in a different way, right? I mean, like a simulation order. This is the only degrade. You want to do something else. If you want to compare the depolymogen, you could go. Yeah, yeah, you could. I mean, even class. I mean, even classically they exist a lot more partial orders, like where you not just process the output system, but also the input system, for example. You could do like twirling or something. And then even this less noisy partial order, there's a lot of variation. Usually you need to regularize it. It's just in this application you don't need the regularization. When you look at capacities or something, you want a regularized version of this less noisy version. But they always sit like in between degradability and  So I was wondering that what is what is the version? So so there's uh the data positioning is like a positive group. So this other guy