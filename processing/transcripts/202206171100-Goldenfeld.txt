Let me just set my timer here. So I'll try to not overgo my time since I know that I'm the last speaker. All right, so my talk is in some sense a follow-up to Sebastian Funk's talk earlier in the week. And I'm going to be telling you about three different stories where Stories where computer modeling and so on was critically important in some of the activities that I was involved with. But I'm also going to try to make the point that what we need is not better modeling, but better understanding of social activity, better ways to communicate, and better things that we ought to be calculating rather than just trying to predict the usual epidemic curves. Epidemic curves. So these are the collaborators I want to acknowledge right at the beginning in case I forget at the end. This is a group that basically historically self-assembled right at the beginning of the pandemic in late February, early March in 2020. As you can see, it was based primarily in physics. Physics. Sergei Maslov and I are both physicists. These are students who are physicists in cosmology and astrophysics and biophysics. Alexei Tachenko, who is a soft matter physicist at Brookhaven, and Ahmed Jalbana, who is a sort of physics-oriented civil engineer. And what we were working on was essentially grow out of our work modeling. Modeling for the state of Illinois, for the governor of Illinois, in fact. Back in early March, we had predicted that the ICU beds in Chicago would be overflowing if the governor did not shut down the state quickly. And we made that prediction based on very simple SEIR models. The governor listened to us and Listen to us, and after sort of talking with us and fact-checking and so on, due diligence, which took about a day or two, the city of Illinois was indeed shut down early. And in fact, a catastrophe of the sort that was happening in New York City and elsewhere in the world was prevented in a way. On the back of that, we started a we basically dropped everything we were doing and started what would end up being a nearly two-year project on On COVID modeling for public health. So, I'm going to talk about three different stories here. The first one is just a little bit about numerical methods, since I know people here have been focusing on that. I'm going to be, I'll show you, I'm not going to go into very many technical details. People can read those things in the papers, of course. I will just make some remarks about predictions. About predictions, what limits them, but nevertheless, how modelling can be useful in public health. And I'm going to advocate for a different way of reporting results and modeling different quantities. And my perspective from that comes from my experience in modeling the risk of financial derivative securities many years ago. Many years ago. I'm sorry, my phone is beeping continuously as people message me. In the second part of the talk, I'm going to talk about a recent paper that came out on eLife and before that, a previous one in PMAS, showing you how we tried to model stochastic social activity, taking into account heterogeneity in the population, the dynamics of the population, and in particular, the baseline question. The baseline question that we started to try to understand was based on our very early experience in the pandemic, which was why was it that many very good epidemiologists were making predictions that were wildly off, predicted peaks that didn't happen much earlier and much lower than expected. And we tried to understand that, and that led us into thinking about heterogeneity, which, of course, many other people, some of them here at this conference. Someone here at this conference talked about and stochastic social activity. And then, in the last part, I'll talk about a moonshot that we did starting in about May of 2020. So at that time, I was at the University of Illinois. I'm right now at the University of California, San Diego, where I've been for just a few months. But what we did at the University of Illinois during the summer of 2020 was we devised a unique library. A unique large-scale surveillance testing procedure that we applied to the entire campus doing COVID PCR testing using PCR, of course, but using saliva rather than nasal fluing. And that enabled us to do fast and frequent testing, which I assure you had a very significant outcome on the health of the campus community and the surrounding testing. Surrounding the town and the county as well. So, and this is what was just published about two weeks ago. And so, this is the first time I have talked about this since the paper has been published. Okay, so let's get into it. So the question I really want to ask here is in what sense can modeling be useful? And the modelling that we did was really reported in the paper primarily on the left. This is the model. Primarily on the left, this is the modeling that we used in trying to understand the pandemic in Illinois. What we did in this work, which I will say something about, is essentially ager infection model coupled with calibration using Markov chain Monte Carlo. And so I'll just say something a little bit about that. And then on the right is a theoretical paper trying to understand whether or not the concept of herd immunity is. Of herd immunity is really appropriate when you have time-dependent heterogeneity. I'm going to show you that, in fact, it really isn't. I think people understand now that herd immunity is not a very useful concept, particularly when you have a viral evolution. So, what did we do in our modeling at the time that we were doing this work, and is even more the case now, as people are shutting down data collection, is we Collection is we felt that modeling based on cases was a waste of time. So we basically modeled on deaths, ICU, and hospital utilization, as well as cases. And so we fit these four streams continuously and simultaneously, and were able to match them so that the result was consistent. We used an adhedial infection model, which An agent of infection model, which many people have done before. There wasn't anything particularly new in that. We generalized, made some technical improvements to this and we did a calibration of parameters. There's like 23, 26 parameters, something like this. We're using Markov chain, Monte Carlo methods, and then we used Bayesian inference on previously reported parameters. So this is the sort of topic. So, this is the sort of topology of our model. It's very standard, so I'm not going to spend too much time discussing it. Here, I just want to show you our posteriors. So, what we would do is we would then do calculations, which are very, very compute-intensive. They would take about a day on our local cluster and then compute the posteriors, and then we would do a forward sampling based on the model that we had produced from that. That. And we would essentially do forward prediction of multiple trajectories, and then from that, make our predictions. Now, you know, we did a lot of stuff there, and I'm not going to talk about all of it, but the thing that I want to emphasize is how difficult it is to predict the only thing that really matters in a pandemic, which is the peak. And the reason I make this rather strong statement is because when the pandemic is exponentially growing, it's pretty easy to do any kind of model to fit it. Easy to do any kind of model to fit it and get that exponential growth when the pandemic is dying away. Again, it's pretty, it's not too hard to model that. But being able to get the peak reliably and in terms of magnitude and location and time is very difficult. And here we showed how, even despite a fairly sophisticated modeling effort, here the uncertainty in the In the predictions in the peak, how they changed as one fitted data through points in time that were later and later and later. So what you would expect is that the later that I fit the last data point that I used, if it's a long time ago, then I have a huge uncertainty. If it's not too far ago, then the uncertainty is going to be less. But what was surprising to What was surprising to us was the magnitude of these uncertainties. And we were working with the state of Illinois other modeling teams who are working and advising Illinois Department of Public Health. We found that their estimates were even, their calibrations were not as sophisticated as ours perhaps, and the range of estimates were absolutely huge to be essentially completely useless. Completely useless. Now, we weren't the only people to make this point. I want to do a shout out to Susanna Magrubia's work and her group, who wrote a very nice PMAS paper where they really essentially did the same type of calculation. And what they really tried to show was that the range of predictions can be Predictions can be so huge that one can't even tell whether there is going to be a peak or not. And they talk about that in detail here. And what they try to predict instead is whether what is the probability of a peak or what is the probability of confirmed infections being less than some number as a function of time since the first confirmed case. And this is something that I'm going to emphasize again in a few minutes, which is that In a few minutes, which is that we should be switching from trying to predict epidemic trajectories to making probabilistic statements, just like one is used to in weather forecasting, you know, there's a 30% chance it's going to rain today, that kind of thing. So I definitely would encourage you to find this paper if you wish to understand that in more detail. Now, one of the things that we did in our service work, we were asked to do this sort of calculation. This sort of calculation and it answers the question: Will my hospital COVID availability be exceeded in the next four weeks, let's see, or two months or whatever? The idea being public health wants to know, do we need to arrange for overflow beds and so on? And this is a very expensive thing, very early on in the pandemic. Without talking with modelers, the state of Illinois famously spent about $15 million on. $15 million on providing excess beds that, in fact, were never needed in a facility in Chicago. And so, this is a very important question, both from the point of view of public health and also from the point of view of public finances. So, the way that we approach this was to, and I call this hospitals at risk because I was used to a concept called value at risk in financial mathematics. And so, basically, what you do is Basically, what you do is you project forward scenarios from your model that's calibrated to fit the historical data on ICU, hospital occupation, and deaths accurately. And then you try to compute the probability density for their occupancies forward in time. And then what you try to do is, well, now you've got a terminal hospital occupancy distribution, if you will. Distribution, if you will. And then one wants to know with what acceptable probability will that be exceeded. So, for example, in our case, the Illinois Department of Public Health was interested in knowing when we would be in danger of overflowing the 75% occupancy that they were prepared to reserve for COVID patients. And so we could go ahead and do that sort of calculation. I'm going to show you hyper. I'm going to show you hypothetical sample data here. This is just, these aren't the real data. So let me show you this here, actually, these data actually are real. So this is the model calibrated to occupancy being a very good fit in one of the regions of Illinois. And then what you do is you project forward in time, and here is the range of predictions. And so this is. Predictions. And so, this is the threshold that you want to make sure that you are under. And you can see in this particular illustrative example that we predict that in this particular case, within the time window of interest, there is no probability of really exceeding that threshold. This was a scenario where at that particular time, one was seeing a large growth in the pandemic in Arizona, Florida, and Texas. And so you can say, well, what would have happened? And so you can say, well, what would happen if we were doing something, if something like that was happening in Illinois? So now you would have a range of future trajectories, which would be shown in here. And then you can see by integrating the area over here, you can work out what is the probability of exceeding the hospital capacity. And so this is the sort of way that we would typically report out these sorts of calculations, the risk of exceeding. Calculations, the risk of exceeding the hospital ICU capacity in different geographical regions. And as time went on, as you got nearer and nearer to the time window that people were interested in, you were able to just to see how the pandemic was developing and give information. In fact, you didn't have the opportunity of being able to do rigorous testing of how this worked, and that would be something that I think would be an important thing. Think would be an important thing to do and to establish. But empirically, it was very helpful to the people making decisions and seemed to be useful. And certainly in the financial mathematics literature, financial risk management, these methodologies, which I first encountered nearly 30 years ago, are still highly used and are the basis of capital requirements and other things like that. Other things like that. So I think this is something that the epidemiology, mathematical epidemiology community should perhaps focus on, perhaps more than they have up to date. So what I've told you then is that one should calibrate models to reliable data. There are a huge number of adjustable parameters and one it's a hard but not impossible task to calibrate good models to that. The predictions To that. The predictions of peaks is very hard, if not impossible, practically, even if you simplify the basic epidemiology. And perhaps the most useful contribution that epidemic modeling can make is providing probabilities of actionable events. And I think this is something that would be worth focusing on in the future. Okay, so now I want to get to part two. And the question that I'm trying to answer here is: why is it To hear is why is it that one gets waves and plateaus in pandemics? And why are the peaks always predicted to be at the wrong time and the wrong height? Now, of course, I'm overstating it here. When I say always, I am sure that that's not, you know, that should not be taken literally. But it's very easy to make the wrong predictions there. And there are many examples of that which happened during COVID and actually eroded in some places. In some places, public's trust of science. So, this is obviously something that we have to get right and have to understand. Are there any things that we are missing? Now, I think it's very well understood that the basic premises of most models are not quite correct. One has to take into account super spreading, which gives you short-term overdispersion in statistics, and we sort of In the statistics, and we sort of roughly know how to account for that in the way that we make our models. Perhaps more tricky is looking at long-term persistent heterogeneous susceptibility, which reflects a diverse population's social propensity. In other words, some people socialize more, other people don't, and also some people are more susceptible than others. And during the pandemic, quite a few groups. Quite a few groups, of course, recognized that this was something that really needed to be important for COVID. Of course, the epidemiology community had understood this issue going back to the early 2000s, of course. So this was nothing new in principle. And so there was a lot of impetus for development in that direction. And one of the new things that we were able to contribute was taking into account Was taking into account the stochastic behavior of individuals as they have some long-term persistent social propensity, but then they make short-term temporary fluctuations from that due to say lockdowns or deciding that they want to be more cautious and what have you. So I'm going to talk, I'm not going to talk so much about this particular paper, I'll talk a little bit more about the succeeding. Paper. I'll talk a little bit more about the succeeding one. But this is a paper which was in PNAS 2020 or 2021. So what we did was look at the persistent heterogeneity in a population. Again, study that in an age-infection model. And like many others have done, we basically ended up with a Ended up with a parameter which we called the immunity factor, which in our sense quantified the heterogeneity of the population. And I'm not going to go through how one calculates it. It's relatively simple to do, particularly if you assume gamma function distributed heterogeneity, which many authors did at the time, some of them speakers here. Speakers here. The end result of that is that when one tries to model the susceptible population, it and the reproduction number, it gets modified by an exponent lambda. And that exponent lambda is a way to parametrize the heterogeneity. And that's shown in this curve over here. Now, what is Now, what is the implication of that? Well, you can take empirical data, and we took here empirical data from New York and Chicago and also other places, and we found that this sort of model was able to fit the data very well. And from now, we could read off this heterogeneity parameter. And so, that gives you a thing which is kind of analogous to the The herd immunity threshold, which we hear is HIT. So, the herd immunity threshold, as other people have calculated, goes down the greater the heterogeneity one has. And what was different in our modeling was not just that the Herdiminsi threshold went down, but it also, in some sense, it failed to even exist. And the reason is that what happens is What happens is you get a state of transient collective immunity, which in some sense is fragile. If you take into account the time-dependent social behavior, once you've reached this sort of point here, then what happens is that state then also dies away and leads to a subsequent wave as well. So I haven't got a slide to show you the wave, but Slide to show you the wave. But so, this, so what we can see from this early calculation that we did is that there really isn't like herd immunity where the epidemic goes up, then comes down again, and then it's all basically over. It will continue on. And so we wanted to then try to understand, you know, not just that you can get subsequent waves, but something a little bit more about what drives that. So that investigation. That investigation was published at the end of last year in eLife. And I'm going to show you a bit more about the calculations that we did here. We were very honoured that our paper was edited by Mark Ripsich and in ELIFE produce an evaluation of the paper, as well as putting online all the referees reports and things like this. And things like this. So, what we did in this paper was we made a model of stochastic social behavior coupled to the COVID-19 dynamics and coupled to the heterogeneity that we had introduced in our previous work. And the question that really got us thinking about this was not just about the epidemic peaks, but we noticed from our working. But we noticed from our work in Illinois and then looking, of course, looking at it elsewhere, that RT basically fluctuated around one for very long periods of time. So in other words, what we call a plateau. And, you know, a very simple model of a pandemic, you know, just a simple SIR model, we just predict it goes up and goes down and it's all over. And obviously, that's not what's happening here. So we wanted to try to understand why and be able to make quantum. Be able to make quantitative predictions. So, basically, in a sort of SIR framework, it's been very well understood that epidemics are self-limited through the depletion of susceptibles. There's also something else that goes on, which is knowledge-based mitigation. Everybody sits around, they watch TV news, they read the newspaper, they find out that the epidemic is going up in their local region, and so they then stay at home. Then stay at home, mask up, get vaccinated, whatever it might be. So that's what I call knowledge-based mitigation. I just want to mention some of these papers here, including early work by Sebastian Funk, of course. And then there's sort of population heterogeneity, and I'm just mentioning some of the work here, including from Brittany and Gomez, who are here at this workshop. So that's all very That's all very, very important. What we added to this was the idea that one had not just a heterogeneous level of, say, social activity, but that one dynamically approached that sort of long-term level in a stochastic way. So the idea was basically an individual may have different types of social behavior, which are shown here as basically, you know, Basically, you know, sitting at home and not going out, or going out to lots of parties and so on. And the basic idea is that, well, as the infection goes up, people make transitions from being in a state where they're hiding at home and reducing their social activity to a state where they're going back to their long-term norm level of social activity. And so when, so it's not difficult to So, it's not difficult to model this. And the way we did it was essentially through a mean-reverting process. In other words, a kind of Ornstein-Unenbeck process. And when one does that and then couples that social activity to the pandemic, epidemic model, what one ends up with is after some mathematical tricks and so on, which I'm not going to spend your time on. Which I'm not going to spend your time on. What one finds is that one gets a model that maps into the Cox-Ingersoll-Ross model for interest rates in financial mathematics. And so this model can actually be solved analytically, and we were able to do that and do a lot of nice analysis of this type of model. So here are the equation of the model. I don't think I'm going to. Equation of the model. I don't think I'm going to have time to go into these in detail, but the main point is that you have dynamic social activity, the heterogeneity comes in as the sort of exponential, this exponent correction here, just as we and others had done in the past. One writes down the new equations and in this way one can solve the equations. Now, let me just tell you one of the main things. Tell you what, what are the main things that come out of this? So, once you do this analysis, what one finds is that you don't have a herd immunity threshold in a classical sense. Now, in regular epidemic models, you have the phenomenon of overshoot, as I'm sure we all know. That doesn't happen here on these dynamic heterogeneous models. They essentially approach the you know the the the you know the the pandemic uh you know asymptotic attack rate uh uh monotonically there's no and there's no uh there's no overshoot um what also happens is that instead of having like in a regular model where you might have a you know a pandemic that goes up and comes down uh the the because you have this these stochastic This stochastic social activity effects that excite these waves of persistent dynamic heterogeneity as shown over here. And one can actually take these models and understand that phase portrait and so on, and one can quite readily fit them to experimental data, which is in a way that is appropriate. So here is an example of a fit in the Midwest. Fit in the Midwest, where we had a regime where the model was applicable in terms of the sort of plateau region. We were able to make a prediction here. So this is the fitted range, this is the predicted range. And what you can see happening is that the traditional SIR models predict a much higher peak. Here I'm showing you the depths per 100,000, whereas the stochastic social activity model predicts a lower peak. Model predicts a lower peak and, in fact, an earlier one as well. And these are fitted, both fitted into this training set over here. And then this is the outcome. And you can see that the stochastic social activity model has made a significantly improved prediction. Okay, so what have I told you in this part two? Well, I've told you is that heterogeneity leads not only to a reduction. Leads not only to a reduction in herd immunity threshold, but can even destroy the notion of the herd immunity threshold in a sort of traditional sense. One can have a state of what we call transient collective immunity, which we think of as being a state where a particular social network has been exhausted by the pandemic. And then once people start making some small changes to the social activity, the social network gets rewired again. And so, again, the pandemic. And so again, the pandemic can then spread. And then, when one puts into account literally stochastic social activity where individuals have different long-term social propensities and stochastically vary their behavior, that leads to waves and plateaus and makes a prediction about pandemic peaks that are earlier and lower than you might get for more simplistic reasons. Okay, so now I want to, for the last part of my talk, To for the last part of my talk, I want to focus on some boots on the ground epidemiology in a real population. That population is the town of Champaign, Alana, and we were able to mitigate COVID in the University of Illinois at Ubana-Champaign, and also, amongst other things, provide testing of COVID for free throughout Illinois and through schools and universities. Illinois and through schools and universities and organizations throughout the university, throughout the state of Illinois, and even throughout the US. So, let me tell you a little bit about that. So, if you want to read the paper, it just came out in Nature Communications. It's a very large and detailed paper. This is the author list. It's an author list of length comparable to what you would have in a paper. Comparable to what you would have in a paper in experimental high-energy physics. But this just shows you the magnitude of the effort required to do this. So what did we do? Well, we set up a system which we called SHIELD, which had three different parts, target, test, and tell. And let me just explain to you what those were. So the core of this, the center of it, is a PCR test that was based on. Test that was based on the saliva rather than nasal pharyngeal testing. So, the reason for that is, as I'll go through in a minute, first of all, first and foremost, I would say we wanted to be able to do surveillance testing of our entire university population and try to keep the university open during the pandemic. And we succeeded in doing that. The surveillance testing requires one to compute how often one needs. Compute how often one needs to test people, and that was where modeling came in, as I'll show you. And so, because we want to get people comfortable doing this, we really wanted to have a mode of testing that's not intrusive and unpleasant. And the other thing that we wanted was to have our testing not only be convenient, have a fast turnaround, be accurate and scalable. And we succeeded in that. I'll say a little bit about that. Now, my work was primarily Now, my work was primarily on devising the testing protocols and the whole shield mitigation strategy and doing the data analytics during the course of the pandemic. That's what we call the target team. And then lastly, there was the TEL team. So we worked in close collaboration with the Champaign Obama Public Health District. We were very lucky to have an extraordinary. We were very lucky to have an extraordinarily smart and energetic public health department who are very responsive and they were an intrinsically critical part of our team. We developed a way to do test notifications through an app and to use the result of the app to allow one access to university buildings. So let me show you. So, let me show you how that works. So, here in this picture, you can see somebody who's just done a COVID test using saliva. So, what they've done is they have drooled into a test tube. That test tube is actually usually collected on the rack. Takes about two minutes to do that. There is racks, we were doing the testing in the recent summer and fall. Summer and fall in open-air tents. Later on, we moved to indoors. These racks were then, you didn't have to make an appointment, you just turned up. And those racks were then taken by golf carts to our veterinary lab, where we set up a clear certified PCR testing system. Now, our PCR system testing was different from everybody else's. What we invented What we invented, our chemists invented this, was a way to do PCR testing without doing viral extraction, without having to use transfer media and things like this. And so what that enabled us to do was that we did not have supply chain bottlenecks for reagents. It was cheaper. It was safer for the testing stuff. They didn't need to have full PPE. And then we did RTQPCR. And then we double-tested any. And then we double-tested anybody who was positive, and we could do that in about two hours. And then we could get the results out to people through an app. And those who are positive, we also then texted them within 15 minutes of the results coming off the PCR machine and got them isolated and started contact tracing. And in practice, our typical time for processing was about 10, 11 hours. Was about 10, 11 hours, something like that. This is a slide taken from SHIELD T3, one of the organizations we set up in the state of Illinois to use these methods throughout Illinois and throughout the US. So the things that we were tasked to do was who would attest when, how often, and what other mitigation strategies can be effective, and can we open the university in some hybrid way as safely as possible? safely as possible. And the way we approached this was to do agent-based modeling. So normally I'm a skeptic about agent-based model because there's so many parameters, but we felt that we really had to do that because of the huge heterogeneity of the population in terms of its behavior, in terms of its age, in terms of its social activity, in terms of the locations that they visited and so on. And so what we did was we So what we did was we made a social network of the entire university from the schedules of all the students. So there's our social network where the nodes are classes, the edges are students. This is a social network with about 2.5 degrees of separation. COVID-19, of course, will spread rapidly in this network. And of course, social bubbles are connected by social activities outside of class schedule time. And then, of course, one then has to interact. And then, of course, one then has to interact with the university staff, faculty, and people in the surrounding community. Now, one of the things that we realized very early on, as early as April or May of 2020, was that COVID was spread by aerosols. And of course, there were aerosol scientists like Lindsay Mark, Kimber Prather, Jazzy Jimenez, and Jimenez and others who were doing a lot of Morasco and others who are doing really fantastic work documenting this. And so we understood that just by looking at the patterns of over-dispersion and so on. And so we wanted to include that in our modelling. And so, again, I'm not going to go through the details of this. Our COVID models followed each agent, student as they went to class, home, restaurants, coffee shops, bars and parties. Restaurants, coffee shops, bars, and parties. We knew how many people were in fraternities and sororities so we could look at their social activity. We knew how often they would have events and things like this. We used Mars-Weilly type of modeling to model the physics of aerosol spread, viral infection. We knew something about the air exchange rate of the different locations. So we knew something about the spreading propensity. And so we went into a very And so we went into a very detailed model, and then we modeled not just social activity, but the effect of online classes, masks on campus, testing, contact tracing, exposure notifications, and so on and so forth. We did things like we computed how many students would arrive on campus at the beginning of the fall semester 2020. In fact, our estimates were pretty close to what was actually. What was actually observed. I'm going to skip over this. We worked out through our agent-based modeling how the effective reproduction number was renormalized, was reduced by these different mitigations. And I'm showing you a simple example over here of different mitigations and how R gets affected. Gets affected. We also invented simple analytical ways to work that out based on the infectious curve, the viral activity as a function of time, where one can see the effects of rapidly isolating people and seeing which parts of this curve you cut off. And so you can see as a function of even just analytically without doing any computer simulations, what is the effect on the On R0 of testing zero times a week, once a week, twice a week, and so on. And you can see here that the higher the frequency, the greater is the reduction. And so, of course, you need to then know what the R is going to be and see which one is appropriate. And we did that, and other people did similar calculations at about the same time. We also, and I'm going to return to this in a couple of years. We also, and I'm going to return to this in a couple of minutes. We also looked at the effect of compliance, and you can see here that compliance made quite a difference to the effectiveness of any surveillance testing in campus. And that's something that we're going to come back to. Here I am showing you a calculation that we did with 60% compliance, which I think is about what we were really meaningfully able to achieve on campus. Achieve on campus. Again, I'll just very quickly mention this. This is a heat map showing you how speed is important. When we were doing testing, we wanted the test to be fast, we wanted the notification to be fast so that we could isolate people very quickly and quarantine their contacts. And that was because some of the simulations that we did back in May 2020 and so on, we could see very easily that your ability to contain To contain a pandemic eroded very rapidly the longer you took. And our people also did similar types of calculations. So here's our daily cases during the fall of 2020. Here in red is the positivity shown over here. These are the number of daily tests that we were doing. You'll see that on some days we were doing 10,015,000. Some days we were doing 10, 15,000 tests a day. At one point, we were doing 2% of all the COVID tests in the United States just at the University of Illinois abandoned Champagne campus. And one of the things I want to point out is here is the worst event that happened during the whole time that we were doing this. This was a peak in positivity that got as high as 2.8% on one day. Even though that was a very small Though that was a very small positivity by the standards that we are used to taking for granted now, like in the area where I am, San Diego, right now, the positivity is around 10%. Other universities had a daily positivity of about 24% or higher. This was the highest that we ever got to. And our testing very rapidly brought down every growth spurt of the pandemic. Every group spurred out of the pandemic on campus, as I'll say in a few minutes. This was the first day of classes for many universities in the US, and pretty much every university that was doing this, doing testing, had an event of this type. Here is one thing I wanted to point out. We also compared the sensitivity of COVID shields to antigen testing, and we were able to detect COVID much. Detect COVID much earlier than lateral flow assays, and that was important for our football team. These are data that we took to find out how often people were testing, and we found that people were not really testing at the frequency that we required them to do, about 62% of the space 3.5 days or less. So the compliance was about 60%. Wasn't too bad. We measured. We measured cases, we calculated the effective reproduction number using methods like the comment-out method and things like this. I'm not going to go through the details here. You can see that the pandemic was contained. We made these estimates in other ways as well. It's just a very simple back-of-the-envelope ways to estimate the regions of growth and then the regions of how it Of how it died away through surveillance testing. I'm not going to go into this. These graphs just show how we were able to mitigate the spread of COVID from our students to the larger Champion About community. There was essentially no correlation. So, what people had been worried about in the community was that having students back on campus would cause a large number of cases and deaths in the community. That didn't turn out. The community. That didn't turn out to be the case. We did an analysis looking at all communities that were of similar size, counties in the US with universities of above 15,000 students. There's about 250 of those. We did various analyses, which I'm not going to go into. And we saw that there was a 35%, 32% reduction in COVID-19 cases compared. In COVID-19 cases compared to what one would have expected. And in terms of death outcomes, that outcome was a fourfold reduction in death based on what you would have expected from simple regressions. And this is the work led by my colleague, Becky Smith. I'm going to skip this, it's not so important. So I want to end by talking about compliance. So this was This was epidemiology is not rocket science, it's harder. It's harder because you have to deal with people. And this is a headline in the New York Times: a university had a great coronavirus plan, but students parted on. This upswing in positive tests, which got into the New York Times, was an order of magnitude less than what other universities were experiencing, but that didn't mean that the New York Times didn't want to talk about that. So I'm not going to go through this in detail. I'm not going to go through this in detail. We were able to use our modeling to compare our epic curves with what we were measuring. And so we could estimate our compliance in an independent way as well. And again, what we found was that the compliance was about 60%. Now, the great thing about getting into the New York Times, because your COVID testing didn't quite work as well as you wanted for a couple of days in a whole year. XKCD Comic ran a CD Comic ran a cartoon that starts off with apparently some university reopened based on a code model developed by two physicists and talks about that we didn't take into account student parties and they had to shut down and they can't understand why physicists would be involved in this because how would they know what parties are like? And the response to that was, well, actually there were seven physicists and engineer computer scientists and actually, you know, Becky Smith, who's an epidemiologist. You know, Becky Smith, who's an epidemiologist, and other people that we worked with in the health sciences. Actually, we did take into account the number of parties correctly. That was all fine. But what we didn't anticipate and nobody else did was the illegal breaking of health department statutes and local ordinances. And in fact, the university didn't close, it stayed open. So, you know, the point about all this is that compliance is a critical factor and messaging is an important factor. And messaging is an important factor. So, here's my conclusion, and I'm going to end on this slide. So, agent-based modeling using physical principles of NSL transmission can help design mitigations, which are based on surveillance testing. It's easy to do in a closed community like Champaign-Arbana or semi-closed, harder to do in, say, an urban area. Large-scale surveillance testing, even on a highly socially active population, can lead to a statistically Can lead to a statistically significant positive health outcome for that community and for the surrounding community. Modeling needs to be combined with real-time data analysis, lead to actionable information with minimum disruption to the community, and compliance is a huge issue. And I think that if one wants to look to the future of what epidemic modeling needs to do, it's not better numerical methods for estimation and computation and so on. And computational and so on. I think the area that we really need better understanding in is social and political interactions that impact compliance. So that's where I will end, and I'll be happy to take any questions.