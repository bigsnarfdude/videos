So, a little bit about the group here. We're a relatively small group, two old professors, namely me and Jim Dufty. We have currently two postdocs, Angelava Vedamata and Ector Rodriguez, and a volunteer, a former student of Jim Dufty's, who, for health reasons, can only work part-time, so he works with us. A bunch of collaborators. With us, a bunch of collaborators on ground state orbital free DFT, particularly the exchange correlation side, Antonio Cancio at Ball State. In the free energy DFT and the pseudopotentials and so forth, my longtime friend and colleague, Valentin Karasyev at the University of Rochester. Katerina Hillica is his postdoc. Josh Hensk was his student. Sushin Hu is his colleague in the institute. I worked a lot with Danielle Mahidro Reinstitute. worked a lot with Danielle Meji Rodriguez and his name will come up some and of course already mentioned a collaboration that I'm grateful to Michaela. Michella and I did not know each other really and he invited me to join. It was his idea to do the chemical reviews and he should be given credit for that initiative. Then of course there's the long-standing collaboration on simpler exchange correlation functions. Well not restricted to that but with that focus with my colleagues in Mexico who are both the country and the people are very dear to me. Country and the people are very dear to me. Various sources of funding. We've just recently changed the URL for the group webpage. So if you use the old URL, it should still default to it. But if you can, use the new one. Okay. A little bit of motivation. I want to go about this in a kind of peculiar personal way, but just a very brief detour. This paper was with Valentin and John. Was with Valentin and Josh and Siching, and it's a remote in nature to a machine learning paper which claimed that the liquid-liquid phase transition in dense hydrogen is continuous, not first order. And it was done with a machine learning potential with the critique that the ab initial molecular dynamics or Born-Oppenheimer molecular dynamics that we and others had done, which certainly were not, we weren't the only ones, indeed, we weren't the leaders. Was flawed, those calculations were flawed by size effects, that the samples were too small. So when this came out, Valentin took the lead on this. He took one look at it and said, well, I don't believe this. And there's a brute force way we can do this. We can simply go do the big systems. So the small system here is 108, a few hundred particles. Big in this context is 1700 to 2,000. And we did it. 2000. And we did it. And the machine learning potential doesn't reproduce what it's supposed to, and so it's wrong. That led us into a very big controversy because that's nature. But at any rate, the point of that little rambling digression is that what drives me in this is a desire to have, excuse me, there's some way to get rid of this bar. Oh well. Oh well. I want to have a systematic hands-off set of DFT approximations that are computationally affordable for high throughput calculations on large systems and or systems under extreme conditions. So examples are, for example, spin crossover systems where you have 50 or 60 heavy atoms and 700, 800, 900 electrons in the unit cell, and you have a whole family of closely related molecules. Related molecules, but you want to know which one is the best spin crossover candidate. So you have to be able to do affordable structural calculations, calculate Yonteller corrections, calculate adiabatic spin crossover energies, calculate vibrational corrections to the one-half polarized temperature, and on and on. So that's kind of what motivates me. When I learned from Selgate that I was doing the opening talk, I thought, well, I better not restrict this. I thought, well, I better not restrict this just to one of my current obsessions, which is the exchange correlation functional. But I ought to go back and look at the field, if not completely, and it certainly would be arrogant to claim that I could do it completely, at least a little bit more generally. Aha, okay, now that little bar will work. Yeah. So this is a quarter of a century ago, basically. This is a paper that most of us, I think, know and probably have read more than once. Probably have read more than once. Published in year 2000, probably drafted in late 1998, 1999. So roughly a quarter century. And it's basically the state of the art as of that time. Now, you know, not to be unfair, there were a lot of other people that had come along. Bob Parr, Julio Alonso, Malcolm Scott, Agnes Nige, Paul Madden, Mike. Agnes Nige, Paul Med, Michael Teeter, Way Tau, who's here, had worked on orbital-free DFT in one way or another, and many others. These are people that I know personally that came to mind as I was preparing for this. But as a marker, this is a really important paper because it tries to set out in a very systematic way the difference between conventional Kung Shim and orbital-free. I'd like to call it orbital-free Kung Shim because we're always dealing with the non-interacting kinetic energy. Non-interacting kinetic energy decomposition that Liu Sham invented. So, this is a kind of marker of where we were 25 years ago. It's already been alluded to. Three, no, almost four years ago, four and a half years ago, McKella approached me. Winway was his postdoc at the time. Kylu was my postdoc at the time, had gone on to professional roles of their own. Said, why don't we write an updated review? It's been roughly 20 years. Review. It's been roughly 20 years at that point. It had been 20 years since Emily and Jan Alexander had done that paper. And so we went to work, and it took us a mere three years plus a year's struggle with the recouncet referee. And so that's, again, an attempt to summarize where we are. We tried to pay attention to everything we knew about. I had found since we published this three papers that we found. We published this three papers that we failed to mention that would have been germane. I mean, there's lots of papers you could mention. But in our own way, we attempted to summarize what had happened, where it came from, how it fitted in. So we actually went back to Thomas Fermi, which is, of course, where Framley and Alexander started. But we tried again to bring it forward from where they had been. And we hope it's useful and we hope that people will build upon it. People will build upon. And again, if you know of a significant paper that's significantly related to this, that we omitted, please tell me I'm collecting. At any rate, now back to personal motivation. And this, again, sounds like a detour, but we will get to ground state orbital pre-DF2 very quickly. I got into this partly out of intellectual curiosity. I was teaching DF2 to graduate students, I think, in about 1987 or 8. Seven or eight, and I didn't understand the business about the density to the one-half and so forth. So I asked one of the students in the class because I assigned a project to each one of them, but I put up a bunch of topics, and one of them was sort of rudiments of Article 3DFT. And the student actually wrote a pretty nice little paper, which was kind of my primer on this. But what really got me into it was an interest in finite temperature or free energy DFT and simulations. To you, and simulations, ab initio-molecular dynamics again, of the systems at very high pressures and very high temperatures. And here you're talking about multiple gigabar pressures and temperatures measured in electron volts. So, this is relevant, for example, to planetary interiors, the need for the equations of state of helium and hydrogen, sometimes ammonia, and or inertial confinement fusion, where you have a little tritium pellet and you hit it with an x-ray. And you hit it with an X-ray implosion and you get fusion. And the problem in both of these cases is that these are material systems. This one is a material system on a trajectory from ambient to excruciatingly large pressures and temperatures. The difficulty is that these are experimentally inaccessible. It's very difficult to have any experimental data, and so it's very important to have accurate simulations. Well, okay, just warm dense matter. This is a kind of a typical simulation from A kind of a typical simulation from abnational molecular dynamics of aluminum. It shows you how the ions move. The ions are the bluish-gray conduction electrons, sorry, or the blue. The conduction electrons are the orange and core electrons are the gray. And again, the temperatures can be up to 100 eV and thousands of GPA. And the point here for orbital free DFT is if you think more than about, I don't know, a few microseconds, whatever, a typical reflex time of the human. Whatever a typical reflex time of the human brain is, and you're a density functional theorist, you immediately know you're in deep trouble. Because in free energy DFT, Berman DFT, the formulation looks very like ground state Bloomberg-Kahn-Sham DFT, except that you have Fermi-Dirac occupation numbers that aren't simple step functions. And since your computational cost scale essentially is the cube of the number of occupied levels or some constant times that accounting for basic. Constant times that accounting for basis set size and so forth. But I mean, those are comparatively small constants. You know that if you go from zero temperature to 20 EV, you're going to have a lot more occupied states, and your computational costs are scaling as the cube of that. And again, if you need to look at many thousands of MD steps at many different temperatures with many different compositions or any of Compositions or any of these computationally large-scale investigations, you know you're in trouble. So, what people typically do at finite temperature and pressure is that they have a grand potential, they have a non-interacting kinetic energy, and there's a non-interacting entropy term. So, this is not what you see in the grand state. And what you need is an exchange correlation free energy that's a functional both of the density and an explicit. Of the density and an explicit function of the density, temperature. Sorry, then you take the functional derivative and set it to the chemical potential, and you get the Euler-Derrange equation, and of course, your exchange correlation potential is this. And so, what happens in orbital-free MD is the ambition is to get rid of this step and instead make it this step. So, here's your MD box, as it's called. Here's your DFT code that's actually giving you the forces. Giving you the forces. And when you go to finite temperature, this gets, this is again, the cost of doing this is 90 to 95% the cost of any single MD step. And this cost is scaling as a cube of some difficult number, like the number of occupied bands times a constant. So what you'd like to do is just replace it with the single Euler equation, which is the orbital free. And the point of this set of remarks is this is exactly the same problem. As the ground state. In other words, I need not only, oops, this blue thing got moved. It's supposed to be over here. Sorry. I need not only the exchange correlation energy, but I also need the non-interacting kinetic energy as an explicit function of the density. So the entropy piece, which is easy enough to calculate anyway, it's just an ideal gas entropy. But the temperature at the ground. Temperature at the ground state calculation is essentially the same problem. And in fact, there's no way that I can do this and get it right if I don't have this, the ground state limit, correct. Okay. Otherwise, I have an intellectual incoherence that takes the groundwork or the validity out from under all the calculations. So my personal saga journey to getting to grand state orbital free DFT. Grand State Orbital 3 DFT started out of the pure intellectual curiosity about trying to get rid of the Kung-Sham orbitals, but still use the Kung Sham decomposition. But then it was really driven 15, 18 years ago when I got the opportunity through some DOE funding to work on free energy DFT. So, okay, so we know the grand state density functional theory. We know from Malvy's constrained search how to construct a version of this universal function. It's not the same universal function. It's not the same universal functional as Hohenberg and Cohn defined. This is my favorite picture of Walter Cohn. And we also know from Lou Sham that what we have to do is reorganize the kinetic energy into the non-interactive kinetic energy, put that introduced difference over here, put it with the exchange energy, put it with the Coulomb correlation, and lump that whole thing and call it exchange correlation. But now we don't get to use these orbitals. Don't get to use these orbitals, and we don't get to use this decomposition of the density of the orbitals. And to do that, you end up with an equation that has basically the Cohn-Sham equation, except that it's got an extra potential in it. And so this slides I use with audiences that typically don't know much about OFDFT. So it's got this, what's this question? But this, you know, we all know it. You do this decomposition. You use the lower bound of the bite-secker kinetic. Use the lower bound of the Weitzacher kinetic energy to the non-interacting kinetic energy, and then you have this Pauli term. This is a very nice paper by Mel and William. It was really very formative for me and Valentin and others. And so, you know, we know the Vaislock sector, we know the Thomas Fermi. And then the world splits. And the world splits in this sense into one-point functionals, which depend upon the density, the gradient, the Laplacian. The gradient beloved. Don't know of anybody that practically has gone beyond a Laplacian. And this is actually our group's focus because we really want computational speeds, so we want something that's simple. This is a simple three-dimensional integral, which means when we take the functional derivative, we end up with scalars, not with something we have to integrate. And that gives us a computational speed. We give up, I'm going to come back to this issue, we give up on shell structure. We give up on shell structure, at least in the informal sense that it's generally assumed that this can't give you shell structure. The larger part of the orbital-free DF2 world, I think, has, although there was a fair amount of backup, when we got into this game in a big way in 2009, 2010, there were a large number of generalized gradient approximations for this enhancement factor that had been parametrized or fitted or obtained in some. Or obtained in some, I would say, not disrespectfully, but as a summary kind of semi-empirical ways. And so, you know, one of our foci was to try to understand whether those things that were in the literature, I think there were six or seven of them, actually worked. And the answer is no, they didn't. You can come back to that. At any rate, if you go back to Emily and Jan Alexander's article, much of the emphasis on the majority is on two-point functionals. Is on two-point functionals. And I've written it here in a relatively general form as having a density dependence as well as an explicit spatial dependence. And this is in many ways much slower or it requires much more calculation. It's often parameterized to particular binding types. Emily Carter was particularly motivated by very practical materials problems on how to distinguish amongst various Crystallographic phases of silicon, how to distinguish amongst different alloys of steel. So they used forms for which they could do sensible parametrizations, among other things. That certainly was not their entire program by a long shot. And they also, one of the properties of most of the work on these is that they eventually have the homogeneous electron gas response function as a calibration. Function as a calibration. So that's kind of the way the world breaks down. Again, having been asked to do the opening talk, and thought, well, I should talk about things other than I put in, or in addition to what I put in in my abstract. So my abstract is number four, but we'll go through these three relatively promptly. But I hope that they arouse some interest and provoke some questions and maybe some progress. First is some mathematical questions about two. Questions. About two and a half years ago, we were invited. Walter Bach and Leejeevan Sitta invited us as a group to write a paper in Letters of Mathematical Physics. I want to be careful here. I think Jim Duftee among our collaboration is the best mathematical physicist of the bunch. I'm pretty good at functional analysis. I am not really deeply skilled in statistical mechanics. I do okay at it. At it, but um, so but none of us pretends to be a mathematician. So, the way this paper is written is to try to, from a physicist perspective, say to applied mathematicians, and I think Elliot Lieb is your audience, okay? That's a pretty intimidating audience, but never mind. Here are some problems that bother us, okay, and about which we could actually use as a community some help. And we structured it in various ways. So, one of the Structured it in various ways. So, one of the big sections of this paper is about the existence of functional derivatives. It turns out, if you follow this literature, I should make a digression here that I haven't planted. We actually ended up with a referee who was one of the most helpful referees I've ever run into. And this referee wrote us something like two and a half pages saying, here are papers that I don't think you know about, or if you do know about it, haven't paid enough attention to them. And in essence, we took that referee report and rewrote the entire paper. Referee report and rewrote the entire paper end to end. And it is a much better paper for the contributions of the referee. And whoever it was, I agree. And so are the rest of us. At any rate, there is a relatively short section about orbital EDFT. And one of the questions, and it's a somewhat arcane question, but there's some papers back in the late 80s, early 90s, that say that it's impossible to have a one-point approximation that has Approximation that has a certain degree of spatial derivative. I stated that awkwardly. I don't think that's true. And in fact, Kai and I wrote a paper pointing out that at least one instance, the argument is incorrect. The larger question, I think, in an example is this one. Is the conjecture about the non-appearance of shell structure in one-point approximations true? And if so, under what conditions? And if so, under what conditions? In other words, if it's generally assumed that you have to have a two-point approximation and something, some kind of response that would give you shell structure or in the solid. I've never seen that proved, and as far as I know, it's never been proved. The arguments better than hand waving, I guess, have been given. But if it's true, is it? But if it's true, is it generically true or is it true because the one-point approximations don't satisfy certain conditions? So that's a question. The next question is, suppose the answer to this is negative. In other words, suppose, in fact, in principle, you could get shell structure. Does this implicate strange signs in the gradient expansion? And that is a rather obscure or opaque. Obscure or opaque remark, but the point we've run into, and I'll try to remember to come back to this when we talk about deorbitalizing orbital dependent exchange correlation functions. One thing that Tony Concio and Hector and Daniela and I have run into is that one way to get improved deorbitalization is to use a functional form that corresponds to the functional form of a truncated gradient expansion, but then alter the coefficients. Sometimes the coefficients have Sometimes the coefficients have quote unquote the wrong sign relative to the proper gradient expansion. So we're trying to understand how one-point approximations work. Okay, that's a somewhat exotic set of questions, but here's another one that I think would be very valuable for us as a community as a whole. There is a conjecture in Lee's papers. This is the one that's published. As far as I know, this paper never was published. Far as I know, this paper never was published. If it was, I don't know where it is. And the conjecture is that the non-interacting kinetic energy lies below the sum of the Thomas-Fermian and the bicep kinetic energies for the same density. Now, we have enforced this locally, somewhat like the way the Leebox remound is enforced in exchange correlation functionals. It's known that that's not right. And in fact, Not right. And in fact, this was a dispute. Well, dispute is a disagreement we got involved with with Emily Carter. And in fact, she and her group are right. We were enforcing it locally. And you can find examples where you go and you construct the Cohn-Sham kinetic energy density, and you look at it, and it doesn't stay below the sum of the Thomas-Fermi kinetic energy density and the von Weisser kinetic energy density. And it's WIP and company in this paper discuss it. Now, Now, all I've done with this two slides is point out that this is a few things that come up very explicitly in Orbiter 3 DFT. What this leaves out, of course, is the generic question about the existence of functional derivatives. It also leaves out something that, as far as I know, has almost exclusively been discussed by Paul and Schubin, namely the issue of interrepresentability. And you will find in the literature without a whole lot of Without a whole lot of kinetic energy density functions that are clearly not interrepresentable because they come in under the corresponding Kung Shan non-interacting kinetic energy. So I think this is both ignored. It's easiest to test post facto, but it's fairly easy to at least take a hard look at it when you're building an approximation. And the question in the back of my mind is, are there additional Are there additional constraints that could be elicited from this for the construction of new and better approximations? Okay. As one says in Spanish, Cambiendo de Tema, changing themes. This is a collaboration recently with a student of Baquella. It emerged from, I had the privilege of going to Rutgers-Newark and visiting Baquella's group, and we were talking about the fact that. About the fact that in the Wang-Teeter kinetic energy density functional and in many others, there's this reference density. And the idea is that it's rho zero, and it's, pardon me, it's evaluate, it's an average, supposedly, density for the bulk. And of course, this has the weird problem that it means that you can't really find and apply these kernels for molecules. Well, if you're trying to study how materials are formed out of molecules, that's kind of frustrating. Small data molecules, that's kind of frustrating for, as the topic of this workshop says, widespread utilization. So what we did was we said, what happens if you just tune this density? Okay. Long tater has all sorts of problems. It's density independent, which means it's variationally unstable. It doesn't scale properly with uniform density scaling, but it's computationally cheap. Scaling, but it's computationally cheap and it's remarkably effective, which makes it sort of appealing. So the idea is: tune this density to something that would be sensible, but wouldn't necessarily be tied to a sample average. One possibility is to tune it so that you minimize the difference between the orbital free density and the con-sham density integrated over whatever the appropriate volume is. Another would be that the two kinetic energies work out correctly for the correct density. out correctly for the correct density and the third one would be that the energies work out correctly and and it's very interesting when you do this and and you know we'll come to the disadvantage of this when you do this and you do volume scaling this is um BCC and cubic diamond silicon when you do volume scaling to kind of track out the equation of state in a quick way what you find is that the conventional density which is this guy here okay Here, okay, right, doesn't work at all. This one, the so-called dense, which is this, doesn't work really well, but it works pretty well around the minimum. And the other two work really well. You can also go and just look at the differences in the density. This is the orbital free density from these three non-standard parameters. Non-standard parameterizations of rho zero for body sending tetragonal silicon at the experimental lattice constant. And then this is what you get when you do the traditional, the rho naught equals n over b. And the fact that this is the difference between the cone-sham, the conventional cone-sham density, and the orbital free density, and this case is obviously worse. You can go down through this table, and what you will find is that that's the case over and over and over again. I won't bore you with. Again, I won't bore you with it. This is an existence theorem table. It's here, it's in the paper, you can read it. What you also see is that these parameters, these by the way, have to be multiplied by 10 to the minus 2. These parameters are generally slightly smaller than the conventional one. Now, there's an exception here. There's another exception down here somewhere. They're not big shifts, okay? And yet there's a remarkable sensitivity. Okay, you say, well, wait a minute, Sam. You're telling me. Well, wait a minute, Sam. You're telling me that you get these lovely improvements by knowing what the conventional con-shim answer is. Why don't you just do conventional con-sham and go home? Well, here's one example. Suppose I have to do a whole bunch of Born-Oppenheimer MD calculations, go in, take a look at the range of volumes you're going to be working on that are plausible. Do a few static lattice calculations on the system of interest, okay, at a low volume, an intermediate, a high volume, whatever. Figure out Whatever. Figure out what tuned, how to tune the rho zero from that, and then plug it into your orbital 3DFT, AMAD, your abidial molecular dynamics, and go run your simulations. Have we tried it? No. Do I think it's going to work? Yeah, I think it's going to be helpful. Is it a cure-all? No. But it's a way to make an effective use of a somewhat otherwise inadequate but nevertheless cheap function. But nevertheless, cheap functional. Okay. One-point functionals. This is where we've done a lot of work. So we introduced two of them. This is the Lio-Karassev-Trichia, and this is Liu-Karasis. This is when we did it for the ground state, and this is when we got it for the free energy functional. And this is so we do constraints. This is the positivity constraint for an earlier version that I haven't. For an earlier version that I haven't plotted for you called VT84, what we did was a cusp condition, and we pointed out that if you have a generalized gradient approximation, which means your one-point functional depends only on the density and the gradient, then the best you can do to satisfy the cusp condition and these constraints is to make sure this expansion for an atomic-like density is positive. Okay, that's the best you can do. It's still an ugly potential. And so. And so, what we did was the various other box would ban according to Augustus and Robus. And so, by the way, it's very obvious, and I think Andrew's title talks about cuss conditions, so it'll be interesting to see what he has to say. At any rate, when Kai was in the group, we had by that point realized that we had this peculiar property that Peculiar property that to satisfy the cusp condition, our enhancement factor for the polyterm had to have this bump in it. So, this is the enhancement factor basically as a function of, ah, I can't get rid of this thing. Anyway, as a function of s, okay, so you can see it. When you have a pseudo-density, which has zero slope at the origin, then you're dealing with a very different constraint. And Kai did a lot of investigation. Constraint and Kai did a lot of investigating and a lot of clever numerical experimentation and came up with this remarkably simple, simple enhancement factor. So we now have a breach of universality, right? These functions are supposed to be universal. This one is supposed to, in fact, is not universal. It only works with cusp densities. This one only works with densities that are flat at the origin. Okay. So we've given up on universality in the hope of improved performance. Universality in the hope of improved performance. So, the cuspidone we've applied and compared to the second-order gradient approximation, APBEK, is a modification of the PBE exchange form due to Lucian Constantine and friends. And this is simple cubic hydrogen. This is old. This is 12, 14 years ago. But it was important to point out that this is the conventional cone-shamosol. This is what we get from our functional. What we get from our functional, and all of these other functionals do not give a bound system. So, even though this is a significant lattice constant error, at least it's a bound system, and at least the forces near the bottom of this energy well are halfway reasonable. So, that was big progress for us at the time. Later on with LKT, Kaiser, what we realized was that if you do mean absolute relative errors for the molar bottom, the equilibrium volumes, the energy, the bulk moduli, and these are. The energy debt moduli, and these are his percentages. And you compare him, for example, to Juan Carter, which is parameterized for semiconductors but doesn't do too badly on metals. And you go down here to LKT, you realize that these errors are not too bad. I mean, they're better on the metals. This was the parameterization domain, and it's at least roughly competitive. So here's a one-point functional that's doing roughly as well or better. Roughly as well or better than a two-point parametrized function. And it takes fewer iterations to converge, and each iteration is faster. So we're sitting there thinking that life is wonderful. Well, life is not wonderful. And this is a point I want to make. This is a plot of the pressure versus lattice constant, okay, for BCC lithium and for FCC aluminum. And for FCC aluminum. And it's done with conventional cone-shim and LDA with a one-electron pseudo-potential, that's the blue, conventional cone-shim and LDA, and a three-electron pseudo-potential, in other words, a so-called all-electron pseudo-potential. That's the black red boxes. And then it's done with LKT and the one-electron pseudo-potential, and that's the continuous pink curve that you almost can't see here. And then it's done with LKT and a three-electron potential. With LKT and a three-electron pseudo-potential, and the system collapses. Same story in aluminum. I won't go through the details, but it's essentially the same story. You do it in aluminum as if it were a three-electron system, and it works. You do it as a 13-electron system, and in conventional con-shim, it works, and in LKT, it fails. What we have here is a problem in which the pseudo-potential for our simple functionals is basically keeping the core. Basically, keeping the core intact, and so it's forcing the remaining density outward. That is, in fact, what we've got, in fact, with these successes is a very interesting combination of cancellation of assumptions. I don't want to call it cancellation of errors, but it's cancellation of assumptions. This is stuff that Valentin and Katerina and I cannot get this stuff, are working on right now. We're writing a paper about this that's going to appear in the electronic. About this, that's going to appear in the electronic structure volume that is devoted to orbital 3DFT. We suspect that this occurs with all other modern-point functionals. We don't know about two-point functionals. The problem is non-additivity. The cone-champ kinetic energy, if you have core and valence electrons, the cone-sham kinetic energy is obviously additive this way, but it's not additive when you do it as the core valence density separation. Corvalence density separation, which is in essence what you're doing when you're doing these fewer than all electron pseudopotential. I should notice that La Tomeca and Olga, I don't believe she's online, she's supposed to speak at this meeting as well. They ran into this problem before we do. They ran into it in the context of PAWs, and there's a very informative paper 10 years ago that I think as a community we probably need to go back and pay more attention to. That I can pay more attention to. Valentin and Paterina and I are working on non-additivity. Typically, what you see in the literature is a non-additivity correction for the exchange correlation. This goes to Steve Louie. I can't remember the other author. But it's typically a relatively small thing because it's a small correction to a relatively small fraction of the total energy. That's not true for the non-interacting kinetic energy, as we all know. Energy, as we all know. So I don't know, as I say, whether this supplies about two-point functionals. I think it's an open area, but it's one that's causing us a fair amount of intense concern. Okay, finally, the topic for the original lecture. How long do I have? I have about 10 minutes. I should be able to make it. This, to tease my quantum chemist friends in the audience, this is what the quantum chemistry community. What the quantum chemistry community is doing to us. They are proliferating orbitals as fast and furious as they possibly can go. They're putting in what they call exact exchange. Exact exchange literally is not quite. It's single determinate exchange, but never mind. And so here we are struggling to try to take the orbitals out while our colleagues, primarily interested in molecules, are feverishly putting them in. And so this is a frustration that occurs. This is a frustration that occurred to me, and I wasn't the first one to give credit where credit is due. I think the first example of what we call deorbitalization is functional, in which they go in and they take a kinetic energy dependence and they replace it with an explicit density functional. The next example is in Purdue and Constantine, and then it sort of disappears until we picked it up. Disappears until we picked it up. And I'm embarrassed to say that in our first paper or so, or the first time I wrote a proposal on this, I didn't realize those two antecedents, but subsequently did. So here is the Purdue-Schmidt Jacobs ladder of functionals turned into a Mexican pyramid by my friend Victor Vella. I say borrowed, it's probably more correctly stolen. We go LDA, put in gradients of the density, we get down to roughly 10 kilocalorie per mole total energy counter errors in molecules. Go to meta-GGAs and Go to MetaGGAs, and one of two things happen. Typically, you put in a functional, which frequently is this chemical region indicator alpha that is dependent on the con-sham kinetic energy density in the explicit orbital dependent form. Now, in principle, you could, in fact, also at the Meta GGA level, or alternatively, you could put in the density of Laplace. But the great bulk of the literature is in this form. Okay, so it's the Okay, so it's the con-sham kinetic energy density of point R minus the bisacher divided by the Thomas Fermi. Well, what is this? Well, this is just the enhancement factor when we get all done because the enhancement factor is this minus this times Thomas Fermi. So then the Thomas Fermi divides out and you're there. Okay, this is what shows up in MVS and scan and R-squared scan. In TPSS and TALMO, there's also another indicator which has caused us an enormous amount of Which has caused us an enormous amount of grief, although we finally figured out how to solve that problem. And I won't spend time in this talk about it. Okay? So the idea that occurred to us was, well, with this orbital dependence, we've got this LK2 or we've got DTH4. Is it good enough to replace this with this? Something that depends on the density and the reduced density gradient. And the answer, the short answer is no. That doesn't work. At least we haven't been able to make it work. Danielle Mejia-LeBries did this work. Did this work about seven years ago. But what we were able to do was to say if you put in a functional of the density, the gradient, and the reduced to density Laplacians, okay, so that you stayed at the meta-GGA level, but without the orbital dependence, that does work. And an example, so what we did was we took various forms of the kinetic energy density function. Well, this is Purdue-Constantine, okay, and then we optimized. Okay, and then we optimize the parameters in them. I'm going to not, I mean, this is detailed mechanics, but basically we change the parameters without changing the form so that they delivered better fits to the alpha, even though they might not give better results for the kinetic energy, the cone-cham kinetic energy itself. Okay, now it turns out that was a very clever idea. It turns out that works usefully. It also turns out that it actually can invite you. That it actually can invite you, but we'll come back to that if I have time. So here's scan. Scan is this enhancement factor that basically has a gradient, a generalized gradient approximation with a very elaborate scaled variable in it. Okay, so this looks like PBE with a really ugly argument. And then it has a switching function and a smoothing function. And what we do is we go in, we end up putting different parameters. Up putting different parameters in here, okay? And we end up doing the same thing in R squared scan, which is a different, since another version, it's a smoother switching function, and it has a regularized alpha in it. Okay. Again, leaving out a bunch of details, but what we find in the molecules is that we can, in fact, get heats of formation. This is the original scan. This is the G399 set on the molecules. G399 set on the molecules. This is deorbitalized skin. So L means Laplacian. So, yes, it's not the same mean absolute deviation in the heat of formation, but it's not bad. We've lost seven tenths of a kcal. With R-squared scan, we lose essentially eight-tenths of a kcal. On bond length, we get almost the same result. On vibrational frequencies, we actually do a tiny bit better. The hitch here, and one that is really a And one that is really a sign, I think, of the Laplace independence is that you have to use really, really fine grids. So these are differences, okay, in mean, absolute, relative, error with respect to this number. And this number was generated with the NWChem huge grid, okay? So extra fine costs you a little bit, but coarse or medium is just useless. In the solids, what you get, and this is a different data set. What you get, and this is a different data set, but you basically get decent bond lengths, you get decent bulk moduli. Again, I'm sorry, I do not know how to get rid of this silly slide, but anyway, you get reasonable results for the cohesive energies. Okay, all right. Another interesting thing that happens is that you get actually better results. Scan and scan L over magnetize elemental 3D. Over magnetized elemental 3D solids. Sorry, scan and R-squared scan, over-magnetized. Scan L and R-squared scan L actually do better. The chromium 2 is very complicated, and these do better, but they don't do great. What happens with the deorbitalized spin crossover, and this is the adiabatic energy difference between a low spin and a high-spin state, is that this deorbitalized functional doesn't work near as well as this one, the parent functional. But the trick here is. Parent functional, but the trick here is you can calculate the geometries with this and then go evaluate the adiabatic spin crossover energy with this. So now these are all cone-cham calculations, mind you. They're conventional. These are just finding out what happens when you get good at the orbitals. Again, not everything is wonderful. PCOPT in the R-squared scan L breaks constraints. Purdue and Kaplan tried to put the constraints back in. It turns out for lattice constants and Lattice constants and various other properties that improves over our R squared scan L. They cut the errors roughly in half or better. But when they come to molecules, it's much worse. And again, I'm just going to do this very quickly. So at this point, I gave you a kind of a surprise. One of the things, and this is where we'll conclude, one of the things we did when we first started deorbitalizing, we tried MetaGGA to make it very simple. Meta-GGA made very simple, which is Yahweh's sun influence. And it's a fascinating meta-GGA, but it's a rather bad one. Mean absolute deviation, 18 kilocalories per mole heat of formation. Not bad on bond distances, pretty bad on frequencies. When we deorbitalized it with concierge and optimized, now concierge red is basically a recovery of the second order gradient expansion. We got a surprise. Things got better. And we published this, and nobody paid a lot of attention. And nobody paid a lot of attention to it except our friends in Vienna. They said, Well, but it doesn't work when you do it on solids. We'll come back to that in a minute. Well, I come back to it now. So we went and did the solids here recently, and the answer is that, yes, it does improve about the same when you do the solids. I think the difficulty is that they're using the Veen code, which has a number of approximations for evaluating meta-GGAs. That's again off into technical details that are probably not germane here, but they're warning. But what we interested was if you put back interesting was if you put back in constial red with the gradient expansion proper parameters you get almost the same results. The orbitalization actually obeys more constraints and it's almost as good and the story is the same when you go to the solids. You improve the lattice constants from the parent. This is the parent that depends on the orbitals. This doesn't depend on the orbitals nor does this. This is the, okay, you improve on the cohesive energy, you improve on the bulk modulae. On the bulk bungee. You lose on the band gaps, but that's not a surprise because this is generalized con shim, which means your band gap is going to be better. These are errors, okay? So you want these numbers to go down. So overall, this is a case where you can actually improve the performance of the meta GGA. It's almost as good as R squared scan. Not quite, but almost. Okay, so I'm going to leave with that. This is a set of calculations using the Durham. Calculations using the deorbitalized scan with RVV10 and doing born Openheimer MD, various temperatures. This is what happens when you do the insulator metal transition at hydrogen. And the question is: what happens if you do classical hydrogen and deuterium and then do quantum hydrogen or quantum deuterium with path integral M D, and then you do quantum hydrogen? And then you do quantum hydrogen, and you can see the isotope effect. And what you see here, it's hard to see, but the experimental numbers here and our calculations with these deorbitalized functions, okay, and that's the crucial point. Our calculations are about as good as any that have been done. Okay, a list of some issues, the requirement of a local pseudo-potential, the core collapse in one-point functions. That's what I In one-point functions. That's what I showed you. The identifying and exploiting of tuning opportunities. We gave you a long teeter example, but I think there are other tuning opportunities. Think of Roy Bear and his tuning to the IP theorem through the exchange correlation, for example. That's not a paradigm we can adopt directly in orbital free d of t, but the notion of tuning to a physically meaningful or constrained property is important. Constraints on kinetic energy density functions. The Lieb conjecture. Functions, the leap conjecture, others, the lack of native PAWs for meta-GGAs. It's a semi-buried secret that most of the calculations with meta-GGA exchange correlation functions use PBE, PAWs, too many consonants. The control of Laplacian numerical instabilities. This is something we're still working on. We now have a situation in which, at least for the SCF, we can actually go. SCF, we can actually go faster than we can with the parent functional. But when we put it in the Born-Oppenheimer MD, the instabilities going from one step to another are such that our deorbitized functions still are not as fast as the orbital. On the other hand, they're at least derivatized, which means that they can be run free DFT. Okay, that's my story. I appreciate your listening. I appreciate the honor of having a chance to talk with you, and I look forward to hearing as many of the talks as I can stay with. Thank you very much. Talks as I can still with. Thank you very much.