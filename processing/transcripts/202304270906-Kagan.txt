Um and so I guess the questions that at least if you're on one of these high energy physics experiments you always get when you're getting your reviewed by your experiment and you have a machine learning tool and someone inevitably asks what's the uncertainty on that machine learning model and I often don't even know what's meant by that. So today we're going to sort of try to talk about what kind of uncertainties could that mean, what happens if a model doesn't perfectly fit the data, and when does that matter? There's sort of another direction of this which isn't about Direction of this, which isn't about how well the machine learning model fits the data, but more about what happens when there's existing systematic uncertainties, and then you're running your machine learning model on some data that has systematic uncertainties, and that's what Tomaso is going to talk about. So that will be the next talk. There's also a nice review of some of the stuff in the PDG. So this is sort of the broad picture where we'll start from. This is sort of supervised learning. And roughly speaking, you have some data, let's call it your features X and your. Let's call it your features x and your targets y, and you want to learn a function of x that predicts the features y. So it's sort of tasked as an optimization problem. But once you sort of pick your set of functions, like a neural network with some architecture, you sort of specify a family of functions you could possibly model. And then you sort of start somewhere randomly in that space and you run an optimization algorithm like gradient descent and you try to minimize some loss function that compares your prediction with your labels. And so you do that and you arrive at some point. And so you do that and you arrive at some point in the space, trying to do the best you can to model some true function. And we'll sort of talk about in a little bit, you know, what happens or the fact that, for instance, the true model may not even be in your family of functions. So that's sort of machine learning in one slide as well. Okay, so what are the things that people care about in machine learning or talk about when they talk about uncertainties? And so I realized over the course of this week, and also generally in talking to physicists, that term Generally, in talking to physicists, that terminology is sort of wildly different in all these places. So, a lot of this talk is somewhat just translating terminologies. So, in machine learning, they typically talk about two kinds of uncertainties. One is called aleatoric uncertainty, and the other one is called epistemic uncertainty. So, aleatoric uncertainty here corresponds to the fact that the thing you're trying to model may not be deterministic. It may be a random process. So, in this example, we're trying to model the blue dots with some function here with green. Dots with some function here with green. This is temperature versus time, for instance. And just the fact that the blue dots are not sort of deterministically following some line but have some spread around that, that's aleatoric uncertainty. The randomness in the data. But on the other hand, there's this idea of epistemic uncertainty, which is coming from your lack of knowledge about the system, about the data, or incomplete information. So the sort of extreme example of epistemic uncertainty is the places where you Epistemic uncertainty is the places where you have no data. You're trying to make a prediction about what's going on in this region, and there's no data, and so you're trying to, then the error here is sort of what are the possible things that you could have gotten. Maybe in the middle, maybe places where you just don't have a ton of data, and so your ability to really determine the value of the function in that part of the space is not great. Okay, I sort of add a third category here. It's also sort of talked about as a third category in machine learning often, and that is what I've called domain shift. And that is what I've called domain shift. So, in this example, what if you started looking at temperature versus time on a different day? Suddenly, the relationship is very different. So, your green model is not a very good model of the red data. So, that's sort of distribution shift or domain shift. There is sort of relationships between epistemic uncertainty and domain shift, and we'll try to talk about that in a few slides. But so, roughly speaking, these are the kinds of things you worry about. So, to dig into that a little more, aleatoric uncertainty. More. Aleatoric uncertainty is often called statistical uncertainty, which is extremely confusing because statistical uncertainty has definitions differently in statistics. In this case, it's considered irreducible. And so what it really means is, for instance, or what they really mean in this context is if you're modeling, for instance, y is equals f of x plus epsilon, where epsilon is some noise term, and that noise is sampled from some distribution like a Gaussian that has some sigma, this sigma is the This sigma is the aleatoric uncertainty. It's the fact that it's not a deterministic function, that y is not a deterministic function of x. Okay, so this is just some examples like this is an unfolding matrix, but basically, you know, for a given true particle energy in our simulator, there's a distribution of energies you might see in the detector. Or if you're trying to predict what's in this image, you might get sort of your posterior may have some distribution of answers. You know, especially this blob down here, you have no idea what animal that is. And that's sort of. No idea what animal that is, and that sort of inherent uncertainty from the fact that it's not a very clear image. Okay, epistemic uncertainty, on the other hand, has to do with effectively with how well you can fit your data. And so it's often sort of broken down into two terms. So the first is we optimize the system to try to find some, to estimate some model. But if you had more data, you might be able to find a better model in that model class that's closer to the true model. And so there's sort of a difference there between the best thing you could have gotten. There between the best thing you could have gotten and what you actually estimated, and that's often sort of dubbed estimation error. So, and then of course there's the challenge that the true function you're trying to model may not be anywhere in your function class. And that's sort of approximation error. Now, these are often sort of considered reducible. If you add more data, you may be able to estimate the model better or use a more complex class of functions that may get closer to actually containing the true function. Okay. Okay, so domain shift is basically the case when the data that you're training on does not correspond to the data that you're testing on. Now, typically, that's sort of outside the scope of most of those other uncertainties. Usually, you're sort of assuming your training and testing data are drawn from the same distribution. And when that doesn't happen, things can go awry. So, in this case, you can train your animal image classifier on nice, clean data. As you add noise to the data, the classifier is accurately drops forcibilously. So, these models can be very sensitive. So, these models can be very sensitive to differences in training and testing data. Now, this looks very similar to something we've been talking about all week: systematic uncertainty. So, for instance, when we take our data and we train it on a simulator, but if that simulator, while I train a model on a simulator, we look at sort of the distribution that we, you know, this is the simulator's output when you apply it to a bunch of, or sorry, the model's output when you apply it on a bunch of data, for instance. But since the simulator isn't a perfect model of the data, you're There isn't a perfect model to data, your model's predictions are not going to be a perfect model. If you apply that model on simulation and real data, they'll look different, and there's sort of a shift there. And so, that's sort of when we see these shifts in high-energy physics analysis, we spend lots of time trying to characterize them and develop systematic uncertainty estimates. But this is also sort of coming from a lack of knowledge about what's going on here. And so, sort of conceptualizing all these uncertainties also sort of have to ask the question, what do you mean by model? The question: What do you mean by model and why do these concepts that seem similar in a lot of places are getting different definitions in different places? So, for instance, in physics, when we talk about the model, we're typically talking about our physics model, our sort of ground-up understanding of physics that we put into a simulator. The model is sort of the data generation process. In machine learning, there's not necessarily a good knowledge of the data generation process of images of cats. So, instead, you fit a model to the data using. A model to the data using relatively little inductive bias or sort of background knowledge in your design and optimization. So you're just sort of trying to fit things to data, and you're often assuming that your training data will be the same as the data you're eventually going to apply this model to. And so you end up with this idea of epistemic uncertainty being, as the concept of a lack of knowledge, leading to different things. So in a physics model, if we don't exactly know how the shower works or how to calculate something, this leads to a sort This leads to a sort of a data simulation mismatch, and that's eventually what we call systematic uncertainties. In the machine learning world, it's much more a lack of knowledge of what are the best parameters to fit the data. And so even though these concepts are the same, it sort of leads to different sort of ideas about uncertainty. Okay, so how about when we start talking about high-energy physics data analysis? So this is sort of a very condensed view. A very condensed view of what data analysis is doing. So, we have, as we saw at the beginning, pattern recognition of particles, selecting data, classifying events. You can sort of summarize that as basically trying to find a summary statistic that takes all the detector elements, so 100 million sensors, and for what we usually do, we try to turn that into one number that we're going to use to perform statistical inference. So, that's what we'll call T, our summary statistic. And that's basically the definition. And that's basically the definition of this variable on the x-axis. Okay, so we found a good variable, and then what we typically do is we make a histogram and we compare simulation here in red and blue with the data in black, and we use that as effectively a density estimator to compute a likelihood and do in colliders frequentist parameter inference. And one of the things that's important here is that, as a histogram, we sort of know what the density should be, it's sort of like Poisson. Sort of like Poisson distributions for each bin. And we know how the Poisson mean depends on our physics parameters of interest. Okay, so we can sort of then think about machine learning that may come in in this part, sort of defining a summary statistic, versus machine learning methods that may affect our sort of modeling of the likelihood and whether that may, for instance, lead us to need to worry about uncertainties. So I think for basically all those I think for basically all those tasks that basically go into defining the summary statistic, what that's effectively going to do is determine the sort of power versus size relationship of your statistical test. But it's not going to make, it's not going to be wrong. If you choose a worse statistic or a worse variable, if instead of the four electron mass in this Higgs analysis, you chose something that was basically completely insensitive to the difference between the signal and the background, you just wouldn't have a very good statistical test, but nothing would be wrong. Test, but nothing would be wrong. And so, this is sort of a question of optimality, and it's not really clear that there's any concept of uncertainty on, for instance, machine learning models used to find patterns of the detector, ignoring systematics for the moment. However, there's lots of machine learning models that may affect effectively your y-value predictions in these histograms. Things like making a fast simulation tool, things like background estimation techniques. So, anytime you're affecting the y-value predictions here, you're The y-value predictions here, you're basically affecting your prediction of the mean of the Poisson distribution in that bin. And if you get that wrong, you're basically building in a sort of misspecifying the model. So that's certainly something we have to be concerned about in terms of uncertainties. So did we learn a good fast simulation or backward estimation technique? We'll eventually determine how well our model will be usable for statistical inference and how compatible it is with the data. So that's certainly a place where I think we need to be worried. That's certainly a place where I think we need to be worrying about uncertainties on our machine learning models and how well we fit them to our training data. But it's sort of curious because we'll talk a little bit in the sort of second half of this talk about some of the uncertainty quantification techniques, but in a lot of cases, it's not clear that you actually want to use them. So as an example, in many cases these for instance, your inability to fit a good model may just look like what we often talk about in terms of systematic uncertainties. In terms of systematic uncertainties. So here's my quick example. Our simulation is not a perfect model of the data. So here's some distribution. A simulation is in blue and the data is in pink. But we have calibration procedures to correct them and we use data. We have a good control over what's in our data so we can correct our simulator. Now we want to make our simulation faster so we fit a machine learning model as a fast surrogate. Great. But the surrogate is now neither a perfect model of the simulator or the data. So you could say, okay, I didn't. Data. So you could say, okay, I didn't fit a perfect model of the simulation, so I should have some uncertainty because that'll affect sort of my predictions of backgrounds and things like this. So we could go and use some epistemic or model uncertainty quantification techniques. Great. Alternatively, we could also just treat this as another simulator, imperfect simulator, imperfect model of the data, and use these calibration procedures. And however, we would define a systematic uncertainty associated with that. So even in places where you might say, okay, we need some uncertainty. You might say, okay, we need some uncertainty on this model. Well, maybe we should just say we need to go calibrate that model to our data. So there's not always a clear picture of when you actually need these uncertainties, but I'd say that's about as far as I've gotten. You know, sort of a case-by-case basis when you're in doing your HEP statistical analysis and deciding what you actually want to do. That being said, we'll talk about some examples now of how you actually quantify these things. Okay, so uncertainty estimation approaches. How am I doing in time? Okay, so. During the time. Okay. There's a huge number of uncertainty estimation or quantification techniques. We're not going to go through them all. We'll talk a little bit about ensemble methods and Bayesian methods. But actually, the difference between aleatoric and epistemic uncertainty modeling is actually sort of interesting. Remember, aleatoric uncertainty is that intrinsic randomness in the data, which we can often model with probability distributions. We have sort of, the data is going to sort of even tell us what those distributions may look like. Distributions may look like. And so, not surprisingly, if you don't have a deterministic process, if you want to model a distribution, you can use basically probability distributions in the machine learning world. So, instead of, for instance, predicting a single value of your target y for a given input x, you can predict the parameters of a probability distribution. So, in this case, the outputs, I'm going to say, well, I'm going to model the uncertainty of my data with a mixture model, Gaussian mixture model. And what I'm going to do is for every input, I'm going And what I'm going to do is for every input, I'm going to predict the mean, variance, and relative weight of those two Gaussians. And this is called sort of a density network or a mixture density network. So now your neural network's job is to predict the parameters of a probability distribution conditioned on the input value. So that works pretty well. But you have to choose what distribution you want to be using. In this case, it's a mixture model. More flexibly, you can use generative machine learning models. So generative machine learning models are basically trying to approximate some density. Approximate some density. And effectively, they do that by trying to transform noise into data. So you want to take some noise distribution, find some mapping, some function of some noise here, Z. It might be a Gaussian that you choose. You want to find some function that's going to map noise into something that looks like a data sample so that the distributions are approximately the same. There's many generative machine learning models out there. Perhaps one that's used the most in this context is called a memorializing flow, which basically says that this transformation leads to Basically, it says that this transformation needs to be bijective, so you can guarantee it has an inverse. And you basically want to choose this function so it's very easy to compute its determinant. And the reason you want to do that is because then you can use the good old change of variables formula to explicitly evaluate density. So if I know p of z, because it's sort of some sample, and then I can compute this determinant of this function, then I can evaluate the probability of the data point under this model. This is just sort of a cartoon trend. This is just sort of a cartoon transforming data from your noise into something that looks like a data sample. We're not going to get into the details of the model, but they're just very powerful tools. Here's an example of using them for modeling the distribution of some scattering process in high energy physics, so E plus E minus to three jets. And this is sort of all the variables that you'd need to model in that process. And this is sort of the fact that the machine learning model, all these different The fact that the machine learning model of these densities can do a very good job of modeling this density and allow you to sample it faster. Epistemic uncertainty doesn't have such a clear picture of how you should do this modeling. It's not clear how you model what you don't know. Maybe there's some good discussions we can have about that. But for instance, if we have some uncertainty about a lack of best model because we don't have enough training statistics, we have to sort of figure out how to answer the questions like, what if the The questions like What if the what models or networks could my method have fit, or what are the uncertainties on the weights of that I fit to my data? So those are the kinds of things you might sort of think about here and there's sort of prescriptions to do this method ideas about how you might be able to go about estimating this I wouldn't say any of them guarantee coverage in any way so one of them that's quite popular is called deep ensembles. Effectively what you do is you randomly initialize them Do you randomly initialize the weights of your model many times and train them all effectively in parallel? And then you sort of take the average and the variance of all those models, the average being a prediction and the variance being your estimate of sort of what kind of predictions you could have gotten of your model uncertainty. Sometimes that's coupled with bootstrapping, sort of randomly sampling the data when you're training each of these models. So the, oh, right, so here's an example. Tudor talked about this yesterday with optimal transport. It turns out With optimal transport, it turns out you can also use sort of an importance weighting technique. So, roughly speaking, you want to predict some data in some region, some distribution in some region of data, and you've got some other controlled data. Instead of defining an optimal transport map, you could sort of define a re-weighting procedure, importance weighting procedure using neural networks. And then, effectively, the question is: well, if I want to model data in region D by re-weighting data in region C, Data in region C, where the reweighting is some neural network, what if I got the wrong weights? So, this is an example from this same analysis, actually, from a different experiment from Atlas, where we derive these weights. And this is sort of if you reinitialize the model that estimates these weights and retrain it, you sort of get this distribution of, this is the relative weight prediction. So they can vary a fair amount, which would basically give you this gray band of variations of what the yellow prediction could look like under basically retraining. Look like under basically retraining the model. I can't guarantee that those error bands are sort of some coverage about where the true model or the data truly lies, but it's sort of what you can get with ensemble modeling. The other very big class of tools is Bayesian modeling. So instead of just having a neural network that has well-defined or deterministically defined weights, you assume that sort of each weight has a distribution on it. You define a prior on that. On it, you define a prior on that. Then you'll compute both a posterior on the weights and you'll use some sort of density model for your prediction. So, this density model is basically your aleatoric uncertainty, just like we talked about before. You say, okay, my target prediction, given some data and given some set of weights, should be a Gaussian, and we'll model the parameters of that Gaussian. But then you also have to model this weight posterior, the posterior of the weights given the data of the training paper. Data, the training data. And typically, this will be estimated using some summation. But the real challenge here is the fact that it's basically extremely difficult to compute this posterior over the weights. There could be millions of them, and there's intractable integrals involved in this process. Since you can't really do this exactly, there's many sort of approximation techniques, and it's especially challenging because it's sort of well known that these are the space of solutions for these. The space of solutions for these weights is highly multimodal. So, you know, there's things like variational inference, stochastic gradient, long-term dynamics. We're not going to talk about all that, but there's a huge variety of techniques that either sort of try to explore the variations around one mode or try to find the sort of peaks of many modes of possible solutions. Okay, good. I'm almost started. I'm almost starting. So, there's not been a lot of work in sort of Bayesian neural networks in Heinrich. Neural networks in high energy physics, as far as I know. Please correct me. If you know of others, here's one example: again, trying to fit some scattering distribution using a normalizing flow, but also sort of approximating a posterior over the weights of that neural network to try to say, okay, well, even though I fit this sort of central model in blue, there's some variation around that given that I only had a finite set of trees. Okay, so sort of to wrap up, I think it's important. I think it's important to sort of think about how and where we might need to worry about machine learning model uncertainty or machine learning uncertainty with energy physics. There's a lot of research into how you estimate that. It's not clear that they're always, they guarantee coverage or they're well calibrated. Some examples in the backup. So we sort of need to examine this for each application we want to apply it to. Certainly there are places I think that it's probably necessary to really be thinking about this kind of uncertainty, things like fast simulation or machine learning based. Fast simulation or machine learning-based background estimations. You might want to worry about uncertainty in a data acquisition process for real-time decision-making. I thought it was very interesting in Anne's talk, we were talking about simulation-based inference, and there was a whole part of the model just for calibration. Because otherwise, if you just simply interpret the sort of likelihood ratio model without calibrating it, who knows what you're actually getting out? Yeah, anomaly detection, and there's probably many other places it shows up. So, I think we can certainly think. So I think we can certainly think about where we need to apply these models, if we need to apply them, and if we do, sort of how cost efficient are the current methods for our needs. So that's sort of where things stand at the moment. Thanks. Can you maybe do one or two questions directly for this talk? Anybody? Francesca. Yeah, I was just curious about this way where you re-initialize the models and kind of run through to get the uncertainty. It seems intuitive, but does it have some like deeper statistical analog, or is it more just a kind of empirical? I mean, I think it's sort of connecting with the idea of sort of like bootstrap uncertainties from statistics. I think the real challenge is whether you can guarantee. The real challenge is whether you can guarantee that it has what you get out has any real meaning in this case. Because you don't even need to resample the data and you get some variation. And in fact, they tend to be better when you don't actually do the bootstrapping on the data. So I think it's the idea of, yeah, I mean, I don't know that there's an exact guarantee that this will give you something meaningful. Just what exactly do you mean by the English market? Oh, you so when you s whenever you're gonna train the model, you take a random sample for the initial value of all the weights? Okay. And it's just resampling that initial value and restarting the training procedure. It's not the kind of tuning part, it's that. You could sort of do that, but it's not typically what's done. So you don't resample the data. You don't resample the data, you just resample the way. You don't resample the data, you just resample the weights, but the data is always fixed. You can resample the data. Empirically, it's not been shown to give any additional benefit. Thanks a lot, it was a really nice talk. So, this might be coming out in a discussion, in which case you can tell me to keep thinking and come back to it later. But when you kind of went through the different types of machine learning uncertainty and kind of acknowledged that we sometimes visualise them in terms of the same. That we sometimes visualise them in terms of statistical and systematics, but they're actually coming from different sources. Can you say a bit more about situations in when doing that parallel could lead you to either the wrong conclusion or to do the wrong thing? I'm particularly thinking about, you know, when you correlate things or is there a situation where if you were to treat your machine learning uncertainty in the same way as you do systematics and I don't know. Systematics, I don't think I don't improve a little bit of going around because that one is a cool idea, but I think. Oh, okay. Let's see. I think, well, you know, I guess you could sort of think about, so this modeling uncertainty, you could sort of think about using one of these uncertainty quantification techniques, and it would give you some variations of your model predictions. I guess you could profile them. Again, it's not guaranteed that the band that you get out has some sort of coverage. The band that you get out has some sort of coverage guarantees. But alternatively, if it was like a fast simulation, you can go and calibrate it. I think that would probably be a much more safe procedure. I think in general, when you're talking about this sort of aleatoric uncertainty, that's just sort of when you want to take into account the fact that there's not one answer that you're going to get out. So that's, I think, a little bit more easy to sort of implement. Yeah, I guess I'm just trying to understand the risks of meeting. To understand the risks of me as a non-machine learning person, just almost making that kind of almost direct mapping, if that makes sense. Well, so my sort of my statement here is if you're doing this, if you're basically going to do something that's going to determine the variable you want to test, you don't want to look for your signal in, you probably don't need an uncertainty. If you're doing this for like background estimation or signal, then you need to do something, right? Either calibrate it to data or sort of quantify the uncertainty, and then you somehow. Sort of quantify the uncertainty, and then you somehow need to check whether that uncertainty quantification scheme actually gives you some sort of coverage of things that you need to cover. Like if you estimate a background, you still need control data to check whether your uncertainty quantification technique actually would have encompassed the background and that control data. So I think it's not so different in terms of what you practically do. It's just sort of thinking about what. I guess a little bit part of this is also like just because you use a machine learning model doesn't mean you need to go and Use a machine learning model doesn't mean you need to go and estimate a bunch of uncertainties, right? But that may not have answered your question. No, I might have more in about three or four hours. This is already good. So this was the questions to the talk. Now we are starting the discussion session.