Thank you for putting this together. All right, good morning, everybody. I appreciate being invited to this workshop. It's been a lot of fun. Cards group, and I currently, the past 10 years or so, I've been collaborating with Kent Larson's group, which is the group that Stephen and Tilly used to work at when he was at MIT. All of the projects that we've been talking about are very interdisciplinary, involve a wide range of different types of works and skills and disciplines. I primarily work on the lower end of this, so it's somewhat slightly more upstream from the talks and discussions we've been having at this meeting. The areas of focus in our group, I'd say about half of my work, is global health, developing tools for poor clinics and doctors. Tools for poor clinics and doctors, community health care workers, but also we do a lot in consumer health devices in the home. And because of the aging population, and other factors, we reached an important milestone in the mid-90s, which is at that time the burden of non-communicable diseases is now higher than the burden of communicable diseases. So these are diseases such as cancer, diabetes, cardiovascular, et cetera, including mental health. Etc., including mental health, you know, depression is a very large burden. And many of these are increasing over time, so it's an important health concern. And what is the role of technology? So I think of health and technology sort of this way. So health, you know, this is your time trajectory of your life. So we're here, or you're here, and then some point in the future you'll be at point B. So our lives can go in many different directions. So I think of this, you know, you so. You know, some of these trajectories can lead to healthy outcomes or healthy states. Some could be unhealthy. So I think of this as a navigation problem, similar to like GPS. You can also think of it as a controls problem. So you're trying to nudge the person to go into the right trajectory. And, you know, so for example, you're trying to avoid maybe diabetes. And to do this, we use technology and we use an architecture and different platforms. So, you know, I call So, you know, I call this digitally mediated therapy. We did our first prototype system back in 2009, and it works somewhat like this. We have wearables that are measuring different things on our body, and then we have a phone that's running algorithms and detecting the state. So there's a closed loop feedback happening right here, and the phone can present some interventions. But then you also have another feedback loop that's happening on a longer time scale that involves a clinician and a doctor. And the clinician can see the data. And the clinician can see the data that's in the cloud, and he can edit that data, modify the dose of these interventions, et cetera. More recently, we've been working on some more challenging architectures. This happens to be an active project I'm doing with Harvard Med School and Mass General Hospital, where we're trying to do interventions on children. And because children don't have mobile phones, we're intervening through this mediator, which is their parent. So the parents have the phone, and Have the phone, and you know, it adds more complications as you might imagine. So, you know, we have this paradigm that we're working with. But the technology can be not just your phones and wearables, but also all the technology around your house. Nowadays, houses have many different gadgets that are now finally start to work together, which is an exciting time. So, this could be compressed and simplified into this, which is basically you can lump all that technology together into this one box. All that technology together into this one box, which is IoT and AI, which includes everything, and you have this feedback loop. So, the other philosophical point I'd like to make is that I think of technology as training wheels, right? So, when you learn to ride a bike, you put on the training wheels, but once you learn to balance and to ride the bike, you can remove those training wheels and then just ride a bike on your own. You know, there's disagreement over this, but I feel that I really care about. But I really care about improving and building people's agency and self-efficacy. So I feel that once, it's not always possible, but when you can, it's nice to, at some point, work towards the goal of removing the technology so that let the person manage their health and disease on their own. So what are some of these tools and what are some of the categories of technology that we use? So of course there's mHealth is probably the most popular. Probably the most popular. We look at all this type of information that can now be digitized. I don't particularly like the word digital biomarker because I don't know what digital actually means. Is it an analog biomarker that's digitized or is it coming from a computer? So I just like to use the word biomarker. There's behavioral biomarkers, physiological biomarkers, there's clinical and there's technology biomarkers. But we looked But we look at all of these different signals that interact with the body, such as everything radiated from Wi-Fi, microwaves, even the smell coming off of your body, sound, infrared, light. And interestingly, a lot of these signals can be used in reverse to do an intervention on a person. So some examples, we know that low-frequency sound couples to your cardiovascular system. In the past, we've made some wearable noise. In the past, we've made some wearable monitors to measure our environment, including sound. Here's an example of a project that this student did for, I think, his master's thesis. So it's a headset that you walk around with, and if there's any unpleasant sounds, this system will filter those sounds in real time and turn them into pleasant sounds. So it's sort of like a reality filter, if you will. And you can see a If you will. And you can see a small YouTube video of that. This back in 2006. We also know another kind of random example. We know that the limbic system is coupled to smell. So some people are interested in using aroma sort of kind of to maybe change our mood or influence our mood. And airlines are starting to use this. So there's one PhD student at the Media Lab, Judith Amorris, who's. At the Media Lab, Judith Amoris, who's using this in a closed-loop sort of system, where if her wearable detects stress, you know, as, and you could define stress in different ways, but you know, looking at skin conductance and heart rate variability, it'll then, she has a little device that sort of dispenses smells and it'll dispense a pleasant smell like lavender to kind of relax you. We also know, and a previous speaker mentioned, circadian. Know and a previous speaker mentioned circadian rhythm, and we know that blue light in particular suppresses the production of melanin, so or melatonin, sorry, and this can modify our circadian rhythm. And we know that circadian rhythm is very important for a variety of things. The way that our body metabolizes medication and metabolizes food, so it's related to weight gain, and a lot of this can be altered. This can be altered or modified or improved by adjusting the lighting in our house. So, for example, as some of us, as we grow older, like our circadian clock gets a little bit weaker, so sometimes you want to reinforce that so you have stronger circadian rhythm. This can be done through technology. An interesting research that also came out of MIT is sort of this emerging treatment, which is still under development for Alzheimer's. Development for Alzheimer's. So basically, by stimulating various sorts of frequencies in the brain through lights, you can clean up some of the plaque amyloids in the brain. And you can look up this article. But it's pretty fascinating that there's very deep effects that light and sound in our environment can have, which are sort of unpredicted. Hardware platforms, we do use some commercial platforms, especially ones that are programmable, like the Samsung Galaxy Watch, which lets you have access to the raw data and allows you to run your own code. But we make a lot of our own hardware, and we also make hardware for the home and connect with different devices. So, you know, we call this the Internet of Things. Some of the custom wearables that we make, so we do the usual thing that we've been talking about in this conference, sort of calibration. About in this conference, sort of calibration. You know, this is one of the students that I shared with Stephen and Tilly. So, we do a lot of calibration of the accelerometer on a vibration plate, but also do a lot of testing with students and different types of physical activity. And then, you know, the usual stuff that we've been talking about the last couple days. But we've also added sleep, and we have a couple really nice wear sensors, so we can detect wear time pretty well at low power. Wear time pretty well at low power and low cost. But the other thing that maybe is a little bit special is we measure affect, and as many of you know, there are important brain-gut pathways. And because of this link between our digestive system and the brain, we know that our affect affects our weight gain. I recommend this paper here as an example. An example. And we know very well the autonomic nervous system, how it impacts our different organs, parasympathetic, sympathetic. But most of this in the past has been done just through heart rate variability. But we like using skin conductance. And I learned a lot about proper use of skin conductance, which is not trivial. And it depends on many different things and how to interpret skin conductance data through my work with Ros Picard at the Media Lab. At the Media Lab. And there's an example of skin conductance signal 24 hours. And here's an example of one week's worth of skin conductance data. One thing I'd like to point out for those of you doing sleep research is the signature of skin conductance during sleep is very distinctive. So we can actually detect sleep also using skin conductance. And the other, one of the chronic problems or classic problems of Or, classic problems of skin conductance is you don't know whether the affect or whether your sympathetic activation is positive or negative, right? Like, I can get a big peak if somebody told me a joke, or I can get a big peak if I'm angry, and I don't know which one is which. So, one thing you can do is people have shown that skin conductus is ipsilateral, not contralateral. So, you can, so right corresponds to right and left correspondence. So, right corresponds to right, and left corresponds to right, left. If you wear two of these skin conductance signals, it turns out that the left side of your body is suppressed when you have negative affect. So, this has been a very interesting finding that we're trying to develop further. So, this, for example, somebody who they were going through grieving, so one of their close relatives passed away. The left side is left hand, sorry, the red is left and blue is right. And blue is right, and you can see even during sleep, the left side of the signal is suppressed, and then a few days later, they're back to normal. So, there's an interesting thing about skin conductance, which this kind of opens up other possibilities to look at valence. Another classic problem is screen time. Most of this is done through self-report. So, we have a wearable that measures not just total light, but the Not just total light, but the actual different colors of light, the components of light, red, green, blue, and IR. And the insight here is if you don't have a TV in the room, even if you're moving around, all the colors move together, right? But if you have a TV in the room that's showing some video, there will be some changes in the light because on a large screen, the colors are changing. And we use this and a variety of different algorithms to predict and detect if there's a screen, if you're in the room with a T V. Obviously, we can't guarantee that the person is actually looking at the TV, but most likely they are. And then we can output the probability of being with a screen, and we are able to distinguish between TV and computer. Now, the accuracy of this is not that great, but it's better than self-reporting. Not that great, but it's better than self-report. And it's also better if you have, so this is on the wrist, but it works better if you do a pendant or something that's always pointing forward. Another classic problem is respiration. So respiration is very important because I work a lot in substance abuse, drug addiction, and many types of diseases and even mental health states can be detected by your respiration rate. So, although many wearables promise to give you respiration, it only works for regular breathing, but for irregular breathing, you really need a custom sensor. So, you know, traditional respiration monitors are like this, you know, capnography, you can use PPG, things that you put in your nose, you can wear respiration belts. But we use radar. So we build our own little radars. You can make a tiny little radar circuit that you clip onto your shirt. It works through clothing and it measures not. Through clothing, and it measures not only your heart, but also your respiration. So, the low frequency is a respiration, the high-frequency pulses are your heart. The other area that we do a lot of work on is what I call enhanced EMA. So, what do I mean by enhanced EMA? So, there's a traditional, you know, I assume we're all familiar with the traditional EMA questionnaires. There's actually a photo of a phone we used in our first just-in-time intervention study. And back then, these little phones. And back then, these little phones were available, which were really nice, but you can't find them anymore. So, what do we add? So, in addition to obviously measuring the responses to the questions, we're interested in, you know, can we add some other measures that improve the fidelity or give us some feedback about the fidelity of the responses and the physiology of the person? So, one of the things we do is we record the elapsed time. So, basically, we measure when the person presses the button and the elapsed time between the button. The button and the elapsed time between the button presses, and the amount of time spent on each screen. And so, if somebody is just pressing next, next, next, and not really thinking about it, we can actually remove that or weight that less. And that helps us with seeing which responses can we really trust or believe. The other thing we do, you know, an observation is we spend a lot of time staring at our screen. So, if you're staring at your phone, what can you do? So, we looked at, also in Ross. Also, in Ros Picard's group, we developed this algorithm that lets you measure heart rate using the camera, which was mentioned in a previous talk. We were one of the first groups to implement this actually completely running on a phone. So we integrated this into EMA. We were able to measure your heart rate while you're just even speaking or not speaking, but basically holding it in your hand. In your hand, and here's some of the responses. So, as you can see, it is sensitive to motion, but we were within seven beats per second or so, seven beats per minute in accuracy. But we've moved away from this because a lot of people, when you sign the consent, are a little bit scared to do anything that involves cameras. So, we're now using this, which is the accelerometer. So, most cell phones have an accelerometer, and you can actually use the accelerometer to derive heart rate, and it's actually just a Heart rate, and it's actually, you know, just as good as using the camera. And it's completely passive. This works even if you're holding the phone in your hand, or even if it's in your pocket, you can get your heart rate and respiration just from the accelerometer and the phone. If you want to be more sensitive, you want more information, more precise information about your pulse, you can ask the person or the subject to actually put their finger on the phone camera. And with that, you're able to get these really nice pulses. These really nice pulses. This is your photopletismography pulse, and we're able to mine this for additional information. I won't explain the physiology behind this, but I could send you a thesis if you're interested. But the point here, just to demonstrate how sensitive this is, each one of these lines is an average pulse of a person. Each line is from a different subject. All of these, so this is one group, there's another group, all the same age range, 20 to 25. Age range, 20 to 25, these are MIT students. The only difference between these two groups is this group exercises regularly, this group does not. And you can see just from the shape of the pulse, we can clearly distinguish who exercises regularly, who's a runner and who's not. And this is reversible. So at a younger age, our vascular system has a fair amount of plasticity. So you can actually move from this group to this group if you start working out. If you start working out, so that's something else we're building in. And we have a score by doing a variety of analysis on the pulse signals, we can actually derive a score, and we're able to, people are able to use that to sort of track their progress. The last thing I'll mention related to phones is: I think the future of medicine is molecular sensing that's used in smell and odors, so we call that olfaction. We know that dogs, We know that dogs are very good at detecting smell, and you can rent or buy dogs today that can detect each of these different things. Even something like stress and fear. So the smell of fear is actually a real thing, and you can train a dog to actually detect fear or detect stress. But most of this is used for things like epilepsy, so the dog can tell you before a seizure happens. The dog will warn you so that you can sit down, etc. So, we're trying to do that on a machine. So, my collaborator is Andreas Merschin. You can look him up, Google Wired magazine, Andreas. There was a nice article on him last summer, which also had a video. He develops hardware that we're trying to embed into the phone, and we're currently working on a prostate cancer project that can detect prostate cancer just from the smell. Lastly, related to EMA, we have, as you know, the EMA, we have, as you know, the smart speakers are now becoming very popular. And we're now taking a lot of our EMA work and moving it to smart speakers because, for example, my mom doesn't like using, she's over 80 years old, she doesn't like using phones too much, but she really likes using the smart speakers. These are very easy to use, so I think a lot of the EMA work can be easily migrated over to voice assistants. And it also is good for children. So children don't have phones, but they really love using it. Don't have phones, but they really love using these devices. So that opens up new opportunities for research. On the Internet of Things, as I mentioned, we're putting sensors everywhere, including sofas, chairs, cars. Beds is a very popular place. There are a lot of commercial products now. We make some of our own. We're also building smart mirrors that use a variety of sensors behind. There's a half-silver mirror. You could use a variety of sensors that do a scan. That do a scan of you. One of those sensors is the thermal camera. And we know from the thermal camera we can detect things like breathing. This is your nose. So when you exhale, your nostrils get warmer. When you inhale, they get cooler. This is used a lot for baby monitors. We built this app so we train a classifier to detect the face from a thermal image, and then we zero in on the nostril. And compared to a respiration belt, we get really good results. It tracks, we use radars, and we use. Use radars and we use voice assistance. So, the other thing we do with voice, besides NLP, listening to the words, you can actually measure your voice quality and sound. So in particular, one of the questions we ask is, do you have a cough? And let me hear you cough. And then if the person coughs, then we analyze the coughs, we do cough segmentation, then we analyze the cough, we fit it to a decay exponential, and from those parameters... And from those parameters, we're able to get some pretty interesting and useful clinical information that helps. I don't expect you to read these numbers, but the point is it enhances our prediction for a diagnosis for a variety of diseases. Like we can distinguish between asthma and COPD better if we include the cough data. Rich? Yeah. Let's take another couple of minutes. Yeah, this is the home stretch here. So, areas of collaboration. So, I think. So, I think one large area that's emerging is what we call digital phenotyping. And, you know, we have many analogies with DNA and genomics, et cetera. But now, the way that we use our digital devices in our home and so forth leaves a digital fingerprint, and it's unique to the person, and we can mine that data for a variety of things. Some of this started. Some of this started early work at the Media Lab. We had this platform called Funf. This code was later used by David Moore and other people to build additional platforms. And now there's a growing community of people doing digital phenotyping on the phone. So basically we, as I mentioned, we get things like heart rate just from the accelerometer on the phone. But we also measure obvious things like GPS. We measure calls sent and received, SMS, text messages. And receive, SMS text messages sent and received, how you type, etc. There's a lot of measures that you can get just from your phone and from the environment around you. We complement this with this. So we have some games that are based on standard psychological paradigms that do neurocognitive measures. But we put it in the form of a game to make it more engaging. And with this, we're able to measure, we have specific tests, specific game modules that measure things like impairment, impulsivity, cognitive bias. Impairment, impulsivity, cognitive bias, etc. So, this helps us guide the digital phenotyping data. Because in the digital phenotyping data, we're just trying to take a bunch of data and trying to see what maps over to specific psychological constructs, like anxiety or depression. But this is a complement to that. So, the bottom line is all this stuff generates a lot of data, and there's a lot of challenges, but also opportunities for data analysis. And if any of you are interested in any of this stuff, Of you are interested in any of this stuff, feel free to reach out and I'd be happy to explore collaboration. So, thank you. So, Rich, do you have APIs for those games? Do you offer APIs that people can develop those? Many of our tools have an API, so you can take our module and embed it in somebody else's mobile app. Right. Yeah. Yeah, and we just go to Right. Yeah. Yeah, and we just go to your website and look, and we contact you. So we do have a lot of our apps in the Play Store, but you can't use them. They're sort of like in a locked mode. We only put them on the store for distribution. But if you contact me, we can set you up so then you can, or talk to your team so you can use them. Does your screen time sensor sense tablet usage as well, or is it just television? Yeah, for, because the screen on a tablet and for Because the screen on a tablet and phones is small, if you're detecting that type of screen time, that does not work very well with these light sensors, but you can do it by running another app in the background that monitors the usage. And for example, we have an app that detects if you're holding it and if there's a face in front of you. So you can actually monitor or verify that the person is looking at the screen. So some of the um the smartphone things you're doing, I'm specifically thinking about the the pulse or uh with the with the fingertip. Has any of that been validated? Yeah, we we've validated it with some of our patient population in India with a cardiovascular hospital. So we we primarily work with patients with different stages of atherosclerosis. Yeah. And is that published? We have a paper that's close to being published, which validates that. So I can. I was thinking there have been some apps, like the Instant Blood Pressure app was another one that went through, and then it was later, I don't know if it was ever validated, but it was later shown to be really poor and dangerous. And so I'm just wondering what steps you're taking, because this is all really exciting, and I don't want to sound negative, but I'm just curious what steps you're taking to make sure that they're actually precise and valid measurements. Yeah, no, absolutely. I completely agree with you. So I completely agree with you. I care a lot about clinical validation, and that's why MIT doesn't have a med school, but I collaborate. Most of my work is with outside MIT, with doctors. And we wrote a review paper on the blood pressure and PPG. And we talk about the instant blood pressure as well. The blood pressure, these sort of measures can give you relative blood pressure, but absolute blood pressure, I don't. Absolute blood pressure. I don't believe any app that claims to give you absolute blood pressure because there's too many variables. But looking at the shape of the pulse, that's something that's pretty solid and it's reproducible across ages. So we've done it from like 20-year-olds up to 80-year-olds. The the open finance you just uh talked about. I I guess maybe in twenty eleven when it was developed, uh store. Was developed or table and were a bit more relaxed. But what you could do now, they've really tightened everything. So, do you have a solution around that? Yeah, yeah. So, the privacy is a moving target. And yeah, back in 2011, it was crazy what Google let you do on the phones. There was pretty much no privacy, and you can track everything the user is doing and what programs are running. That's become more strict, but our More strict, but our digital phenotyping platform uses the latest Android permissions and API. And also, with regards to GPS, for example, and calls, just to let you know in case any of you work with GPS, we anonymize it because, for example, GPS, we're just interested in the relative motion. We're not interested in absolutely knowing where the person lives. So, we add a random offset. So, we actually, when we look at the GPS data, we don't know where the person. We don't know where the person actually is. We just know their relative motion. And with the calls, all of our numbers are hashed. So I can tell you that you receive three calls from three distinct people, but I don't know what the phone numbers are. So we do take steps to kind of ensure privacy. Yeah, so we're sort of collecting some of this data. And right now you can't collect call logs from any platform. Um and and the GPS you can collect Apple. But the hack will prompt you all the time, saying there's this app, sensing your in the background. Yeah. Yeah. So like these platforms are moving to a direction where you can't really do monitoring, even though in theory it's amazing. In practice you can't actually utilize these platforms. Yeah, uh a lot of the processing needs to be done on the phone and then sent to the server. Like you can't send to the server raw pictures. Server raw pictures, but you can do through processing locally on the phone, and that gets around some of the concerns. But yeah, Apple is a little bit more strict than Google. Since we're running a little bit late, I'm sure that Rich is around and he he could talk to you over over a cup of tea. Uh let's thank him one more time and we must be uh quite a bit late at night.