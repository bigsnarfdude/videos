Thank you, Ling Long. I would also like to thank Helen for this invitation. My generation, we kind of miss the trend of machine learning and then we are catching up, we are kind of learning some aspects of machine learning. So it's a great workshop for me to be here to learn some new stuff. This work started. This work started in collaboration with C. Lou and Tom Viteritz. Tom is a well-known computer scientist, and C. was our former PhD student, now a postdoc at Fred. Okay, so maybe I'll start with an announcement. So, you know, we have this International Indian Statistical Association. They have their conference, you know. They have their conference happening in India in December, between Christmas and New Year. So if any of you are interested in going, let me or any of the organizers know. The place is actually in the southern tip, slightly southern and western tip. It's kind of beautiful, you know, next to the ocean, Arabic and kind of Indian ocean. So these are some pictures that I got from. Are some pictures that I got from kind of Google image search? And it's like one hour kind of flight to Maldives. And you know, so some of the places are beautiful. Cochin itself, you know, is like an island chain. So and you know, there are backwaters, mountains, close by, etc. So now coming back to the talk, I'm going to give you an overview of this anomaly detection kind of Anomaly detection kind of methods in machine learning. At least the way I understand it, the machine learning community, they are kind of they're interested in this problem for various applications where invariably they are dealing with maybe high-dimensional or complex data. They are really into kind of scalable computation. And so imagine that, you know, maybe into it, you know. you know maybe into it you know um you know the the the the tax company you know they have their all data you know for millions of individuals and they really want to run these algorithms on them and then somehow you know there is that trust worthiness and so or reliability so in some sense to me you know maybe these algorithms or machine learning approaches also need some good statistical properties so that's Statistical properties. So that's how I want to view it. So I'm going to talk about these statistical properties on that. Now, the way this algorithm is developed is, you know, essentially, you know, what they are trying to do, they are trying to kind of assign a score, they call it anomaly score to each of the data values. And somehow they want to identify through the scores which are kind of anomaly points, data points or kind of bad points. Kind of bad points. And it seems there is a connection to our statistical estimation theory. So we can kind of try to understand these machine learning approaches through our statistical understanding and kind of see what is going on. So I'll definitely give you a couple of examples and the way it works. There are benchmark data sets developed by the machine learning community. So I'm going to run the algorithms. So, I'm going to run the algorithms, a couple of algorithms on a couple of data sets and see what is going on. So, in a big picture, this is what is happening. These kind of anomaly detection problems arise in many contexts. It could be like fraud detection, it could be computer security issues, data cleaning. Maybe sometimes we have bad data and we want to remove them. It could be scientific discoveries, but they could. Discoveries, but you know, they go beyond even environmental monitoring or sensor networks and things like that. So, what we have in a high level that we have maybe a random sample of data, x1, x2, x. These are coming from some mixture distributions, and these are like high-dimensional, maybe complex. And so, in this setup, in this mixture. In this setup, in this mixture kind of setup, a small fraction of the data is coming from one population, and then the other fraction is coming from kind of more standard population. And so G0, that represents the probability distributions of normal data points. G1 represents the distributions for the unusual or anomaly data points. And this guy, this alpha, is some kind of long-run fraction of anomaly. Kind of long-run fraction of anomaly points. So that fraction alpha is unknown. Typically, you know, in application, you know, all these are unknown. Alpha is G1, Z0, they're unknown. And the other good thing is in this machine learning setup, there is often actually a sample of normal data points. So there is actually a reference set of variables. And that's one of the kind of good things. And so that's the setup. Now, what happens is. Now, what happens is for these kind of machine learning approaches, because the data are coming from high-dimensional complex structures, there is no interest in kind of estimating the distribution functions. Maybe it's too challenging. So, that's not what is typically done. But what is typically done is, even though sometimes there are density-based methods, I haven't seen much. Methods. I haven't seen much use of that. You know, there are some machine learning support vector machine algorithms, cluster algorithms, even hidden. But mainly what people use nowadays are some kind of normalized MONI scores through a series of different types of machine learning algorithms. And I'm going to describe some of them. And then, you know, methods are kind of evaluated in, you know, I don't think there is a consensus. Sometimes people are interested in talking. Sometimes people are interested in top K outliers. Sometimes people would say we have anomaly scores. Middle list choose a cutoff. If the score is high, we will say this data points are anomaly points. So there is some decisions taken on it. And what are kind of good anomaly scores? There are kind of no clear-cut definitions, but one idea is that we have these outlying observations and we have inly or normal observations. And we have inline or normal observations, and so in that anomaly scores, in that scale, we should see some kind of gap between inline and outline observation. How to achieve that? That's kind of a million-dollar question, but there is no kind of principle way to target, to go there. So, but you know, this, you know, the literature is pretty large. I mean, if you type like anomaly detections on Google Scholars and loopholes. On Google Scholars and look for machine learning kind of papers or conference proceedings, you will see hundreds and hundreds of them. And so, you know, one way they understand anomaly score is these are easy to interpret, you know, kind of score besides the outline less of the data values. All these algorithms, they are kind of scalable algorithms. So, from a computational point of view, they are exciting. But there might be some loss of There might be some loss of information, and you know, that's maybe the original problem is too challenging, and so maybe that's okay. And in terms of the actual algorithms, machine learning algorithms, you know, now there is a long list, but here are some, you know, they are interested in some ensembles of mixture models, with Gaussian mixture model. There is an algorithm called isolation forest, it is very popular, is different than. Popular is different than kind of our random forest. Something they developed based on kind of Tukey's half-space idea, they call it half-space mass algorithm. Here is something very popular known as lightweight online detectors of anomaly, you know, based on random projections and things like that. And you know, the whole setup, you know, actually, you know, gives rise to many kinds of interesting statistical questions like what this algorithm. Questions like what these algorithms are doing, are there any kind of principle way to understand this? So, just to give you kind of an idea of what some of these algorithms are trying to do, so when they are considering some Gaussian mixtures, maybe they are thinking about one Gaussian distributions, and then we know the density function, and we can evaluate the density function at a particular complex. And if it's coming from two Gaussian density, And if it's coming from two Gaussian, like mixture, mixture of two Gaussians, again, if we provided we know the densities, we can kind of compute the value of the densities. And so if there are like three mixtures, we'd be able to compute. And so what this algorithm tries to do, it just, you know, looks for kind of different mixtures and takes some kind of averages. And it's very much like the density-based ideas, but it's a question, you know, for kind of high-dimensional. You know, kind of high-dimensional data values, whether we can estimate these densities well. But if we can, in principle, we'll get an anomaly score based on this kind of averages. This is what isolation forest does. It's a depth-based idea. So, typically, what it does, it kind of assumes that data values are in some kind of hyper rectangle and then it goes for random partition. And so, it partitions the data points till kind of they are isolated. Kind of they are isolated. So, here is my rate point. You know, it requires maybe two random partitions to isolate. Maybe if we kind of repeat the process, maybe it requires three partitions to isolate. Maybe in another way, it requires two partitions to isolate. And so now for kind of different instances, we are just looking into this kind of number of partitions we need to kind of isolate that. So, that's one way to understand this isolation. To understand this, this isolation kind of forest. So, this D1, D2, these are the, as I said, number of partitions. There is another way to understand this one. Maybe we can think in terms of tree structures like binary trees. And so, you know, the partitions will invariably give rise to tree structures. But this D1, D2, they can also be understood as depth of a particular point on the tree. And then it's like what we are getting is some form of average. We are getting some form of average depth, and that's our kind of annual score. So, maybe a point you know that we can isolate easily, maybe it's one of the outlying points, and so the depth will be small. So, this hub space-based idea is like you know, you have, you know, you are kind of thinking about a convex hull of the data points in a high-dimensional space, and maybe you have a mechanism to randomly generate the half-spaces, and then what did you do? Spaces, and then what did you do? You know, you have this point, you know, you have a half space, and you are kind of counting how many data values are there on the side where the data point is. And so, you know, there are kind of two points here, four points here. You will just say, you know, I am going to assign a mass 2 over 6 on this side. If this is my kind of half space line, you know, now again it is 2 over 6, but if it is this line, it is 1 over 6, and again, you do some kind of average. Average. So these are kind of techniques that is done. Maybe one more. So we have this lightweight online detectors of anomaly. What it does, it just chooses a kind of a random directions and then it projects the data value along the random directions. And then it will just try to draw a histogram. And so then you get some weight for a particular point. And then you just look into kind of the weight or the height for kind. The weight or the height for kind of different random projections, you again take kind of averages of them. So, maybe in the absence, you know, they try to compute some density or something similar. And then, you know, obviously, we get an average of them. So, all these algorithms has one interesting property. You can compute them very fast. You know, even for millions of 10 million data sets, you know, with hundreds of different variables, you can run. Different variables, you can run the algorithm and you get something. Now, if we think like this reduction is happening and then we have this anomaly scores, we still kind of have to kind of understand what is going on in terms of statistical properties. So, what happens is, you know, from the original data x1 to xn, typically, you know, we will have this y1 to yn. Now, these are anomalous, and obviously, if the original data Scores and obviously, if the original data is coming from a mixed set, so the anomaly scores will also come from some mixed distribution. Now, these are one-dimensional distribution. So, you know, the score for kind of, you know, if the anomaly score is y is coming from, you know, one distribution P1 that represents the anomaly scores for the unusual points, and P0 is the anomaly scores for the kind of non-profits. Anomaly scores for the kind of nominal or normal data points. We still have alpha here, but you know, it's a question whether, I mean, ideally, you know, that alpha should carry. Now, there is something tricky about the way some of these machine learning algorithms are done. So, we also have anomaly scores for the normal data, right? Because we have a reference data set, so we will also get this one. But there is something tricky about some of the algorithms, so they don't. Out some of the algorithm. So, they don't necessarily just run the data, you know, kind of independently and compute these ensembles of course. Like for isolation forest, you know, they will run the, they will construct the binary tree with a set of points and then they will kind of run the normal nominal data points as well as kind of the mixed data point kind of separately along those trees. And what happens is, you know, in a sense, these cores may not necessarily be actually an IID sample. Be actually an IID sample, it may become actually more like what we call an exchangeable sample. And so, one thing that happens through these reductions is best to assume that you know our anomaly scores actually are an exchangeable sample from this kind of mixture model rather than an independent or like you know random IID sample. But still, you know, all the issues are the same. You know, we are interested in estimating this alpha, the fraction of anomaly points. Alpha, the fraction of anomaly points, that's in the data, and also you know, maybe you know, we do not know P0, P1, etc. Now, this kind of thing, you know, will remind you of like hypothesis testing problem. We have kind of normal null hypothesis, we have alternative hypothesis, as if we are kind of interested in estimating the fraction of alternative hypothesis. So, so, so that will, you know, this idea will help you that, okay, you know. This idea will help you that, okay, you know, in some sense, we know this problem in our statistical domain, and so I'm kind of going to take an approach along that line, like more like hypothesis, this is testing that. Now, even though we have a kind of a reference sample and kind of a mixture sample, this is a two-sample problem. There is a simple way to reduce it to a one-sample problem. And the way to do it is essentially we look into the reference kind of anomaly scores. Of anomaly scores, anomaly scores for the normal data, look into their empirical kind of distribution functions and then essentially try to compute kind of p-values. Now, p values will be discrete, but you know, we can add some kind of uniform random variable to make it continuous. And then what will happen, we will have in terms of this p-values, you know, now it will become a one-sample problem where one of the distributions, so you know, here. So you know here this guy was unknown, this guy was unknown, this was alpha. But we can assume this to be like a uniform 0, 1 distribution. And these are our like p values. These guys are like p values. Some are coming from null distribution, some are coming from alternative distribution. So that reduction is not that difficult, but it's actually one of the good things. The other thing is this kind of p-values because we are using These kind of p-values, because we are using the empirical distribution functions, they will depend only on the rank of the anomaly scores. And so, this is also kind of another kind of good aspects of that, that what we are computing that essentially depends on range and giving some. Now, there are still kind of challenges here. So, for example, if we really want to estimate this alpha parameter, you know, so that. Parameter, you know, so that reduction, I'm assuming that, you know, one is uniform, the other represents the anomaly scores for the unusual points, and alpha is the fraction. There are still what we call the identifiability issues, meaning that this mixture can be rewritten as another mixture. And so, what it means is that what we can in effect estimate is not alpha, but we can estimate what we call alpha naught. Alpha naught is the minimum. Alpha naught is the minimum values of gamma for which this guy is a valid CDF. So you can kind of solve P1 and if you solve P1 you will get this probability distributions minus 1 minus kind of alpha times P0 over kind of alpha. But then for minimum value of gamma, if this is a CDF, that is actually estimable and identifiable. Otherwise this mixture is not identical. So one of the challenge here is that the problem is only identifiable. challenge here is that the problem is only identifiable if alpha equal to alpha naught otherwise you know alpha can be greater than alpha naught and then you know we'd not be able to estimate you know identify you know all the anomalies and this requires a condition and typically it says that the the support of p1 should be should be should be strictly contained in the in in in in 0 and 1. Now comes the you know if we are thinking about our statistical theory there are ways to kind of build on it. Ways to kind of build on it. Here is one bit and kind of a little bit from our semi-parametric theory. We kind of kind of get an kind of empirical estimates of these anomaly distributions, probability distributions. And this need not be a CDF in strict sense. So what we can do, we can look into that kind of L2 distance or kind of Euclidean distance and maybe we can isotonize it. And once we isotonize it, we are going to get an estimate. If we are going to get an estimate of the anomaly distributions. And so, again, maybe there is a picture, but for the sake of time, I will kind of skip this part. And what happens is this distance for a value of gamma has some very interesting properties. This invariant under monotone transformation, meaning that it will depend only on the ranks. So, sorry. So, yeah, and that means that for exchangeable anomaly points. You know, anomaly points, we can treat them as kind of also IIG points because everything depends here, depends through the lens. The other thing is, this is a non-increasing convex functions and it has some very interesting behavior. It's essentially zero for gamma greater than alpha naught, but you know, it's a positive kind of function for gamma less than alpha. So it kind of goes down and it kind of becomes flat, and we are interested in that kind of elbow point. And so, for the elbow point, you know, And so, for the elbow point, you know, there are again ways to do estimation theory. Here, we have some non-standard rate, like you know, we can think about some cutoff and you know, when it hits the cutoff, you know, that's our estimate. And so, there is a way to estimate this guy with kind of square root of n over like some constant time log, log, and rate. And, you know, so, so, so, so, this theory is kind of well understood due to a very nice paper by Roeth Batro and. Paper by Rohit Patro and Bodhi Sen from Colombia. So we can do that. Now, where we are kind of going from here, for example, you know, maybe we are interested in what we call local false discovery rates, right? At the end, because we want to maybe set decisions rules, maybe that controls false discovery rate, or maybe use local false discovery rate to kind of set a cutoff. And so, you know, this problem initially, maybe we are interested in this kind of, you know, local false discovery rate. Uh, you know, local positive discovery rate computation, but that's in high dimension. Instead, what we are getting is maybe a proxy of that, right? So, maybe this is kind of simpler, but it's still required. So, we can estimate alpha, this is uniform, so that's constant, but we still need a density as and again, you know, if we as if we somehow can assume that this is concave, then we don't even need those bandwidths and etc. We can just look into those convex least concave measure enter, you know, kind of from the isotonized regression, you know, we can kind of get. Vigra, and you know, we can kind of get an estimate of them. Now, why are these useful? So, to summarize, you know, all these ideas seem very reasonable, but the algorithm depends on some key steps. One is that there is no big loss of information for that anomaly scores. Identifiability is QE, so we need the support of P1 to be content in 0, 1, and then maybe it's a desirable property that this. Property that this anomaly score distributions, you know, that's like a concept, you know, on its support. But the good thing is, these are actually statistically testable steps. So we can use statistical idea to kind of figure out each of these steps. And that's why I feel like the statistics can play a role in, you know, essentially in providing some insight about the reliability or trustworthiness of this algorithm. So, you know, how much time do I have? Five minutes? Do I have five minutes? At least five minutes. Okay, so I'm going to give you a couple of like numerical examples how this works or this does not work. So these are kind of benchmark data sets again created by machine learning people and so one data set they call it cover type. So you have to assume that these are national forest areas and then you know they have many different land covers. So they are using 54 different attributes. Different attributes, not necessarily I have a list of all these attributes. You know, some of you know, and then these are land croppers, so I do not know, you know, one may mean something, two may mean something else, etc. So maybe conifer, maybe some other, etc. And so the way this experiment is done, some of the land cover type is kind of hidden. And so that is like our anomalous kind of data points, and then we are going to train. Of data points, and then we are going to train on the other land cover types, and they are like normal. And so, we have 5000 kind of you know nominal normal data, and then the mixture contains 2000 anomalous points, maybe some overtype that are not there in this set, and then 3000 that are typical. Now, I am going to use two of these known algorithms. And so, when we generate the anomaly scores, you know, for the, you know, here you see it is like a mixture data for the A mixture data for the kind of mixed data set, whereas for the reference data set, it's not a mix, it's like a unimodal kind of one. And so we kind of feel that maybe there is a way to kind of figure out the fraction of anomalous force. Now, this is that distance functions. Remember, we mentioned about this L2 distance that we use to minimize for different values of gamma. And it's like an elbow function, it's a convex kind of non-internal. Function is a convex kind of non-increasing function, it goes down and it becomes zero essentially. And we are interested in kind of this point. And you can see that this point is very close to like about 0.4. So it's kind of hitting the estimate. So somehow the second algorithm, that random projection algorithm, is exactly 0.4 for whatever strange reason. And here is a little bit of kind of intuition. So if we are kind of estimating the If we are kind of estimating the distributions of P1 and its density function, so essentially all the anomaly scores they are kind of between 0 and 0.4. There is almost nothing beyond. So here we see that the support is contained in, strictly contained in between 0 and 0. And we don't need even statistical theory, you know, because we can test whether the support is strictly contained in or no. We need maybe the, we need to consider the maximum value, and maybe we need to apply some extreme value theory. To apply some extreme value theorem to kind of figure out, but that's not needed, you know, you can kind of see that support will be strict. So, here it seems like there is no loss of generality on any of these steps. Looks like this is also fairly concave, if you know, like decreasing function, if not like a very nice concave, but somewhat concave. So, it's kind of doing very well in this example. So, here is another example they call kind of later recognition data sets. So, we Later recognition data sets. So we have these 26 letters, right? So 13 of them were used or maybe faster, you know, I think 13 of them were used for kind of defining these are like normal letters. And then the other 13 were kept aside thinking that those are kind of anomaly kind of letters. And so the algorithm would never see them. And so same story, you know, we have 5000 kind of nominal data and then we have a link. Data and then we have a mixture data containing 2000 anomalous letters, like you know, and then the 3000 kind of standard letters. And so, again, here number of attribute is 16. I am not sure exactly how they are kind of created, but imagine you have images of letters and then they are using some particular 16 features to create these data sets. Now, in this case, what happens is we have these anomaly scores for the mixture data. Anomaly scores for the mixture data, anomaly scores for the kind of normal data, they look pretty similar. They are not that different. It's not like bimodal or kind of mixture anymore. And here actually, you know, the mode is that, you know, there is a problem of kind of identifiability. So if we look into the distance functions, it's no longer 0.4, it's kind of this is, you know, we can only estimate alpha naught. So it's hitting more, more like, you know. Is heating more like you know, in one case, the estimate is 0.16 is in the random projection algorithm doing slightly better is 0.26, but is not anywhere close to kind of 0.4. And there is a way to check that. For example, we can plot, we can estimate the distributions and the density function. It looks very flat here, but you know, I was looking at the number. So, even you know, at 1, the density estimate is like 0.03 something. Like 0.03 something. So there is still a little kind of value here. And that's exactly showing that this problem is not identified. So identifiability is the key. Is it actually contained? Maybe or kind of decreasing. That is probably happening. There might be other ways we can see some loss of generality, but that's also testable, you know, in our statistical sense. So with this, I will kind of conclude saying that here we have some interesting kind of machine learning. Some interesting kind of machine learning problem. We can purely look at it from a machine learning angle. So, you know, my student and Tom and I, we wrote a paper in JMLR that focused mainly on the computer science aspects, like PAG guarantees, you know, all kinds of bounds and etc. So the computer scientists, they really like that. But how do we understand what is going on, etc.? I think this is where statistics can play a role. And I think maybe for some of the message. Know maybe for some of the machine learning algorithms, we have, you know, if we can connect with our literature, we have also ways to kind of understand their reliability, trustworthiness. And at least we can see when it works, when it does not. It may be a challenging task to come up with a new algorithm or the algorithm that will kind of improve and kind of say, here is the best possible answer, right? Because that's what we statisticians, you know, spend our lifetime thinking we have to come up with kind of. Thinking we have to come up with kind of the best possible answer. Maybe that's not possible, but maybe this is one step. I'll stop here. I have, you know, one or two things.