Okay, perfect. Thank you. Okay, so as we know, genomics data in general has been, there are many technologies now and we can get a lot of data out of that. And ideally, doctors want to use these massive data to develop more personalized medicine initiatives. And so, just to give you an example, I'm going to be working with this. I'm going to be working with this data set from prostate cancer, trying to find a proteomics biomarker in this case. And there have been many studies that have found very strong association between proteins and prostate cancer glees and score. However, we are not sure if those associations are really have a causal effect on the gleason score, or it's just association between that. So, ideally, we want to identify those. So, ideally, we want to identify those causal relationships to diagnose cancer aggressiveness in this case or any other disease. So, a simple way to model that can be to think of a linear regression model. But we know that when we have data from an observational study, with that linear regression, we will not be able to establish causality. So, a way, and this problem also happens not only. Problem also happens not only when there are confounding factors but also when there are reverse consolidated problems that even measurement errors. And mathematically, what is happening is that the covariates, some covariates or all covariates are correlated with the error terms. So that assumption that we usually have in least squares estimator is being violated. And as the previous speaker was saying, the estimators are biased, but are also inconsistent. So we cannot establish the consolidation. We cannot establish the causality relation that we are looking for. So, one option to one solution to that problem is to use instrumental variables estimators. And I go quickly through this because it was previously introduced. But the main idea of instrumental variables is to try to divide the signal that is in that X. This is the hope to know what part of that X is uncorrelated with the error term from the part that is correlated with the error term. Correlated with the error term. But how can we do that? How can we separate those two parts? So the instrumental variables can help because instrumental variables are not any kind of variables. They have to satisfy three important assumptions. So one is that the instrumental variables needs to be associated with your covariate. That way, that relation will be able to get the part that has some signal there. But the other part will be that is not correlated. Part will be that it's not correlated to the error term. So it can be correlated to y, but only through the path of that covariate x. So in that case, you can get that uncorrelation that you are looking for. So this is just a visual effect that one way of getting that is to project that covariate in the space of the instrumental variables because the instrumental variables are not correlated with the error term and thus the projection will not. term and thus the projection will not be correlated. And we know that one way of getting that projection will be through a least square estimator. So that's how we can interpret the two stage least squares. In the first stage, you just project into the space of instrumental variables and then you get the prediction or the predictors or that projection and you put it into your second stage, which is what Dr. Waipen was. Dr. White Penn was showing to us. And it can be proved that this two-stage instrument, a variable estimator called two-stage squared is consistent, is a consistent estimator of the original regression parameters. So this is not new. It has been first introduced in the 30s by Philip Wright, although there are some controversial opinions that it was his son who was a genetic statistician. A genetic statistician Sewell Wright, which was the one that first talked about or introduced it to status quaries estimator. Now, more recently, instrumental variables estimator has been used in health data. And I have a couple of references here. And on top of these ones, our previous talk was giving us even more recent references on instrumental variable and deep learning. Instrumental variable and deep learning literature. Okay, so what is different from Mendelian randomization? Not too much. It's just that in Mendelian randomization, the genetic variants are used as instrumental variables. And the justification is that those are randomly assigned during meiosis. So they will satisfy by design the assumptions of an instrumental variable. The of an instrumental variable. So, in my case, what I want to see if I can find any protein that will be associated with prostate cancer aggressiveness. And what I will do is germline variations as instrumental variables. Okay, now the problem is that the dimensionality of the data that I will be working with will be very large. Will be very large, so I cannot just use that to stage least square estimators. I will have many instruments to choose from, and I will have many possible proteins to choose from, although only some of them are going to be relevant for the problem that I want to explain. So what can we do in that case? So, one option, sorry, I went too fast there. Uh, did I need something? Okay, uh, so I need to find valid instruments and when I'm measuring a lot, uh, and I, yeah, I already said this, and only a few of the proteins are going to be codel. Okay, so what I will try to do is to use a variable selection method, and I can and try to combine Mendelian randomization with a variable selection method to try to find those instruments. The main problem. Those instruments. The main problem that we face is that this is what we call the good, the bad, and the ugly, because we know that some instruments are going to be good instruments, and a representation of that is given by this thing here. So there is an association of the instruments with the Y, but only through the path of the covariant. There's no other back door that connects those two. Now we have also. Now, we have also the bad ones, which are the ones that are not really associated with X. And the instruments of the variable selection method can probably help us to distinguish those two, the ones that are not really associated with the covariate. The main problem are the ones that they are associated, but they also have another association with the wise. And those are the ones that we call the ugly because those are the ones that are the most dangerous ones, the ones that a variable. Dangerous one, the ones that a variable selection method will get in, but they are going to damage the whole instrumental variable process. And something similar happened at the level of that second stage when we have to find the good proteins in my case, and a variable selection can help to distinguish those goods from the bad. Okay, so we design a new estimator that we are Design a new estimator that we are calling Thrive that has three stages. In the first stage, we used POST-LASU, which I'm going to call it later PLASU, to regress protein expressions on the genetic variants. I'm explaining what is POSLASU in a second. In the second stage, we are going to use those predicted expressions to regress them with the clinical outcome of. With the clinical outcome of interest, and use another time the post-la suit to make a selection of what are the exposures that we are most interested in. But there's also a third stage. It's not a stage, but there's a third check where we do a sensitivity analysis to test if there's any violation of the instrumental variables assumptions there. So before I go. So, before I go into the penalization levels, I want to say what Post LASU is in case you don't know it. So, Bellioni was the first one to introduce that methodology. And what it does is it uses LASU to select the variables, but then once the variables are selected, it uses a least square estimator. Because basically, we are trying to address bias in our coefficients introduced by confoundings, but then if we By confoundings, but then if we apply LASU, our estimators are going to be biased again. So Benone was proposing this post-LASU estimator to remove the bias introduced by LASU. It has some limitations in particular when LASO is, if we are selecting more variables than where we can fit with stability at least square estimator. But we explore other methods in that case. Explore other methods in that case. I'm not going to go into those today. I don't have enough time. Another technical detail in our algorithm that I'm not going to explain too much is that tuning these models is also very challenging. Deciding on the level of penalization is tricky because what happened in the first stage is not necessarily the optimum for the second stage. So in our algorithm, we design the level. We design the level of penalization in a way that we are looking simultaneously at both stages when deciding what is the level of penalization. With all that, that is what we call Thrive and we compare it against other estimators. Okay, so I'm going to show you the simulation, the results of a simulation study. It's not this, it's less than this, but we. Is less than this, but we basically look at having a sample size of size 500, running 500 times the simulations. The concentration parameter there measures the signal to noise ratio in the variables that we generate. We try with 90 and 180. The results I'm going to show today are with 180. And then the endogenous variables, endogenous means that the covariate is correlated with. The covariate is correlated with the error term, so you need to address something there. Um, they uh we try with 5, 10, 50, 100, and in the paper we have 500 as well. The number of instruments are the ones that I'm going to show you today how they grow from 5, 10, and all that. But another thing that we need to know is among all those that we have, how many are relevant? How many are real are the ones that we want to detect? Are the ones that we want to detect? So there are, we tried with one instrumental variable per endogenous variable or five instrumental variables per endogenous variable, unique and distinct. And then we are also going to be in a case where we have only five important variables. So five among five or five among 10, 5 among 50 or 100. Okay, so the estimators that I'm going to go. Estimators that I'm going to compare. So, one will be the least square estimator, which is basically ignoring any confounding in the data. The LASU, which is also going to ignore the problem of endogeneity, but it's going to at least select variables. Then the two stately squares, which will acknowledge the problem of endogeneity, but it's not going to make any selection. So, as the number of variables increases, Number of variables increases relative to those that are important, it's going to fail as well. And then we tried having a regularization in the first stage, but not in the second stage. That is Berlioni's estimator. So he didn't try any selection in the second stage. The last two that was suggested in 2015 by Lin, Feng, and Li, and those will have an L1 regular. Those will have an L1 regularization in the first stage, followed by an L1 regularization in the second stage. And then our method, which is the, as I told you before, the post-la-soup, post-la-soup, but also with the sensitivity analysis and with the different grid to find the penalty parameter. And we included in all cases an oracle which will be a two stately squares on the true variables only. On the true variables only, because this is the simulation, we know what they are. So, in this plot, I'm showing you the case of having 10 proteins of which only five are relevant and having only one instrumental variable per these proteins. In the x-axis, I'm showing the number of instruments, and in the y-axis is the mean absolute deviation of the estimator. So we can see. Estimator. So we can see there that our estimator is very close to the oracle. As the number of instruments increase, it doesn't affect that. And that's because we are doing that selection and we can identify not only the relevant instrumental variables, but also those that do not violate any of the assumptions. The LASU-LASU, which is also doing Which is also doing a selection, is having some, it's having more problems. And here we are looking at the map, so you can expect the bias in those, right? Like it's using LASU, so the estimators are going to be biased, and thus it's going to affect the median absolute deviation of the estimators. You can also see the two-stage square here, that at the beginning, it's okay because we're in a low-dimensional setting, but as the number But as the number of instruments starts to increase, then it gets really bad, and as bad as the least square estimator that was ignoring endogeneity at all. Okay, so then what we also looked at was how good we were in terms of variable selection. So here in the y-axis, you have the Matthew correlation coefficient that is a combination of the true positives and the true negatives to have a unique measurements of those. Unique measurements of those. And our estimator is doing very well in terms of not only identifying the true positive covariates, as well as discarding the ones that are not relevant here. And the solid versus the dotted line means if we are, it's the kind of grid that we put into consideration. So it's kind of a very detailed thing here, but you can see that we can. Here, but you can see that we can improve depending also on how we make that grid of lambda in the penalization to select. Okay, then in the second case, we increase the number of endogenous variables. We still have five that are relevant. And you can see now that both PLASU and LASU, the PLASUSU or LASU-LASU are not doing bad. Are not doing bad, although the other methods are doing so bad that we are a little bit out of scale here in the lower part. Plusu Plusu is still doing better. But we have other simulations where we continue increasing the size of the endogenous variables. And as that size of the endogenous variables increase, then that bias that LASO introduced is less relevant. It's more important to identify what are the relevant variables than the bias that introduced. Than the bias that introduces in the estimator. Of course, in this case, the two-state least square estimator is not going to do well, and it has the same problem as the least squares or the LASO ones. Okay, this is again in terms of variable selection. The Plasso is better than using the Lasso-Lasso bit. Okay, and I know I don't have time. Okay, and I know I don't have too much time, so I want to finish by presenting just a few results of the real case. So, this data set comes from the Cancer Genome Atlas, and we have a response which is the GLISEN score to measure the severity of the tumor. Our exposures are expression level of 196 proteins that are measured by the RPPRA. The instruments are germline. Instruments are germline genetic variants, and these have been measured on the same patients. That's something that is also different from our previous talk. But the germline genetic variants are measurements on the same patients as the proteins and as the glision score. And the sample size is 352 patients that we have data on. So here are some results that compares the so plasmo. Uh, the so plus 2s is our estimator, uh, is the thrive here. So, zero means how many zero coefficients we have, and plus means how many with a positive association. Last to one stage is when it does not include any instrumental variables estimators. Again, zero means that the covariate, the protein was discarded, and plus and minus are the signs of that association. Of that association. So you can see that although LASU and also ours discard many of the proteins, we are discarding much more. And we identified only two proteins associated with the glisons core, and both have positive association. The same happened when we go through LA SU, LA SU. There are more. There are more proteins that are found by Lasu-Lasu that Plasu-Plasu, it's not finding relevant. And I'm going to show you a little bit how the plots of those look like now. Okay, so in these plots, this has been proposed by Burgess and other co-authors in 2018. And what they look at is if the assumption Is if the assumptions of those instruments are good assumptions or not? Because at the end of the day, if you don't have good instruments, you're not going to get good causal effects. And so what it looks at is it compares the genetic association with protein expression versus the genetic associations with your outcome. So you want those to lie in that diagonal line and to be similar because that will mean that you have. Because that will mean that you have a monoton, it will satisfy the assumptions of having these monotonic causal effects. And you can see that for both proteins identified by Thrive, that those assumptions seem to be well satisfied. At least we have a strong evidence here, visual evidence that those are being satisfied. Now, when you look at proteins identified by Lasso-Lasou that were not identified by Plasu-Plasu, you can By plasso-plasso, you can see that those monotonic hostile assumptions are not being well satisfied, are much weaker here. Okay, so with that, I just want to thank Drs. Joe Watson and David Keplinger. They were my students when we were working on this project, but they graduated before we published our paper. But they're still work in progress, and we're working together to try to. Working together to try to get this paper out. And I want to conclude by what I presented today. So when we have new omics technology, we can measure many more features and that stimulate a lot of biomarkers discovery. But the focus of the talk today was not only to find associations, which is most of the bulk of the studies that we see in genomics, but also to find causal. Also, to find causal relationships in the data. We are proposing a new estimator that we call Thrive because it's three times robust in terms of all the steps and the selections. And instrumental variables in that case have been combined with variable selection methods so that we can not only select relevant instruments, but also select relevant exposures and do sensitivity analysis. Do sensitivity analysis on their way as well. So, Thrive integrates in a nice way genetic and probiotic information in these prostate cancer progression analysis that I show you today. And I want to say that I did not present anything on deep learning. So, deep IV, as the previous speaker is one of the references, but in fact, Waypen was much better. But in fact, Waypen was much better than me citing deep learning and instrumental variable estimators. So, yeah, so with that, I want to thank you all for being here. And this is a picture of Stanny Vancouver. If you have time to come and visit. Thank you.