Set of variables that I denote y, y1 up to yp. And our goal is to estimate their partial dependency structure, which means estimating which pairs of variables are dependent conditionally on the rest of the variables. And the graphical model will be a representation of this dependency structure, where each variable will be represented. Where each variable will be represented by a node, and each edge will represent a partial dependency. So, for in this small example, there is an edge between y1 and y2, which means that y1 and y2 are dependent conditionally on y3, and there is no edge between y1 and y3, which means that they are independent conditionally on y2. So, this is a standard problem, and the motivation. Problem and the motivation problems that we are aiming at is applications where the number of variables p is large. And this appears in many genomic applications, for example, in gene-wise association studies, where the variable can represent the level of expression of some gene. And also in neuroimaging data, the variables can be the level of activation of some regions of interest. Of some regions of interest. So, to simplify the problem of inferring the dependency structure, we can assume some distribution for the vector variable y. So, in a standard setting, we would assume that y is a multivariate normal vector, which we suppose here for simplicity to be centered, and with some unknown covariance matrix Ïƒ, which is a p by p positive definite matrix. Definite matrix. And in this case, the problem of graphical modeling is simplified because it is well known that if you basically, if you parametrize the multivariate normal distribution with the precision matrix, so with the inverse covariance, then the conditional independent statements between pairs of variables will be encoded in the zero entries of the omega matrix. So whenever an entry omega ij is equal to zero, this means that y and This means that y and yj are independent conditionally on the rest of the variables. So, these conditionally independent statements are encoded in the graphical model and can be also encoded into some binary latent variables which are denote z and which will represent the graphical model. So, our goal will be to estimate the precision matrix omega and the corresponding graphical structure that I denote z given That I denote z, given a sample of the vector y. So, one useful fact that we will use is that under the assumption that the vector of observation is Gaussian, then the distribution of one variable conditionally on the others can be also written or is a normally distributed variable. So, the density of one variable, for example, the Of one variable, for example, the last one yp, conditionally on the rest of variables that I denote y minus p, is a univariate normal distribution where the mean is simply given by the inner product between y minus p and a vector that corresponds to one colon of omega, actually the off-diagonal colon, off-diagonal coefficients of the p colon of omega, which I denote with this notation, actually divided also by the Uh, actually, divided also by the of the divided by the diagonal coefficients. So, this is for the mean, and the variance of this normal distribution is the inverse of the diagonal coefficients. So, equivalently, this means that I can write the distribution of one variable as an inner product between the vector y minus p plus some noise that is Gaussian with variants. That is Gaussian with a variance omega pp minus one. So, what this conditional distribution gives us is a connection between Gaussian graphical modeling and linear regression in the sense that a variable can be expressed as linearly dependent on the other variables. And we will explore these similarities between Gaussian graphical models and linear regression for. Linear regression for accelerating Bayesian computation in this model. So, what I will talk about now is first how we can do sparse Bayesian inference in the context of high-dimensional Gaussian graphical models. Then, I will propose R algorithms or MCMC algorithms based on metropolis within Gibbs steps. And for this, we will propose two types of modes, which can be Of moves, which are can be either local or global. I will explain what it means. And finally, if I have time, I will present some numerical results that show that our algorithm is much faster than state-of-the-art Bayesian algorithms. So, first, how can we do sparse Bayesian inference in this context? So, in the context of high-dimensional graphical models, which means when the Which means when the number of variables pH is large, it is reasonable to look for a sparse solution, which means a sparse graphical model z, so with a small number of edges in the graphical model. Note that in this case, the space of graphical model is sub-exponentially in p because it is of order 2 power p squared. So, in this case, recovering a sparse graphical model would also Recovering a sparse graphical model would also be more interpretable. And in this context, it is also useful to use the Bayesian framework first for integrating prior information on the variable relationships and their dependencies. Also, because with the Poisson distribution, we will be able to quantify the uncertainty remaining on the precision matrix omega and the associated graphical model Z. For example, one way of quantifying this uncertainty would be to prove. Quantifying this uncertainty would be to provide the sets of graphical model Z or to provide the marginal variable inclusion probabilities, or we could also provide some credible sets for the parameters, either marginally on each coefficient of omega or on the full matrix omega. However, there is a cost to that, is that Bayesian methods are generally more computationally intensive than frequentist methods such as penalized maclides. Such as penalized maximum likelihood approaches, such as a GLASO. And also, there is an additional issue: that specifying a reasonable or sensible prior distribution is not straightforward in this context. And I will explain why. So, actually, to design a prior distribution in the context of Gaussian graphical models, there are many two approaches that have been proposed so far. The first one is to use the conjugate prior. The conjugate prior for the multivariate normal distribution, which is the G Wishard distribution. It is defined in a hierarchical manner by first specifying a prior pi on the set of graphical models, so pi of z, for example, setting it as a product of Bernoulli distribution on each edge, and then setting the distribution, the price distribution of omega given z as. Given Z as the Wichard distribution, which is a distribution parametrized by the number of degrees, which is by B, and the positive definite matrix D, which is called the shape parameter. The drawback of this prior is that the normalizing constant is generally intractable unless for specific decomposable model. So computing the posterior distribution implied by this. Computing the possible distribution implied by this prior requires some computational engineering and also to make it really fast, some approximation on the Poissier distribution. Another approach is to use what I call almost factorizable prior, which are priors on omega that are defined through univariate distribution on each of the coefficients omega ij. So in this case, pi of omega would be proportional to a product of a univariate distribution pi Lj, which could enforce sparsity through the form of by using some shrinkage distribution pi Lj. But it's not like this valid prior distribution on the space of positive definite matrices. We need to add also the shape. We need to add also the shape or the parameter space constraint, which is imposing that the matrix should be positive definite. And one common approach that uses this type of almost factorizable prior is an approach proposed by Wong in 2015, which specified this univariate distribution on each coefficient as a continuous pipeline slab. As a continuous spike and slab, so a mixture of two normal distributions, one which is called the spike, which is the first one, which has a small variance S0 squared, and the second density is the slab, which has a variance S1 squared, which is typically chosen as large. And the interpretation is that any coefficient omega ij that is small will be interpreted as coming from the spike and quantum. As coming from the spike and considered as not significant and not included in the graphical model, and a large coefficient will be interpreted as coming from the slab and significant for the graphical model. Actually, this continuous spike and slab prior distribution is a sort of approximation of an ideal discrete spike and slab where the spike is replaced by a direct measure at zero. So, meaning that in this case, actually. meaning that in this case actually the the the posterior distribution the prior and the posterior distribution of the space of matrices would be supported on truly sparse matrices and this is a prior that is sort of ideal in the sense that it has also a provable near optimal asymptotic property so a near optimal posterior concentration rate so we will focus on this type of spike and slab prior Slab prior. So, first, the posterior distribution that will be our object of inference is classically defined as proportional to the Gaussian likelihood times the prior distribution. To compute or to sample from this Poisson distribution, Wong proposed a colonwise GIP sampler for the continuous pack and slab prior, which runs quite fast for a P of order 100. Of order 100. And that was already a great advance for the field of Bayesian computation for Bausian graphical model. The problem is that the implied posterior for the continuum spike and slap suffer from a lack of consistency and power, which means that in general, the posterior distribution on the true graphical model will not converge to one, even if the number of data samples goes to infinity. Goes to infinity. This is the case because, in general, coefficients of the precision matrix that are truly non-zero but small and smaller or of smaller than the spike standard deviation, this coefficient will not be included in the model even with infinite amount of data. And a small example to show you that this happens even in very simple examples. So, for example, even with Even with a model with only three variables, where you have on the true precision matrix two on the diagonal and the coefficient rho on the off-diagonal coefficients. Then, if I compute the posterior distribution on the true model versus the value of this value, this parameter row, which represents the smallest value of the coefficient in omega zero. Then, for the continuous packet, Then, for the continuous Pyke and Slab prior, unless rho is sufficiently large, I will have a very low probability apostle on the true model in comparison to the exact spike and slab, by which I mean actually the discrete spike and slab. And in terms of power, I will have the same behavior that the continuous spike and slab prior will underperform in comparison to. Will underperform in comparison to the discrete spacing slab. So, one additional point is that it is possible to recover consistency with the continuous spacing slab by making the spikes variance goes to zero as n goes to infinity. So, this is possible if I decrease the spikes variance. That in then in the previous example, I will have the Poisson distribution on the true model going to one. On the true model going to one with enough data. However, there is a problem with doing that: when you decrease the spikes variance, then the MCMC algorithm, the GIP sampler, will suffer from poor mixing. And this is a problem that had been also observed in standard linear recognition. I won't maybe add more detail on this. So, our contribution would be to propose an efficient Be to propose an efficient MCMC algorithm targeting the posterior distribution implied by discrete spike and slab. For this, we will use a metropolis within KIPPS chain. And to accelerate or to have a good mixing of this Markov chain, we will propose two types of move, either local or globally informed. And we'll show that is computationally efficient. So, first, the global idea of our MCMC chain is to use chain is to use GIPS type of updates. So here the GIPS will operate colon-wise. So at each step of the GIPS sample we will update one colon of omega and for this we will use a proposal distribution which here I denote Q indexed by the colon for which it's proposed a new sample. So in the first step I will update the first column, in the second step the second one, etc. And I will loop over the column. etc and I will loop over the columns to update to obtain samples from for the full omega. So to propose samples, ideally in a pure Gibbs algorithm I would use the conditional posterior distribution of one colon of omega given the race in order to update one colon of omega. And actually the first result that we provide is to say that To say that under a reparametrization, we can actually sample the non-zero coefficients of one colon of omega according to a normal gamma density. So, this is one of our first results is that given a vector z that will indicate which coefficients are non-zero in the colon of interest, so say at the colon omega j. Then give. Then, given Z, the posterior distribution on the reparametrization of the coefficients will simply be normal gamma, where the normal distribution has the dimension of the number of non-zero coefficients. So, on the dimension of the L0 norm of these vectors. But the posterior distribution on this indicator of Z is this untractable target distribution. Tractable target distribution, which form does not matter much for now. The thing is, it's intractable because the normalizing result is unknown. And for this, we will use a metropolis testing step to sample from this intractable target distribution on the vector of indicator variables. So, our first proposal for that is to use random wall, which is a type of proposal that is standardly used in discrete space MCMC. MCMC. So, our goal is to propose an approximate sample from this untractable distribution. And the random proposal will correspond to burst-death swap moves, which proposal kernel has this form. But what it means simply is that the kernel propose to move to a new model by either adding a non-zero coefficient or removing a adding or removing a non-zero coefficient or either swap a one with a zero. So this is a type of local mode that is quite standard. So this is our first proposal. The second one, which will allow to make global moves in the space of models, is what we call linear regression inspired in the sense that we, in this case, we use We, in this case, we use the connection between Gaussian graphical models and linear regression by observing that the target distribution that we are actually trying to sample from is analogous to the posterior distribution under a linear regression model and a conjugate spike and slap prior. So, actually, under the model where one variable is regressed linearly onto the others, and by specifying the vector z as the indicator vector, As the indicator vector of the non-zero regression coefficients. Using the conjugate spike and slap prior for linear regression, then the posterior distribution on this vector of indicator variables has this similar shape, which is also an interactable distribution, but in this case, it's much nicer because efficient samplers have been designed for this standard linear regression problem. For example, the Temple GIF sampler by Ezan Elan Roberts or the Inform bus. Iran Roberts or the informed death sampler. And moreover, this posture distribution that we use as a proposal distribution inside the metropolising step will be independent of the current step of the Markov chain. So in this case, our proposal will be an independent proposal, and also for each column, it will be independent the proposal for each column will be independent of the proposal of the other column. Proposal of the other column. And we call it globally informed in the sense that it is still informed by the data. One other observation on this global proposal is that because for each colon index, this proposal distribution is independent, then we can actually sample from this proposal in parallel and also as a pre-component. And also, as a pre-computation step before starting our Gibbs iteration, iterating over the columns of omega. An additional trick that we use is that the proposal distribution using this connection with linear regression can be potentially too concentrated compared to the distribution we are targeting. And to overcome this problem of overconcentration. Overcome this problem of overconcentration, we can add a tampering, which means basically exponentiating our proposal distribution by a power beta between 0 and 1. So if beta equal to 1, that will mean we add no tempering. And beta tending to 0, which means that we propose uniformly across all the possible models of indicator vector of dimension p minus 1. We do that because in We do that because, in general, for metropolitan steps, it is better for mixing to use a not too concentrating proposal distribution. That would lead to better mixing properties of the Markov chain. So, to summarize, our almost parallel algorithm will run like this. So, first, as a pre-completion step, we will sample from P-independent MCMC chains. Independent MCMC chains that will sample from our proposal distribution inspired by the linear regression problem. And after this, we will start our Gibbs iteration. So for each iteration, we will iterate over the columns of omega. And for each colon, we will first propose a model of indicator vectors through a metropolising step. So either proposing for more global proposal. Um, global proposal or form or local proposal and accept it or reject it, accept it with the probability given by the metropolitan acceptance ratio. And then once we have a sample from the model, we sample the coefficients or the reparameters coefficients using just a multivariate normal gamma distribution. A multivariate normal and a gamma distribution. And this is how we And this is how we unfold the updates of the matrix omega. Do I have five more minutes or am I already running out of time? Well, if there are no reactions, maybe I'm still on time. So maybe you can quickly present some results that. Some results that quite show that our method is much more efficient, especially in sparse high-dimensional settings. So I will consider P between 20 and 200. And I will consider also a sparse precision matrix that will be tri-diagonal. And I will compare two of our algorithms. So one that is, we call serial geometric. So, one that is we call serial GIPS, that we only use local moves. And a second one that we call parallel beta, which is our method with global moves and some tempering parameter beta. And we compare our two proposed algorithms to the state of the art, which is implemented in the PTGraph package, which is a great package for Gaussian graphical models when using the G-Reichard prior. So there are The G-Richell prior. So there are two actually variants of the algorithm in this package. So the BDGraph function is a continuous burst time MCMC that sample the precision matrix and the model. And the second function is the bdgraph.mpl which uses some approximation with a partial likelihood. And this algorithm actually only samples the model, so does not sample the Sample the model so does not sample the precision matrix. And for this comparison, we only compare the algorithmic performance and not the inference performance, since actually our method will target a different position than the one targeted by the BRAF package. So we will compare clock time and mixing times. And for the clock time, we argue that our comparison is quite fair because all the methods are actually implemented in C. So here are our So here are our results comparing first the serial Gibbs algorithm with the bid graph method. So on the left panel I have a proxy measure for the mixing or for the mixing time. So the lower is better on the log scale versus the number of variable p. And so our algorithms is for all values of p mixing better than the the bd graph package. The BDGraph package. And in terms of cut time, we are also faster than the BDGraph. So it's only slightly faster than the BDGraph.mpl, but this algorithm only samples the models. And in fact, sampling the precision metric would have an additional computational cost that is not included here. Then we have Then we have also evaluated the advantage of using global moves and temporary. So here I compare our algorithm using local moves, the serial GIPS, with three versions of our algorithms using the global moves with three values of the temporary parameter. And what we observe is that, so on the left panel, I have the clock time. On the left panel, I have the clock time versus P, and using the parallel proposal actually can really decrease the computational time. Actually, we have this thing that when we add tempering, because then we have to compute the marginal likelihood, we have more marginal likelihood to compute under the proposal distribution that requires some additional cost. So the parallel. So the parallel trick also can slow down a bit the algorithm, but we still win for large values of P. And in terms of mixing, which is here measured in terms of expected jump distance on the right panel, we also see that adding this global move through the parallel proposal greatly improves the mixing of algorithms. And in particular, when we add the temperature. So, the take-hart message here is that with parallelization, we can reduce the clock time of our algorithm for large values of p and also we can get better mixing with tampering or proposal distribution. So, this is the summary of the talk. So, what we have proposed in this project is new MCMC algorithm to accelerate Gaussian graphical model. Accelerate Gaussian graphical model when using the discrete spike and slap prior. And all these algorithms are actually already implemented in the MOMBF package, which is a package for Bayesian variable selection maintained by David. For our MCMC, we have proposed two types of moves that help to get better mixing that the state of the art. And in particular, for this globally influenced For these globally informed moves, we can leverage a two-buff of algorithms that are implemented for liner aggregation, and then that can also accelerate our almost parallel algorithm. And in the final stage of this project, we are analyzing the mixing time of our MCMC and we are proving that RMCMC actually enjoys fast mixing time, which means mixing time that are polynomial in the number of samples. Polynomial in the number of samples and the dimension. And with this, I thank you very much for your attention. And I'm very happy to take any questions on this talk. Thank you very much. Deborah, can you hear us? Can you hear us? No, I can't. Very clear. Suddenly, yeah. Okay. Sorry, we had some connection problem. We heard audio perfectly, yeah. Just you couldn't hear us. Thank you very much. Any reactions? Any reactions, comments? Hi, Deborah. This was really interesting. So, if I understand right, you're doing essentially all these parallel linear regressions. And I'm wondering, at what point are you sort of post-processing all the time to make sure you're staying positive, definite, and symmetric? So, that's the trick of the reparametrization. So, when you use a parallelizer, When we use a parallel proposal, we only use them for proposing the indicator vector. But where the positive definiteness constraint appears is when we sample the parameter. But under this reparameterization, actually, this is guaranteed because by sampling from a gamma distribution this parameter VJ, we actually guaranteed that the That the proposed column will be positive definite. The whole matrix will be positive definite. So. Okay, thank you. Thank you. Any more questions? Comments? Hi, Debs. Thank you very much. So I think I, this is a more understanding because I think I lost a bit. So you started with talking about why this inference doesn't. Is like inference doesn't work well for the continuous spike and slab, but then in your conclusion, you were talking about the discrete. So I think I lost the moment where you or maybe like if you can re-explain. Sorry. Yeah, sorry. So there is like consistency and power issue with the continuous spacing slab, but you don't have this problem for the discrete bike and slab. So the discrete spike and slab is probably. The discrete spargence lab is probably consistent on the on the true on the data generating truths. And your algorithm is for which of like for the discrete spec answer. Thank you. Any more comments? Any online questions? I don't see any. And let's thank Deborah again. Thank you. Thank you very much. 