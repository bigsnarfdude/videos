You can stay away for another three minutes. So this is a talk that it's about a work that was done quite a while ago, but we are trying to finish a German paper that comes out of this with some new designs. So it's my excuse to get back to the talk is my excuse to get back to this paper. And the motivating example is going to Motivating example is going to be bandit value estimation and we'll see how that kind of forced us to get some new concentration inequalities which are quite nice. Actually is a score. My computer may go to say it, but you should not. Alright so my collaborators in this work have been Ilya, Claire and Andras and they're all at be And UNRASH, and they're all at deep mind, and the results in a bunch of papers. So, let's start with the motivating example. So, this is contextual bandits. Quite a bit of notation, but it's a really simple thing. You have a context that arrives, it's a random context, an agent or learning algorithm or whatever chooses an action, it's from a finite set, it's okay different values. It's okay, different values, and the reward is received as a response to that. And there is a stochastic model that governs the rewards, and the rewards are in zero-one interval to keep things super simple. And we are interested in selecting policies which have a high value. What is a policy? It's just a map from context to actions. So, reacting to the context that is received. The context that is received, policy chooses an action, it's going to receive a reward. The expected value of the policy, the expected reward that the policy generates as a result of seeing the stochastic context is the value of the policy. I'm going to use u of pi to denote the value of the policy. And other notation is that little r is going to Going to denote the immediate reward that is received given context X having been chosen action A. Setting clear. It's just standard contextual benefits. And we're going to be in the so-called bad setting. But as my colleague would say, mistakes have been made. So you're not empowered to choose the actions. Someone else was choosing the actions. Maybe some policy was followed. This is the specific setting that I'm going to look at. Some policy was followed to collect some data. And this data set was collected. This is the data that we have to work with. This tapos, XI, EI, RI. It's super simple, just IID. And then we are also given this policy that was generating the data. So these probabilities are accessible. So, these probabilities are accessible to us: of like what's the probability of choosing an action? So, it's like point evaluation is possible of this policy. And we're also given some other policy that we need to evaluate, and point evaluation of that policy is also possible. And the goal is to figure out the lower bond, the high probability lower bound, and the value of this other policy. Questions? S is the data. S is the data. Right, yeah, good. Yeah, let's throw the whole talk as it's going to, you know, the data. So, here the lower bond, we want to make it as data-dependent as possible, but it should be a high from the lower bond, and of course the lower bond should be close to the value, so then we are successful if we can do that. And so, one standard approach for approaching this problem would be to maybe come up with some way of just estimating the value. Some way of just estimating the value of the policy and then study the concentration of the destimator and use a concentration result to get an overbound and the value of this variable. And this is indeed the approach that most of the people follow, or most of the literature follows. We are going to do this as well in this talk. And there are many ways of doing this, so I'm going to start with something super simple. Something super simple, the important sampling estimator that everyone probably knows. Then, there is, I'm going to say a few words about something which is kind of dominating the literature of value estimation in this literature, this doubly robust estimator. But our focus of attention is going to be the weighted importance sampling estimator today. So, to start So to start very gently, what is this importance fading estimator? Well, it's whatever you probably already had in your mind of when you saw this problem that you can just calculate these importance ratios of like what is the probability that would have been assigned to the action given the policy that I want to evaluate divided by the corresponding probability on the behavioral policy, calculate all these weights and compute the weight. And compute the weighted average of the reward, it's an unbiased estimator at least of the true value. It's easy to see, but we need a lower bound, a value lower bound, because the value lower bound is motivated that we're going to be pessimistic about the value, and then after that we maybe select the policy with the highest lower bound. That's kind of like the standard thing that people do. So can we get some good concentration results? Get some good concentration results for this. And the problem is that you have all this context, and because of that, if the policies are disagreeing, you know, like given the context, then this WI could be habited. So it's going to be maybe difficult to get tight concentration for this. Maybe even the variance is going to be large. Maybe the variance could be infinite even though the reverse were finite. So maybe this this estimator is not the best estimator. Is not the best estimator after all. So, people are trying to fix this in various ways. So, one of the standard fixes is to introduce a little bias or truncate the weights and that way you can control the tail behavior of the weights. And you can try to make this work. It's really hard to tune the lumbar in such a way that you get good concentration. You get good concentration, and like you have adaptivity to other properties that you want to adapt to. So, we're not going to do this. Other people are trying to do this. The endurance estimator is this doubly robust estimator, and it's here because it is so popular. It's really just a matter of control rates. So, you come up with some other function that kind of tries to estimate the rewards. So, you're subtracting from the offset reward the estimated rewards. Estimated rewards, you apply the relating to that, and you have a different estimator, which is like if this was the reward function, this would be just the value of the policy. So you combine this to, this is probably robust, and statisticians really love this. Because it has some really nice asymptotic properties, and it's also unbiased for a number of to do different settings, so that's why it's called doubly robust, and robust is. Doubly robust and robust this way. So, in the statistical situation that people are interested in, often the weights themselves are estimated. And so, there might not be this exact ratios. You estimate some densities or whatnot. So, the weights are estimated, and then it becomes questionable whether this is a good thing to do. And then the result would say that if either case, if the weights are If either case, if the weights are correct, like they are correctly weighting, then this is unbiased. So that would be the situation that we are in. Or if the reward function was correctly estimated in an unbiased way, then it's also unbiased. Kind of negative. And the effect should be that the variance is reduced if we have a good estimator. Of course, if eta hat is zero, then we get back input and space. Back in Poten's weightings is kind of like, I've been making any progress with our concentration problems. Well, it's not antarically, right? Like, this is kind of going in a different direction in addressing the issues with the variance. So, we're just going to abandon this for the current talk. And concept of weighted importance sampling estimator. This is everywhere in the Monte Carlo literature. People are very early. People very early on observed that this estimator has much nicer properties in certain ways. You are giving up something though when you're using this estimator. So the logic of this estimator, if you haven't seen it, is that all the w's i's have an expectation of one. And so you imagine dividing both the numerator and the denominator. denominator by n and the lower flash numbers to ask you that like this is it should be ball part good estimator so at least that's kind of like the reasoning for why the bias goes away and this is not completely unreasonable and but at the same time at the same time this is a much better estimator in terms of some whole variance control so in particular if the rewards were in between 0 and 1 the variance In between zero and one, that the variance of this never blows up. So the variance is always control. It's like the heavy care situation is basically gone. If you have some very large weight, you divide by that large weight, so everything is under control. So you gain some bias, but the variance is at least under control. And you can get actually an upper bound and a variance in terms of the In terms of the weights. And this was kind of the starting point for our journey into trying to get some concentration results for these estimators. So what happens in nice situations, this empirical, so this quantity here is an empirical quantity that's completely under your control. Like you can calculate it. And this tends to be very nice and small in Nice and small in a lot of examples. And so the estimator has these nice properties. And we want to have a concentration result where you have some quantity like this, like this outer bond that you can get from affronchen that controls the concentration result. So that's the goal. So what I also show here is that in the Monte Carlo literature, this expression appears. Here, this expression appears without the expectation, and people call that inverse effective sample size because, I mean, the sample size should be governing the variance, so like the inverse of the sample size should be the variance. Like they don't use the Afron-Schnein argument, they use some other asymptotic argument to argue that this is the correct quantity. But yeah, it turns out that you can just use Afron-Schain's function. Just that's up to factor four. Up to factor four, like you use losing there a little bit. But then the other question is: how do we get this high probability hormones? And so here it is. So we can do it. This looks pretty messy, but underneath it there is acute inequality. And so this is my excuse to talk about this. This is a bit messy. But still, it should be understandable. So here is our estimate. Understandable. So here is our estimator. So this is the normalizer, Z. And we're going to have a variance proxy, which is kind of like an F from Stein variance proxy. Like on the previous slide, you have seen that we basically had the sum of the W's normalized, so the normalized squared weights. So that was the variance proxy that was coming from Franch. And if you believe that that's a good variance proxy, Believe that that's a good variance proxy. We kind of almost have this, except that we have to introduce a shadow sample, which is a bunch of independent copies of the weight, which we can do. We are conditioning everything in a context, so you can just ignore the context as well. So you can introduce this. You can introduce this independent copies of the weights, and then you are appropriately modifying the normalizing constant. So it's basically the same quantity that we had, except that we have this independent copy, added to the original one, and we take conditional expectation given both the context, which is very, very helpful because we don't want to calculate anything with respect to the constant, no concentration. I mean, it would be. I mean, it would be kind of challenging to do such things. And we are also conditioning the first k of the way. So it's kind of like it's a little bit asymmetric. So that's not necessarily the nicest property that it has, but this is what we can get. So the first k, not the first k minus one. It's the first k. Yeah. Right. So this is not. This is not integrated out. And so it's mainly like you have to copy the expectations with very normalized terms. And then as you have more and more terms, you have to do more integration. And okay, we also have this estimator is biased. So we need to somehow control the bias. And for that, And for that we have this quantity, this beta quantity, which is basically the inverse of the expected inverse of the normalizing factor. Condition and the context. And so this is the form. Okay, so this C and C prime, they depend on universal constants and like whatever. And like whatever quality you want to choose. So there are some tunable things. Oh, that's the value that we want to lower about. So that's the value we wanted to estimate or work on. So that was the value of the policy. Wait, why is data in front of UWIS? It's because it's biased. Because it's biased, so we're correcting for the bias. So this beta is hopefully going to be really close to 1. So remember what I told you about the concentration here. So Z is concentrated around N, so this is N divided by N, it's like, ah, it's maybe close to 1. And then you take the inverse of it, maybe that's still close to 1. So indeed. Oops. I mean, not if it's a Vital, like. Here you're conditioning in a context. So the problem is the habital that if like along the context you get some really bad tails that go somewhere. So for this context, So for this context, if the two policies agree, you're good. Otherwise. But it's a finite quantity. It's going to be controlled. In the heavy toe case, you have one of these WK dominating the Z variable. It doesn't seem like the V changes things much from the expression we had on the previous slide in that case. I think that what the V. The V changes from the previous. From the bound that you had on the previous square. Right, right. So here in the, like, yeah, like, this is pretty close to what we have there. Yeah. Okay, so again, so I think that we are kind of safe from the habitatileness because we're kind. The habital is because we're conditioning on the context. So the habitalness, like if you have this fixed context, the policies can only disagree so much. You know, like if you have no control along the context, then the disagreement could be arbitrary large. And so this is what it like, okay. This is what it like, okay. It would be nice to know whether this quantity can be improved or not. I don't know that. But it's an empirical thing and then that kind of it doesn't suffer empirically from the issues that come with habitat and the concentration bonds that would rely on bondedness of the weights regardless of context. Like, okay, that's problematic, but It's problematic, but if you can do something like this, then that is kind of a good idea. Alright. Okay. No truncation, no hyperparameters. It's really important that the concepts are essentially fixed here. Okay, we need to know a bunch of things, and then if you straightforward trying to compute this, this is like. This is like too heavy, but you can approximate it using Monte Carlo law. So it's a good attempt. So how does this work? And where does the new concentration energy come in? So we try to bond this difference and to bond this difference from below we do this. We do this scoping, or like we subtract and add, and we're gonna have a value which is just context-dependent value for the policy. So that's the value of the policy at the context. So we have some concentration here going on because the contexts are randomly chosen and everything is under control because the rewards are under so that's nice. The context concept The context concentration is dealt with here, and the rest is conditioned on the context. So, the next term is just the bias term. We need to control that. And then there will be the concentration term, which is like an interesting term. Or like, okay, the bias term is also, I think, quite funky, interesting. So, how do we deal with the bias term? Well, long story short, it's a version of Harris inequality then. It's a version of Harris inequality that you can use here, but you might almost think that you can just decompose this by saying that, okay, I have, this is one of the functions, it's a W, and the other function of the W's is just one over the normalizing constant, and then one is increasing, the other is decreasing. Highest inequality can be applied, except that the A k's are coming in, so conditioning on the x k. So that's a little bit problematic. Xk, so that's a little bit problematic. Basically, we have to redo the proof, but if you redo the proof, it still works out. And the base case requires another Harrison equality. So I don't know whether there is a more elegant way of doing this or a more elegant version of Hardison equality, but this is what we have to do. So for the concentration, alright, so we have all this trouble. What am I going to do? Going to do. Alright, so we're going to have a new Afrinstein-Tele bond, and so that's where the concentration bond is going to come. So this is the bond that we're going to have. And this is very cute, I think, because it has a very short proof. I will be able to present the whole thing, the whole proof. Like this is a short talk, and you will get the proof. You will get the proof. It's simple. So, what does this say? So, it's the usual drill that you have some non-linear function at your sample, which is composed of independent elements, and you calculate the deviation from the mean. You want to control that in terms of some nice quantity, like a variance-like quantity. So, here we're going to have this is what we propose as the variance proxy. Proxy. So, what is this? So, S of K is the sample where you have a shadow sample again, and the case element of the sample is replaced by, but is independent copy of the case element. And so that's the variance proxy, and then the result says that for any For any x non-negative and y positive is probability, this exponential, apart from exponential small probabilities, the deviation is controlled by the variance plus this correction term y. To choose the y wisely, 1 over n would be working, for example, for us. Like think about this kind of Wirsting scale that we want to choose there, or maybe something even smaller, huh? I don't care about constant y, don't I take y equals b? Don't I take y equals v? V is random, so it's like this cannot be random. Yeah, so y has to be a constant. And we indeed have a version where we are replacing this with some version of the variance proxy in expected. So you could do that. Like you say, oh, let y be the expected value of v, then you get some pretty nice scale. Pretty nice scaling. So, for running a nice integral, that's a nice form. And you get some nice scaling. Notice that for choosing y not quite, right, it has kind of a small cost. So it's like missing that long. So it's not bad. Alright, so how do we get this? Well, okay, before we do that, how do we apply this? Well, you can just choose F. you can just choose F to be this big important sampling estimator and then some algebra is going to give you this upper bound on the variance proxy that we had in Ethereum and then you choose y to be let's say 1 over n. That's a crude choice. We have some better choices. That also works. And so how do we prove this Effronstein tail bound? We're going to use some methods that I learned I learned about from Victor Delapana's book on self-normalized bonds. And so we're going to show that this denatal and the square root variance proxy are so-called canonical pair. And for the canonical pairs, there are some really nice tailbones. So this canonical pair just means that the second variable kind of acts like as a variance proxy in a version like. A variance proxy in a bursting light bulb. And so, how does that work? So, a pair of random variables is called a canonical pair. If the second one is non-negative, well, you expect the variance proxy to be non-negative. And this expression, which should be very familiar to everyone, we worked with this exponential moment and then trying to get empty bonds for. For deviations of random variables, is bonded. So it's kind of like the usual, it's almost like the sub-Gaussian property, except that we have to move this e to the lambda squared b squared inside the expectation. Because b is random, right? Like, so we have to move it to the other side, and then no, we are demanding that. Side, and now we are demanding that, okay, this normalization should hold. That this is upper bounded by one. What is F again? Which F? Like in the theorem, in the main theorem, in the new? Oh, okay. So that's it just uh just this function. Actually with R I, okay, that's okay. Yeah. Yeah, that's it. And then, yeah, I didn't show how you do the color colation for the variance proxy, but you must be wondering what happened. I'm just trying to understand if it's chaining like an empirical Bernstein and classical Effranstein or if it's different from We couldn't make it work. I mean, we have some versions of these inequalities in our paper. Yeah. And I mean, like, you can make it work, but it's much looser, significantly looser. So I do have some figures at the end. All right. Okay, so if A and B are kinetic a pair, is a kinetic appear. Is a kind of a pad, and really nice things are happening. This is one beautiful inequality. And here is another one, which, well, we can't completely take credit for it. It's basically okay as some theorem in the same book, but we're improving the proof and the constants a little bit. So this inequality basically says, it's like you can recognize the same form as we had seen before, right? form as we had seen before, right? Like that A, dervations of A can be bounded in terms of the square root of B squared b side. So how do we prove this? This is such a cute proof. So you just start with Markov's inequality and so from that you get immediately that apart from exponential small probabilities, a random variable x can be upper bounded by its cumulon function. It's Kumo on function basics. That just Markov pre-written on the channel's my code. And then once you have this, you know that, well, I need to get this control for A. How do I do this? Well, you choose X in a wise manner. And the choice for the X is just going to be the log of the exponential of the expression that we know we can control for a fixed number. We can control for a fixed lambda, but we're going to plug in random lambda, which is chosen independently of E and V from this Gaussian distribution. And you supply Markov's inequality and do the calculation. Rearrange, and you got the inequality. So it's pretty nice. Just Gaussian integration. So that's uh that's that's how you get from these canonical pairs uh nice steel inequalities uh with this method of mixtures. And uh what remains to be shown is that delta n square root VR is a colonial pair and that's basically works the same as the proof of the bounded differences inequalities. So it's basically you have to study It says basically you have to study this, and then you're decomposing the data in these different terms. And then you can be clever. So maybe I don't show this because it's kind of like really the standard steps. There is a symmetrization argument that you need to do at the end, but it still works out and everything's fine. So in conclusions, I use band-aids as an excuse to uh as an excuse to get have some fun with some some inequalities, Harris inequality and maybe like interesting new Effronstein-like Tay inequality. And we have various more complicated versions of this like Pagbase variant and whatnot. I didn't talk about those. And I think that the proof is like the status normalizing accurate this using this kinetic appearance is really beautiful. This kind of appears is really beautiful and nice. So, really delighted to have learned that from this book. And the inequality is okay. Yeah, it's much better than anything else we have ever seen. But for example, it doesn't exploit if the reward has a small variance. It's like it's still like not that great. Like, I don't know how to fix that. Um, bootstrapping or whatnot. Um bootstrapping or whatnot. We have to do some stuff. And yeah, I I'm big curious about whether you think this could be applied to some other settings. All right, that's it. Thanks. I'm I'm gonna make a blasphemy. Uh have you tried this in practice? There you go. There you go. So this was on a synthetic example. So, but we do have some non-synthetic examples as well. And so the black curve is this Effernstein bond. And the Embrage-Benstein bond, this various levels of subtruncation, are shown here. And this is the And this is the estimated, this is the true value that you want to get to. And the lower bound is shown here. So you see that like some of these guys never do anything. And we are pretty early on taking off. This is the sample size for various settings, like less and more difficult problems. So it generally feels pretty good. This is an empiric example, and I don't know, like, here are some other empathy categories. It are some other empirical results which should be synthetic with the ball letters as usual mean that we are the best. Okay. Thank you.