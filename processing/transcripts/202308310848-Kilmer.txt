So let me just start with the goal of my talk today. So in the first place, I'm just going to assume that I have a matrix A with some block structure, which will be pretty general in the sense of what I mean by block structure. And our goal is to try and find an approximation to that matrix A. We'll call it A hat so that the difference hat so that the difference in the Frobenius form is small. The idea being that perhaps we will want to either use A hat in place of A in some places where it might be appropriate or some maybe A hat has some nice further structure that we can take advantage of in the computations. So the way that we're going to do this is somewhat unconventional. We are going to map what I will call non-reducation. Map what I will call non-redundant blocks of the block matrix into a third-order tensor. Then I'm going to take an approximate tensor decomposition, and I'm going to take the tensor factors from that tensor decomposition and map it back to my matrix, and that will produce a hat. And the reason that I want to do this is because this process ends up allowing me to find additional Find additional latent structure that is not obvious by staring at the matrix itself. And in particular, today I'll show you two different types of structure that it's possible to get depending on the tensor decomposition you choose. And in one case, you can get this block-low-brand structure. In another case, a chronic or sum structure. And I'll give a few examples where these things might be relevant in practice. And on top of getting this additional. And on top of getting this additional latent structure, these decompositions turn out to preserve the high-level structure that's already present in the matrix that we start with. And towards the end of the talk, I'll talk about how this actually can be generalized to a multi-level block structure. Alright, so the features of our approach that are interesting, we first have to define what we'll call it, a matrix. What we'll call a matrix-to-tensor mapping that is invertible that requires only knowing the block-level structure of the matrices. And as I mentioned earlier, and I'll show an example of this in a moment, what I mean by structure is quite general. The method that I'm introducing is agnostic to the type of tensor decomposition that I'm using. So, if you do any work with tensor decompositions, if you Google, Decompositions. If you Google tensor decompositions, there's a whole alphabet host of different algorithms that you can use. I am not advocating for one over the other. I will just be discussing two particular decompositions and show you the structure that you get out in those cases. But if you want to substitute in your favorite method, please go right ahead. The interesting thing is the way that we define this matrix to tensor mapping. To tensor mapping means that we will be able to preserve error. So the error that I get in the tensor approximation will be exactly the error that I get in the matrix approximation that I'm constructing, which is a nice feature. What I'm describing in today's talk, I'm ignoring symmetry and positive definiteness or semi-definiteness. Definiteness or semi-definiteness. I decided it would take too much machinery to discuss in detail how we do this, but I can adapt the method to preserve symmetry, positive definiteness, or semi-definiteness. And the other thing about this framework, this approach, is that it actually subsumes some results that are in the literature about certain types of matrix approximations and, in some cases, improves upon those results. And fruits upon those results. So I struggled with how to write the outline for this. And then, in a stroke of brilliance, Arvind came up with this picture. And this is one of those pictures worth a thousand words kind of things. So the basic idea, so here's an example of what a structured matrix I might consider, right? This looks like it must be blocked toplets, for example, with some block symmetry. I'm going to take the non-redundant blocks out of this. Redundant blocks out of this matrix. I'm going to put them into what we will call a third-order tensor, so a three-way array. I'm going to use some kind of tensor decomposition, and then depending on what my application is and what I want to get out of this, I'm going to map it to a sum of chronic or products decomposition or a block structure factorization. And the colors here indicate that this high-level structure is also preserved in the two. Preserved in the two factorizations there as well. Yes, the structure, the HL structure means that the block, the or blocks, for instance, are equal or not? These are equal in size. All in size, but not in the components. So this and this and this would be the same. I consider this one of the non-redundant blocks. Okay. I have some specific. I have some specific examples. But the main reference for this is a paper that appeared in Symax just last year. And really what I'm hoping is at the end of the talk that you all will tell me that you have other applications where this might be applicable or ways to generalize what I'm doing for your application. So it's a little bit of an advertisement. All right. Alright, so what I said this is a fairly general notion of what do I mean by structure. So here's two little cartoons. In this first example, I'm just showing something which has no more structure than being block tri-diagonal. So with the colors, I've tried to indicate that none of these blocks matches any other block in the matrix, but it is block tri-diagonal. In the other case, I have something again which looks Case, I have something again which looks like it's you know highly structured in the way that we tend to think of structured. It's a block toplitz matrix. So keep that in mind because I will be talking about examples of this form as well as more structured cases. But the key ingredient, as I said previously, is I have to develop this matrix to tensor mapping, and I have to find a mathematical description that will allow me to get this inversion. That will allow me to get this invertible mountain. It captures this structure. So, this is what we decided upon. So, in this, throughout most of the rest of the talk, my matrix A is a real matrix, and it's got L by Q blocks of size M by N. And what I'm going to want to do is select non-redundant. Select non-redundant blocks. I'll label those A sub K. And let's suppose that there are P such non-redundant blocks. So I list them here in this set that I'll call script A. And together with script A, I have to have a way of helping to represent this redundancy. Well, and so I have this other set, script E, and these E's are just basic. E's are just basically weighted entries. They're either zero or they're one over square root of eta k, where eta k is the number of appearances of a k in the matrix. And I'll show two examples of this in a moment to make this more concrete. And if I have decided what these things ought to be for a particular instance, then what we have is a Then, what we have is a way of putting the matrix back together from these two sets. So, we call this notation, this struct with a script E, and the input script A. We define it this way, as the sum. Again, P is the number of non-redundant blocks. And so I can always, from these two sets, get back a matrix. And in this case, I'll get back a matrix A. I'll get back a matrix A exactly. And for what it's worth, this is an interesting decomposition because it decomposes a matrix into terms that are orthogonal in the trace form. So here's a very simple example, a block diagonal matrix. And I'm assuming that none of the blocks matches any of the other blocks. So, and the blocks are themselves square. The blocks are themselves square, then I just need to, I would store all the blocks on the block diagonal in my set A. And the E, the weights are one for every one of these outer product looking things. And I could put the matrix back together this way. If I do something which is blocked toplitz, but it's not necessarily block symmetric. Necessarily block symmetric, then everything I need to know about this matrix is in the first block row and the first block column. So I can just list those here. I do have to choose an ordering. Doesn't matter actually what ordering I choose. I just have to keep track of it and be consistent throughout. So this was the ordering I chose for that list. And in this case, you see that the set E has The set E has each one of these matrices has the individual weights associated to how many times it appears on its respective diagonal. Alright, so in order to tell you about what I want to do, I have to say a little bit about tensors without going too far down that rabbit hole. So we are working right now with Right now, with a third-order tensor. So you can just think of it as a three-way array. And there are many ways that you could slice and dice a tensor. Some of the useful ones for purposes of this talk, I might be talking about frontal slices of a tensor. So those are, again, just matrices slicing front to back, and we can use this convenient malar colon notation for that. Lateral slices. Lateral slices of a tensor and the fibers of a tensor. There are column fibers, row fibers, and tube fibers, and I just indicated what the tube fibers look like in this case, sensing vectors but into the board. So, the way that our matrix to tensor mapping is going to work, I have to have some way of getting the information from The information from those two sets, script A and script A, into a tensor. And so, what I'm going to do is define two operations, the twist and the squeeze operations. Squeeze is actually, you know, MATLAB command will do this for you, but basically the idea is that I can go back and forth. There's sort of one-to-one correspondence between a matrix and its counterpart twisted into the board. So, the matrix to tensor mapping, we've got this set script E, we've got this set of AKs, and we just basically fill in lateral slices with the blocks, the non-redundant blocks that I'm keeping. So I'm just and again, I'm just kind of doing it in order that I put in the list. Order that I put in the list, but in reality, it doesn't really matter which ordering I choose as long as I'm consistent. And then I need to be able to get back. So if I'm given a tensor script X, then the way that I get my matrix back from that, knowing this set script E, is to apply that script function. So now I can. So now I take the lateral slices of my script X tensor and I squeeze them. So they become a collection of matrices, which I then fill in over here in this formula. And it's invertible. It's an invertible memory. If I start with A and I don't do anything to the tensor, then I get back A. So the algorithm is pretty straightforward to Is pretty straightforward to describe, right? I just need to determine what script E and script A are. Then I can define my tensor in this way. I have to compute a tensor approximation. I'll get to that in a moment. We'll call that tensor approximation script T. And then I define my matrix approximation using the tensor approximation, at least implicitly, and pushing it back through that transformation to get A hat. A hat. Which tensor approximation? That's a good question. How good is the matrix approximation that we get from this? Before we enter this delicate thing, even earlier I understand that there is a choice on how you twist the matrix, in which direction you put the matrix in the tensor. It turns out it will be fine as long as I'm consistent. The framework that is in my mind from some other tensor work is the thing that made me decide I was putting them in as lateral slices. If you wanted to put them in in a different way, you could do it, especially with the tool component. It won't make a difference. It won't as long as you're keeping track of what you're doing. You would just need to define the matrix tensor mapping and the consistent. Tensor mapping in a consistent way. Okay, so I want to be able to answer these three questions basically, and in order to be able to really talk in particular about the third one, I do have to consider a few different types of tensor approximations. And so, what I've done here is picked two that are sort of the most famous, the most widely used, probably. Most widely used, probably types of tensor decompositions. So, in the first case, this candy-comp parifact or CP decomposition, this is a decomposition of a third-order tensor into what we call rank one tensors. So basically, the idea is that a rank one tensor is written in terms of a three-way outer product. It's just the generalization of an outer product of two vectors, except now since you have a third vector. Except now, since you have a third vector going into the board, you're going to get a copy into the board of that rank one matrix multiplied by whatever those weights are. And in practice, you know, trying to find the exact tensor rank of a tensor is actually a hard problem. People don't do that. They just guess what an R, a reasonable R should be, and they try to solve this problem. This problem. And Hans talked about the way that you might solve a problem like this would be with an alternating least squares method, maybe with Anderson acceleration attached to it. Doesn't actually have to have a solution, but we just kind of ignore that and do it anyway. And then we can collect these things here, the YI, the ZI, and the HI, into what we call the factor matrices. And of course, what's interesting. And of course, what's interesting here about the CP decomposition that makes it unlike some of the other tensor decompositions is these things can have possibly dependent columns. And that's something that I can take further advantage of when I do compression if that actually happens. But R here is what we'll call the tensor rank. And then there are some interesting papers that describe where this originated, and then some of the Originated and then some of the annoying issues that come up with CB decomposition. The other decomposition that I want to talk about is a TEPR or TEPR3 in this case, and in particular the higher order SVD variant of the TEPR3. So this is actually an equal sign because I can do this in polynomial time. The way that I can do this, for example, if I wanted it in If I wanted to enforce that these three factor matrices, U, V, and W, had orthonormal columns, would be to compute the full S V D, for example. This is typically what happens, and then truncate terms to get to a, what we call a multi-rank R1, R2, R3 approximation. So I should say maybe a little bit about how this actually happens. There's basically, you've got this third-order tensor. There are three ways that you can unfold that into a matrix. We call that the mode one unfolding, the mode two unfolding, and the mode three unfolding. So where these things come from is that they're the left singular matrices from an S V D of each one of those unfoldings. So there's a bunch of matrix factorizations under the hood of this. Under the hood of this. And then these three things are used to, you know, project, they apply to this original matrix, and you get the core tensor from that. So we'll call that a multilinear rank R1, R2, R3 approximation. When we do these three individual SVDs, we throw terms away. This process of doing these three SVDs and throwing Doing these three SVDs and throwing things away is not optimal in the Frobenius norm. So it's not like the matrix SVD you get an Eckert-Young theorem. However, there are upper bounds on this. We say it's quasi-optim. So to summarize, I've taken my matrix and I have mapped it to a tensor script X. And I want to use one of those two tensor approximation schemes. Tensor approximation schemes to come up with a factorization, an approximate factorization T. All right, and again, the CP approximation, I will end up with three factor matrices that each that have R columns, which are not necessarily linearly independent. And then the Tucker approximation, I get this core, which is R1, R2, R3. And I have three factor matrices, which are tall and thin with orthodox. Which are tall and thin with orthonormal columns, and they have R1, R2, and R3 columns, respectively. So, the interesting case when this whole technique is going to be valuable will be the case when the rank that I've chosen for the CP approximation is smaller than the smallest dimension. And in fact, I should have said this earlier for CP, that there are some upper bounds on what the rank has. There are some upper bounds on what the rank has to be. The upper bounds are kind of large, like it's the product of the two smallest dimensions or something like that. So I'm really hoping for an R that's smaller than any one of those dimensions. And for Tucker, the really interesting case is when I pick the truncation term, at least one of the three truncation terms, to be smaller than its corresponding image. Okay, so here is the theorem. Because of the format of the tensor approximation that we have, if I have these mappings, the invertible, the tensor to, the matrix to tensor, and the matrix to tensor and the tensor to matrix mappings. And I get my approximation A hat from the approximation to the tensor. To the tensor, then the errors in the Frobenis are equal. So that's kind of a nice feature, and the equality is tied in with the way that I did the weighting in the description. That's the script E terms. Now, how am I going to get this additional structure? I need to go back and look at the matrix to tensor mapping. I mean, at least formally, this is how it works. At least formally, this is how it looks. But in those factored forms, I don't want to take those factored forms. I have factor matrices and I maybe have a core tensor. I don't want to make the tensor and then slice it up and parse it back out. It doesn't give me anything. The whole idea is you never explicitly construct the tensor approximation t. You just have implicitly the representation in the factor terms, and then terms and then using that information we get structure that's in that squeeze of T term that we can push forward into the to get the rest of the restrict approximation. So for example by doing that process I can show that both if I use either the CP or the Tucker decompositions that my approximation A hat would correspond precisely to Have correspond precisely to a summation of Kronecker products. This, the number of them, well, tau will be R, the rank of the CP approximation if I use CP. Tau will be R2 if I use the Tucker approximation and compress on the second mode. CJ will inherit the original structure at the block level. So if the matrix originally blocked taplets, Originally blocked topelets, CJ is blocked toplets. If it was originally blocked tridiagonal, the CJ will be tridiagonal. And the DJ, that actually may have some low-rank structure itself, but it depends on how you truncated the different. And that's just a nice, again, a visualization of the possibilities for the results here. For the results here, this would be the case that I do get some additional low-rank structure in the DJ time. Let's see this in practice. Well, we just naively took a bunch of block tridiagonal matrices out of sweet spars. And that's all we knew about them. They were blocked tridiagonal. And we put them into a tensor and we used a higher order SUD or a tucker, but we only compressed on one. Or a tucker, but we only compress it on one mode. So that's where the cross 2 and the V come from. That means I just have a single factor matrix. The G, the script G, that's the representation of the core. So this G thing will be, everything here was square end by end blocks and block, the overall structure was square as well. And what we can see here is that we got. Can see here is that we got an amazing amount of compression. So, for example, in this last one, we're storing 3% of the original non-zeros of that tri-diagonal matrix in our representation, and we can approximate it to machine precision. That's relative error there, right? And that's with a sum of our terms. So, you can imagine what this means in terms. So, you can imagine what this means in terms of storage and how fast that you can compute now matrix vector products with this approximation, right? Because you can use rules of Kronecker products to do this very efficiently. So that was nice. This one here has an asterisk by it because, in fact, it was so large that we took just a chunk, a big chunk out of it, and we ended up using a randomized HLS. So the compression is amazing and the relative error approximation is amazing. These things are still tridiagonal. And if the blocks were themselves originally tri-diagonal, the fact that we didn't try to preserve the DJs would be as well. So in many cases, these problems, when they have this block structure, they have Have this block structure, they have extra structure inside. I mean, the blocks have extra structure, even the spikes. Of course, all this competition, I know that with tensors, everything is full. But any way to explore this excess or low rank, I mean it's an extra block, not the extra blocks, but low rank. So, this in this procedure, essentially, it's kind of if there's low rankness, it's sort of finding it automatically through the It's sort of finding it automatically through the factorization. It is true that, for example, if these things were themselves tri-diagonal, then the tensor that I have is also very sparse, so I can immediately reduce the amount of computation I overhead in computing the tensor approximation. Yes? Bringing that up. The other type of structure that we explored for its particular That we explored for its potential practical use was to use a CP or a Tucker decomposition. And what we could get out of it was this, what we were hoping to get was a block below rank structure. In this case, this matrix, each block of this is a copy of U, and U has orthonormal columns. Likewise, W has orthonormal columns. And the matrix in the middle repeats the structure again. Again. So let me show you an example of this. So I have to apologize in one way. A is not now the big A. A is just one of these four components to this linear time invariant discrete dynamical system. And the matrix that I'm trying to approximate, I call H, because it's going to be blocked angle, actually. So I want to recover these individual components. These individual components from this big block Hankel matrix that's formed with the Markov parameters. And it's actually fairly large in practice. So it turns out what we need in order to be able to recover the components is a low-rank approximation to this thing, and then I can recover, for example, that little A that leads to similarity. So I said the HOSVD, we did this one just with the HOSVD, involves doing SVDs of three different unfoldings of that tensor. These are the singular values for each one of those three range compositions you have to do to get it. Here's the dimension of the tensor. And we took, this is the multi-rank approximation that we took to the tensor. And we used this technique and we came up with the And we used this technique and we came up with this thing we called H tilde of S. So now what we have to do to complete the process is to look at this innermost matrix here. So it's just a very small version also of it's still Block Hampa, but much smaller. And we do one more post-processing step of using something called the randomized ERA algorithm to basically. Randomized ERA algorithm to basically get an approximate S V D to the term in the middle. And then we have to recover, I said, use that to recover the components of this dynamical system. So these are the spectra of the original matrix A, little A, that appeared in the dynamical system. This is using one of the most well-known, the standard ERA algorithm, and the black is using our approximation. Approximation, better float-and-point operation count, a lot less storage, a lot less runtime. I mentioned we could adapt what I was saying to the symmetric case and also to preserve either a semi-definiteness or a positive definiteness. I didn't want to put all the machinery on the slides, but our motivating example there was doing block low rank approximation to space-time covariance. Approximation to space-time covariance matrices. And in the example that we have in the paper, I think we compressed to like we have a representation that's like 0.3% of the original space-time covariance tensor. So that's very nice. What I want to just show in the last few slides here is how this does generalize to multiple levels of structure. And because this is the result, it shows how some of the things in the literature. Of the things in the literature get subsumed by this framework. So, the idea here is that now I have my matrix A and I have L levels of block structure. And at the finest level, the matrices that I'm going to consider will be size M by N. Now, M and N could be 1 each, but they might not be. They might not be. So the type of block structure, again, general. I do have to require that at each level it's the same structure. So if at the top level it looks like it's toplets, that's fine. But at the next level, it could be just blocked right diagonal, for example. So on each level, it needs to be fixed. So as a motivating example, I'll just consider this case. I'll just consider this case. This would be a two-level example. So at the coarsest level, it looks like it is block symmetric block toplets. So the non-redundant things would be this block, this block, and this block, and this block. So four blocks. And I'm assuming that within each one of these blocks, there's an additional structure. Say it's block diagonal. Diagonal. And so, I guess those were zero blocks, just block tridiagonal. So, I've got five blocks within each one of these three things that I need to store just to represent this matrix. All right, so this is the ugly slide, but I wanted to just put it up for completeness. I wanted to just put it up for completeness. If you get nothing else from this slide, at least it should evoke memories of the two-level version of this that I, that's the single-level version of this that I had up earlier. Now we have to represent the structures at each one of the levels. So I've got sets that look like this for each one of the levels, which again are weighted in terms of the number of times they appear within that level. Within that level. And now it gets a little complicated because I'm going to map these things to higher order tensors, not just third order. So I do need a notion of twist and a notion of squeeze. I can't really illustrate it with pictures so well. But just note that if I start with a level L block, a level L. Block, level L structured matrix, then the order of the tensor will be L plus 2. So in the example that I was showing you where capital L was 2, then I would get a fourth order tensor, and these would be, that would be the indexing scheme that I've chosen for that fourth order tensor. It doesn't matter so much, again, the indexing scheme as long as you keep it straight. And then I can always go back. So this is still an invertible mapping. Still an invertible mapping as it was before. Well, decompositions such as CP and Tucker are well defined for tensors of higher order as well. So I just essentially do the same kind of thing again. So, and it turns out, as before, we can prove that the error in the Frobenius norm in the resulting matrix approximation will match the Frobenius norm error in your tensor approximation. Tensor of the last one. So, where I think this was interesting, remember I said that m and n could be one. So, you're at your finest level. You could be thinking of just individual scalar entries. So there's a body of work in the image processing community where they tried to capture optimal block toplets, toplets, block approximations. Topplets block approximations or block toplets plus ankle with topless plus ankle block approximations. And they show how to do this. These are the references to those works. It's very nice work. But a lot of machinery in those papers went into figuring how to construct the matrix that you had to factor to get the optimal approximation with the weighting and so forth. Here we get that for free. Here we get that for free. So, this model that I've described just tells you how that waiting comes about naturally. In this work that I did with Jim some years ago, Jim Nagy, we tried to generalize this kind of approach for a third-order case. So we observed that if you have a triply toplitz matrix that's banded at each level, if you take the interior column, fold it up into tensor, do a tensor decomposition, you get a chronic. Do a tensor decomposition, you get a Kroniker approximation. But it was not optimal in any way, and we didn't know how to weight it or anything. Again, this framework gives us the appropriate weightings. If I use an HOSVD, I can get an approximation that's guaranteed to be quasi-optimal. So I had a lot of fun with this project. I think we've come up with this potentially. Up with this potentially really useful method, I hope, for lots of applications that will generate these new structured matrix decompositions through this approach. And we've got bounds on the matrix approximation error in terms of the tensor approximation error, which you can control. You know, as we were talking about earlier, you can exploit some efficiencies in the underlying structure in the tensor situation. Underlying structure in the tensor said you have to decompose an offline component. So you reveal with this process this latent structure that's lurking under the hood as well as retaining the structure that you had at the beginning. You can do this in a way that preserves symmetry, generalizes to multi-level. And we're looking again, want to tell me about additional applications, I would be happy to hear that. People always don't. Hear that, people always tell me: oh, you do tensor training, yeah, you can do tensor training, but you do tensor training. And I had a master's student who is now working as PhD at Emory do some work on this with a hierarchical structured matrix, trying to extend it to that case for a type of fractional PDB. That's challenging because the blocks are different sizes, so you have to do some. Are different sizes, so you have to do something different. Anyway, thank you for your attention. Questions? Please? In the middle, you had a reference to a paper from 1927, I think. Yes, CP decomposition. The CP decomposition originated, as far as anyone knows, to that paper in, I think it was psychometrics literature. What was it used for? Literature. What was it used for? Psychometrics. Some kind of factor analysis, I think. But I find this really impressive that all started from the thought of formalizing this. Please. Yeah, so if you compare with a low-rank approximation of just your non-redouble loss, like also intuitively, like. Like, also intuitively, like, what's the advantage of doing the tensor decomposition rather than decomposition per block? And I guess intuitively, you could expect there to be an advantage if somehow there is some relation between blocks. And so, is there any intuitive insight in that? That is essentially what we're exploiting. I think if they have absolutely, I mean, if they were completely random, I don't, you're not going to get too much. I mean, I think if we're worse case. I mean, I can give more worst case bound based on what the tensor decompositions give you, but it won't be necessarily practically useful. And numerically, did you do comparisons with decompositions just per block and accuracy or efficiency? I mean, I'm sort of ignoring any structure on the blocks and letting the tensor decompositions tell me what it is, is basically what it is. What it is. So we haven't looked at, you know, well, what if I did low-rank thing? Because intuitively, you would see in many applications, I think there are relations between the rules. And then I guess potential decomposition kind of discovers that and makes it more efficient. Like there is potential for this kind of approach compared to decomposition for a lot, but I guess it would be interesting to try. It would be interesting to try to maybe understand that more I would perhaps see your results on problems use far and all these problems are they it that may actually be the case but no I mean I Be the case, but no. I mean, I kind of wanted to see if this would work as a black box method. And so it could well be the case if they were constructed. Somebody knew they were chroniker, you know, some of exactly that many chroniker products. I have tried this on other problems, like spatially invariant and blur, where the blurring operator was constructed by a known number of different terms, and I can recover exactly that number of terms with this approach. That number of terms with this approach. Extending to blocks with different sizes sort of hard. Can you say a bit more about? Just that the tensor framework is requiring the dimension. Yeah, right? So we just need different tensors. But you could take the largest one and zero out. Yeah, so Mitchell tried a little bit of that. So Mitchell tried a little bit of that, but one of the matrices that we had had very nice, it was like powers of two size, different size blocks. And so basically we're looking at things on the same level, which would have all the same dimensions. But I think in the code that generates those matrices, if you were to change some parameters, then we'd have some problems and you have to do some zero packing. Yeah. Yeah, just a reality. So are there any connections between what you've done and Manon's work on the nearest chronicle form? On the nearest chronicle form of a matrix, where you can, like, even a matrix, try to find the nearest chronicle form, just one term like B chronic that minimizes the distance between the two. So you mean like what's So uh you mean like what's in the Piziantis and Van Loan paper? Yeah. Yeah, I mean so this this can basically return those results. Okay, so it can be seen as a generalization kind of. Right, so the references I had, the work that Jim had done with some of his former students, those all were using that model, that original model in that paper was specific to those structures. With specific to those structures, and they could do it with like a single term or with multiple terms and so forth. So, yeah, have you thought about, or maybe only worked on, other norms? Like, if you take a matrix norm that is a reasonable, I would say, the forbiddenness norm, like the spectral norm. Have you looked at that problem? How this translates for, for example, for your third-order tensors. So, the equivalent norm for the tensors corresponding to the two-norm for the names. Correspondingly two long formula matrices. I have not thought about that. No, it's a good observation. I have not. But it would be extremely hard, because that's already going to be hard. You can compute that. So at least for the computers of computation, we can stop here. Thank you. We continue with one discussion. You continue with Martina. You have a chip for the time to do it. I did say one thing just because it came up without very different history. I just don't know how to do it. So in a germanic algorithm, no, no, you say a germanic algorithm for blank source separation. So 