All right, so thanks for the organizer for inviting me here. It's been a very good learning experience for me. So job ready. So today we're going to talk about a guided hobby model for decomposition. So as you know, cell type proportion is an important health indicator. For example, if you have a control antasis, they might exhibit different cell proportions. So knowing the differences of cell proportion can help us understand the disease activity. Actually, all. So, as we know, single-cell sequencing has been, single-cell RNA sequencing has been quite popular method here. But the cost is still very high compared to bulk-bit. So, there are only a few studies for large-scale patient-level single-cell RN sequencing studies. So, the goal here is to do computational cell type deconvolution. So, the idea is that you have very The idea is that you have a very abundant bulk gene expression data sets from GTEx, TTGA, or from GEO in general. And then you want to take some of these single-cell RNA as a reference group. And then you can deconvolve the bulk data into cell-type proportions. So however, this is not a new problem. So actually, a cyber sort has been around for a while. So which use purified bulk data. So the idea is that you train So, the idea is that you train a non-negative least squared regression model, and then you have these purified profiles from a set of markers. And then you try to figure out the linear coefficients. So these coefficients being non-negative as constraint, so proportional to the cell type proportion. So then you try to fit this linear coefficient to make up the proportion over the gene expression that looks like the BOP data. So that's the earlier methodology. So our contribution here. So, our contribution here with respect to the existing methods are threefold. So, first, we want to infer the cell type-specific distribution from single-cell reference data, so we abbreviate it as a CTS. And also, we want to infer the sub-cell type distributions under each of the cell types. So that's another contribution directly from the single-cell RN distribution reference as well. And thirdly, we want our method to enable CTS differential expression analysis directly from. Differential expression analysis directly from the bulk data, where you can compare case and control to see what are the differentially expressed genes under each of the cell types. So we treat the cells as a document using sort of a natural language kind of idea. So cells are document, genes are vocabulary, and then the read are the tokens. That is, each read you know which gene they come from. So the idea here is that this is a kind of a just a very standard. Kind of just a very standard probability graphic model for linear allocation. So the contribution that we made here is that we're trying to make the alpha, which is the hyperparameter for the topic prior, to be asymmetrical. So the idea is that, let's say, I want to dedicate top A1 to the blue cell type, top A2 to the green cell type, and top A3 to the yellow cell type. Okay, so the rows here are my cells. So now we want to leverage the cell type label information. So now, suppose I know this. So now, suppose I know this first cell is the blue cell type. So now I want to put the higher hyperparameter 0.9 on that cell type so that I can actually anchor topic 1 for that blue cell type. And then the second cell is also a blue cell type, I put 0.9 as well. And then the third one is a yellow cell type. So I put 0.9 on top of 3. So in this case, I can anchor each of the topics to a specific cell types. So LDA in general, there's an identifiability problem. So you train LDAs on the topics. So, you train LDAs on the topics. You cannot directly identify with a specific cell type. But by anchoring each other topics through the hyperparameter in this way, you can actually identify each topic with a specific cell type. So that's the hyperparameter set. And then if you're familiar with the LDA, essentially you go with the sort of topic mixture. So this is for a cell D, for example. So given the hyperparameter, and then you want to sample from a Dirt distribution the theta cell D. So in here, we can imagine each cell. So, here we can imagine each cell, which is the row here, is a mixture of cell types. Because of the way that we anchor the hyperparameter, we can see that the cells actually are labeled with a cell type, actually exhibit very high positive mixture probabilities, but not black and white. So, indicating the model expressed some uncertainty about the noisy cell type. As we discussed here, some cell types are not 100% defined. So, that's sort of the topic mixture, theta here. And then, given theta, we're going to sample. And then, given theta, we're going to sample for each of the reads. So, it's an RNA sequencing, you have the reads. So, each read has a read, what's the origin of cell type B comes from? So, that's a lean variable VID, for cell D. So, giving that lean topic, and now we're going to sample from this gene by topic distribution. Let's say the topic is one, one of the cell types, and then we're going to sample one of the genes across these topic distributions. And then that gives us the observed gene ID. Observed gene ID. So that's X ID. So that's where I really map to that gene. So that's sort of the indicator for that. So here, these unshaded nodes are the main variables. So that's something that we're trying to infer. We're trying to infer the theta. We're trying to also infer the gene by topic distribution, which give us the cell type specific distributions. So giving this problem, I felt like I'm missing something, I don't show the math. So in here, the inference algorithm is actually very, very simple. Because the Very simple. Because of the conjugacy of the directional act of multinomial, we can actually first integrate out the theta of the plot. So that we don't need to infer those linear variables, but we can recover them later. So in here, the only linear variable left is this topic assignment, which is proportional to these two terms here. The first term, the most important piece, is this alpha dk. So that's where we introduce that hyperparameter where we observe the cell belongs to that cell type k. And then we have a high value for alpha for that observed cell type. For alpha, for that observed cell type, and a low value for the rest of the alpha values. So that's where the alpha comes from. I can see that in here is driving the topic inference. If you know that this cell is a specific cell type, the probability that specific read coming from that cell type should be higher. So that's the kind of parameter here. And then the end here is sort of collecting evidence. So that is for this cell D, I want to know how many reads has already been assigned to top container. So that's a sufficient statistic here, excluding the Statistics here, excluding the read i, because the read i is the one that I'm currently inferring. So that's an excluding notation. And then here, this is a global parameter. So that is, if I know the gene is XID indexed by the XID as observed, I want to know how many reads has already been assigned to topic K for gene W. So in here, index XID could be equal to W. So that's the global topic distributions. So giving these two terms, you can then just run like So, given these two terms, you can then just run, let's say, deep sampling, but in our case, we run variational difference. So, we alter between the first step and the second step, you can consider as an E and M. And then we monitor the evidence overbound until it converges. And after it converges, we can recover the link variable theta. The expectation of theta is computed just normalizing the counts. And whereas the expectation of phi, which is the gene by topic, is also computed usually. Topic. It's also computer using CC templates. And then we don't stop there, and we can also infer sub-cell types. So, how does that work? So, basically, how it works is that, for example, you want two topics per cell type. So, now I can dedicate topic 1 and 2 for the blue cell type. I can dedicate topics 3 and 4 for the green cell type. I dedicate topic 5 and 6 for the yellow cell type. So now for the first cell type, which I know is a blue cell type. So, now I can split the value. So now I can split the value, right? So previously I showed 0.9 for that. You know, Blue Cell had then split into 0.45 for each of the subcopics. And then for the rest of the topics, just randomly sample from uniform distribution with a very low upper bound. And then for each of the cells, we do that. So that enables us to actually infer the phi distribution, where this is also still the gene by topics. So now for each of the topics, you had a sub-sound type. That is sub-cell type. So, in here, this is an example that we infer from the Panque single-cell data set. So, where we use five topics per cell type. So, I'm going to get to that in a moment. So, once we have this sort of gene-by-topic distribution from single cell, and then our ungodling is trying to become more bonded, that is, we take this gene-by-topic distribution here, and then we take another LDA. In this case, we 65, because that's something we estimated from single cell. That's something we estimated from a single cell. And we're trying to estimate theta in this case, where x is the read count from the bulk RNA sequencing data. So the idea is basically the same as the previous slide, where we're trying to insert a topic for each of the reads from the bulk data. So in this case, we don't actually have an asymmetrical prior anymore because for bulk data, we don't know the proportion. So we have a flat alpha there. And then in here, we're just fixing the phi for the topic gene. The topic gene, gene, topic-specific distributions from the single-cell RNA-sequencing data. And then we alternate between these two steps, and in the end, we can recover the theta, which is the cell type proportion from the bottom group. So regarding the number of sub-cell type topics per cell type, so that's something that we don't know. It's a hyperparameter. So we have experimented that through a simulation. So here we tried on three different data sets, confidence, the breadth, and the So, pancreas, breast, and the rheumatoid arthritis of blood data set. So, essentially, we're experimenting these Pearson correlation as well as experiment correlation as an evaluation of the inferred proportion with the ground truth because we simulate we know the underlying self-proportion for the bulk. And then we evaluate sort of the performance as a function of the number of sub-profits per cell type. So, you can see that in here, when we reach those five, When we reach those five, the model tends to increase and then flatten. So that's the number of topics we choose for each of the cell types. So some of the data set seem like there's no improvement, it's okay. So it depends on the data sets. And then we conduct the kind of comprehensive experiments using the real bulk data where we know the ground truth through flight cytometry, for example, some of the bulk data you know the cell proportion. The cell proportion. And then the experimental movie data set, we try on five different data sets. So, where for each of the bulb data sets, we use one of the reference single-cell data sets completely separate from the bulk data. And then we assess the experiment correlation for the decompile cell proportion with respect to the ground truth cell proportion. So, in here, we experiment three different variations of our model, the GPN. We use all the genes, so without any filtering, 20,000 genes, and then we use a pre-fast. Genes, and then we use a pre-processing gene that is removing some of the housekeeping views. And then we use the highly variable genes and we'll assess which one actually performs better than each of the data sets. We also compare with the SALTR methods, including the E-Seq, Cyber4X, MUSIC, Bass, Prism, and BISC. So overall, here the experiment higher the better and the RMSE the lower the better. So we see that we in general we come See that we, in general, we confer a very competitive performance compared with the other system methods. And this is another way of evaluating the model that is you can evaluate how well the model deconvolves per cell type. So that for each of the cell type, you can then calculate the correlation of cross-samples for that cell type. And then for each of the data set, you can evaluate how well the model performed for that cell type. And then the roles are the same method that we compared in the previous slide. Method that we compared in the previous slide, and then each of the columns here are different data sets. So, in here, the intensity is showing the proportion of the Pearson correlation, whereas here the intensity is proportional to the numbers of the this equal mu square error, so still the higher the value. And then this top bar here is basically the general overall proportion in the bulk data. As you can see, that the higher the proportion in the bulk data, the better we perform. The better we perform, so which means that we have more evidence about that cell type proportion, right? So, which is also more effective. And then, here the sidebar is showing the overall performance where we're averaging the speed run correlation of fast cell data sets. So, in general, the three variants of our model perform favorably compared with the existing methods. Based prism is one of the second best methods, which is also based on copy models, with a different sort of formulation. Also, suggesting that Copy model works quite well on this type of degree. Quite well on this test, type of deconversion test. And then we also try on the purify block where we know the cell type label. So in this case, it's very simple. We just take the cell type with highest probability as our prediction. So now we can assess the prediction accuracy in this case. So what we did is we experimented on two different purified bulk data sets using two different references. One is HPC, human blood cells, the other one is PBMC, another sort of reference. Another sort of reference data. So, overall, we see that our model, especially the one that uses finite variable genes, actually performs quite well, where the second bias mention is still a base prism. And then we took a look at the sub-cell type topics. What did we learn from the sub-cell type topics per each of the cell type? So, in here, we see that we use five topics per cell type. We see there's a sort of top gene, and then you can see this is a block that. Gene, and then you can see this block diagonal, which is very interesting within this cell type. You see, there are top genes, right? Here, the rows are the genes. And then the label showing is that the known marker genes for that cell type. As you can see, some of the cell type, we actually recover a lot of marker genes. And then when we perform the enrichment analysis of the five topics per cell type, we see that they're all very much enriched for the marker genes. So this is one of the leading topic with the leading edge analysis. Okay, uh, I have three minutes, right? I haven't come to the best average. No, we have to make sure we are on schedule too. So, yeah, so and then we correlate the proportion with the hemoglobin. So, that is, as I showed you earlier. So, that is, as I showed you earlier, so this is a proportions indicator of health. So, this is a hemoglobin level in the type 2 diabetes patients. So, now we've identified three cell types that exhibit significant correlation with the hemoglobin level in the type 2 diabetes patient. And then what we did is, as I mentioned in the third contribution here, is that not only can you use a cell type as a guide, you can also use a phenotype as a guide. So that in fact we can directly run the bulk data using phenotype as a guide to learn phenotype. Phenotype as a guide to learn phenotype-specific topics. So now we can combine the two. The idea is that here I can train the cell type-specific topic distribution, and then I use that phi to initialize for each of the phenotypes. So here I have a pink phenotype, we have this dark-red phenotype. So I initialize that with the same set of CPS copies. So now during the fine-tuning, we use the phenotypes from the box data as a guide. And now we're going to fine-tune the file on each side, right? And then the difference. On each side, right? And then the difference between the phenotypes for the same topics indicating the differential express genes. So now to give you a more concrete idea here, so we apply to the breast cancer data. So here the cancer subtypes are the basal and the ER causal. So now we learn, for each of the phenotypes, we learn a set of cell type distributions. As a column indicate, these are the topics. And then we take the difference between the basal and the ER positive. Between the basal and the ER positive. So that gives us what's up-regulated in basal and what's down-regulated in ER. And here is almost a mirror image: what's down-regulated in basal and what's up-regulated in ER. Notice here, we actually did that in a cell type specific way. And then we performed sort of permutation over the phenotype label. We obtained the empirical null distribution, and then we obtained the differentially expressed genes based on the p-value. And then here, these are all the genes we detect based on p-value. These are all the genes we detect based on p-value of 0.05. And then we label the genes if they also agree with the DC2. We just run DC2 directly on the bulk data, and then we can identify differentially expressed genes. So indeed, here, most of the genes we detect are also in agreement with the DC2 genes. And then we conduct the sort of gene side enrichment. We take the genes that are upregulated in ER positive. We see what are the genes that are enriched. We see what are the genes that are enriched. In Luminal 2, indeed, so the energy gene side is ER early and ER late. So these are the gene side which makes sense. And then in the other side, where the down-regulated gene are positive but up-regulated in basal, we identify in the basal cell type, the enriched genes are enriched gene size or the T2M checkpoint, E2F, and several. So these are also sort of reasonable gene set to be enriched in that specific sort of subtype of Sort of subtypical case. So, with that, I'd like to summarize the work. So, we incorporate the cell type information as a topic prior to guide the CTS topic inference. And then we infer sub-cell type topic distribution from single-cell reference data. Using phenotype as a secondary guide, we can then infer the CPS distributions per phenotype, and which enable us to do differential analysis in the cell type specifically. So, there are a few future works. So, there are a few future works we want to scale up our method to directly model on the type of sapien, one of the largest ALS data, to infer all the cell types there, as well as sub-cell types. We can also apply it to A-type data. And also, you know, sort of having a sufficient number of the topic per cell type is a little bit arbitrary, so we're exploring a data-straight process, which allow us to do this stick breaking process, where you can actually infer what's a sufficient number of sub-cell type per topic. Number of sub-cell type per child. So I would like to acknowledge this is a very small group of people that work on this project: my postdoc, Swama, and also my master's student, Michael, on this project. So the preprint is available on BioRxiv. So the second revision is under review in Genome Biology, so Google Plus. So the code is also open access and also the format agent. Thank you for your attention. Thank you for your attention. I don't want to cut into the coffee time. So let's save the if you have questions, post it in Slack channel. Oh, we can save some coffee time, yes. No. I'm sorry. So our next speaker is Kai Yehuang. 