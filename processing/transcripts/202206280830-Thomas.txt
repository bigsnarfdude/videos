Thank you for the invitation. And so this is a joint work with Philippe Navaux and Anna Benamou. And so the goal here is to provide actually non-asentoting bounds, so non-ascenting guarantees for probability weighted moments estimators. So the probability weighted moments are really motivated by ideologists and applied statisticians. So they are widely used, especially in hydrology. Especially in hydrology. And they give a lot of information about the distribution of the sample. And the good thing is they have explicit and simple expressions for extreme value distributions. So either for the GV or the GP distributions. So they are defined in this way. So if you considered a real-valued random variable with CDF F. Variable with CDF F such that we have a first order moment. Then the priority weighted moment is defined as this quantity. So the expectation of x time f of x to the power of r times f bar x to the power s. And then you can choose different values of r and s and have different information about. And the good thing is that. And the good thing is that it's always defined for any integer RS if you have the hypothesis that you have a first order moment. And so they have a strong link with the order statistics. So for example, if you consider a sample X1 and a P sample, and you consider the corresponding order. The corresponding order statistics of X1P until XPP. Then you can define, so the expectation of the QS or the statistics of your P sample can be expressed in terms of the probability weighted, the PWM, with, so R is equal to Q minus one and S is equal to P minus Q. P minus Q. And of course, you can reverse and you can express a probability weighting moment in terms of the expectation of an order statistics. So, of course, if you want to build PWM estimators, you have different options. So, for example, if you take the deficient, so the first one on the first slide here, well, you will just Here. Well, you will just take the empirical mean and replace F, so the distribution by its empirical version. So you have actually this estimator for BWM. And you can also use the fact that it's linked to the expectation of another statistics. So here it's, so you have a sample of size M, N, sorry. Of size M, N, sorry. And you consider, so you build M, well, you build actually P samples, so samples of size P inside your huge sample. And so you take the empirical mean of the, so it's just a way to construct and build some QF or the statistics of a P sample. So you have a huge sample and you consider P samples inside, smaller, and then you consider the QF order statistics. You consider the QF understatistics and you take the empirical mean, you have the expectation of an understatistics. So, actually, these actually, this estimator has a strongly used statistics, which is good because it can actually provide a lot of good properties. And furthermore, actually, if you have no ties, you can actually express so the TQP. So, the TQP as a linear combination of other statistics with a coefficient. So, weight here, AK, which is actually defined this way. So, this is actually very good because it seems very simple. And we can have the feeling that it will be quite easy to actually study. And it's actually very simple to even implement and compute in the moon. So, that's actually a good profit. A good problem. And actually, our question is: can we obtain actually finite simple results? So that will be actually satisfied with a large probability for the PWM estimators. And what we actually want to have is minimal moment conditions because we want it to be to be actually to we want to be able to actually have properties even in the heavy tail case. In the heavy-tail case, so we want to read many minimal moment conditions. So, to obtain actually finite sample results and to have non-asymptotic bounds, we will use actually concentration theory. So, concentration of measure phenomena. So, it's basically it means that for any functions of many independent von. Any functions of many independent random variables that do not depend too much on any of them can concentrate around its mean values. So, for example, the first concentration inequality that we know is the Markov inequality. And then you have the Chebyshev inequality. So then you can see here, of course, the smaller the variance of X is, of course, the better the bound will be because then, of course, X will be the more. X will be the more concentrated around its mean value. So, of course, when you do concentration theory, well, results and attain, the first step actually to have the variance bound. Because if you have a variance bound, then thanks to the Chemichev inequality, you have an equality that's actually true with large probability. And then the second step is to have deviance inequality and have inequality of the times. Well, inequality of the times, so the two one or below here. So, for example, what we call a sub-Gaussian variable. Um, so it means that he has it has a term that is more concentrated than the Gaussian term. So, basically, um, here, so the bound here, the expectation, the exponential of minus t square over 2b, that's actually the tail of a Gaussian distribution. Okay, so then what you say is that your more. That you're more concentrated than the Gaussian distribution, which is why it's called a sub-Gaussian distribution. And then you have what we call a sub-Gaussian. So the V here represents the, so it's the variance factor. So if you are in the Gaussian case exactly, V is the variance of the Gaussian distribution you consider. But sometimes it may have, so first thing is sometimes you're not, well, it's impossible to prove for any. It's impossible to prove for any distribution, but what may happen also is that you may be able to actually prove some kind of bound like this, but the V is too large. So it's much larger than the variance of your distribution. So then you lose in terms of the variance factor. So what you want to have is that the variance factors should be as close as possible to the variance of the distribution F in general that you are considering. General, that you are considering. And sometimes, even you don't have any idea of the variance of the distribution you're considering, but you're able to prove a variance bound. So, one of the goals here and the objectives is that it's to be able to prove this kind of bounds, but with a V that is as close as possible as the variance bound that you have find in the first step. And sometimes that's impossible, and so on. Sometimes that's impossible, and well, in general, actually, even. So, then you can prove that the variable is sub-gamma. So, if you look at the two bounds here, it's the same bound. The only difference is that you have a term here in the determinator, which is C plus C T. So, C is a scale factor. So, V is still the variance factor. So, actually, a sub-Gaussian variable is a sub-Gamma variable, but with C is equal to zero. C is equal to zero. So it means that you prefer to lose a bit on the scale factor C, but you have a variance factor which is closer to the real variance of your distribution. So here, so actually in the two papers of Bouchon and Thomas, so especially in the paper of 2012, we proved that the order statistics, so prove that the order statistics so the order statistics of the the the x uh qp all sub gamma okay so we prove that they are sub gamma under the hypothesis that so the h here is sorry i forgot to to define it so it's the hazard rate so it's the ratio of the density and the survival function so one minus the cdf function and this assumption assumes actually that you are in the grumble distribution That you are in the Goomba distribution domain, sorry. So you are in the light, even well, in the light tail case, and even more because you should have, well, you should be more concentrated than the exponential distribution, because the hazard rate for the exponential distribution is constant. So it means that you are more concentrated than an exponential distribution. So it's a very strong assumption. And actually, if you use the Skynus technique, Actually, if you use the Skynus technique, because the TQP can be expressed as a linear combination of the statistics, you can actually prove that it's also sub-gamma. But what we are not happy with is the fact that we have this huge assumption saying that we are more concentrated than the exponential distribution. And as I said, so it basically means that you need all exponential moments. And it's a super strong. And it's a super strong assumption, especially when you deal with extreme value theory. So we wanted to have something to be able to have, well, still non-asymptotic results, but with really minimal moment conditions. Okay, so that's exactly what I just said. So the goal here is to obtain those kinds of results. And actually, this is not a new problem. This is actually a very classical issue in extreme concentration. In extreme concentration theory. And so, for example, if you take the empirical mean, the empirical mean, you know, with the central limit theorem that it's so it's asymptotic normal. So it should be a sub-Gaussian, actually. You expect it to be a sub-Gaussian variable. But actually, if you want to prove sub-Gaussian inequalities for the empirical mean, then you need Then you need you still need the assumption of exponential moments, but the central limit theorem is the hypothesis or just the assumptions is just that you need the second order moment. So what actually has been introduced is what we call the median of means technique. So it means that you will actually define a new estimator with the median of methodology. And we are actually what we want to do is thanks to this. want to do is thanks to this actually method to design a sub-Gaussian estimator for the PWM under the assumption that the only assumption that we will have is that X has a finite circuit moment, which is actually not a strong assumption because we know that the PWM estimators are actually asymptotic or normal under this exact assumption. So it's quite expected. Expected. Okay, so how does this median of means method work? So it's quite simple, actually. So you just, so you need to choose, the statisticians need to choose actually a probability delta. So satisfying some assumptions here, technical assumptions here. So you take your sample, size n here, and you divide your sample into blocks. Okay, so into m blocks. So block, the m actually is. block the m actually is exactly uh basically the log of uh two of a delta uh so it's joints uh joint subsets so b1 bm and so of course what we will do is recall that we want to have an estimator for the qf or the statistics of a p sample so we need to check that the the blocks are at least of size p, otherwise it will some have some kind of problems. Some have some kind of problems. And here you will, so inside and within each box, we will define the so used statistics estimators of the of the so the expectation of the q of the statistics. So here is exact is it that's actually the TQP estimator, but inside inside a block. Okay, um, and then you take them, so you have an estimator. So you have an estimator in each block, and then you take the median of all the estimators you will have. And thanks to these techniques, actually, we will be able to have sub-Gaussian inequalities because here we have a median, and the median actually is sub-Gaussian. This is how basically, so that, well, it's not that the median is that's actually no sense, but what Actually, no sense, but what I mean is we will deal with the banormal distribution, which is actually a sub-Gaussian distribution, and so that's actually how it works. And actually, this technique was first introduced for the empirical mean. Okay, so we just adapted it to the PWM estimator. Okay, so then we were able actually to obtain and to derive concentration inequality. So, concentration inequalities for this actually median of means estimator alpha hat. And we have here, so the variance factor here. And so, actually, this inequality is true, actually satisfied with probability one minus delta. The problem here is that I skipped basically the first step. So, I told you that the first step was first. So I told you that the first step was first to obtain a variance bound and then do the divience bound and then to be to well to check that the variance factor here was close to the variance bound that we had found before. The problem here is that we were not able actually to have the variance bound. So we can't really check actually how it works. But we still have But we still have the third, well, non-asymptotic bound for a PWM estimator. And so just to finish, we have two examples of applications. So the first one is a link with the extreme value theory. So I just go. So we will on the so we consider the G V case, so the maxima approach here. And so of course. Here. And so, of course, what we want to do here is to be able to estimate the xi here. And so the classical actually estimator here satisfies, so it's a solution of this equation. The problem is that there's no explicit solution. So it's not very easy to then obtain an unasymptotic bound for an estimator of psi. This actually estimator has been widely studied. There's a lot of asymptotic properties. Under the hypothesis, that psi is smaller than one half, which actually is exactly having one, well, the finite circle on the moment. So there are very few finite sample results for the PDM and US estimators. So there has been one actually, at least one work. At least one work on the explicit variance expressions in the GP case. Okay, so there's actually a there's actually you can actually show that psi can be expressed this way. Okay, so now it's basically it's the solution of another equation of the equation of the same type here, exactly. So it's not exactly this one, but and but this one is actually. And but this one is actually you can inverse it so it's you have an explicit formula for psi in terms of PWM. Okay, so that's all the so that's that's actually just PWM. So and so you can deduce an estimator for xi which is actually here. Okay, so the goal here is because we have results for the alpha hats then we hope A hat, then we hope to have a result for the psi hat, which we have. So that's actually the inequality here. So the Cn delta here, so it's here. So the maximum of, so that's actually the variance for the alphas, alpha 1, 1, alpha 2, 2, and etc. So that's actually the variance factor. So we showed that actually, so sorry, it's Showed that actually, so sorry, it's n here. So psi is actually sub-Gaussian, but for now we still do not have any Barnes bound, it's quite not easy to obtain. So something to have in mind is that so this actually equation is only true if S is a DV. If S is your G V. So if F is exactly your G V, then you have this equation. So if F is not exactly a G V, then your estimator will estimate, well, will be close to some quantity, which one would be close to Xi, but not exactly Xi. So you have a spell of misspecification bias, which is just because you have, well, the Xi is it's a It's basically an asymptotic. It comes from the asymptotic, so we are not exactly a GV distribution. So that's actually something. And okay, so that's actually good because, so it's not exactly the PWM estimators we have that we use that is widely used, but this estimators actually close. Actually, it's close to the regular estimator. And for this one, we are able to actually propose non-SM13 bounds. So the goal is really, the problem is that for the regular estimator, it's impossible to have those kind of results with minimum actually moment conditions. And then we have also an application for poverty. So, for a probability score, for a forecast score. So, it's the continuous, so it's a continuous ranked probability score. So, it's a scoring rule. So, because actually, so most weather forecast centers, they produce not one value, they produce like a full distribution or a sample of an ensemble of possible values. So, we need to compare. So we need to compare the forecasted distribution F with Y, which is a realized observation of F. And so the scoring rule allows us to actually assess the forecast performance with respect to the observations. So the definition of the COPS is here. Okay, so like, so here you have maybe no idea what is the link with the P. Maybe no idea which is the link with the PWM, but actually, it can be shown that it can be expressed in terms of PWM. And so then you just have to replace, so you can estimate it with the estimations, the estimators of the PWM. And then we are able to actually also provide non-asymptotic bounds for the estimators of the CRPS. Okay, that's it. Thank you for your attention.