And really the driving force of this project were these two PhD students in Princeton, Hu Jin Chang from Wan Hei, and of course Jiego from Princeton. And you can find the two references here. Okay, so I'm not an expert in fractons, so I will not give you a summary about fractons. I'm sure you got people. Fractons. I'm sure you've got people in the audience which are much more expert than I am. Maybe from my background of fractional automotive effect and anti-month measurement, what was interesting in this story was going beyond two dimensions and especially the fact that you have also simple models like just a generalization of the Tori code in higher dimensions like the X cube or the Ha code and that these And that these fractons have this extensive degeneracy of the ground state, which is something which is unusual from the 2D perspective. It's not completely unusual when you think about fractional tomorrow effect, but not in two dimensions, but in four dimensions. That was also one of the motivation to look at this kind of system. And more precisely, we were interested in the question, okay, what is the impact of this extensive degeneracy of the ground state in the anti-combat behavior? In the antenna behavior? Especially, is there any correction that we can relate to the ground state of Germany? And once again, the funny part here is we have all these nice toys models which are pretty simple to handle. And the question we tried to address in the first paper was: if you have the ground state of these object codes, how can you write a tensor network description for them? And once you have the tensor network description, how can you extract Once you have the tensor electronic description, how can you extract the antenna entropy? That's basically what I will try to discuss today. So, a brief analytic about the study as a code, and just a presentation. So, we have a lattice, you can think about a square lattice, a cubic lattice, where you have spin one halves. They can be on the vertices of the lattice or on the links, depends on the model that you're looking at. You can have more than one spin, also. And you have a bunch. And you have a bunch of maximum commuting operators that will help you to build your Hamiltonians. So I is just, you can have different type of operators. R is a position on the lattice. And the O, I, R are emission operators, usually a product of Z and X polymatrices. And they all mutually commute. And they square to one. We are 21. The good thing is, if you have such a model, it's pretty simple to build the ground state. You just start with all the, for example, for the spin down, which I, as a notation, I use the zero here, and you act with all the operators like this, the one plus the O by two, properly normalized state. And you can easily check that if you act with any operator on this state, you get the state itself. So that's how you build the ground state. The ground state. Let me give you a few examples of these studies of code. Maybe in one dimension, the simplest one is, well, beyond the Ising model, is the one that you is in the 7 University class and the AKLT model. So there it's just a product of three operators, the C and X and the C. So you have here three sides. Not the right picture. Not the most convenient picture here. The most convenient picture here. You have, of course, the truditory code. There you have two types of operator: the vertex operator A V, which is just the product of all the Z's. In that case, the spin, they live on the links. And the Placet operator, just the product of the four X body matrices. You can generalize this in three dimensions, you get three Tori code. This is the vertex operator and the three types of bracket operators. Of bracket operators. The X-Q model, which is one of the type 1 fractum examples, looks like the 3D toy code, except that you switch the role of the bracket and the vertex. The placet operator will be just the product of the x, so this cube here. And you have three types of vertex operators for H XZ, X Y and Y Z. And the hard code is a bit more complex, you have two spin per living on the vertices, but that's the same class in I'm sorry. Um but this is the same class in kind of story. So um tensor network, how we uh build it well you know that um if you have this is your any spin configuration and you can decompose your many body states on this basis and you can always rewrite this um many body states with weights which are built from uh matrices of tensor that you contract. That you contract. I'm sure that you're more, and you're familiar at least with the matrix product state, which is the case for the one-dimensional problem. So the object that you have here is a matrices with an upper index, which is related to the physical spin, like the spin I have up or down, and two virtual indices, H1 and H2. And the C that you have here is just a contraction of all these matrices. So that's how you. So that's how you build the one-dimensional version of this. For the two-dimensional problem, it depends a bit if you put your spin on vertices of the square analysis or on the limbs. But that's the same story. This is the example where the tensor will have four virtual indices, H1, H2, H3, and H4, and one physical index, S. And you can do And you can do the same story for the case where your physical indices are on the links. There, you will define it slightly differently with the physical degrees of freedom that you make. All your matrices will be like these two spins, like S1 and S2, and you still have four virtual instances. And you can, of course, generize the story to higher dimensions. So, how do we go from the stabilizer code Hamiltonian to The stabilizer code Hamiltonian to, or more precisely, to the graph set of these guys to their tensor network description. So, what we did was to use a very interesting trick is to decompose this tensor into objects. And we introduce this new quantity, which is a projector from this physical spin to the virtual one. And the idea is that the virtual degrees of freedom is also a spin one-half in that case. Also, a spin 1 half in that case. And this projector does a simple thing. If the physical index has a given value, then i and j should also have the same value. So instead of up and down, I use 0 and 1, but it's exactly the same story. So this object is 0, is 1, only if these three guys are LT. And now any of these tensors I've mentioned, be it in two-dimension, in one dimension, the spin or the vertices or on the links, I can always decompose. Links. I can always decompose these tensors in this way. So I have, I introduced two of these tensors in this example because I have two physical indices. So two of these projectors, the two of these G's, and this T here. So graphically speaking, I take this tensor with the four virtual indices and the two physical ones, and I split this guy into three components: two projectors, G, and G and this object T. And the point is T has only virtual indices, no physical indices. In this language, acting on the physical spin and see the action on what it means on the tensor is pretty simple thanks to the property of this projector here. Let's say I have to act with a z operator on an With the z operator on a physical screen here, it's actually like acting on virtual index to the left with the z or to the physical index on the right. Once again, because this guy proposed that all the three are identical. Similarly, if I flip the spin, the physical spin, it's like also flipping the two virtual ones. So now we can come back to the story of the finding. Of finding the tensor network description of the ground state, or let's say, for example, the 2D Tori curve. So, what I need to find if this tensor network is the ground state of the Hamiltonian is, well, that if I take any vertex operator or any placet operator, this tensor network should be mapped to itself. So let's start with a vertex operator. Remember, the vertex operator is you act with the product of all the z's. Of all the Z's. So let's say you have this, you just look at one, what's going on around one vertex, and you have all the four Z's, and you can transfer the action from the physical spin to the virtual one. So this acting on a given vertex, on the tensor, is like, on the physical index, is like acting on the virtual one like this. And if I want to satisfy this equation, what I need is this property. Basically, the t-tensor, if I act with all Basically, the t-tensor, if I act with all the z's at the same time, should be equal to itself. For the placet operator, it's the same story. Of course, I have to consider now four t matrices, but I can transfer the action of x onto the virtual links. And this will give me a set of equations like this. Now, I just need to solve these equations. And it's pretty simple, just for example, by looking at this one. Simple, just for example, by looking at this one, that what you will require is that the sum of all the indices, all the virtual indices of this t-tensor, should be if it's equal to zero, it's equal to zero if the sum of the virtual indices are equal to one, mod two, and it's one if they are equal to zero, mod two. So summing all the indices here tells you if the value should be zero or one. You can extend that to the 3D toy code. It's exactly the same story. You look at the action of vertex operators, all the three vertex operators and the blackett operators, vertex operators and the three blackets, and you get a set of equations for t that you can easily solve in this way. And this is the knife or the trivial generation of the 2D case. If you want to do the execute model, then it's a bit more involved. Involved. In the sense that you get, you write down the equation, and the way to solve it is to impose the constraints. So remember, we are dealing with a three-dimensional tensor. So we have hx, hx, bar, y bar, and hx bar. And basically the sum they And basically, the sum, the constraint that you have on these indices of the tensor, is per plane. So along the xy plane, the indices should sum up to 0, mod 2. On the xz plane, it should also sum up to 0, mod2. Or on the yz plane, it should sum up to 0, mod 2. So if you satisfy this equation, it's equal to 1, otherwise it's equal to 0. For the hack code, it's a bit more involved because, once again, the indices are on the vertices, but you can repeat the story. So, we have constructed this tensor into a description for all these models. Now, the question, once we have these guys, how can we extract, how we can compute the antenna naturally? Was this a constructive way to take any stabilizer code and associate to it? Stabilizer code and associate to it a tensor network? Or is it just that it works in these examples? Let's say, you know, getting the equation, it should be okay for all the stabilizer codes, I would say, because it's just applied the same kind of construction. There is also an assumption here about the projector, how it acts on the dimension that we have. You know, the dimension that we have for the bonds is the same under the physical one. So, if you take a more complex absorption, like one of this co-cycle I might talk later about where the bond dimension is a bit higher than just a single spin, it's a bit more tricky in that case. But in principle, you can extend it to all the cases. But what we did was wait to solve for at least these models. So, how do we compute the antennas knowing uh this denser network? Um This denser network. In general, what you do if you have one many body states and you want to compute this antagonal entropy, you start by performing the single-valid composition. So namely, you cut your space into two regions, A and A bar. And you can always rewrite it if your bases are factorized nicely into this expression here where the alpha A. Here, where the alpha A only lives in the region A, the B alpha bar only the region A bar, and you have this matrix in alpha and beta beta, and you perform the SVD on this one, you extract the Schmidt weight, and this is how you compute the antenna shot. Basically, you have this to perform the SVD, you have a new basis for the region A and A bar, which is satisfying this condition here, and from these positive weights you can extract the anti-lancho. So, if you have a tensor network description, So if you have a tensor network description, it's almost naturally writing your state as an S V D. In this example here, a square lattice where I have a physical spin here, living on the links, when I cut my system into two regions A and A bar, the boundary here of the region A will cross all the many links and I can check, I can track which links are cut. I can track which links I cut. And these, the label of these links, or more precisely the value that they have, will give me a natural notation for the decomposition of my tensor network into this expression here. Now, this is not an SVD because in all generic T, these T are not orthogonal, and also that's not true in general for the T. So that's not true in general for the T of the region A bar. So it's not in principle an LNS V. It looks like it's not. The thing is, for the TNS that we have talked about and for some natural cuts, what we can show is that the description that we have used directly provide the single-valued composition. And at the end of the day, computing the entropy is just counting. Computing the entropy is just counting how many states we have in this sum. So, let me be more accurate here. How do we satisfy the SVD condition? Well, basically, if you take two states here, corresponding to two different configurations, they are orthogonal. And this both comes from this projector. Remember that the projector enforces that you have the same indices for not only the physical spin. The physical spin, but also the virtual one. I put the physical spin on the A side, but it's, you can put it on the A bar, it doesn't matter here. So basically, these states are orthogonal because if they do not correspond to the same, the T and T prime configuration are different, then the physical spin will be different at the boundary. So the two states will be orthogonal. That's how you show that you satisfy the Geyser equation. You can do the same for the bar region. Equation, you can do the same for any bar region. You can also prove that the coefficient you have does not depend on t. So, overall, this equation is an actual S V up to a global factor. So, to check, the only question is what is the value of this coefficient? And since they're all the same, it's all about counting how many states you have. The point is, not all the configurations give you a state, a value one by valid mean. A value one by this by that I mean, it could be that this guy is up to two zero. And the way you can see it is when you contract, you will enforce. So let's say you have two tensor here. Maybe I should write it down with a contract of Now, with the contract of H4 here, 5, 6, 7. If you remember the case of the Tauric code, we have the requirement for the T tensor that the sum of these guys should be equal to 0 mod 2. Now, if you start contracting two of them, you will have a constraint on H1, H5, H6, H7, 8, H3, and H2. So, actually, let's be more explicit here. For the 2D terms, For the 2D Torrec code, you just have the requirement that the sum of these guys should be equal to 0 mod 2, should be equal to 0 mod2 to get a non-zero state. And you can count how many configurations will give you this property. Namely, imagine that your system as you go through, this is the region A and A bar. And you have L links, L, L, L, L, going through the cart. Well, how many configurations do I have? How many T's I have? I have, in principle, 2 to the power 4 L. But not all these states have a non-zero norm. And how many do I have? Well, I should only consider s I should only consider the configuration where the sum of the coefficients are equal to 0 minus 2. So I divide by 2 actually the number of configurations. So that's why the number of non-ULT is only 2 to the power 4L minus 1. And this is how I get back the Eantech entropy for the 2D Torrey code. That means the area load, the 4L, times log2, minus the topological correction minus log2. And it's a very nice way to see. And it's a very nice way to see that this topological correction comes from this non-local constraint on the system. You can extend, I mean you can do the exact same calculation for the 3D Torrey code, and you will get the usual result. Now we can apply this to the X-Cube model. So for the X-Cube model, it's a bit more diff uh it's a bit different story. Remember that we have a constraint Remember that we have a constraint not over the sum of all the indices of the tensor, but only per plane. So let's say now I take the region A to be a cube and I have L square bones going through each face of a cube. So in principle I have six L square bones. 6L square bonds. So this gives me this down here. But I have to take into account the constraint. And for that, remember that I have a constraint per, let's say here it's a yz plane and I have L of them in this direction. I also L constraint for the XY plane and L constraint also for the YZ plane. So that's why you So that's why you have to reduce this number by 3L. But when you do that, you also over car the constraint, and this will exactly give you this minus one correction. So that's how you get the antennality for the X-Q model starting from the TNS. And you see the funny story, I mean, the relation between the correction that you have here and the ground state degeneracy of the X-ray. And the ground state degeneracy of the XP model on the three torus. So I don't show the slides here for the HAC code. It's slightly more complex and the kind of cut, which is natural, is not this very nice cue, but you can exactly perform the same type of calculation and get the same results. So as a summary of this part here, So, we have a systematic way to build the stencil network for a given of this stabilizer probe. At least when the bond dimension is 2. So, that answers your question about the jet construction. And the way we build it, basically, it provides a natural way to perform the S V D. And then leads to natural ways to extract or to compute the antigen entropy for these guys. These guys. And what is the funny part here is the correction of the antangle entropy for these factor models, which is in L compared to the, let's say, the visual topological three-dimensional code, where the corrections are also constant. Okay, so the last, whatever, ten minutes? Five? Ten minutes. Ten minutes. Let me briefly Let me briefly tell you why we were also interested in the research mention for study as a code. So it's a slightly different story. Once again, we go back to the story of writing down many body states either as a tensor network or a different object. So we want a compact way to encode information of the many body states. In principle, how many coefficients that you need to describe such a many-body quantum state? Such a many quantum state, well, as many as you have the dimension of the Hilbert space. So if you have a bunch of spin one-half, you need two to the n coefficients here. And there is like this long story now of trying to encode many-body quantum states as a neural network. So basically the way to do that, well say, if I have a configuration, I need to find the coefficient. And a neural network is just a massive function to do that. Is just a massive function to do that. You provide one configuration of, let's say, spin, and it gives you the coefficient that you have in your body-betty state for this configuration. And you know it's always doable, mathematically speaking. You can find always a network that will do this encoding. So the question is not, I mean, if it's doable, but what is the most, is it the most efficient network to do it? And then how convenient it is to do any calculation with it. So, there are two ways to tackle this problem: a purely numerical one, which is say the neural network is like a massive virational wave function where we have all the intra neurons that we can, which are just not, that we can tune, and get the best visional wave function. So, it works, get a large field working on this. But we also know a lot in Konesman that we like simple. In Connect Manuel that we like simple models to understand how it works, how it takes. Think about the AKLT model, for example. So we would like to find a model simple neural network to understand how it works. And especially what we are interested in is the relation between neural networks and matrix product state. So what we have considered is a very, it's a simpler, or at least simple class of neural network that is infused machine. Network, the received for some machine, which is you have your visible spins and you have some hidden ones, and you write your wave function as a sum over all the configuration of the hidden spins of some Boltzmann weight. And what you need here is the coupling between the hidden spins and the visible one, which is encoded in this doubling weight, which should be Weight, which should be somewhere here. And you can also add some X-field on H, the virtual one, and also bias for the e-dom spin at the visible one. Once again, it's not a problem of is it doable to encode any many quantum spin this way. The theoremists say it's doable. The question is: how do you do it efficiently? Wait, I mean, you're. Wait, I mean, you're restricting the positive wave functions here, I guess, or these weights don't have to be real numbers? They don't have to be real. Oh, okay. So there is a large treasure already tried, I mean, showing how to do this kind of construction for simple cases. You can do that for the AQLT model in one dimension with an SWSO code. You can do that for the PTIF code. You can do that for the Lauffen state, and so on and so forth. But once again, we are more interested in the relation between this and matrix product states. So I think I could skip this talk, this slide, because it's basically whatever we talked about previously, just with a slightly different notation. But the idea is the following. If you look at how we encode the information when it comes to metrics product state, it's about the entanglement between the two region A and B. That's we'll give it information about the virtual or the virtual space. Virtual or the virtual space. And if you think about the network and you try to cut your system into two regions A and B, it's the same story. It's about the connectivity, for example, between the hidden spin of the region B and the visible one in the region A. So the amount of entanglement will dictate structure of the neural network. And what we have considered is this class of one-dimensional study user code to see what is the optimal description in terms of The optimal description in terms of restricted Bosswood machine and matrix product state. So, this is an example of how you can basically map an RBM to a matrix product state. So, you can group together in an efficient way the visible spins and the hidden one, and what you have is some of this hidden spin will play the role of the virtual indices in your matrix product state. State. So basically, you have this mapping from the RBM to this kind of metrics product state, where these are physical indices. We are grouped together three spins. And what we play the role of virtual indices are some of the hidden spins. Nicola? Yep. You're assuming localities for the weight system that work? I will focus on translational environment system. So in that case, I have exactly the In that case, I have exactly the same tensor in the case of the NPS for every I have to define the unit cell. But those bonds from the visible layer to the hidden layer could be arbitrarily long? In principle, yes, but for the model that we consider, the studies are really one dimension, you only have a finite branch range. And you can always define a human cell by grouping together enough physical spin to have only nearest neighbor coupling in the system. And if you do such a construction, the matrix website that you have, that you obtain, has a very specific property. Its rank is equal to 1. So the question that you might wonder, so when I take this RBM and I build the MPS, the MPS has this property of rank 1, or the tensor of window rank 1, can I just invert the property and say, well, if I have a Say, well, if I have, let's say, an MPS where the tensors are the length one, can I get an optimal RBM out of it? And that's basically the question that we've tried to solve, at least in the case of these WSO codes. So, to answer the two questions I the two questions The two questions that we solved is the first one is: when can a ground state of a translation environment study as a code can be expressed as a rank 1 M plus? So forget about RBM for the time being. Well, what we can prove is a relation on the rank of the tensor and the contents of operators of this WSO code. So without entering into the details, just by looking at the Hamilton unit, you can define these two quantities D. Define these two quantities, D, which is number of pairs of operators, and this NZL, which is also just the number of operators that contain Z and identity. So these are two objects that you can directly derive from the expression by looking at the expression of the Hamiltonian. So you can get a bound on the rank. And for a large class of stabilizer codes in one dimension, Stabilizer codes in one dimension, actually, the rank, this bound saturated rank, and it equals to one. Especially for the class of the one ecocycle model, which are just generalization of this AK alternative model written as WSO code. So that's one example here. All we put together so that the ECLT model would just be this. ZXZ model. So this one is a natural generalization where you extend a little bit the unit cells and cover more spins. And you can write down the ground state as an RBM. And you can derive the MPS and tell the MPS also rank one. So basically, the message here that you should remember is this relation between RBM and between RBM and optimal ARBM and optimal MPS is tightly related to having this property of a tensor of being a rank one. So basically if you have this property you can really find an optimal matrix product state and an optimal support machine, by that I mean with the smallest number of hidden spins and the smallest connection. Okay, thank you. If you try to look at higher-dimensional RBMs, do you wind up getting some nice constraint on the text on a like a pep state or construction? You mean so by higher dimensional, looking at a two-dimensional problem or just increasing? I think it should be doable. At least people look at that for the 2D Tori code. Now, the kind of constraint that you, I'm sure that you also get, at least for the 2D Tori code, the same kind of constraint on the rank of the tensors here. Now, for all the generically speaking, I'm not sure about this. Even for 1D system, if you look carefully, this is a statement we can make accurate only for a specific. Accurate only for a specific class of study. There are no other questions? I think this could be very good. No, I'm going to often break until ten before people talk, so that's what's possible. This one needs to be a little bit of a time, so the arbitrary lossy button. Great, so that's how much you can do.