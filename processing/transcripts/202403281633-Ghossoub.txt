The idea of finding uh robust optimizations to uh to problems of uh risk sharing. Okay, and you'll see how the robustness is going to happen here. It's through the preferences of the individuals. I'm going to try to tie it back to Stephen's talk, because essentially the preferences that we're going to use for characterizing optim are going to be robust distortion waste functions. But I'm going to start with something a little bit more general. This is, by the way, Jean-Twork with Michael, my PhD student. Work with Michael, my PhD student, who's brilliant and going to be on the job market soon in case anybody's interested. Can you teach me a little bit about this type of absolutely? There you go. Okay, so the basic idea here is what can we infer from a basic assumption on preferences which is preserving Which is preserving the concave order. Essentially, a relation to the status. I'll show you how that is for that. So, short concavity is simply defined as a functional that's concave with respect to the, that's monotone with respect to the concave border. This class of utilities is wide enough if you impose some additional properties on top of short concatenity to account for a large class of robust utilities. For instance, of the You to do these, for instance, of the form of variational preferences, and more derived some extra conditions. These are not the preferences that we're going to use to characterize optima. This is just one example of a class of true concave TVs, but there's much more here. Two points that I want to mention throughout the talk is so there are many representation results that are fairly classical by now, and I'm going to try to summarize all of them before we move on to characterizing options. Before we move on to characterizing optiva. And I'm going to show you one extra representation result that we had to use in this paper simply because it made more sense in our screen context. So in itself, it might be interesting for some of you. It was definitely interesting for us, and we had to use it in characterizing Delta. So that would be the first sort of contribution here that I want to mention. The second one is actually characterizing the and not just the existence, not just qualitative properties in the sense that this is an identical level. In the sense that this is an automatic collection, we have the combinator, but actually, I want to try to give you a semi-explicit form for what they look like. It's not going to be an actual close form, but very close to that. And we'll see how they arise in a couple of minutes. One, indeed, unsurprising fact is because of what show concavity is, clearly a common tone improvement theorem is going to play a major role in characterizing optima, and there will indeed be common tone. And there will indeed be common tone. So often I would very talk, there will indeed be common tongue. But I'm going to go one step further and show you exactly what they look like. What functions of the aggregate endowment they will need, right? That's the point. Which one function of the aggregate endowment they will need, okay? So this is the basic setting. We're going to look at a pure exchange economy. Think of it as a bit of an extension of a risk-giant market simply to allow for negative additional governance too. But later on, actually, you know, I'm not going to. Later on, actually, you know, I'm not going to have time to talk about this now, but in the paper we have an application to a risk-sharing object. So, we're going to start with a collection of acceptable positions taking for now to be at infinity. We need an assumption of non-atomicity on the basic space, but I don't think I need to sell this here to this audience. Economists are a bit sensitive to this, but I think here we should all be fine with this assumption of non-atomicity. So, we have end. Atomicity. So we have n agents in this market, each with an initial endowment xi, which could be, which could take negative values, right? This is the whole point, right? We're not in a risk-sharing market in the sense that these are not risk exposures, they are imbalanced. And the aggregate endowment is S. That's the sum of all the XIs. The idea is: how can we find a way to reallocate, optimally, hopefully, this aggregate endowment S among Endowment as among the antigens. That's a classical problem in kinetics. So the set of acceptable allocations of the eigenvalue is the set of all n-dimensional vectors in x that sum up to the initial eigenvalue. And we want to redistribute this in some way that's preferable for all individuals or the criteria. Okay, so this is the basic setting. So obviously to talk about optimality, we need some sort of way to measure the preferences of Sort of way to measure the preferences of the individuals, and we're going to assume that these individuals have utilities of the form UI over script tax. For now, this is all we're assuming. So UI doesn't have to satisfy anything at the moment. Some properties that might be useful later on, I will introduce them as we go. Okay, so two basic definitions here so that we're all on the same page. An allocation is set to the individual rational if it improves upon the status quo for everybody. Status quo for everybody. It is said to be perito-optimal if you cannot find another individual irrational allocation that improves upon it for everyone with at least one strict improvement. Now I have to say that sometimes you might see a different definition in the sense that sometimes the POs are not necessarily IRs, but I like to normalize it so that you start with the individual rational locations. One way to go around this, if you like, is. To go around this, if you like, is to eventually look at the intersection of PO and IR if you don't define PO to be a subset of IR. And that will be basically what you'll be looking for. So to avoid taking intersections and whatever, let's just assume that we're only restricting two individual irrational allocations and we're choosing the rate to optimal ones from among these individually rational allocations. Okay? So we're going to denote by script IR the collection of all individual rational allocations, script ER. George rational allocations, scriptly go for the prior to optimal allocations. Now you see that in order to characterize these, obviously we need to know more about the UIs, right? We need to know some of their properties. A basic thing that we will be imposing all throughout is monotonicity, and then we'll add on top of that as we put. So monotonicity is clearly a monotonicity with respect to the pointwise order. Some other things that you're all familiar with, I just put it here on the slide so that we're all on the same page. Translation variables. Page, translation of variance score, cache, invariance, if you talk about risk measures, concavity, was it homogeneous, law and variance on, and comment on cavity. We're going to come back to these throughout. We're not going to impose all of them at the same time, but I want to show you that in the next few slides, I want to show you what happens if we start with monotonicity and shortconcarry and add fewer of these on top of each other. What happens in terms of representing the UI until we eventually get to robust distortion? Get to robust distortion measures that Stephen discussed earlier. But robustness can be in a slightly different sense. So, obviously, one classical class of utilities that are used in the future are the monetary utilities. So, these are the complex risk measures, which you want, the variant of risk measures. But here we need concatenate because we talked about wealth rather than risk exposure. So, these are all the monotone translations. So, these are all the monotone, translational variant, and concave function, how do we call them? Monitoring utilities. So, the problem of finding parity to optimal with low invariant monitoring utilities is a fairly classical one. There's a few papers that have contributed to this. I'm not going to cite all of the literature here. I'll mention a few. Obviously, Juliet et al., Filipovic, and Sweden looked at low-invariant monitoring to these. They were able to show the existence of mitochima. Of my to optima and the conutonacimi, they did provide some characterization in a very specific case. And the hope that I have here is to provide something a little bit more general than that. Ravanati and Swindland relaxed translation invariants and obtained very similar results. Obviously, there is the work of my colleague Erodu and his co-authors, where they studied to optimize a convertible market under dual utilities and rate-dependent utilities. Depending on degrees. So, our work builds upon this to some extent, but extends it beyond that. And obviously, there's the classical work of Lutheran co-authors, Pario Caroui, etc. If you want a very good summary of the field in terms of existence and optimality, obviously the book has a fantastic reference, chapters ten and eleven and I I recommend this I recommend this for my graduate students and I don't get a cut out of the sales. And I don't get a cut out of my sales. It's just that it's a nice book to have. No, you might. So, moving on, I want to spend a few slides talking about Schulkon Cavity and then we'll see how to apply this to the director of tonight. So, a few basic things. As I mentioned, Schulkon Cavalier's monotonicity with respect to the concave order. There is, on an unatomic space, there is a tight relationship between Schulkan Cavalry and Between Schochon Cavality and low invariance, if the functionals are concave, and these are fairly classical results. This is just to show you the relationship between preserving SSD and being so concave. I mean, intuitively, this is indeed what risk aversion is, is monatonicity and preserving the continent one. A more interesting result is the following. And again, keep in mind, we're always assuming here a non-atomic space. This only works on a non-atomic space. This only works on a non-atomic space. If you start with a functional that's concave and upwards semi-continuous, weak star upwards semi-continuous, then true concavity and low invariance are basically the same thing. And you see now how you could recover a lot of the results in the literature on low-invariant monetary utilities as soon as you assume a person continuity and concavity. And concavity is typically hidden, obviously, in an assumption of having monetary utilities or a completely convex response. So this is obviously always a So, this is obviously always in the background to some extent. And so, you can see that you can build on the classical results in the low and low invariance setting using your concavity. Another interesting representation here, when you only assume monotonicity, there are true concavity on top of concavity is this dual representation of these true concrete functionals. And here you don't have the classical sort of expectation, but you Have the classical sort of expectation, but you have this worst-case expectation if you want to look at it this way. So, relating this to what Stephen was talking about earlier, you can think of this as related to a distortion risk measure. And this is indeed the approach we're going to take here. So, later on, instead of writing this m's over the y's, we're basically going to take your robust set, your robustness sets, if you want, would be a set of distortions. Would be a set of distortions, distortion functions that you construct from these y somehow. And I'll get to this in a second. Okay? So this is one basic result. If you add on top of what we have already established, if you add translation invariance, obviously you can refine this representation. Now you can restrict your uncertainty set if you want slightly. This is still a fairly classical result of Dana and 3D. And you see here. And you see here how we can interpret basically what we had before as so what we had before here on this slide, you can interpret that now indeed as a distortion of mind or a distortion or a YRE functional physics, right? So we're talking about votivities here, not misfits. So that would be your YRE functional with respect to this distortion phi of y that you define from your y and the sense, right? This is indeed what Stephen was talking about. Indeed, what Stephen was talking about earlier in his talk. Obviously, he didn't present it in this way, but this is the idea. And here, integration is in the sense of shape. So this is still fairly classical. I want to show you a slight variation that we have to work with here, which turned out to be useful in setting risk clearing, and that's in front of me. So if you have a monotone concave translation environment, so a monitor utility that is upper That is upper-semi-continuous, short-concave, and positively homogeneous. That's equivalent to assuming basically that you have a low-invariant, positively homogeneous one-to-e-to-d by the previous results, right? So this and this are equivalent under this assumption, right? So this relates it back now to the classical literature that some of you might be familiar with. So under these assumptions, you can get a really nice representation of the functional as indeed a robust distribution. Indeed, a robust distortion risk measure. But now, here the robustness is with respect to the distortion function itself. We're not measuring robustness in the sense of distribution being somewhere around self, right? It's just in terms of the actual distortion functions. And these are all complex distortion functions. So in a sense, you have a two imps here on top of each other. But we don't need to worry about that. What I want to focus on is that the robustness is with respect. That the robustness is with respect to the distortion function t. And that t is chosen just like before. Think of this t as the phiy that we obtained here through the y function, through this construction that Stephen described earlier. Okay? So think about the distortion here as coming from the quantile of that one. Yes, Roger. So how does this result differ from the Kasuka representation? Oh, it's indeed the quantum. Representation. Oh, it's indeed the Coswoka. Exactly. No, it's a bit of a refinement of the set here. So Kosuoka's set is a little bit larger. We had to restrict it a little bit. But indeed, this is the idea, exactly. I mean, this is indeed what Kosmoka was talking about. Right? Low-invariant or coherent resonance. Thank you for that. Any other questions about it? Okay. So moving on, I'm going to skip this. Moving on, I'm going to skip this. There are ways to improve on this result, but it's not too important. So, one other thing that I want to mention is when you restrict now your random variables to ones that are common atomic with the stuff, and why we're doing this. Keep in mind, we have a short concavity, so we we're monotonic with respect to the complex orders. So, the common term improvement theorem is going to applicate eventually. Prior to optimal, it would be comradotone. Product optimal would be common ton. They would be all in one legs function of the aggregate government S. So there will all be sort of common tongue with S. So this collection is going to be important and characterize often up. And that's why I want to spend a bit of time on this before showing you our results. So we can refine slightly the proposition that I mentioned earlier and that about the mention that's related to COSMOCA in the sense that now if you restrict there's a time. You restrict, there's a type over here. This should be x upper arrow. This should be this x, right? So if you restrict the u to that set, then you obtain this nice representation where now the set of t's is a little bit nicer than the previous one, at least for the application. It would be the point-wise closure of the set. This is the phi h that you obtained from the pointile as before, but now you select the h from your big set H, intersect it with the set. So you take the common atomic. This set. So you take the common atomic elements of the set H, you take their quantile, you construct the distortion like we did earlier, and you obtain this slight modification out of the distribution. So in terms of representations, this is all I wanted to mention for these utilities. So from now on, think of them as these robust instortion risk measures, and we want to try to find perito optima for this class of preferences. And the hope is to be able to get something like a crisis. To be able to get something like a crisp representation, not just existence. One interesting tool is always the relating Optima to maximizing your social welfare function. And indeed, this is going to hold here by completing. That's pretty much all you need. Let's consider this problem for a given set of ways along the I's and call this the social welfare function for that set of depletion rates. That set of Gish rates. Classical results, which are easy to replicate here, is that if all of these y's are monitoring and concave, then something is very optimal if and only if it's optimal for this, for a given choice of weights in this set. This is not surprising. If you indeed, instead of using concavity, if you use transition convariance, all these weights could be taken to be 1 or 1 over n. Doesn't really matter. Doesn't really matter. And again, this is a fairly classical result in this literature. So there's really nothing new here. And now, if we restrict the market to the Commercial market that I was mentioning earlier, Rodu worked on with Luten, my co-author, Alan Fanger. Then, obviously, we can talk about the concept of colour-to-merit optima, and later on I'll show you that they will lead to the same utility possibility. To the same utility possibility set, and under some conditions, they will be exactly the same sets. Not only do they lead to the same utilities, but they will be exactly the same under, say, strictual concavity. Again, more or less a fairly classical definition here, but let's define a comrade on periodo optimum to be any comrades allocation that's individually rational and upon which you cannot improve by another comradon allocation with a strict improvement for at least one agent. With a strict approval for at least one agent. So it turns out that you can also obtain a social welfare characterization of Commercial Optima, but here now obviously you're going to restrict to Commerce on allocations that are individually rational, not the set of all Commer Ton allocations. So now let's talk about relative efficiency if you assume only concurrently and show concality, and later on we're going to add some more properties for that. So let's start with So let's start with this basic result. So this is the utility possibility frontier, meaning the set of all utilities you obtain for all agents for an optimum. And this is the comatonic utility possibility frontier, the set of all utilities you obtain for all agents for a common atomic paradox optimum. So the thing that should not be surprising based on the comaton improvement theorem is that under shoot concavity, these are exactly the same, right? That's one of the key points here. That's one of the key points here: is that if you use monotone concave and shown concave, PO is not empty if and only if CPO is not empty. PO intersected with AC is indeed CPO. So if you take a paradox optimal allocation, and if it happens to be convolutonic, that it is indeed a combat on paradox optimal allocation, these two would be the same. And under the additional assumption of strict short vocabulary, you can show that these two sets are exactly the same. Are exactly the same. Not only do they lead the same activities, but they're exactly the same. So if we add a couple more properties here, I still have hope to finish on time. If we add a couple more properties here, we can get some nice crisp representation. And this is the basic result that I wanted to discuss here. So let's define G to be the set of all these vectors of functions gi that are all non-decreasing and some to the identity. Number decreasing and some to the identity. So this will come from the one-literature property of periodoactiva. This should not be surprising while we're introducing this set. Okay? So with this set being defined, then we can obtain a nice characterization as follows. So let's suppose that every one of these UIs is monotone, concave, shoe, concave, positive, homogenous, and transition invariant, and also continuous. So, and I'll restate this later on in terms of low-invariant more. Of low-invariant monitor utilities. But for now, let's go with that. So we know, therefore, that these have this representation, which is the stressed scope. These are robust dual utilities, if you want. That's why we call them robust GRA dual utilities. And obviously, in this case, I shouldn't say obviously, but you can show not in a too complicated way that the set of CPOs and the set of POs are non-MT, there will exist a solution to this problem, and you see. Solution to this problem, and you see why this is important in a bit. This is how we're going to get this crisp sort of semi-close form representation of periodo optima. And in the setting, period to optima are going to look exactly like this. So an allocation is going to be periodo optimal or combinatoric period here, or that's the same thing, if and only if they look like this. So, up to some sort of a translation here, this sum of the Ci's is equal. This sum of the Ci's is equal to this lower bound of S. In the context of a risk-sharing market, this is zero, right? Because all of your XI's would be positive. You don't need to worry about the negative part. This would be zero in the context of risk-sharing, but here it's slightly more general. So your colony term grade diopter will all look like this. The CIs have to sum up to this S lower bar. Gi's are of the form that we're going to construct like this in this procedure. So bear with me. procedure. So bear with me, I'll explain the procedure. Let t i1 star to tn star be one solution to this problem that we showed that I showed you earlier here on this slide. The problem of finding a set of distortions to minimize this integral. You'll see why those can be useful on a bit. So let's say we obtain a solution to this, call it t1 star tn star. And let's define And let's define the set for every given x, let's define the set LX to be the set of individuals, the set of agents in the economy, for which Ti star applied to this upper tail of S coincides with the maximum among all of these agents. Think of it as the worst case in terms of passing, or something like that. And for every agent, let's define the function hi to be any function, and here you have infinitely many choices are clearly, any function such that the sum over Lx will be 1 and the sum over the complement of Lx in N is 0, almost surely. And let's construct the Gi's to be basically the integrals of these H i's. These will be your one-dimensional functions at a very optimal. Functions at a very top. So if you're familiar with this literature on optimal insurance with this sabotage condition, this is how it's typically done at that literature. At least this is how Tim and I have worked in this direction for for a while and this is quite coming a little bit from that literature as well. Does anyone have a question? So, this is how you can construct your optima when your preferences are of the robust distortion risk measure function. Now, we can get some special cases here, and maybe it's not worth spending the rest of the time on these special cases. I just want to mention a couple of things. There's one way to restate this as follows: if you have possibly homogenous, low-invariant monitoring utilities, so think of them. To the utilities, so think of them as low-invariant coherent risk measures, then this is basically what we said earlier, we stated once. So you can show that CPOs and POs are not empty. Something is CPO if and only if it looks like this. If you have structural concavity, then this will also be the set of all varietal. Okay? Now, granted, this is not an actual cross-borne expression because you see how this construction depends implicitly. Construction depends implicitly on the Ti's in here optimizing as well. So it's hard in general to get an actual cross-form expression at this level of generality. You need some additional assumptions, maybe on what the TIs look like. Or maybe if you don't have robustness, which is a special case that you can explore here, if you have only the RA tool. So I'm not going to have time to go over that special case. I just want to mention a couple of more results and then we can stop here. Can stop here. Obviously, if you add homotonic additivity, then you get your tool of utility, there's no robustness anymore. And this is a fairly classical result. Obviously, you can see that all of this has been already shown into parts of the rotation. I'm going to skip that. This is the same characterization as before, but without the robustness. So you don't need to worry about that problem that we call problem star and finding an optimum to problem star. Lotting them into problem star, the individual TI's could be used automatically to construct your LA's, not the TI stars that you were digging for. Okay? So I'm going to close with this. How to restate this in the language of risk measures. So suppose that you have, obviously there's a lot that I'm going to skip over, how to redefine parental optimality, how to redefine individual rationality. So instead of the UI's you have a rho i and the inequalities would be exactly in the opposite sign and everything would be exactly the same side. And everything will be right. You just have to reverse the inequalities. So if all of the individuals have conotonic additive low-invariant complex risk measures, coherent distortion risk measures, if you want, then co-Is are of this form. CPOs will be, so the common temperato optima will be the solutions to this social welfare maximization with no weights. And same for the POs. They will be non-empty. And all of these CPOs will look exactly like this, just like reconstructed code. Exactly like this, just like reconstructed coding. So think of this as a complement to usually, but when you impose additionally comatonic additivity. I had an example here of expected shortfall just so that you see what it looks like. In that case, obviously the MX would have a nicer representation because we know exactly what the TIs look like. They all look like this. And you get this nice result, which is This nice result, which is to be expected based on previous work of Rodrigue and authors. So, this will be essentially what gets you your informal compilation. If you construct your optimum this way, it extends a little bit part of the result of this paper. I'm going to stop here, but so the conclusion, I'm not going to. So the conclusion I'm I I'm not going to go over this. A quick question. So in the beginning, when you set up the domain, you set up the domain at HIFI, right? Yes. So in HIFIT, you can automatically have a two-progress, right? So the lower semiconductor. Property, right? So the lower atomic infinity is constantly. But no, but you need the space to be non-atomic, otherwise. So one implication is you're right. If you start with short concatenity, you need some continuity to get low invariance. But once you have low invariance, you're automatically going to get convex ideas of two properties. And you're not going to need that continuity. Because the space is not empty. So when you have this low invariant convex, which means you're going to need Right, but you already got something. Yes, you need that bony, but you need that only for sure. But if you start with show concurrency, isn't it? Already KV1? No. Common law imply the concern, right? Equally law implied computer law, concave order implied equality in the I don't know if if you're uh if if you're you you need so if you're concave So, if you're concave, one direction does not require it, which is the direction you described, the other direction to get this equivalence that I showed initially. I'm not saying about the equivalent implied only marks. That probably doesn't need any information, I think. No, I did. But the other, yeah, exactly. No, that's correct. But for the other direction, you do move that. Yes? You showed us, you know, maybe I'm a 14-second slide in a presentation for the virus. Representation for the variational preferences. Yes, as an introduction. So, is there a link between what the results you got for print automatically and so on to the original preferences? I didn't see it. Yeah, so I mean, for more on this particular representation, this paper of Raveni and Switland gives you a perfect discussion of the spectrum. Discussion of this, but they do not characterize any host form. They just show you the existence and homotomicity of errato optimization. This is indeed what this paper does. They talk about a subclass of these preferences, the law invariant subclass of these preferences. You still have to determine how, where, and what form the roundtable will take tomorrow. So expect an email.