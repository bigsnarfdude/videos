The quantum central limit theorem and monotonicity conjectures related to entropy, a bird's eye view, and the view result. Thank you. Thank you for the opportunity and very nice day here. This talk is, I consider this as a quantum probability talk again, but it takes roots from a lot of information theory and optical sciences and a lot of other sciences where they try to send. Where they try to send data through an optical fiber, let's say, and then try to maximize the entropy or minimize the output entropy, these kinds of questions. They're about the capacity of channels, okay? And a lot of problems are, problems are really pure mathematical problems, but solving these kind of problems can give a lot of applications and a lot of understanding about several different kinds of channels, channel capacity. So, we'll see all these things a little bit. So we'll see all these things a little bit. And this is an ongoing research which we haven't solved the whole, there are a lot of problems and we just have some small ideas here and there and the research is on. Yeah. Sure. So this is the outline of the talk. We talk about the classical central limit theorem and entropy power inequalities, and then a short introduction on quantum probability, which I might Probability which I might skip. Quantum Fusion-Hudson-Central Limit Theorem, Montonsity of Entropy of the Observables. This is the part where we have some small result. And then we talk a little bit about quantum entropy power inequalities and entropy photon number inequalities. Okay, the first section: classical central limit theorem and entropy power inequalities. So, we all know the classical central limit theorem. If you are given IID samples, If you are given IID samples, let's take mean 0, variance 1, and this normalized sum will converge to a Gaussian state with the same variance as the one which you started with. If you start with a mean 0 variance 1, the converging Gaussian also has mean 0 variance 1. Then if you notice this one, the first term is just your started x, x1, let's say, let's call it x1, and second term is x1 plus x2 divided by root 2. X1 plus X2 divided by root 2, X1 plus X2 plus X3 divided by root 3, etc. Okay, then Shannon in 1940s, when he in the seminal paper where he laid out the foundations of information theory, showed that the differential entropy of the second quantity, that is x1 plus x2 divided by root 2, is bigger than or is equal to the differential entropy of the first quantity. And the differential entropy is just integral of f log f, where f is the probability density. probability density function of x. And this inequality and following entropic power inequalities were used to describe channel capacities of some white noise channels. Then, even though he had a proof that proof was not extremely rigorous, Stam in 1959 gave a rigorous proof of Gave a triggerous proof of this inequality. Then came Lieb in 1978, gave even shorter proof and gave us this conjecture. Oh, sorry, this is something which should have come earlier. From here, we can easily see that this also holds. From 2 to 1, if you know, if you just replace this by 2 here, then you can get 2 square here. So, once you know 2 to 1, you can always show y 2 to the power k is bigger than or equal to y 2 to the power k k k k k k k k k k k k k k k k To the power k is bigger than or equal to the power k minus one. But then Lieb conjectured that this happens at every step. That means in the central limit theorem, the Shannon's differential entropy increases at each step, okay, in analog to the second law of thermodynamics, where a process is there and the entropy increases over the time. So, even though this could have been a conjecture from this time, it was From this time, it was Lieb who really wrote it down in an article. Then it was 1978 and it was an open problem for a long time. And in 2004, Hartstein, Ball, Bharte, and Navar, I just say ABBN, they proved this inequality, not just this inequality, they proved more general versions of the inequality. And now this is the story of the montonicity of Shannon. The monotonicity of Channel's relative entropy under the central limit theorem. A related inequality is called entropy power inequalities. So, what is an entropy power? So, the inequality simply says that exponential of the relative, the differential entropy, two times the differential entropy of x plus y is bigger than or equal to exponential of the relative entropy of x plus exponential of the relative entropy of y. Of course, so that two factors also there when you have independent random variables. So, entropy. Random variables. So, entropy power means is defined like this. Okay, entropy power is exponential of. So, if you have an n vector x, if you yeah, if you want to just take n is equal to 1, and p of x, that is the entropy power of x, is defined as exponential of 2 over n the differential entropy divided by 2 pi e, 2 pi e. This is because the relate the differential entropy of an Relate the differential entropy of a normal random variables with mean zero with variance sigma square is likely n over two log 2 pi e sigma square. This is the relative entropy of, so sorry, not relative entropy, the differential entropy of a normal random variable. And the entropy power of x is the variance. Actually, if you take these things to the other side, so 2 over n will come here. And then if you exponentiate and what put a 1 over pi, that is what 1 over 2 pi, that is what this thing, entropy power. So entropy power is actually. Entropy power. So, entropy power is actually the variance of the normal random variable with the same entropy as your starting random variable. Okay. So, entropy power of x is the variance of a Gaussian random vector with same differential entropy as the one you started with. All right. Then, there are a few equivalent forms of entropy power inequalities in the classical case. So, this is entropy power of any real linear combination. Of any real linear combination of independent random variables is bigger than or is equal to, you can just take out those a squares outside, so these real numbers. Or that is also equivalent to the Shannon's entropy of this thing is bigger than or is equal to the corresponding Gaussian random vector with the same entropy. Okay. Or these two are equivalent. If you also have mode A summation A square is equal to one, you have a third version of. equal to one, you have a third version of this inequality, which says that simply says that entropy of this linear combination is bigger than or is equal to summation a square h of x is the same like this one. Okay. So these are the equivalent forms of classical entropy power inequalities, which are all settled by the ABBN paper in 2004. All right, that was a big breakthrough. And then what are the applications? Why do people care about these things? Lot of applications. In fact, a number of code. In fact, a number of coding theorems for Gaussian channels are proved using this. The simple proofs of these channel capacities are given by this entropy power inequalities. Very classical real analysis problems, but gives amazing applications like capacity of additive noise, non-Gaussian additive noise channels. Then there is something very interesting: this MIMO, multiple input, multiple output channels. Yeah, here, this one. A lot of wiretap channels, a lot of them, a lot like that. Now, Like that. Now, when this quantum information theory was developing, similar questions like the channel capacities of corresponding quantum channels came out. And then people started looking at how these things were done in the classical case. And several inequalities like the entropy power inequality were conjectured. And many of them are ruled. The literature is really huge and vast, developed mainly by engineers. Developed mainly by engineers, physicists, optical scientists, quantum information theorists, all these people. But still, there are a lot of unknown questions here. So I like to put this into the setting of quantum probability, which we discussed yesterday. I'm not going to describe everything in detail. When is my 12:15 is oh, I have some time. Okay, good. So let's. Time okay, good. So let's start with a state which is a post-traid trace class operator with trace one, and pure state is just this one. And if you have, and like we discussed yesterday, if you have an observable that is a self-adjoint operator, this observable quantity of a physical system gets a probability distribution on the real line. It is just like when you try to measure it, when the state system is in the state row and you try to measure this observable, you get a distribution, classical probability distribution. That is precisely what. That is precisely what the distribution of x in the state rho. And that can be that comes probably in the next slide. But the spectral theorem for the self-adjoint operator enables us to describe all these things in a mathematically rigorous fashion. Okay, so a distribution of an observable is just mu rho x e that is a classical distribution on the real line. This is a real, this is a subset of the real line Borel subset. Line Borel subset. This and this is the spectral measure of your x, and trace c x rho, that is the mean of your projection corresponding to the set E in the state row. Just that. That is the distribution of an observable in the state row. And you can talk about the expectation, just expectation of that classical measure, classical probability distribution is your expectation of your random variable, quantum random variable. Similarly, any moments and variance. Moments and variance. Then, when we talk about quantum central limit theorem, the most fundamental version of quantum and central limit theorem was proved in Fox space by Hussian and Hudson. To describe that, we need a little bit about these objects, the Fox space and fundamental operators, position, momentum, etc., on the Fox space. And we saw a lot over the last few days about these things. As a short reminder, you can talk. reminder you can talk if you are in a finite mode setting that means your degrees of freedom is n uh your phase space is a two n dimensional real Hilbert space real vector space and then the Fox space is symmetric Fox space over H and you have these operators known as either displacement or whale unitary operators they are exponential of the field operators okay and then you this is a very since these are unitary operators This is a very since these are unitary operators, you can talk about all the other operators using that. These are all familiar formulas: position at the jth, sorry, momentum at the jth coordinate, position at the jth coordinate, annihilation, creation, all those operators satisfying the CCR relations. All right. Then comes the definition of a, what do you mean by a Gaussian state? We have seen it a lot yesterday. I'm talking only about mean zero Gaussian state for simplicity in this talk. Gaussian state for simplicity in this talk. So, a state is mean zero, one way to say is that its quantum characteristic function trace rho w x plus iy is equal to exponential of negative of a quadratic, okay, positive quadratic. Then you say that the state is Gaussian. This is also equivalent to say that the rho is Gaussian if and only if the distribution of the field operators is a classical normal distribution for every u. So, these are equivalent things. You can talk about Gaussian states either using their covariance, I mean. Covariance, I mean, using their quantum characteristic function, or equivalently, you can look at all these field operators and look at the distribution of these field operators with respect to your given state. If all of them are Gaussian, that is an equivalence, okay, equivalence. I wanted to stress this because this was not discussed in the meeting. People know it already, but since we were seeing only this line, I thought, okay, let's say talk about. This line, I thought, okay, let's say talk about this as well. Okay. Then comes the Qshan-Hudson central limit theorem from 1973, I guess. For that, they need to define what is meant by a beam splitter unitary in the optical sciences literature, but in mathematical form, it's a very simple thing which we already know. If you have a, your phase space is Cn, and if let's say you have a unitary U. And if let's say you have a unitary U on CN, okay. Unitary, okay. Then there exist another unitary. There exists a unitary, let's call it gamma u for this board wherever I'm writing, but I will not use this notation later. Gamma U unitary. On your box phase such that gamma u w of let us say some little u and gamma u dagger is equal to w of u u for every u in your C n. U in your Cn. These are known as the second quantization unitary operators, or they can also be defined like gamma t also can be completely defined by gamma u acting on exponential vector is equal to u exponential vector at u v. I did not define what is an exponential vector, but these things exist. These are not. Things exist. These are known as sometimes called as second quantization unit tree. But in some specific cases of this unit tree, operators are very important for optical sciences, especially your communication through a lousy channel like fiber optics or whatever. Yeah, even though my background is mathematics, since now I work at an optical sciences department, I get to talk to these people, you know, and I also teach a course in the optical sciences department from which I picked up all these things. I picked up all these things. So there is something called a beam splitter unitary. So beam splitter is a gadget, it's an instrument which you can see in an optical lab. This prism kind of a, you know, like an instrument like that. It can take two inputs of light and it gives out two inputs out. And it gives out two input outputs of light. And the input of light, when you, if you are having, let's say, some coherent states alpha and coherent state beta is going through this beam splitter. This is how they notate it usually. Okay. And then whatever comes here at the output, so this alpha beta, if you think of it as a vector, the output will be just if this has a transmissivity lambda, I'll talk about that later, u lambda of alpha beta. u lambda of alpha beta so gamma u lambda u die so u lambda of alpha beta the coherent state u lambda of alpha beta will be coming out of that thing so basically what i'm trying to say is that this second quantization unitary which we have has a physical representation in an optical sciences lab i when i realized that i found it really cool you know anyway so that unitaries can explain how to define convolutions of states okay so for Okay, so for these optical scientists, this convolution is just some passing some lights through some light through this beam splitter and doing it again and again. Okay. So that is why I just wanted to explain this thing. This u lambda which I represent here is actually gamma of a unitary and that unitary is going to be this unit. This is a unitary two by two. Let's say we are talking about two by two matrices, one mode. This is a unitary matrix. mode, this is a unitary matrix, and that unitary matrix gets a second quantization, and that second quantization is what they call as beam splitter unitary. Okay. And you can also talk about write it in terms of annihilations and creations of the both the modes like that. All right. So this unitary, so now onward just to be clear, I'm not writing the gamma every time. I'm just writing u lambda itself as the second quantization unitary which acts in your. Quantization unitary, which acts in your fork space. All right. Then we can do it. It's very easy to define the quantum convolution using these unitary operators. If you are having two states, rho and sigma, and lambda is a number, which is usually known as the transmissivity or loss of the beam splitter you have, then the quantum convolution of the state rho with the say sigma is defined as you take rho tensor sigma. You take rho tensor sigma, apply this unitary transformation, and then take partial trace second with the second output. This thing is called the quantum convolution of rho and sigma. This thing has similar properties like your convolution of probability measures. That is why it is called quantum convolution and it comes up in the central limit theorem as well. Now, in terms of the characteristic function, the characteristic function of I think I made a mistake here. This is the characteristic function of rho star lambda at z. Okay, that's the characteristic function of this guy. That is what I wanted to write here. That is chi of rho star lambda sigma at a point. Okay. He is actually the product of the characteristic function, but there is a factor coming here. This is like the convolution of normal probability measures, usual probability measures, where, yeah, sorry. So, what I want to insist is that, or I want to highlight, is that it is like the convolution of two measures. If you take the Fourier transform of the convolution, you get the product. Here also you get the product, but there's an extra lambda factor coming here. All right. And this is the convolution of any two states. It's a Lambda convolution. Then you can talk about something in normalized convolution. Normalized convolution. Normalized convolution. Normalized convolution, just define it for two states with it. Take lambda is equal to half. If you define it for two states, it's called normalized convolution. Now you can iterate this thing, you know. What you can do is that you can define a k convolution of states rho 1 up to rho k by taking k minus 1 convolution and then taking 1 minus 1 over k here as the as lambda for the next one. So in this way, you can iteratively define your normal, some what is known as normalized. normal some what is known as normalized convolution okay start with two just let's say rho one and rho two row one star rho two is rho one star half rho two and then do it with this formula one minus one at each step this is called the k times normalized convolution of states and if you have single state and if you come convolute again and again we just say that rho star k or take a k times convolution of rho and a modern version of the quantum central limit theorem even though Version of the quantum central limit theorem, even though Hushen and Hudson initially proved the central limit theorem in terms of the positions and momentums, a more modern version of the central limit theorem states like this. Yeah, I'll come back to this thing later. Yeah, here is the central limit theorem. Just like classical probability, if you start with any probability measure, converge with again and again, you will converge to a normal distribution. That's a weak convergence of measures. Weak convergence of measures. Similar to that, if you start with any state on L2 of Rn or a finite mode box space, whatever, with finite second moments, you should have second moments defined for the state, okay? Then only you can talk about these things. Then you convert the state again and again and again. This will converge weakly to a Gaussian state. I think this thing is having some mission. I think this has a role on the other side. Yeah, at least this is working. Yeah. So rho k converts to a Gaussian state with the same covariance matrix and mean. Okay, that is the Fusion-Hertzen central limit theorem. And this convergence, initially I told that it's a weak convergence. Beak convergence of states are written like this. For every bounded observable x, it goes, it converges. Trace x rho k converges to trace of x rho. All right, that's the convergence in big topology. That's a convergence in big topology. This is, in fact, a price norm convergence itself because we are dealing only with states. We can act. Yeah, yeah. So we are talking always about the, after talking about the fixed covariance matrix, that is, with respect to P and Q. Yes. I should have told it earlier. Thank you. Yeah, this convergence that's a minor fact. It's just almost a corollary. It's just almost a corollary. If you have V convergence for states, you will get a convergence in trace norm itself. All right. So this was Cushion-Hudson's central limit theorem. And then there is a. So we started with a, you can start with any state with mean m and covariance sigma s, and then it. No, that is the case when you talk about random variables. If you talk about measures, Quantum variables. If you talk about measures, just measures, think of just classical measures with finite second moments. Okay. Then you convert it again and again, you'll get converts to a normal distribution. That is one version of classical central limit theorem. That version should be reminded. Yeah, but the definition of convolution is what you know giving you that. That's not something which we do by a hand, you know. Convolution just gives you that. Anyway, we can think about it any way we want. Is it convolution in the standard sense or is it convolution in the sense of generalized composition? So, in classical case, standard convolution, in quantum case, this convolution. But covolution is a definition. Yeah. It must have some standard. Are you talking about classical? Yeah, please. Yeah. Take rho of x plus y over the square root of two, rho of x minus y over square root of two, and you integrate out to y, then you get the convoluted stop. Could you please repeat once again? So if we rescale, you can convolve and rescale, but it's still five minutes of five. So it's not just convolutions. Yeah, this is what I defined as normalized. Now it's working. Normalized quantum convolutions. Okay. Yeah, I think we can discuss about the proof or definitions later, but it's a normalized convolution which is defined something like this. Under this convolution, you have the quantum central limit theorem, and it's very easy to prove in this framework. Fusion Hudson took a lot of time to develop these ideas and prove the thing. But if you look at it in this way, it's just one-page proof, you know. So then comes the idea of comes the idea of sorry about that one-human entropy one-hour entropy of a state is defined similarly to the one-human entropy of a classical random variable entropy of rho is trace rho log rho with a negative sign okay just like integral of p log p something like that okay similarly defined and and then similar to the classical case one can show that Classical case, one can show that Von Human entropy of row star 2 is bigger than the Von Hermann entropy of the first one. And just like the classical case, again, you can replace with rho star 2. You can take it to this side and prove it for 2 to the power k steps. So, one-normal entropy increases under quantum convolution. When I say quantum convolution, it is that normalized convolution increases at each step. In the 2 to the power k steps, question: Does it increase? Question: Does it increase at every step? This is the first monotonicity conjecture which I would like to present. This was, I think it was officially, it was first written down in the PhD thesis of Saikat Guha, 2008 PhD thesis, even though this was a problem which could have been thought about earlier. Okay, this question is really unknown, and the interest about this question right now, we don't. Question right now, we don't know really any applications as such for this inequality. It's a very beautiful thing if you can prove, but I at least don't know any applications of this inequality. But it can give, it is believed in the community that it can give insights to other problems related to the endroom inequalities, which have applications. Exactly. That is precisely the reason why people are interested in entropy power inequalities, actually. And under central limit theorem, it can also be shown that entropy actually converges. Entropy of the states will converge to the entropy of corresponding Gaussian state. That can be shown. But whether it converges monotonically is the question. And you have some evidence here that in these steps it happened. And also, you can do some numerics on simple steps. You can do some numerics on simple states and show that, okay, whatever you are checking, it is happening, you know. Yeah. All right. This is a small result, which comes, small corollary coming from ABBN's result combined with the quantum central limit theorem. You can actually show that the differential entropy of the distribution of quantum random variables xp plus yq with the state with respect to the state rho star k. state with respect to the state rho star k increases monotonically. So we have quantum states, you have the field operators, the canonical operators, and you can talk about the and these operators do have a distribution on the real line and then those are classical distributions. Okay. In this framework of the central limit theorem and the proof if you read carefully you can actually show that the differential entropy of these observables will just increase in each step. In each step. Okay. And that is a small result which we have right now. It is not anything surprising, or I don't think that's very surprising, but it's not seen in the literature. That's all. Yeah, so this is interesting. But then a question arises. If you can show that these are, there are p's and q's are in some sense canonical. Okay, so let's think about one mode. You have canonical p and q. And you know that the distribution of any linear combination of p's and q have this involotronic. Have this monotonicity property under the central limit theorem. So, the question is: can you convert the entropy of these observables in some sense? You can optimize over some set or something and then get back the quantum relative entropy of the state. This question is a difficult question, I think. We tried a little bit, we could not find. But if we can find the relationship between the one-normal entropy of a state, there we don't have any observable. Okay, it's an independent, there's a state and it's right here. It's an independent, there is a state, and it's a one-known entropy. But we have some canonical observables which say a lot of things about these states and things like that. And what we know now is that if you consider this class of all canonical observables, the entropy increases. Can you connect these two things? We don't know right now. That's an idea which we tried and we could not get anywhere with that. But if there is a way to do this, then there might be a way to prove the original inequality. We don't know that, so we leave it there. We don't know that, so we leave it there. Sure. Now comes the quantum entropy power inequalities. The two forms of quantum entropy power inequalities were conjectured by Guha again in the same MIT thesis. And one is the entropy of rho star lambda is greater than or equal to lambda times s of rho plus one minus lambda times s of sigma. And another one is entropy again. Yeah. Yeah, this is just like the classical entropy power inequality. I'm not explaining really a lot here. Then, this also remained open for a few years, but it got finally settled in 2014, I guess. Yes, 2014, two articles came almost same time. In the first article, Koenig and Smith proved this inequality, the second one, first inequality, and second one for the lambda is equal to two case. Okay, well, lambda is equal to half. Case okay, well, lambda is equal to half case, and immediately after that, there was a nature photonics article which was written by De Palma, Murray, and Giovanni. And they proved it for all. They fully settled this problem. This problem is no more an open problem, which is finally settled in the literature. But these are not the really fascinating inequalities about these objects. Real useful inequalities, this has some applications, but This has some applications, but the inequalities which can transform the field is actually the so-called entropy-photon number inequalities, which we will discuss now. Entropy-photon number. So, this is so the first entropy-power inequality of classical random variables was you fix the variance of your state and then talk about the entropy. Okay. So, that is like you know, the power, the budget you have. You have this much of budget of power of your signal. This end variance and power are very much. Signal, this variance and power are very much related. And then you want to send these things through this channel. And fixing that power, how much more information can you retrieve? How to maximize your capacity? That is the idea. Here, when you are using optical transmitters, for example, laser light to use, you have a fixed budget of photon numbers. You want to produce the photons and you want to, you have a fixed photon number. Let's say you have n is your budget. You have n photon, so you can send per second. So, you can send per second, okay. Then you want to maximize the capacity of the channel, that is the idea. So, that is why this photon number inequality comes into the picture. So, the idea is if you have this, let us read this thing first. g of x is equal to 1 plus x log 1 plus x minus x log x. This function, let's call it g function for the time being. This is actually the one-on-one entropy of a thermal state with mean photon number x. Sorry, so mean photon number is nothing but Number is nothing but so mean photo number. Let's write is write is write it n bar. This is equal to trace row a dagger A. A dagger A is your number operator, the mean of that operator under this with under the state row, that is the mean photo number. You fix that to be a specific number. And with that mean photo number, if you consider the thermal states with mean photon number, the Thermal states with mean photon number the x, you get g of x as the mean photon number of that, sorry, unknown entropy of that state. Now, you can define the entropy photon number n rho is g inverse of s of rho over n. So, the entropy photon number of a state rho is the mean photon number of a thermal state with the same monomanian entropy as that of rho. This was the exactly same case when you had a classical situation. The entropy photon number, entropy power was actually the Power was actually the fix after fixing the variance, you talk about the entropy of the states, entropy of the observable, oh, sorry, random variables. Yeah. So similar to that, here, entropy-photon number seems to be the most natural candidate. And also in Guha's thesis, it was shown that if you can prove these two equivalent forms of entropy-photon number inequalities, then you can prove a lot of coding theorems about Gaussian channels. Okay, so. Okay, so entropy photon number of row star lambda sigma is this convex combination of the first one and the second one, and that is always bigger than or is equal to the corresponding thermal state with same mean photon number. These two are equivalent is shown, but you don't know whether it is really true, these inequalities. And there is one more thing. We can actually say the first inequality in the APNI implies the entropic power inequalities. Implies the entropy power inequalities, which was proved by Koenig Smith or Pipalma et al. But the converse is not known. Therefore, these inequalities are actually still open. And why do we care about this entropy power entropy photon number inequalities? This will settle the conjectured capacity of thermal noise channel. Minimum output and entropy conjectures. These related conjecture conjectures prove the capacity of bosonic. Prove the capacity of bosonic broadcast channel and many others like that. Privacy of the wiretap channel, etc. Okay, so a lot of questions about channel capacity can be answered if you do these kind of problems. Very fertile area, but a lot of people have already tried and a lot of literature. It's very difficult to go through the literature and know what is known and what is not known because people take specific kind of channels, you know, specific kind of states, specific NBA. Specific kind of states, specific environments, and then prove inequalities. Okay. So, in some special cases, several inequalities are known. But I think photon number inequality is not known for any case, I think. But a lot of literature is there. We need to go through it and find out what is known and not known. But it can give a lot of good applications, real world applications. So, with this, I stop. Thank you. Your definition of the entropy photon number. But interpretation, the meaning of a thermal state with the same phenomenon. But the thermal state is defined also. Yeah, actually, I need to do a calculation. I don't know in my head, but the point is that you can get any entropy, any positive entropy you can get using thermal states. So, if you are given an entropy, just take the corresponding thermal state with that entropy. Given a number, State with that entropy. Given a number, take that thermal state with that entropy. You can get that. You can solve for S basically that thermal parameter. So all of these will also follow from the sharp convolution, Young's convolution equality, right? I'm not very aware of that things, those things. Okay. So the statement is very easy. So the statement is very easy. You have the convolution and you ask the sharp form of the convolution, Young's convolution in the inequality in the quantum setting. And if that is true, that will imply this quantum entropy power as well as this entropy quantum number conjecture. Oh, very nice. I was not aware of that till actually. Yes. So my question is in the Kalaxo setting, the deep conjecture of this Conjecture of this theorem as an alternative short proof by Thomas Coulot using the facial information and the maximum correlations. Yeah, it is the data. I'm not talking about the proof in this kind of in this in the quantum setting using this approach. I'm curious if we have anything like the maximum correlation. The maximum correlation of sum of IP random variables in the quantum setting. Do we have analog results of this? I think there are some analogs, but I could not connect it properly. I looked at Cotard's paper, but I was not able to connect that idea of maximum correlations with the quantum probability one. But there are some ideas in quantum probability like that. If you take your normalized convolution power, there should also be a formula of the characteristic function in terms of the characteristic function of the state of the total. Yeah, I did not explain that actually. Yeah, that is here. That's here. So, if you take the normalized convolution of rho, then the characteristic function of that at u is chi rho u. Of that at u is chi rho u over square root of k to the power k. This is exactly the same like classical probability theory, and that is why the proof of central limit theorem becomes very easy in this setting, in this framework. Is that also possible here? Yeah, actually, the three probability one, we had looked at Shilendekov's article. We had looked at Shilendakov's articles immediately after ABBN. Shilendakov and some other people proved all these theorems for free probability, but we are unable to put that into the quantum probability setting. There are several results nearby, but we are just not able to get this particular thing. That is something which we are looking at right now. Can we really put it into a quantum Markov semiconductor? A quantum Markov semi-group. That is something which we are trying actually. This whole thing, if you can see a semi-group, then probably we can talk about the invariant states and you know, yeah, that is another ongoing pursuit. Yeah. And that is the connection of this talk with this conference, by the way. I was just wondering, just an idea, if the beam splitter, yeah, splitter. Is it related to a squeezing channel in some sort? It reminded me of the operator that you should. Squeezing is when this unitary is a symbolic matrix. Instead of unitary here, you take a symbolic matrix. You take a symbolic matrix, you get squeezing. Okay, and the corresponding channel, you don't have a representation like that. You cannot write this simple representation of the output. You have to go through some phase-based integrals to do that, you know. So it's not straightforward. But squeezing is also a operation which optical scientists are very interested in, and they do it on a regular basis. But the problem with squeezing is that even now, the state of the art squeezing, which we can achieve. Of the art squeezing, which we can achieve in a lab, is just one, where mathematically zero to infinity squeezing is allowed. You know, it's very hard to really do that in a lab. Yeah, but to say the squeezing channel is similar, but you cannot write a simple representation like this. And there is just one more thing. There is an article which I wrote with K.R. Parsaradi in which we give an explicit representation of the squeezing operations, you know, how to write the output of a squeezing operation. Um ovation. Thank you.