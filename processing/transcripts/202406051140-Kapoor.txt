Over the course of this conference we've been hearing from a lot of people and I think one sort of upshot is that AI ML models are not just oracles for predicting the future, instead they intervene on a number of domains. So one question is what does this intervention look like? And the sort of set of domains that AI and ML are potentially useful for has sort of expanded and there are claims so much so that there are claims that these technologies are quote unquote general purpose technologies. Quote, general purpose technologies. Of course, this term is somewhat ambiguous. And so, what we're interested in is printing down which domains will adopt this technology and how will they specialize and what are the implications for welfare and performance. And we try to answer these questions using a simple model. The basic ingredients of the model that we're interested in are generalists sort of producing a technology that may be adapted and used by one or more. Adapted and used by one or more domain specialists. And already we have sort of an interesting what we're calling strategy feedback loop here, where any individual domain specialist strategy will depend on the capabilities that the generalist has developed, but the generalist's investment level depends on which potential downstream users of the technology will take up the technology. So there's some interesting dynamics at play here. So, we model this interaction as a game. The game proceeds in three simple steps. The last two steps are: first, the generalist and then the specialist each invests some amount of effort in the technology, improving its performance at some cost. And then it brings some revenue. So, before any of this occurs, they have to agree on how to share the revenue. And we use bargaining and bargaining solutions to come to this agreement. So, bargaining solutions are a foundation. So, bargaining solutions are a foundational concept in economic theory. The setup is quite simple. Given k players labeled from 1 to k with utilities, they have to agree on some parameters x. And how do you agree? There are normative approaches to this question and descriptive approaches. But broadly, like the way that we apply bargaining solutions is by specifying some joint utility function and then optimizing over that joint utility function to now. That joint utility function to navigate toward a bargaining solution in the sort of Pareto optimal set. And here are three examples. You might want to maximize the sum of utilities or the minimum of the utilities. And these are various solutions that have been proposed by economists over many decades. So here's a depiction of the various bargaining solutions that we find. And in particular, we find that even if the ratio of the costs of the two players Of the costs of the two players are quite high. Bargaining will still lead to meaningful revenue sharing and investment by both players, which perhaps provides some intuition about why we see something like OpenAI not just claiming the entire AI pipeline, but entering into some agreements or opening up the model for downstream practitioners to use. And this was that last plot was just with one domain specialist, but we have recent work sort of. We have recent work sort of generalizing these results to multiple domain specialists. And I don't have time to talk about this, but they're really interesting geometries of the sort of Pareto optimal set and bargaining solutions being the sort of like stems that move through the Pareto space. Finally, the last sort of finding I'll highlight here is that we find that any domain specialist will fall into one of three buckets. They'll either abstain from using the technology altogether. Abstain from using the technology altogether, they'll free-ride, meaning they take it off the shelf without investing additional heifer, or they'll actively contribute to the technology. And these depend on certain conditions of their cost and revenue functions. So just in conclusion, we set out to sort of understand this interaction, this intervention. How do technologies intervene in a variety of domains? And we sort of study these fundamental behaviors of adaptation and joint production of a technology. And joint production of a technology. We use bargaining as a way of modeling this sort of interaction, and we classify specialist strategies depending on their cost and revenue function. As a future direction, we're really interested in policy and regulatory interventions, including liability regimes. And we're also interested in describing the technology using a vector of features instead of just one feature, which we've been calling performance. Thank you. Mark can come down, but does anyone have a question prepared? I have a question, which is sort of like the domain-specific, like the sort of the game, the dynamics of the game. Do you guys consider that there might be domains where there's a higher cost to fine-tuning than others and how that would affect your business? I think the game is a model. Oh, yeah, you can see that. Yeah, so depending. Yeah, so depending on the cost of a particular domain, they might not take up the technology at all. And so if it's prohibitively costly, for example, to enter or use the technology. And so we actually use like zero and first order approximations of the different costs of the domains to determine which of the three sort of buckets strategies that they fall into. I think Michael Morematic curiously, like have you thought of maybe how that would be operationalized in different ways? Operationalized in different ways, like the idea of making that. I think people do sort of play that game and they have that trade-off, but I'm curious, just like pragmatically, what kind of data you think that would actually go into informing that kind of decision, even at a university level. Yeah, so what I think is sort of nice about this finding is it uses marginal information. So, at zero investment, what's the sort of intercept and slope of a cost function? So, this is sort of like an MVP or like zero. Sort of like an MVP or like zero investment read of like the cost function of a domain. And so we think this is actually pretty easy to like to implement empirically, but we haven't done anything along those lines yet, but I think it could be really cool to do empirically. Thank you, Ben. All right, okay. We have Maud of the PhD student at the MIT Institute of Data Science and Systems. Of data science. Oh, and society. Data institute, data systems, and society. IDSS. I said it wrong for a year. Hi, everyone. It's so good to be here today and this week. It's been such a fantastic time with the conversations. I'm Hamad. I'm a PhD student at MIT, and today I'll be presenting some joint work with a great team of co-authors, Van Mary, Neil, Lauren, Lester, and Allison. I work a lot on the equity of experimentation, specifically in the context of health, and today I'll be talking a little bit about our work. And today I'll be talking a little bit about our work, Should I Stop or Should I Go? on the early stopping of randomized experiments with heterogeneous populations. That's the wrong button. Oh, great. Let's send the button. Oh, there you go. Oops. Sorry buttons. Yes. Oh, can I go back now? I love you. Again, sorry. Let me just do it again. Sorry. Alright, there we go. Okay, so, sorry about that. So, randomized experimentation is a goal-standed method to identify causal effects. So, this could be in a clinical trial where you want to test the efficacy and safety of a new drug. This could be in an A-B test, where you assess the effect of a new product feature on system usage and other metrics. Product feature on system usage and other metrics, or maybe more relevant to this audience, an RCT where you test the effect of a new predictive model on human decision making in potentially a high-stakes domain. So best practice for a randomized experiment is typically to specify the sample size and duration ahead of time. So you say, I'm going to run this experiment with 2,000 participants over the course of two months. Do the experiment, collect your results, and then at the end, analyze your data. However, experiments often need to be stopped. However, experiments often need to be stopped early if the treatment has an unintended harmful effect. So if you're running a clinical trial and it turns out that your drug actually increases the rate of serious side effects, you likely want to end that trial early to protect your experimental participants. Or if you're deploying some AI system and it's worsening human decision-making, you likely want to do the same thing to protect the people who are affected by the decision-making process. Now, there are a bunch of methods that do this. These are collectively called stopping tests. They've been studied from sort of Stopping tests. They've been studied from sort of 1948 with Abraham Wald to today. New methods have been developed. And these methods typically dictate when you can stop an experiment early for harm. These methods are designed to do two things. First, if the treatment is harmful, they're designed to detect that harm with high probability. And if the treatment is not harmful, they are designed to not stop the experiment. So if you think of the null hypothesis as being the treatment is not harmful, they're designed to maximize statistical power, which is really that first condition. Statistical power, which is really that first condition, whilst controlling the type 1 error rate, which is the second condition. Now, these existing stopping tests, standard practice is to apply them homogeneously to the whole population. So you aggregate all your data, run your stopping test, decide whether the stop will go. However, the motivating question behind this paper was: what if the experimental population is heterogeneous? And specifically, what if you have a situation where a minority group in your experiment is harmed by the treatment, but the treatment Is harmed by the treatment, but the treatment may not be harmful in aggregate. And so that led to this question: which is what happens if you run these stopping tests if only a minority group of participants is harmed? And what we found is that all existing stopping tests were not able to stop experiments where this was the case. So if a drug, for example, is harmful only for elderly patients, but not for the rest of the population, what we found here on the y-axis, we plot stopping probability, so the probability of stopping the experiment. On the x-axis, we show the harm. On the x-axis, we show the harmful treatment size, so the magnitude of the harm. And each line here shows you results for a different size of my neurotransmitter. And if you look at the line that's boxed in yellow there, where you have 10% of participants harmed by the treatment, so elderly patients over the age of 65, the existing stopping tests will never manage to stop the trial, or very rarely manage to stop trials. And so then the natural question is: what can we do about this? Can we adapt existing? About this, right? Can we adapt existing stopping tests to better detect this harm? And so that's the focus of our work. We developed CLASH, which we believe is the first broadly applicable solution to heterogeneous early stopping. Clash stands for Causal Latent Analysis for Stopping Heterogeneously, but the real reason we called it Clash is because the papers called Should I Stop and Should I Go? If you have a chance to make a DD's punk rock reference, I think you should. The Clash works in two stages. The clash works in two stages. Stage one is causal estimation, and stage two is a stopping decision. So, in the problems of this workshop, you can kind of think of stage one as being the prediction element, and stage two as being the intervention, where you're intervening on an experiment, whether you want to stop or continue. So, in stage one, you use standard methods to estimate the probability that each individual participant that you've recruited so far is harmed by the treatment. And then in stage two, you use these estimated probabilities to reweight any existing stopping test. Rewate any existing stopping test in a way that makes them more sensitive to subgroup HOM. This is some more details on how this method works. I won't talk about this here. I'm happy to talk about this more detail in the poster session. But I think the key is that both stage one and stage two use existing methods and requires minimal overhead over existing experimental procedures. So there are three key considerations with Clash, right? So I told you already that we want it to have two key properties. We want it to have two key properties. One, if a group is harmed, if a minority group is harmed, you want Clash to be able to detect that harm reliably. On the other hand, if no group is harmed by the treatment, you don't want Clash to tell you to stop the experiment, right? Because that's a sort of false positive. And third, a more pragmatic consideration is we want this to be easy to incorporate into existing experimental procedures, which is really to ensure that practitioners can use this. Use this. And so, this is a quick summary of why this works. I don't think I have time to really dive through the evaluation, but I'm happy to talk more about this at the poster session and hopefully convince you that our method achieves all these three conditions and yields you effective heterogeneous topping in conditions where you have a minority group of participants being harmed. So, I'll end there. I'm excited to discuss more of the post-accession. Does anyone have questions? Does anyone have questions? A quick question or two for that? Go ahead, Dan. So the stage one, you have to estimate energy, stream effects, and just curious how do you think that? So you can use any method that gives you a point estimate and a standard error around the point estimate. So we're agnostic to the specific method that you use, but anything that gives you those two things will be sufficient. You could use linear regression that you use. Use a linear regression. The easiest way of causal forest is another thing that you could use to do the same thing. And in experiments, we tested both primarily causal forests, but linear regressions were the software sounds. Thank you, Maud. So next we have Sasha Pierre who's a patient-student after instant in computer science. All right. Does it work? Okay, perfect. We'll see how it goes. All right, so I think a lot of the discussion we've had so far today has been in some sense descriptive. Punky's work is a little bit on the normative side, but here I'm going to focus on something that's much more explicitly normative, which is what are the ethics of designing social media platform experiments. And what I mean by that is basically the sort of experiments described on this slide. The sort of experiments described on this slide. I think perhaps the most famous or the most infamous example is around 10 years back when Facebook released this study where they were changing the emotional balance of a certain number of posts that you saw on your newsfeed. The study received immense sort of critical negative feedback, and I think people had a number of concerns with it. So these concerns were things about the lack of informed consent. So many people expressed that people could not reasonably opt into being experimented on in this way. Into being experimented on in this way. There were concerns about an unexpected purpose. So when you log on to social media, you don't really expect to be experimented on in this way. You don't really expect that the platform is treating you as a guinea pig, so to say. And finally, there are concerns about concrete harms to subjects. So for people who are facing mental health issues, for example, this very experiment itself could have caused concrete harm. And I think around the time, a lot of the tech ethics community also had the same reaction. In fact, this is joint work with Angelina. In fact, this is chart work with Angelina and Arvind. So when Arvind used to teach this course in this case in his tech ethics course, I think he had all of these concerns being the primary focus. I think he likes to joke that nowadays he teaches this for the exact opposite reason. He likes to teach us as an example of how he went wrong in his analysis of social media platform experiments. So in general, over the next five minutes, I want to point out a number of flaws in the way we reason about platform experiments, and especially the tech ethics community has reason. Especially the tech ethics community has reasoned about platform experiments and how we can get beyond some of these. In doing this, my aim is not really to defend Facebook and how it conducted this particular study. I mean, there were a number of flaws. Matt and I have been discussing a lot of them over the last few days. But I do think there is a case to be made for running more of these platform experiments in the sense that I think the tech ethics community was directionally wrong in its prescription to avoid platform experimentation out of them. So the first thing I want to say. So, the first thing I want to point out is that a lot of the reaction to this study over-relied on moral intuition. So, moral intuition