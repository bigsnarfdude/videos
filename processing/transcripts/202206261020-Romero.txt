So so hi everyone. So thanks a lot to the organizers for putting this workshop together. I'm going to talk about the following topics. The sample complexity of Thompson's classical estimator and more precisely about the mean square error and benchmarks when estimating Gaussian. When estimating Gaussian stationary series, then that will lead us to the topic of the number of tapers versus the bandwidth of the estimator. And second, I'm going to refer to similar problems on domains, where we take samples not on the real line, but on a domain. And here, the main topic I want to refer to is the difference. Topic: I want to refer to is the difference between computing the Sleepian tapers or the tapers periods and actually computing the multi-taper. So let's start with the classical multi-taper. So this is the standard setting. We have a time series, let's say it's real and zero mean, and we observe n consecutive values. And this is the And this is the multi-taper. So we take k-taper periodograms and then we take the average. And the periodograms are tapered with, of course, with the Slepian tapers. So the other parameter around is the bandwidth of the Slepian tapers, W. And this is, of course, an estimate for the spectral density, which is the Fourier series of this correlation function. So, the parameters are the number of tapers and the bandwidth of the Slepian tapers. And some questions one may keep in mind are how to choose the parameters, what's the mean square error, and what's the sample complexity, meaning how many samples are needed to achieve a certain mean square error. And moreover, we can ask whether certain specific Whether certain specific choices are optimal for this kind of estimation problem. So, in order to discuss this formally, the first thing I want to do is to introduce a concrete error metric. So I'm going to refer to the uniform norm. So that's the if S is the spectral density of the time series and S hat is the estimated spectral density with any Density with any estimator, the uniform norm is the maximal deviation between those two over all possible frequencies or over the fundamental domain, right? So these are periodic functions in this setting. And I would argue this is the right error metric to consider. Of course, the problem formally depends on the choice of the metric, but I would argue this is the most meaningful one. And one important motivation to consider this metric is that this corresponds to the spectral error for covariance estimation. So concretely, this number here, the L infinity distance between the spectral density and the estimated spectral density, is also the maximal absolute value of the eigenvalues of the difference between the true covariance of the time. Covariance of the time series and the estimated covariance by means of the estimator S hat. So the Fourier coefficients of the estimated spectral density. And this is, of course, also the same as the maximal error in little L2 norm that we accept when we compare the action of the true covariance on a vector and the action of the estimated covariance on a vector. The estimated covariance on a vector. So, this is a meaningful metric. So, the corresponding mean square error is the expected value of the square of that quantity. So, the expectation of the square of the L infinity distance between the true spectral density and the estimated spectral density, the expected maximal deviation or the expected square of the maximal. Square of the maximal deviation. And this, well, this is in my opinion the true meaningful metric as opposed, for example, to something that maybe is more common in the literature, the expected quadratic error for a single frequency. That's a weaker metric and maybe not so meaningful as the unit. Maybe not so meaningful as the uniform one. So, to discuss Minsk Cuero, I'll stick to the most classical setting one can think of. The time series is Gaussian and the spectral density is C2. And then we wonder to which extent we can estimate the spectral density of time series with n observations. So, for this problem, for for this problem there is a clear benchmark i'm quoting here a result from 2013 so here we see the quadratic expected value of the l infinity distance between the the true spectral density and the estimated one so what i'm calling the mean square error and then we take the the maximal error over Similar error over all possible spectral densities with respect to a certain estimator, and then we consider the best performing estimator. So it's a minimax risk for this problem among time series with the C2 density. So that's the normalization to be able to define this risk. And it turns out that. This risk. And it turns out that if we have n samples of such a process, the exact sampling rate of the exact sample complexity of this problem is log n over n to the 4 over 5. This symbol here means that the ratio between the two quantities is bounded above and below. So they are certainly constants in this estimate. And what we prove with my colleague Michael Spekwacher is that Thomson's classical multi-taper achieves this optimal rate. So if you let S hat be the multi-taper, then you hit exactly the benchmark for this particular model of the spectral estimation problem. Gaussian time series with CT. Gaussian time series with C2 densities. And actually, I should say Thomson's multi-taper can hit the benchmark because we have to choose parameters. So to hit the benchmark in this particular setting of the C2 density and a Gaussian process, the number of tapers and the bandwidth need to be linked by the usual time frequency product equation. Product equation. So the number of tapers needs to be the maximal one allowed by the uncertainty principle. And moreover, there is one specific choice of the number of tapers or equivalently of the bandwidth that leads to this bound here. And well, these results actually follow from an explicit bound for the mean square error. For the mean square error that is valid for an arbitrary k under this relation between the number of tapers and the and the bandwidth. And optimizing k in that equation shows that we can hit the benchmark in this asymptotic sense. Okay, so I want to stress that there's hidden constants in this symbol here. And the first thing I want to mention is that Want to mention is that the literature on the multi-taper is full of references to other choices of the balance between the bandwidth W and the number of tapers K. So these are typically mentioned in relation to the so-called high dynamic range, where the smoothness of the spectral density may be May vary with the frequency, which is somehow not accounted in this model where we take a uniform bound on the C2 norm. So I wish I have with respect to this question is I would very much like to see a formal model justifying other choices of K and W. Just as we have here a precise benchmark for Benchmark for this kind of spectral estimation problems and a result showing the optimality of the multi-taper with this choice of K and W. I would very much like to see a similar analysis showing the formal advantage of any other choice of K and W. And some definite first step in this direction I've seen recently in this paper here that's that That can be found in archive, I think. Yes, can be found in archive. So, here there are some analytical estimates showing why one may want to consider k below this critical value. But I would like to see a full formal model and a precise minimax rate. So, that I want to propose for For discussion, and I want to mention briefly something about the proofs here. So, the bias of the multi-taper can be written in this well-known form. So, it's an integral average of the true spectral density with a certain integral kernel, rho. And this is called the spectral window, which The spectral window, which is the average of the absolute value square of the Fourier series of the Sleptian papers, where we take k of those. And it's been observed already in Thomson's original paper in the 82 that this one is rather flat on the target frequency band and then it's small. So this figure that I took the leave. That I took the liberty to take from the original paper is in logarithmic scale here. So this is quite flat. So this can be used to do a heuristic analysis of the bias, of the multi-taper, because one can here replace this spectral window by the bump function on the target. Bump function on the target frequency band, and then once we have a fair average, we can do any sort of estimate involving the smoothness of the spectral window to come up with a precise number for the bias. And together with my colleague Daniel Abreu, we gave an integral bound for the deviation. Integral bound for the deviation of the spectral window from the indicator function of the target frequency bound. And the bound one gets on this integral deviation is log n over k. And again, this is valid exactly when the bandwidth and the number of tapers are linked by the maximization of uncertainty. So this one here, okay? Of uncertainty, so this one here, k equals n w. So, this to some extent justifies this picture here. And again, the use of this equation is that, well, since this is an estimate for the deviation in an integral sense, one can replace the spectral window by this bump here and then quantify exactly the Quantify exactly the difference because this is precisely an integral average. And for example, if you plug this estimate, it's very easy to obtain a weak mean square error bound. So to argue that the expected quadratic error in the estimation of one single frequency is n to the minus 4 over 5. And to get the strong To get the strong mean square error bounds where the supremum is inside the expectation, one needs to use something a bit more sophisticated about Gaussian process. So some form of concentration of measure for quadratic forms. But these are just technicalities. So this is how one can show that the multi-taper hits the benchmark for this particular estimation problem. Let me Let me say something more about this. So, the main, of course, the main truth is the profile of the concentration of the celebrant typers, this famous plateau picture. And there is a very important work that I should mention from Lee and Rosenblatt, where they calculate the bias for individual tape per periodograms. Taper periodograms and then they argue or they give some sort of heuristic arguments for the bias of the multi-taper based on the so-called Scheb asymptotics for the Slepian eigenvalues, so the eigenvalues of the tablets operators whose eigenfunctions are the Slepian functions. But they don't have estimates. And the reason is that And the reason is that what they do is they study the or they suggest an estimate on the bias of each taper periodogram based on the behavior of these eigenvalues. But it's very hard to take an average over all those estimates because those classical estimates for the asymptotics of the eigenvalue. The asymptotics of the eigenvalues of the Slipian functions, they work in a regime which is incompatible with that average. So what we did with my colleague is we noted that estimating the average of the Slepian functions or the absolute value squared of the function. Absolute value square of the Fourier series of the Slepian functions is much easier than estimating each particular function. So actually, the proof of this L1 deviation estimate is almost straightforward. It's almost trivial, as opposed to estimates for the bias of each taper periodogram. So that will be the first message I want to convey. The first message I want to convey: that the analysis of this average is much, much easier than the analysis of each individual periodogram. And the second thing I would like to talk about is a multi-taper on domains. So here the problem is the same as before. We get a real We get a real stationary process, but instead of considering the integer points, so a time series, we consider a grid of dimension D. And the true new component is that we observe this process on a rather general domain omega. And the geometry of omega plays, of course, a role. So, there is again a multi-taper estimator for this problem. So, formally, it looks the same, but the difference is that we taper the observations with the Slepian tapers adapted to, well, say a D-dimensional bandwidth set, so a cube minus W over two, W over two, or it could be a Over two, or it could be a disk or a sphere, it doesn't matter. And the time concentration is given by the acquisition domain omega instead of a simple interval as we did in the case of dimension one. So this is exactly or quite similar to the context of the talk of Frederik's mom. I think maybe there was even another talk about this. So, the two parameters here are the number of tapers and the bandwidth. And the natural choice is the one that saturates the uncertainty principle. So, the number of samples, the cardinality of the domain omega, times the measure of the bandwidth. With target set. So, the questions you may have here are the same as before: how do we choose these parameters? What's the mean square error? What's the sample complexity? Is there any sample optimality? But besides that, there is something very particular about the multi-dimensional setting. There is a real or claimed numerical instability or a problem. Instability or a problem a stability question with the calculation of these Lebanese tapers when the domain omega is possibly regular, which is a setting that is actually quite relevant in practice as explained in the talk by Frederick Simone. Maybe I would like to give you another motivation for this problem with rather irregular geometries, omega. This is something I got into while working together with my colleague Joachim Anden. So this is a problem where we receive, say, an image on a certain domain, but we know that the image is actually concentrated in the central part of the domain. So there is signal plus noise in the middle of the domain. Plus noise in the middle of the domain and just noise outside. And we would like to use the outer part of the image to calculate statistics of the noise to hopefully then denoise the inner part where the meaningful signal is. So it's a problem you may have, for example, when imaging when the calibration is totally known. The next time you use the instrument, the calibration is different. So with one picture, we have. One picture, we have to do the full estimation of the noise. We are not allowed to tune the input. So, hopefully, it should work like this. You get the noisy image, you use the outer part of the picture to estimate the noise, and then hopefully that leads to a denoised image. So, in certain fields, the common In certain fields, the common way to do this is to select a little box around the corner and do tensor products of Lebian tapers to estimate the noise here and here and here and here. Maybe take an average of the four corners, and that's an estimate for the noise. And actually, that doesn't fully exploit the data we have because we are not considering, for example. We are not considering, for example, the samples here, here, here, here. So the multi-taper adapted to this domain exploits better the available samples. And so completely the domain omega here would be the complement of the disk within the square, which is where we have samples of the noise, say not contaminated with signal. So there is a certain There is a certain perceived problem with this approach, which is the calculation of the slipt and tapers. Again, we can consider the time frequency optimization problem, where we optimize the frequency concentration of the functions to a target d-dimensional interval and the time concentration to a domain like this one, the complement of the disk within the square. And again, one can argue that the And again, one can argue that the eigenvalues of that problem have a plateau profile, but it is well known that it's not so easy to calculate the eigenvectors. In the one-dimensional case, there is a special trick due to some special symmetries, and I think we've heard about that. But in other cases, you may not have symmetries to exploit. And there is a certain, I would say, confusion in the multi-taper literature that to some extent regards that as an obstacle for the calculation of the multi-taper. So this is another point I would like to bring up in this workshop. One very simple observation we made with my colleague, Chokim and then is that if we take any orthonormal basis of the span Normal basis of the span of the Slepian tapers, and we calculate the multi-taper with this arbitrary orthonormal basis of the span of the Slepian tapers, we get exactly the same result as if we use the true Sleptian tapers. So the individual taper periodograms do depend on the particular taper one uses, but the average depends on the span, as long as the tapers are. As long as the tapers are orthogonal. So, what we call proxy tapers is any other orthonormal basis of the span of the true Sleepian tapers. And the point is that because of this plateau profile, it may be hard to calculate the first Lepian tapers just because it's hard to distinguish from a linear combination between the first one and the second one. But because of this beautiful plateau profile, the problem of capturing the span. The problem of capturing the span of the first so many Islepian tapers is very well conditioned. The conditioning of that is driven by this gap here, which is a good one. So conclusion, the problem of calculating individual taper periodograms may be ill-conditioned in some setups, but the problem of calculating the average is well conditioned. And concretely, this means that if you do SBD, you may get some warmth. You may get some warnings, but you will most likely get the right result because even if the SBD fails to calculate the tapers, most likely it will give you an orthonormal basis for their span. And actually, with my colleague, we propose something a bit more straightforward. We just take the Slepian matrix adapted maybe to any regular domain. We input the right amount of random vectors, just noise. We apply the We apply the matrix, say, two or three times, and then we orthogonalize the output. And whatever comes out from that is surely not the Slepian tapers, but it's a new family of tapers that does the same job, that with very high probability computes the same thing. So it's a standard way of doing low-rank approximation. And this to some extent, To some extent, answers the question of whether this can be computed in a stable way. And by the way, the proof of this equality here is straightforward. This is very easy to show. It's a two, three-line calculation. And the final thing I want to mention is now that we discuss domains, what about these bounds for the mean square error? So here the So here, when we have a domain, besides the number of points in the domain, another important quantity that shows up is the digital perimeter of the observation domain. So the number of points in the domain that are next to a point outside the domain. And we proved with my colleague Michael Spegvach some mean square bounds under a reasonable Under a reasonable assumption on the perimeter of the domain. Meaning that the way to maybe to think of this condition is you should consider, you should think of discretization of an analog domain. There is an analog estimation problem, and then you have a grid and you discretize everything. So if the perimeter of the discretized domain scales as you would expect with the underlying analog domain, Underlying analog domain and the number of tapers is linked to the bandwidth by the time frequency optimization condition, then we can obtain this mean square error bound where n here is the number of samples that one takes. And that requires, of course, a particular choice. Again, for a Gaussian. Again, for a Gaussian process with a C2 density, normalized to one, let's say. This follows again from a mean square error bound valid for general K by optimizing. So here, the third question I would like to bring up is the optimality. What are the benchmarks for this problem? And this is actually something that I don't know. With my colleague, we proved some benchmarks for the problem. Group some benchmarks for the problem of estimating a Gaussian time process on a observed on a rather general domain. And this we did simply by extending or imitating the techniques that work in dimension one. And the bounds we obtain are not really satisfactory because they involve the diameter of the acquisition domain instead of the number of samples, which may be Number of samples, which may be fine in some cases, but for example, for this domain that I mentioned before, the complement of a disk within square, they are not too precise because this domain, for example, has a large diameter in proportion to the number of points. But it is true that if you apply these bounds to a reasonable domain, On a domain such as a square or a rectangle, the performance of the multi-typers, say in dimension two, is very close to the benchmark. So n to the minus two over three, and then the difference is in the power of the log factor. So I have a very partial answer to the question of the optimality in dimension two, only for some specific domains that may not really fit the fully. The fully regular setting that may be useful in some applications. And the other thing I should mention is that the bias analysis in this multi-dimensional setting is again through the so-called spectral window. And even in this general setting of rather general acquisition geometry, one can prove that the spectral window of the multi-taper is asymptotically closed. Is asymptotically close to a flat, an indicator function of the target bandwidth interval with similar bounds, but which now incorporate the perimeter of the acquisition domain. So to conclude, this is the question I would like to bring up. Do we use a number of tapers which is exactly given by the maximization of the time frequency? Time frequency uncertainty product, or do we use less? And actually, I would like to know why one would care to use less. And I would like, moreover, a formal answer to that question with a precise statistical model that is optimized with a different choice than K equal N times W to the D. When K, this question is interesting in dimension one, but is particularly important in higher dimension because as Dimension, because as I argued before, when k equals exactly the time bandwidth product, there are no instabilities in the calculation of the multi-taper. But the same argument does not apply to other choices of k. So what I would like to better understand why one may want to select a different value of k. And in particular, I would like to know if it's about selecting just k. It's about selecting just K Slepeon tapers of the first or the first K Slapian tapers. So, for example, I would like to know if selecting the first K proxy Slepian tapers produces results similar to the ones you would expect when you choose a number of tapers below the critical value. So, thanks a lot for your attention, and I'll be glad to take questions. To take questions.