Thanks so much for inviting me. So, maybe compared to many of the most of the audience, I'm actually from a different background. I'm mainly like from the machine learning community. But in the past few years, I'm very interested. I've been very interested in AI for biology, in particular, AI for drug discovery. So, today I'm going to talk about some of our recent work on geometric deep learning for drug discovery. So, I'm going to So, I'm going to give a quick introduction to drug discovery. So, in general, drug discovery is a very long and expensive process. So, in general, if you want to develop a new drug, it takes like more than 10 years and also costs like 2.5 billion US dollars. So, a typical process of drug discovery is like this. Suppose you have a disease, let's say SARS-CoV-2, you first have to identify a drug target, for example, for SARS-CoV-2. Drug target, for example, for SARS-CoV-2, the drug target is the SMI protein. So, then in the stage of drug discovery, so we're gonna identify a few leading molecules, and those molecules will be validated in the preclinical stage. Most of the time, it will be on animals like mouse. So, if the molecules can pass the preclinical stage, they will be further evaluated in the drug development stage. In the drug development stage, the clinical trials. So, most of the drugs will fail in the clinical trials. So, the percentage of the drugs which are able to pass the three stages of clinical trials is only like as low as 10%. And so that's why we want to use AI in particular, machine learning, to accelerate the process of drug discovery, also increase the success rate of drug discovery. Of drug discovery. So nowadays, like we see a huge opportunity for AI to accelerate the process of drug discovery because a huge amount of data are generated in the biometric domain or in the drug discovery area. So for example, we have a huge amount of like biometric literature, such as PubMed. So nowadays, we also have a lot of like a public database, for example, PubCAN or the PDB database. So a lot of like data from the So, a lot of like data from the clinical trials as well. And nowadays, also, there's also some huge progress in various, for example, the high throughput by assays, which are also able to generate a lot of data. So, by combining all these data, so we can use AI to analyze those data. And so, we can instruct patterns, so we can generate hypotheses, or we can make predictions. For example, we can predict which molecules are more likely. Predict which molecules are more likely to bind to the protein target or to be more likely successful in the clinical trials. So that's why we see a huge opportunities for AI in drug discovery. So talking about data, so what are the most important data in this domain? So they are molecules, for example, small molecules or proteins. So for both small molecules and proteins, they can be represented as graph. For example, a molecule can be So, a molecule can be represented as a graph of atoms. So, each node is an atom. So, the bounds between the atom can be treated as the edges between the nodes. So, we can also represent proteins as a graph. Each node is a residue. And the special distance between the residues can be treated as edges between the nodes. So, besides representing molecules and proteins as graph, And proteins as graphs. So, in practice, we can also represent both of them as 3D structures because naturally, each atom or each atom in both small molecules and proteins can be naturally represented as a 3D coordinates. So, that's small molecules and proteins. So, another very important data in drug discovery is the biometric notch graphs. So, biometric notch graph is essentially a heterogeneous graph between different kinds of like biometic and Different kinds of like biometric entities, for example. So it involves like the connections between disease, drugs, and the proteins or genes. So with biometric knowledge graphs, so we can actually use it for many important applications in drug discovery. For example, based on biometric notch graph, so we can predict the causal relationship between genes and also disease. So this is very important for applications like. Is very important for applications like target identification, or we can try to predict the relationship between disease, disease, and the drugs. So, this is very important for applications like drug republicanism. So, you can see that really in this domain, many of the important data can be easily represented as graph or 3D structures. And that's why we need to use geometric deep learning, graph machine learning, or graph neural networks. neural networks in drug discovery. So my group have been working on different kinds of problems for drug discovery. So in the beginning, we mainly work with graph-based methods. So essentially, we represent molecules as graphs, and then we use techniques like graph neural networks and combine with other techniques for different problems in drug discovery. For example, we have worked on like a molecule. We have worked on a molecule property prediction. So, essentially, here, given a molecule or a compound, we use graphene network to learn the repetition of the molecules. And then, based on the repetition, we can predict the property of molecules. And then, so the second problem we work on is called like de novo molecule design and optimization. So, in this problem, essentially, we try to search for a few molecules which have good. Molecules which have good a few totally new molecules which have good properties. So, in this direction, so we've been using techniques like deep gen models and reinforced learning. So, first we can use deep gen models to generate chemically valid molecules, and then we can use reinforced learning to search for molecules with good properties. Okay, so let's say now with deep joint models and reinforced learning, so we've identified a good molecules. A good molecule like this. And then the next question is: how to synthesize, how to make this molecule? And that's related to the third problem called like retrosynthes prediction or retrosynthesis planning. So essentially, given a designed molecular structure, we want to identify a set of like reactants based on which we can synthesize the target and molecule structure. Okay, so you can see that for all these like. So, you can see that for all these fundamental problems in drug discovery, so we are mainly working with molecules, as I said in the beginning. So, we can represent molecules as graph. And that's why we can use the techniques like graph neural networks combined with other techniques like deep gen models and the reinforcement techniques for different applications in drug discovery. So, I will not go into the details due to the time limit. The details due to the time limit. Okay. So now we are also sort of like trying to go into another direction because traditionally, so you can see most of the time we represent molecules as graph. But now we see actually a better reputation to represent molecules because a better way to represent molecules is to represent molecules as 3D structures. Because the 3D structures are actually a more natural and also intrinsic reputation. And also intrinsic repetitions for molecules because the 3D structures or the 3D conformations determine the biologic or physic activities of molecules. And that's why it's a better representation. Of course, in practice, actually, there's a challenge because for most molecules, their 3D structures are not available. And that's why, either in computational chemistry or computational biology, a 3D structure prediction. 3D structure prediction is always a very fundamental and also important problem. So, in computational chemistry, basically, the task is like given a molecule, a molecular graph, we try to predict its 3D structure. And in computational biology, maybe you're more familiar with the problem, which is really too alpha-to, right? So, you are given an amino acid sequence, and then you try to predict the 3D structure of proteins. three structure of proteins so in my group we mainly work in in the past like a few years we mainly work on like uh uh predicting the three structures of swims of small molecules so as essentially here we're trying to trying to learn this mapping right or in other words we're trying to model this conditional distribution given the input molecular graph g how to how to determine the the the 3d structure of the 3d conformation okay so we've been working on like a So, we've been working on developing different kinds of deep gen models for this problem in the past two years. So, a very challenging problem, a very challenging problem for this problem is that. So, you can see that for the 3D structure or the 3D conformation, it's actually rotation and the translation invariant. So, what does that mean? So, if you rotate and/or translate this confirmation, This confirmation, the coordinates of the atoms will be changed, but it remains the same confirmation. In other words, if you rotate and translate the confirmation, so the probability, this probability shouldn't be changed. So that's something you need to build in your division models. So in today's talk, I'm going to talk about our latest work on developing division. On developing diffusion model for this problem. So, diffusion model recently has been very popular in different areas, including image, speech, or even reverse learning. So, here we present the first diffusion-based model for molecular 3D structure generation. So, there's an essential idea of the diffusion model for this problem is like this. Problem is like this. So, for diffusion models, so here C0 is basically corresponding to your real-world data distribution. So, in this case, it's the real world molecule conformations. And then, so we have this forward process or diffusion process. So, in the forward process or diffusion process, at every time step, we're going to add some noise. For example, from CT minus one to CT, so which is parametrized by this distribution, we're going to add some noise. So, most of the time, it's just a Some noise. So, most of the time is just a Gaussian noise. So, here there are no parameters, which, in other words, this distribution doesn't need to be learned. So, we just add some noise at every tan step until its end. So, we will map the real world distribution to a noise distribution. So, usually, this noise distribution is a Gaussian distribution. So, in other words, for diffusion model, so we're going to map the data distribution from the data distribution to a From the data distribution to a north distribution. So I was maybe I stop at here. Okay, so basically, here I want to say, like, so the diffusion model essentially trying to map the real world distribution to the noise distribution. Okay, so in the forward process, we just add some noise at every turn step. And in the reverse process or in the journey process, so we are going to start from some noise and at every turn step, we're going to do some denoising. We're going to do some denoising. So essentially, we try to eliminate the noise from the current structure until at the end, we can recover the ground truth conformation or 3D structure. So that's the essential idea of diffusion model. So here, so there are some mathematics. Essentially, as I said, in the forward process from C0 to Ct, at every time step from Ct minus 1 to Ct, which is parametrized by this distribution. Which is parametrized by this distribution. So essentially, we just add some noise, add some Gaussian noise. And so that's the forward process. And then what we really care about is really the journey process. So we need to determine. So in the journey process, we need to first determine the prior distribution PCT, PC capital T, and also at every time step, which is a denoising process, starting from C T. From Ct, we're going to eliminate the noise from this structure to get a cleaner structure, which is Ct minus one. And this process is parametrized by this distribution, okay? Because here G is molecular graph, so we can always condition on our input molecular graph. So that's the mathematical formulation of GODF model. So for this problem, a big challenge, as I said in the beginning, is how do we model the How do we model the rotation and the translation invariance of the conformation? So, C0, C0 is all bronchous molecular conformation of input molecule graph. So, as I said, if you rotate and translate this 3D structure, so it remains the same molecule structure. So, the probability shouldn't be changed. And that's something we need to model in this diffusion model. In other words, the probability. Diffusion model. In other words, the probability induced by this generated process should be rotation and the translation invariant. So that's something we need to consider when we try to build this diffusion model for this problem. So this is a proposition in our paper, but the essential idea is that. So in order to make sure this marginal distribution or the data likelihood. Or the data likelihood is rotation and translation invariant, we basically need to satisfy two criteria. First, this prime distribution, PCT, should be rotation and translation invariant. In other words, any structure from this noise distribution, if we rotate and translate it, the probability shouldn't be changed. And also, in the denoise, in the in the in the denoise in the denoising process from ct to ct minus one so this distribution should be uh rotation and translation equivalent what does that mean it means if you rotate or translate this conformation the output ct minus one should be rotated and translated accordingly so that's the essential idea of this model so i will skip the details So, I will skip the details and maybe show you some results. So, we evaluate our models on two standard data sets for molecular graph, molecular 3D structure generation. So, the cuminizes relatively small molecules and drugs and relatively big molecules. So, we can see that our methods are able to significantly outperform existing methods, especially on big molecules. Especially on big molecules like drugs. If you look at this data set, so our methods are able to really significantly outperform existing state art methods. So let me also show you some concrete examples. So here are different input molecular graph. And then you can see that our models are able to generate diverse conformations for the same input molecule graph because this actually makes sense because for input This actually makes sense because for input molecular graph, so in it may have multiple stable confirmations, okay. And here I show you like visualizations. So this visualization basically shows the process of our model to find a stable structure for input molecular graph. So you can see that like in the beginning, so we start from a random structure. In other words, the coordinates of each The coordinates of each atom is initialized randomly, and then at every diffusion step, we try to do the denoising, we try to do the denoise. So we try to correct the positions of each atom until at the end, you can see that like each atom find its right position, and then we will the whole process converge. So we will stop further doing the denoising process. And that's how we are able to find a stable confirmation. Find a stable conformation from a randomly initialized structure. So this is the first part. So you can see by now I mainly talk about using graph neural networks or graph machine learning to model small molecules. So recently, so my group, so we also been very interested in using geometric deep learning for modeling big molecules. Big molecules like proteins. So, for proteins, a very important problem is how can we learn good protein representations. So, this is very important for many applications like protein function prediction or protein-protein interactions. So, we've been very interested in building foundation models for protein replication learning. So, you know, recently there's a lot of interest in the community. In the community for building foundation models in different areas. For example, we've seen a lot of efforts in building foundation models for natural language processing, computer vision, based on models like BERT or Transformers, right? For example, nowadays we are able to build like pre-trint language models in natural language processing or in computer vision. But how to build foundation models for drug discovery, for molecules, for proteins? Probably for molecules, for proteins. So, there's a lot of space out there. So, here in particular, we focus on building foundation models for learning protein repetitions. So, in the community, I know, so there are already many work of trying to build in foundation models based on protein language models. Because for proteins, essentially, so the first thing we can do is like we represent proteins as amino acids. Proteins as amino acid sequence, right? And then we can use like a sequence model, like bird models. So we can pre-train a bird model or protein language model on a huge amount of MNS sequence. And then later, we can fine-tune the model based on some downstream labeled data. So, here the motivation of this work comes from the following. Because, as you know, like a full protein, a better reputation. A better reputation is their 3D structures, which determine most of their biologic activities. So, therefore, a better way to learn protein reputations is based on their 3D structures. So, in particular, in this work, we've been working on trying to pre-train a geometric protein encoder based on their 3D structures. So, we make use of a lot of protein 3D structure data. Protein 3D structure data, so including the original PDB data, as well as the recent release, like 3D structures which are predicted by R42. And then we propose some self-supervised learning techniques on top of these 3D structures. So let me give you a main idea. Suppose here we have a 3D protein structure, and then what we Protein structure. And then what we do is we're going to construct a graph out of this 3D structure. So essentially, in this graph, every node corresponds to a residue. And then, so we have different kinds of like edges or regions between the residues. For example, so we can use the special distance between the residues as edges between the nodes. Between the nodes, or we can use the sequential order information in the sequence as edges between the residues. Once we construct a residual graph, then we can propose or we can develop different kinds of self-supervised learning techniques. For example, we can use similar techniques like mask range model. So we can mask out one of the residual type and then we can try to predict this residual type, right? Or we can Residual type, right? Or we can try to predict the distance between two residues, or even we can predict the angles between two edges, or even we can predict the dehydral angles between like three edges. So here we can propose different kinds of like self-supervised learning techniques to learn this geometric encoder. So we show that by using the So we showed that by using the structure-based approaches, so we are able to outperform the sequence-based approaches. Actually, for sequence-based pre-training protein language models, we need to make use of hundreds of millions of sequence. But with this 3D structure pre-training encoder, we don't need to use that many data. We just need to maybe use hundreds of thousands of structures, which are sufficient to outperform the sequence-based methods. So, another thing I want to briefly mention at the end of ASEAL: in the domain of drug discovery, besides the molecules or protein itself, another very important data is actually their interactions or their interactions with other biomet entities, which is the biometric graph. So, essentially, it's a heterogeneous graph between disease, drugs, and Drugs and genes or proteins. So, with biometric knowledge graphs, so we can import many applications like drug repurposing. Essentially, we try to predict whether there's going to be a link between a drug and a disease or whether there's going to be a causal relationship between the gene and the disease, which can be used for application like target identification. But in any case, for all these applications, the fundamental problem here is how can we to do link prediction? can we do link prediction on graph or how can we do reasoning on on graph based on the current uh facts or knowledge in the in the knowledge graph how do we uh predict the missing facts missing links so that's a very fundamental problem uh on graph so in my group so we've been working on different kinds of like reasoning techniques or link prediction techniques on knowledge graph including traditionally like knowledge graph embedding based techniques essentially we try to learn embedding for every entity We try to learn embedding for every entity, for every region, and then we can reason in the vector space, in the continuous space. And recently, we've been also working on trying to use more traditionally, traditional symbolic logic group-based approaches for reasoning on logic graph. Because the good thing about logic group-based approaches is that they are more interpretable. Because if you make the prediction, we can also tell you why the model makes this prediction. Tell you why the model makes this prediction, right? So, we've been trying to combine in traditional logic rule-based approaches with logic graph embedding-based approaches for reason on NodgeGraph. And also we're trying to automatically induct logic rules from NodgeGraph for reasoning on NodgeGraph. And then recently, we also propose a very general graph neural network framework for link prediction on any kinds of graph. So, I will not go into the details due to the tiny. The details due to the time limit. So, finally, I also want to mention that, like, so in my group, so we also develop a specific machine learning system for drug discovery called TorDrug. Because in the machine learning community or in general, like in the bioinformatics community, so nowadays we have huge interest in drug discovery. But for many researchers or for many students, they don't have prior knowledge. They don't know what important task we should work on. Uh, we should work on. So, in this platform, basically, we benchmark a bunch of like important tasks in this domain, for example, probate molecule probability prediction, uh, pre-training molecule repetitions, the normal molecule design and optimization, et cetera. Okay, and then for each benchmark task, so we provided the open source data set, evaluation metrics, uh, or other data pre-processing pipeline. So, suppose you're a machine learning researcher or by bioinformatics. Researcher or bioinformatics, and then you're interested in this area. So you can start from our system, and you can only focus on developing organs. And so right now, so we only support AI for small molecule discovery, but in the future, we will also release like this, we also will release some modules on protein representation learning. Modules on protein replication learning. So, due to the time limit, I will stop here and take some quick questions if you have any. Thanks very much.