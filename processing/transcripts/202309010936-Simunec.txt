This is a joint work with my PhD advisor Michele Benzi and Michele Nelli who is another PhD student at my institution. So I would like to first acknowledge some financial support from INDAM, which is the Italian Institute of Fine Mathematics, and Moor, the Italian Ministry for University and Research. And in particular, the Moor project paid for my plane ticket, so I have to thank them for real. Okay, so this is the outline. Basically, the theoretical Basically the theoretical part of the presentation will be split in two parts, one on trace approximation and one on the approximation of the product that forms within the trace approximation. So it will be quite high level because I don't have time to go into the details, but I will be happy to answer any questions. So let me introduce the problem. So we start with the matrix, which is, you can think about it as large and sparse usually. It is a positive semi-definite matrix. Positive semi-definite matrix, normalized, so it's a metric, and normalized so that it has trace equal to one. And this is usually known in applications as a density matrix. We would like to compute its form lamina entropy, which is defined like this. So it's minus the trace of A log A. In general, it falls within computing the trace of a matrix function. And well, so since we have a large and sparse matrix, we don't really want to compute this exactly because it's expensive. This is exactly because it's expensive, it would require the computation of either the eigenvalues of the matrix, all of the eigenvalues, or of the matrix function. So it would have a cubic cost in the dimension, which we like to avoid. So instead, we try to use algorithms that are more efficient and that only give out approximations of this quantity. So this is important in several applications such as quantum mechanics and uh quantum information degree. Quantum information difficult. So the methods that we explore have been already discussed in the literature, so we don't really introduce new methods, but basically we specialize these methods to special fields that we are interested in. So the methods that we consider write the entropy in this form here. So this can be actually said in general for any matrix function. So we have this problem here of computing the trace of a matrix function. And we would like to write And we would like to write this entropy as a sum of several quadratic forms with f of a, where these vectors xj are chosen in a certain way. And this certain way can be, on this talk, either stochastic trace estimators such as Hutchinson, for instance, or probably approximations that are specialized for sparse matrices and they use graph properties of the sparse matrix. So I will now give a brief introduction of these two techniques. Brief introduction of these two techniques without too many details. So, stochastic trace estimators work with any matrix B, so it doesn't have to be a matrix function. And basically, what they do is they, so this is Hutchinson estimator, which is the most basic stochastic trace estimator that you can think about. So you generate a certain number of random vectors, which can be Gaussian or Vadimacho, and you compute these quadrants. And you compute these quadratic forms with the matrix B, and you approximate the trace as the average of these. So the reason this works is that each of these samples here will have an expected value, but is equal to the trace. So if you average several of them, you will get an approximation of the trace. And of course, this makes sense when you don't have direct access to big, otherwise you could just read the trace. So our case where we have B equals F of A is one of these cases where computing One of these cases where computing the whole B is expensive, but looking at, for instance, f of a times a vector, it can be done in a less expensive way. Okay, and then in practice, what we actually use is the Hutch plus Plus trace estimator, which is an improvement upon Hutchinson obtained by first doing a low-rank approximation of the matrix in order to reduce the error as much as possible if the matrix has a Matrix has a good Lorentz approximation, and then Hutchinson is used on the remaining part. So, this is a very high-level view of Husserl's plus. If you never heard about it, I don't expect you to really understand, but I mean, it's just to give you an idea. Okay, so as I said, we do this for B equals F of A, and we have right, we don't, we fall into this expression that I showed you earlier. On the other hand, On the other hand, we have probing methods that only work for sparse matrices and for matrix function. So here we actually only focus on the case of B equals F of A, let's say. So the way they work is the following. So given a sparse matrix, you can define the graph associated with this matrix. Basically, if you have an n by n matrix, you have a graph with n nodes, and you put edges between node i and node j. node i and node j, whenever a ij is not good. So this gives you, even a sparse matrix, it gives you a graph. And now what we would like to do is compute distance decolorings of this graph. So we fix an integer of d larger than 1. And a distance decoloring is basically a coloring of the nodes of the graph with the property that if two nodes are of a distance less than or equal to d, then they must be of different colour. Then they must be of different colours. Or, in other words, if two nodes are of the same colour, they must be further apart than me. Okay, so this gives you basically a partition of the graph in a certain number of different colors. Now, we have these colors. For each color, we can define a proding vector, which is simply a vector which has one entries equal to one corresponding to nodes of that colour and zero for all other entries. zero for all other entries. So just to give you a confirmation, so if you sum all these vectors, you get the vector of all ones. And now we can approximate the trace with this approximation here, which we will call tau d, which is simply this sum of quadratic force with sum here, where these vectors are the probing vectors. So again, it falls into the same category as before. Now, the reason this makes sense. Now, the reason this makes sense is this result, which was given in this paper by Francischmann and Schweitzer, which basically is telling us that if we use a distance p-coloring for this kind of approximation, we are approximating the trace of f of a with an accuracy that depends on this quantity, which is the best error that you can get in approximating the function f with a polynomial up to degree d on the spectrum of the matrix. So that's where So that's where all this machinery comes into the actual result. Okay, so this was all present in the literature. Before what we did was specializing it to the entropy function. So we found an improved bound on this quantity for the special case of the entropy function. So here is the result. The way we obtained it is by using that integral expression for the entropy function. You can recognize here. Function. You can recognize here, it looks like a Pouchy stiffness function multiplied by a small degree polynomial. And so we have this result on the scalar approximation of the function f. So here we have this parameter gamma, which basically is telling you how large the spectrum is. Here we have a factor, say a low-degree rational function in D, and this is the main part with the geometric. Geodetic decay indeed. So, this is an, there were already bounds on this function, and this improves on them. So, you can see here a small example. We're still talking about scalar approximation of the function. So, you can see in black the actual error of the polynomial approximation. Here, you don't see the axis, but here is d the distance, free of the polynomial in this case, and the error. And you can see the blue bound, which is our bound, is quite. See, the blue bound, which is our bound, is quite a bit better than the old one. And if we plug in this bound into the previous general bound for the probing method that I showed you, we get this bound, which basically looks the same, apart from this part of our n here, which comes from the other theorem. And yeah, so this gives us a bound on this is the entropy, and this is the probing approximation of this task decoring. So this is the result that we have. Result that we have. Now, this seems to be a decent result in theory, but in practice, it's not so good. So, let me show you just an example. For the moment, ignore the blue lines, just to get the red one and the green one. So, this is the approximation that you get using increasing values of the distance for the angle. So, you see in green you have the error, and in red, the bound. You see that the bound is like two, three, sometimes four. Three, sometimes four, orders of magnitude above. So it's not really practical. And in particular, I should stress that for this kind of problem, since we are computing distance D colorings, it's not like with usual methods for many problems where you can just increase D until you get to the precision that you want. Because when you say move from D equals 10 to D equals 11, you're moving from You're moving from a distance 10 colouring to a distance 11 coloring, which probably have completely different vectors inside it. So you basically throw away everything you've done up to then, and you have to do it again from scratch. All the computations you have to make assumptions. So it's really important to know in advance, to decide in advance, which T you want. So, because maybe I didn't mention it, but our goal would be to say, we have an algorithm that gives you a Algorithm, but it's you, I don't know, 10 to the minus 4 accuracy. So the user gives you an input a certain accuracy, and you manage to do stuff in order to get that accuracy. With maybe some guarantees, if you possibly. So we also have a heuristic for this, which is a heuristic, so it doesn't give any guarantees, but it seems to be much better in practice compared to the bound. So the way this works is that we first compute the first few approximations. Few approximations. So here we compute the approximation for d equal 1, 2, and 3. And then using the bound, we sort of guess how the behavior of this convergence will be. So recalling the bound, we can say, okay, the error would be something like a genetic factor to the power of d divided by the power of d times a constant. And what we do is we, this difference here, we approximate it with tau d plus 1 minus tau d. tau d plus 1 minus tau d, as it's often done. We assume this is an equality and we compute the parameters CK and QK. This is the heuristic that we use. And well I should say the theorem would suggest to use K2, but in reality we found out that K able 3 is better in the experience. So we just take as a guess of the behavior of the convergence the maximum Of the behavior of the convergence, the maximum of what we get if we use table 2 and table 3. I know this is a heuristic, so it's not very rigorous, but I mean, the experiments show that it's pretty good. So, let me show you again this picture. Now, look also at the blue lines. These are the estimates for k equal 2 and k equal 3. So, this one is for k equal 2 and this one is for k equal 3. So, you can see that the one with k equal 3 is pretty close to the actual error. Not good always on upper bound, which you can see there. Upper bound, which you can see there, but I mean, it seems to be quite good. And I will show you more experience later. So if you're still not convinced, maybe you would be more convinced. Okay. So this was all for the problem method. So I don't know if you have questions, you can just stop me now. Otherwise, I'll probably talk to the next part. Okay, so as you have seen, both in the stochastic estimator and in the Estimator and in the probing method, the main problem, I mean, the main computational problem inside is the computation of something like this. So you have f of a and you want to compute that without form with this f of b. And this can be done using projection on trilogue subspaces. So just to remind you what it is, we have a basis, Vm, a Trilog subspace, which is usually orthonormal. And we would like to approximate this by This approximate this by projecting basically. So the cost of this will be the computation of the basis, which depends on which reload subspace we use, and this computation of this small matrix function, which is usually not too expensive since if we're doing things right, m, the dimension of the freelov subspace, which will be much smaller than the dimension of the whole space. So the cost of computing this will be m to the power of 3, which is much less than operating. Which is much less than operations with a usually. Okay, so one very popular choice of critical subspace method is the polynomial one, which at each iteration is increased, multidimension is increased by adding something obtaining with a matrix vector product with A. So this has convergent speed related to polynomial approximation of F. And by the way, we actually know how fast polynomial approximation of F is because of the bar we had earlier. On the other hand, we have On the other hand, we have rational Curial subspaces which are a bit more complicated and are defined like this. So at each iteration, you add a vector obtained by solving a shifted linear system, as you said. And so here, of course, the cost is higher because you have to solve linear systems instead of matrix vector products. But you have a much higher approximation power, let's say, because you're using rational approximation instead of polynomial approximation. So there's a potential for much faster convergence. Potential for much faster convergence. Oh, yeah, and of course, an important parameters here are these ψ and j, which are the poles of the method. And clearly, they are a very important choice to make. But luckily for our problem, we actually can look at the leadership to know what to do. So, if we look at this paper by Massen Rogel, we can see that for Cauchy-Siltis functions, which are very closely related to the function that we consider, Function that we consider. There exists a whole sequence which is asymptotically optimal constructed using the technique of distributed sequences. And so if you recall, our function is a Cauchy-Sylvis function times a low-degree polynomial. So if we combine these poles with just a few polynomial poles, we should get good convergence. And yeah, and we also developed a Yeah, and we also developed uh an a posterior error bound to d detect when to stop in this chromato. So this was already done in Stefan Button's thesis and we extended this to quite other forms. So it's a bit technical so I will not show you the actual result. I will show you only the variable. So focus on the blue lines. These are the conversions for the polynomial prevalent method. The convergence for the polynomial prevalent method. So the line in the middle is the actual convergence, and above and below, you see the upper and lower bounds that we have a posteriori. And we also have a heuristic, which is the triangles, which is very close to the actual error. And this is obtained by just taking the geometric mean of the upper and lower bound. It seems to be working pretty well. But anyway, the also the upper bound is pretty close. So if one doesn't want to use a heuristic, they can just use the actual uh error bound. The actual error part. And on the other hand, in red, we have the resistivities of poles for the rational method. So, of course, these iterations are much more expensive, but you can see the asymptotic convergence rate is pretty good compared to the polynomial one. At the beginning, it's not so great, so it would be beneficial to use polynomial iterations at the beginning, and indeed that's what we do. So, if you com if you combine, say, ten polynomial iterations followed by rational iterations. Followed by rational iterations, you get basically the best of both convergences. So fast, all these iterations cost much less than those. And the convergence is, if you compare here in 20 iterations, you get to 10 to the minus 10, here 10 to the minus 12. So it's very similar to using only rational methods. And again, here, the genetic mean seems to be pretty close to the actual error. Okay, so that's it. And now I can show you. And now I can show you some experiments to put everything together and show you that it should work quite well. So the experiments we do are based on sparse matrices from the Swiss sparse matrix collection. And we constitute the graph entry. So we take those matrices, we ignore the actual entries, we consider them only as binary matrices. But take the largest connective component of the associated graph. Component of the associated graph. We take the Laplacian associated to it, which is a symmetric graph. Of course, we take undirected graphs, so we get symmetric, positive, semi-definite graph Laplacian. And then we normalize it so that it has a unit trace. This will be our density matrix. And we use, as I mentioned, we use Hash ⁇  for a stochastic estimate. And we actually use the adaptive Hash ⁇  implementation in this paper by person called Donomics and Crestman, which is adaptive. Which is adaptive in the sense that instead of giving the parameters of how many fadat forms and whatever, you give them how much accuracy you want and they will do everything. Use their implementation whichever way. Okay, so as I mentioned earlier, the goal is to give an input or relative accuracy and using either bounds or heuristic estimates, stop the algorithm whenever we get to that accuracy. Okay, so these are the test matrices. You can see the dimension goes up to about 1 million. And yeah, the other important thing probably is the filling, which is a factor that measures how much filling there is in the Cholesky factor when you do the Cholesky factorization of these matrices. So 3.6 means that the Cholesky factor has 3.6 times the non-zero entries compared to the original matrix. Matrix. And you can see there are several matrices on which this factor is very small. These are matrices that are road networks, basically, or in general large world graphs, which means that they are very similar to grids, to spatial grids. So you need a lot of time usually to move from one node to another. So both it is easy to find distaste colorings with a small number of colours and also the trusting factorization. And also, the joystick factorization can be done cheaply and applied cheaply. On the other hand, we have like these three darks here with a much higher fill-in value, and you will see later also a much higher number of colors required in order to get distance D colorings. So these three examples will be examples on which the problem method doesn't work well. So just included both cases to give an overview. So these are the four smallest. So these are the f s four smallest matrices. Here we compare for so for each experiment we have two different uh of each test matrix we have two experiments. One is using so this is the problem method. One uses the theoretical error bounds, the theoretical bound on the value of D, and the other one uses the heuristic. So the one with the bound is the one below. And you can see that the value of D that comes out is very high and out is very high and the associated number of colors as well. And often, like here and here, you see that the number of colors is actually the same as the dimension, so it's useless because you would be faster by just diagonalizing. On the other hand, if we use the heuristic, you see the first line, the actual D that is out used is much slower and you can see that the average is quite below the requested tolerance. So it seems to So it seems to be work fine, be working fine also on these matrix. Okay, so on the other hand, we have the comparison between the pre-look bound and pre-look estimate. This is less relevant. I mean, you could just use the pre-look bound and get good results. But just to give some points to the geometric mean estimate, you can see that the first line is with the estimate, the second one is with the bound. Estimate, the second one is with the bound. You can see that using the estimate basically is saving you a small factor of both runtime and actual number of iterations. And the important thing is that the error remains below working. And yeah, I didn't mention here. So from now on, all the experiments are going to be used. Right, so the heuristic estimate on the The heuristic estimate on the on the problem method, simply because it's unfeasible. So notice that here we use epsilon 10 to the minus 3 and we already have 25. Here with epsilon 10 to the minus 5, well the number of colors is not present, but if you imagine the polynomial iterations are all the iterations over all the colors. So if we had used the theoretical bound, those numbers would be much, much higher. So it's simply unfeasible to use. Unfeasible to use in general probing methods without some kind of estimate for the value of T. Okay, so this is a small example for the stochastic estimator, again adaptive watch plus plus, using the same matrices. So here we use two different tolerances and you can see that with the larger tolerance, which is the first line, the time is actually very The time is actually very small, so it's extremely efficient. The problem with these kind of methods is that if you go with smaller and smaller tolerances, the runtime increases very quickly, as you can see in a second by. And the problem here is that these matrices do not have any lower end structure. And so H is not able to exploit this lower end structure in the first step. And so basically, its convergence is reduced to the same convergence as H and C. To the same convergence as Hutchinson, which, as you might know, is very slow as you try to get to higher and higher accuracy because of the convergence of doing simply the mean of samples. Okay, so these are larger matrices. As I said before, they are divided in large world and small world graphs. So these are large world, which again are similar to road networks and these matrices the following method is very good. Following method is very good. You can see the value of d is pretty small, the associated number of colors is extremely small. So, look at this one, for instance. And so, in the end, the runtime is pretty good. This is a breakdown, just to show you how much, on which parts the runtime is most used. And one important thing to notice is that the pre-processing step, which I takes a lot of time, actually, and with pre-processing, And with pre-processing here, I mean the time that it takes to decide which value of t to use. So basically, it's the time to compute tau1, tau2, and tau3 in order to get the estimate. Which is pretty large, but again, think that if we didn't do this, probably instead of having d equal 8 or 5, we would be requested to use by the bound to use d equal 20 say. Because the bound can be evaluated immediately because it's a scalar bound. Once you have Once you have bounced on the error values, but it gives you a value of t which is too high, which is really too big. Okay, so yeah, now we have the problem for the smaller graphs, which are smaller in terms of number of nodes. But as you can see, even with d equals 3, you need a much higher number of colours. So the runtime is, you know, compare. If you compare these with the previous result, they are much higher compared to the dimension of the rule. Higher compared to the dimension of the problem. And on the other hand, we have H, which again cannot go to very high accuracy, but it doesn't care if you have a small word graph or a large word graph. It's very efficient for relatively large tolerances, it's very efficient in all cases. Okay, so yeah, all these methods have their own advantages and drawbacks, of course. There's no perfect method. Let me just before I conclude, let me just I mean, just before I conclude, let me just give you a short generalization that we can have. So, this was all focused on the entropy function, right? As I mentioned in the beginning, everything can be done for another function in the same way by changing some details. For instance, the error bounds for the probing method or how you select holes inside the Russian field method. And one particular example that I would like to mention is that when you have the entropy of not A. Have the entropy of not A, but of a matrix which is itself a matrix function. So if A is file of H, you just plug in file of H and get trace of another function which is defined like this. And in applications, one thing that appears often is this Gibbs state, which is defined by an exponential. So H will be a Hamiltonian. And if you can see here, if you put this function phi inside the logarithm, you get function phi inside the logarithm, you get away. The logarithm goes away. And so we get a function which is very simple, analytic and everything. So using these techniques on that function would be very efficient. So you wouldn't even have to use rational pylon methods. You could just use polynomial ones and you would get very good results. So we didn't really do this in our work because it was getting already too long, but I'm sure in the future it can be done pretty easily. Okay, so these are the conclusions. Okay, so these are the conclusions. So just to summarize the contributions of this stuff, we have found numera priority robounds for the entropy using a probing method. We also have a heuristic to replace these bounds when they don't work very well. And within this method, we have developed a posterior label bounds and estimates for Russian Krinov methods when used for the computation of the data forms. Yeah, and everything can be put together. Everything can be put together to get an algorithm that gives you the entropy up to a precision of epsilon. Okay, these are some references. First one is the actual work we did, which should probably appear soon in Monaji Modematic. And thank you all for the attention. A normal entropy measures some, I don't know, energy content of a graph or something like that. I don't really know that probably. Yeah. Is there a generalization of this so that you can actually compare two different matrices? Because this phenomenon is, I think, it looks like one component of the KL divergence. Okay, I don't really know what that is. Yeah, so, but I guess so, yeah, this is. But I guess so, yeah, this is it uses only one matrix, and I know like physicists use this quantity to compare them, but you know, they first compute the two entropies and then compare them. So maybe something else could be said by looking at them together, but I haven't seen this, so I don't know. May I ask you a question? I wish I could forget. So the what you said what you talked about in the cyber development is related to what I was thinking. The fact that in your trace, it seems that what micros are the smallest egg in markers, right? The non-CO smallest egg in Microsoft. It depends on the function that you use. So the activity function. I'm talking about the trace of minus label. Ah, you mean the original problem. Okay, the original problem or the generalization? Okay, the original problem or the generalization? Well, the original problem is you have this function, yeah, minus 0, z, right? And you want this in the interval 0, 1, because that's where you have the end of values as expected, matrix of plus 1. And this function looks something like this. So there are not really eigenvalues that are very important. I don't know if this answers. I don't know if this answers. So it's not true that only the lowest ending values are important. This is true, though. Or yeah, when you use this function, this is more true, because then you have something like, you have f of z, like minus phi of z log phi of z. So if phi is the exponential, this is phi. So we have this exponential here, and then it's true that the smallest eggaball is important. Yeah, I mean I said the smallest or the largest. I mean it seems that the one in the middle that are most important. Yeah, but but not that important. Exactly. So the ones in the middle are important, but it's not like it's it's you know, the exponentials have very fast decay, so Have very fast decay, so they are more important, and it's very noticeable that they are more important. Yeah, on the other hand, on this function, there is no, I mean, there is no extremely well-defined peak. So, yeah, these are more important, but when we consider these ones, it means a lot. I'm not sure. Exactly. So that's why, I mean, if it was true that only the middle ones were very important, Only the middle ones were very important, then Hatch, for instance, would detect that the matrix F of A is almost low rank, right? Because it would have a lot of, I mean, the important argument values would be broad, but it doesn't indeed. Thank you. Thank you. So uh we're gonna just have a break now, but it's a little complicated, this checkout, because we don't control this. Because we don't control this, but in principle, you're supposed to check out by 11 o'clock, or we're going to start at 10:30. So it's up to you.