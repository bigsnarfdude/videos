Everyone, I'm Anchi from the National University of Singapore. And thank you very much for joining me this afternoon and allowing me this opportunity to share with you some of our recent work on the role of re-randomization in the design and analysis of randomized experiments. So my goal today is to convince you that this new baby, re-randomization based on p-values, aka R-E-P, is indeed worth about 40 minutes of your beautiful afternoon. So throughout the talk, our use star as a proper noun. Or use star as a proper noun, denoting the symbol for statistical significance in our outputs. So, a quick overview of what's on the menu today as a classic three-course menu. We're going to be starting with an appetizer of two existential questions. What's REP and why do we care? And then move on to the main course for a design-based theory of causal inference under REP. We then close with a series of desserts where I, among others, will play my own devil's episode. We'll play my own devil's epic and question the very logical foundation of the covariate balance test in the first place. So let's get started. So to begin with, it's almost a clich√© by now that randomized experiments balance all covariates on average and provide the gold standard for estimating treatment effects. Chance imbalances are nevertheless inevitable in any realized allocation, subjecting subsequent inference to possibly large variables. Inference to possibly large variability and conditional bias. Contemporary scientific journals react by encouraging the reporting of table one of covariate balance, featuring not only covariate means by treatment, but also stars from tests of their significance. Unlike their cousin tables for Unlike their cousin tables for final results, where we crave for nothing more than a series of stars after the p-value, signifying muse, big muse, and possibly noble price. The table one of covariate balance plays strictly by the rule of no star is good news, with large p-values indicating better covariate balance across treatment levels and protecting us from referees' most benevolent nitpicking. The urge to avoid small p-values motivates a very intuitive practice. Motivates a very intuitive practice. So, why don't we just keep generating new allocations until we hit one that eliminates all stars in table one? Formally, re-randomization based on p-values runs one or more statistical tests to check the covariate balance of a realized randomization and accepts the allocation if and only if all p-values of interest exceed some pre-specified thresholds. It, by definition, improves the covariate balance of the allocation of hand, which is an extremely desirable feature. After all, who says that the one real allocation in the hand cannot be worth two, if not 200 hypothetical allocations in the long run? This gives us the very first reason why we should care. REP provides an extremely convenient tool for improving the extremely desirable covariate balance. Covariate balance. Indeed, the magic of R allows us to identify a perfect candidate with just five lines of article in the blink of an eye. As it turns out, such practice is not only smart, intuitive, and convenient, but may also already be widely in use. So we surveyed 53 experimental papers from the journals of the American Economic Association and summarized the Association and summarize the balance tests and p-values in their covariant balance tables when available. The distribution of the 447 reported p-values as shown in this histogram deviates visibly from the uniform distribution with much less mass on the small values. The results echo the findings of Schultz and Brun and McKinsey and suggest that the possibility of selection bias and undocumented re-randomization beyond Document read randomization beyond pure randomization. So, this gives my second cent why we should care. REP may already be widely in use in practice, and popularity itself justifies due attention. Or skeptical as we are after all the drama around the p-hacking and the final results table, an immediate question is, is this just another day of p-hacking in a different table? Different table, what's the price for this apparently too good to be true, free covariate balance? As it turns out, the price that we have to pay for a star-free balance table is that some of the most commonly used methods for analyzing data from completely randomized experiments can be over-conservative when applied to data from REP and thereby deterring otherwise statistically significant findings. This gives the most depressing reason why we should. This gives the most depressing reason why we should care. Avoiding the stars in a covariate balance table can actually harm the stars in the final results. A better understanding of REP is that's the key to seizing both better covariant balance and efficient and well-calibrated inference in one cell swoop. This brings us to the end of our appetizer where we got to see what REP is, as well as my could be very biased three cents about why do we care. Three cents about why do we care? Time for the main course. So, consider a treatment control experiment with two levels of interest indexed by zero and one, and a study population of n units indexed by i from one to n. For each unit, we observe a j-dimensional covariate vector denoted by xi. We invoke the potential outcomes framework and denote by yiq the potential outcome of unit i if received at treatment to level q. Unit I, if received a treatment to level Q. The difference between YI1 and YI0 defines the individual treatment effect of unit I, denoted by tau i. The goal is to learn about the average treatment effect, tau, as the average of tau i over all n units. Alternatively, we can interpret tau as the difference in two population average potential outcomes, y bar one and y bar zero, as the average of yq over all. YQ over all n units. Towards this end, randomized experiments provide the silver bullet for revealing half of the potential outcomes as the raw ingredients for inferring tau. To begin with, the completely randomized design With the completely randomized design gives an intuitive way to allocate the units to different treatment levels. For some pre-specified fixed treatment sizes N1 and N0, it randomly samples N1 units to receive the treatment and then assigns remaining N0 units to receive control. And let ZI denote the treatment assignment of unit I. The treatment and control groups can be represented as I's with ZI equal to one and zero, respectively. zero respectively. So recall that covariate balance across treatment arms is an extremely desirable feature for almost all experiments. So the good news is that the completely randomized design balances all covariates on average and ensures unbiased estimation of the average treatment effect, tau, by simple comparison. The bad news is that there is no such guarantee for any realized allocation. So what if the allocation at hand is in balance with At hand is imbalanced with regard to some important covariates. Rerandomization arose in such contexts and forces covariate balance in the design stage. In particular, it uses complete randomization to generate the initial candidate allocation and accept an allocation if and only if the covariate distribution across treatment arms satisfies some pre-specified covariate balance criterion. REP event is special. REP events a special type of re-randomization with the covariate balance criterion defined by the p-values from some covariate balance tests. Recall that the stars from covariate-wise two sample t-tests are what occupied the last columns of the most commonly seen table ones. Given the covariant vector and the treatment allocation from complete randomization, an intuitive way to conduct An intuitive way to conduct REP is then to run one two-sample t-test for each covariant, and then accept the allocation if and only if the resulting two-sided p-values all exceed some pre-specified thresholds. We allow the thresholds to vary across covariates in case we value balance in certain covariates more than the others, but we can always set them all at 0.05 to eliminate all the stars. And this defines the REP based on J. Defines the REP based on J-covert WISE 2 sample t-tests as B-design to ensure good news, aka no stars in table one. Of interest is its implications on subsequent inference. This can be an impossible question at first sight, given the accountable number of possible procedures for subsequent inference out there. Let's go step by step and start with three commonly used estimators as a small but illuminated. As a small but illuminating first step. So recall that YIQ denotes the potential outcome of unit I if received the treatment level Q. So given ZI as the treatment level that is actually received, the observed outcome equals YI1 for those assigned to treatment and Y0 for those assigned control. The observed data then consists of the covert vectors, the treatment assignments, and the observed Treatment assignments and the observed data. Sorry, the observed outcomes. And the goal is to infer the population average treatment effect how as the difference between two population average potential outcomes, y bar one and y bar zero from these observed data. To this end, the sample mean of the observed outcomes under treatment level Q, which we denote by y hat Q, gives an intuitive estimate. Q gives an intuitive estimator of the y bar q. Replacing y bar q with y hat q in this definition yields the difference in means estimator of the average treatment effect, which we denote by tau hat n. We use the substrate n to signify name in 1923, who show that tau hat n isn't biased under complete randomization. The presence of COVID information further promises the Covariate information further promises the opportunity to improve estimation efficiency. To begin with, the difference in means estimator equals the coefficient of Zi from the regression of the observed outcome of Yi on Zi. So in case you don't speak R is our first language for statistical computing, this expression stands for not Yi is distributed as the sum of one and Zi, but rather the R expression for the linear regression of Yi on Zi with Of yi on zi with an intercept. This motivates two ways to incorporate covari information via regression adjustment. So, in particular, Fisher 1935 proposed add the covariate vector as J additional regressors into the regression and estimate tau by the OLS coefficient of Zi from the additive regression of Yi on Zi and Xi. Lin 2013 proposed an improved estimator as the coefficient of Zi As the coefficient of Zi from the interacted regression of Yi on not only Zi and Xi, but also their interactions to accommodate treatment effect heterogeneity. So this gives us three convenient regression estimators of the average treatment effect, indexed by N, F, and L, respectively. So the NFL here doesn't stand for the National Football League, but rather Neyman, Fisher, and Lynn. So our goal is to figure out their validity. is to figure out their validity and efficiency under REP. To this end, we introduce some additional definition for comparing the asymptotic efficiency of different estimators. So for two estimators that are both consistent for tau, we say that they're asymptotically equally efficient if they have the same asymptotic distribution. And that tau hat one is asymptotically more efficient if it is smaller asymptotic variance. Smaller asymptotic variance. So, as our baseline, standard theory ensures that the three estimators are consistent and asymptotically normal under complete randomization, aka CRD, with Liz estimator being asymptotically the most efficient. In addition, the corresponding robust standard errors from OLS are asymptotically conservative for estimating the true sampling variances. This justifies the large This justifies the large sample inference based on the convenient regression outputs and norm approximation, with Lin's estimator being the most efficient. Of interest is how these results change under REP. So this leads to the one and only one theorem of the main course, revealing the advantage of Linz estimator under REP as well. In particular, for an estimator of tau that is consistent under both CR That is consistent under both CRD and REP. We say that REP improves the asymptotic efficiency of tau hat if it has smaller variance under REP than under CRD. So as an illustration, imagine we have 10,000 independent completely randomized allocations, among which 2,000 satisfy the balance criterion of RDP. Then a distribution of tau hat under CRD is summarized over. Under CRD is summarized over all 10,000 allocations, whereas that under REP is summarized over only the 2,000 that are acceptable under REP. So in addition, let tau hat X denote the difference in covariant means by treatment group, which gives an intuitive measure of the covariate balance. Theorem 1 ensures that tau hat X is less variable under REP than under CRD, such as Than under CRD, such that REP indeed improves covariate balance. In addition, all three estimators are consistent under REP, with Lin's estimator being asymptotically the most efficient. Third, REP improves the asymptotic efficiency of Neyman's and Fisher's estimators, yet leaves that of Linz unchanged relative to CRD. This is indeed a lot of words to digest at first bite. Let's see if I can use one. Let's see if I can use one or 6,000 more words that make them at least slightly more tender. So the histograms here give the distribution of the three estimators over 50,000 initial completely randomized allocations, where the true tau equals zero. The horizontal axis represents the simulated values of the tau hat, and the vertical axis represents the density. The vertical lines correspond to the central. lines correspond to the central 95% quanta range. So the wider this interval, the less efficient the estimator. The first row corresponds to CRD, where the results are summarized over all 50,000 allocations. And the second row corresponds to REP, where the results are summarized over the subsets of allocations that satisfy the balance criteria. All six histograms center at zero. Center at zero, indicating small empirical biases. From the first row, Lin's estimator is the most efficient under CRD, whereas Fisher's estimator can be even worse than the unadjusted difference in means. So cover adjustment alone is not enough. We have to use the right specification to ensure efficiency gains. From the second row, Lin's estimator is the most efficient under our As the most efficient under REP as well. From the first two columns, REP improves the efficiency of Neyman's and Fisher's estimator. So the intervals are much narrower under REP. And from the last column, REP does not affect the efficiency of Linz estimator. So this illustrates the merits of Linz estimator under REP and brings us to three takeaways. First, First, REP improves covert balance, so we should use it for design whenever condition allows. Second, Linz estimator is asymptotically the most efficient under REP and is hence a recommendation for causal inference under REP. Third, REP improves the efficiency of Neyman's and Fisher's estimators, but has no effect on that of Linz estimator. As a result, the strategy of analyzing REP as if. The strategy of analyzing REP as if completely randomized by the regression estimators and norm approximation is perfectly fine for Lin's estimator, but can be over-conservative for Neyman's and Fisher's estimators. Importantly, all these results are design-based and hence robust to model misspecification. In particular, design-based inference conditions on the potential outcomes and takes the treatment assignments as a sole source of randomness in the observed. The sole source of randomness in the observed outcomes. We then evaluate the sampling properties of the estimators over the distribution of ZI's induced by complete randomization or REP, fully known without any assumptions. Even if we compute the tau hat NFL as outputs of linear regressions, we treat OLS as a purely numeric tool for computing the estimators without invoking any assumptions of the linear model. Invoking any assumptions of the linear models. The results that we have just seen and thus hold regardless of how well the unadjusted, additive, and interactive regressions approximate the true data generating process. So in case you're curious about how we get here, here's a sneak peek of the math or math behind. REP keeps the asymptotic distribution of Linz estimator intact as the same normal as under complete. As the same normal as under complete randomization. It changes that of Neyman's and Fisher's estimator to a convolution of a normal and a truncated normal that is more efficient than their respective distributions under complete randomization. A very pleasant surprise that the proof benefits tremendously from the legendary Gaussian correlation inequality. And I don't have time to go into detail today, but and Detail today, but an advertisement for the technical report on archive if you're interested. So this comes to the end of our main course with the combination of REP and LINS estimator being the final recommendation. Good design plus good analysis equals jamming and jam out. Time for some desserts. The dessert menu also consists of three parts. We'll be starting with eight more schemes for REP under treatment control experiment and then explore the possible extensions experiments with more than two treatment arms. We close with a somewhat surprising existential question of whether covariate balance tests are logical in the first place. Let's dig in. So recall that REP based on J covariate wise two REP based on J covariate-wise two-sample t-tests conducts one two-sample t-test for each covariate and accepts an allocation if and only if all covariate-wise p-values exceed some pre-specified thresholds. This defines the marginal rule of REP based on two sample t-tests. We focus on it during the main course because it's prevalent in practice for a balance check and has close connections with table one. Has close connections with Table 1. It's nevertheless only one of a million possible tests out there that we can leverage to measure covariate balance. To begin with, we can test the difference in means of all J covariates together by a multivariate version of the two-sample t-test, also known as the two-sample Hotelins T-squared test. And denote the p-value from this multivariate two-sample t-test. This multivariate sample t-test as P0T. We can accept a randomization if and only if this P-value exceeds some pre-specified threshold. This defines the joint rule for REP based on one single two-sample t-test. When both margin and joint balances are desired, we can also adopt a consensus rule that accepts a randomization if and only if it's acceptable under both margin and joint rules. Under both margin and joint rules, with PJT exceeding some pre-specified thresholds for all J from zero to J. This defines three REP schemes by two sample t-tests with the acceptance rules summarized here, marginal, joint, and consensus. And despite the intuitiveness and wide application of two-sample t-tests in reality, there nevertheless Reality, they're nevertheless not the only way to measure covariate balance between two treatment groups. In particular, the covariate wise two sample t-tests are numerically equivalent to the linear regression of the individual covariant on the treatment assignment. Motivated by the idea of the propensity score, which measures the probability of a unit receiving a treatment given its covariance, we can instead run a linear regression of the treatment indicator on the covariance. Treatment indicator on the covariates, measuring how the covariates values affected the treatment assignment. The resulting coefficients should be about zero for a well-balanced assignment, indicating that no variants play a significant role in influencing the treatment allocation. This motivates the marginal, joint, and consensus rules based on linear regression. Regression. In particular, the linear regression of Zi on Xij gives not only one marginal p-value for each of the coefficients corresponding to each covariate, but also a joint p-value from the F-test of the full model versus the empty model with only the intercept. This gives us three acceptance rules for REP, parallel to those based on the two sample t-tests. On the two sample t-tests, marginal, joint, and consensus. That accepted randomization only if all the marginal p-values exceed the threshold, or the joint p-value exceeds the threshold, or both the marginal and joint p-values exceed the corresponding thresholds. So, Wedevo's advocate's question would be that linear regression is not for binary outcomes to begin with. No problem. Simply changing problem. Simply changing LM to GLM pacifies all problems with a one-click fix. In particular, the logistic regression of Zi on Xi yields not only one marginal p-values for each covariate, but also the statistics for computing the p-value from a log likelihood ratio test of the full model against the empty model with only the intercept. And this yields three acceptance rules in full. Three acceptance rules in full parallel with the ones under the linear regression. And the GLM is just our expression for the logistic regression. This gives us in total nine covariant balance criteria for REP as a combination of three famous statistical models, the two-sample t-test, linear regression, and logistic regression, and three not so famous, but hopefully intuitive acceptance rules. Acceptance rules, marginal, join, and consensus based on the covariant wise and the join p-values, respectively. And of course, we can also go mix and match such that the possibilities are indeed innumerable. So here, we can accept an allocation if and only if all the marginal p-values from the two-sample t-test and the f-test. Test and the F test, the joint p-value from the F-test, of corresponding to linear regression exceeds some pre-specified linear threshold. So of interest, is there implications on subsequent inference? So as it turns out, the three takeaways from the main course corresponding to the marginal two-sample t-test extend verbatim here with no need of modification at all. With no need of modification at all. So, first, we should always use REP for the design. And second, the Linz estimator is the most efficient under REP. And third, analyzing as if completely randomized can be problematic for Neyman and Fisher's estimator, deterring otherwise statistically significant findings. So, again, importantly, the takeaways are the same, but the math is different. And you can Different, and you can check out our technical report if you're interested. So, this marks the end of our first dessert. So, let's move on to the second part on REP4 experiments with more than two treatment arms. So, consider an experiment with Q treatment levels indexed by Q from 1 to big Q and a study population of N units indexed by I from 1 to N. For each unit, we observe one J to We observe one J-dimensional covariate vector xi, one treatment assignment variable zi, and one observed outcome yi. A key distinction is that zi is no longer binary 0, 1, but takes values from 1 to q as a discrete categorical variable. And this leads to two extremely sad news for REP under multi-armed experiments. At first, the three famous statistical models seem no longer appropriate. Seem no longer appropriate for measuring the covariate balance across multiple treatment arms. And second, the linear regressions of Y on ZI no longer work. This, however, doesn't need to be the end of the world. The great news is that the F-test and multinomial logistic regression provide two most intuitive extensions of the marginal TCMPOT tests and logistic regression in the presence of more than two treatment arms. Presence of more than two treatment arms. So, in particular, we can simply conduct a one F test for each covariant and accept a randomization if and only if the resulting J marginal p-values all exceed some pre-specified thresholds. Alternatively, the multinomial regression of Zi on Xi yields Q minus 1 times J marginal p-values corresponding to each covert treatment level pair for the non-reference treatment level. This treatment level, and as well as one joint p-value corresponding to the likelihood ratio test, likelihood ratio test of the full model against the empty model that contains only the intersect. This allows us the ingredients to form the marginal, joint, and consensus rules parallel to those under the treatment control experiment. So, estimation of the treatment effects can So estimation of the treatment effects can then be based on the regressions of the observed outcomes on the vector of the treatment assignment indicators. And again, I don't have time to go into detail today. So another advertisement for our technical report on archive if you're interested. So last but perhaps most importantly, let's explore the very logical foundation of the Cobert balance test. As a bit of a background, Bit of a background. Covariant balance tests before being used for REP have long been the North stars for deciding which covariance to adjust for in subsequent analysis. In particular, we could conduct a one marginal t-sample t-test for each covariate and just for only those with p-values that carry stars. That is less than the notorious 0.05. Such practice, despite being a Such practice, despite being incredibly intuitive and extremely popular, received some rather fierce criticism in the literature. So as Atman most incisively pointed out, in a clinical trial in which treatment allocation was properly randomized, a difference of any sort between the two groups at the baseline would necessarily be due to nothing but chance. Since the randomization proceeds Since the randomization procedures preclude any external influences on which subjects receive which treatment levels, performing a significance test to compare baseline variables is as such to assess the probability of something having occurred by chance when we know that it did occur by chance. Stars, by definition, are nothing but false positive. No wonder Altman called such procedure clearly absurd. Such procedure clearly observed. This leads to a particularly interesting standpoint of Consort 2010, which required reporting of a table showing baseline demographic and political characteristics for each group, but discouraged significance tests of baseline differences, which are not necessarily wrong, but just illogical, superfluous, and can mislead investigators and their readers. So these are the Readers. So these are the original words from this technical report, not my personal opinion. This expulsion of the p-values, however logical, opens up an inevitable rabbit hole. How can we and the naive audience then decide whether a particular allocation is balanced enough without the guidance of the stars? I have no definitive answer to this headache or. Answer to this headache or dilemma, but would still like to offer my could be very biased two cents. So, first, it's okay to conduct the balance test to reinvent the false positives as we keep reinventing the wheels all the time, but don't use them as a basis for deciding which covariates to adjust for in the downstream analysis. Instead, use them to conduct REP and always adjust for all covariates by interactive regression, aka lanes format. Regression, aka links formulation when sample size allows. Again, good design plus good analysis equals jam in and jam out. And this is pretty much all that I have for today. So thank you. And here's the archive link for the key references that I kept advertising for. And please do check it out if you're interested. Thank you so much.