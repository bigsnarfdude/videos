This morning session. It's our great pleasure to have Professor Pritik Shaudri from UPEN to give a third talk in this morning session. So Pratik is the assistant professor in electrical and system engineering at UPenn. His research major His research majorly focuses on machine learning, in particular deep neural networks, robotics, and computer vision. So, today he's going to talk: Does the data induced capacity control in deep learning? Predict, please. Yes. So, firstly, thank you for inviting me to this workshop. I wish it was held in person. So, I've never been to OHACA. That would have been fun. So, today I would like to tell you about some of our recent work on understanding generalization in deep learning. And the way I personally think of this talk is that it is a story of missteps. It's a story of trying to understand where the performance of neural networks is coming from. We know neural networks work spectacularly well in practice, and it has been a little bit of a It has been a little bit of an onerous task to explain using existing theory why they should work, when they should not work, how likely they are to work on new problems, etc. Perhaps one biggest mystery of deep learning is the fact that we train big neural networks with algorithms that look like stochastic data set. The data sets are huge, you have a million samples or so. You have a million samples or so. You have a few millions of weights. And the gradient that we compute at each iteration while updating the weights is not really a very accurate estimate of the true gradient because we're computing it on a very small subset of the data set. And yet, in spite of such an inefficient implementation of and picking one optimization algorithm of many and many others. Many and many others, these things do work well. So, this is a plot. I have taken it from a blog that became quite famous a few years ago. On the x-axis is GigaOps. As we create different networks, these are all networks trained on this large data set called ImageNet. As we build new networks, some of them are larger than the others. Roughly speaking, across time, the accuracy of these networks has kept on increasing. Has kept on increasing. This is how we invent new architectures. And they look quite different. So some of them will be like vanilla convocational networks, some of them have different kinds of operators. This is rash normalization. Some of them have residual connections. And there is really like a very diverse family of architectures that work well on these data sets. The one unifying thread among all these networks is that every single one of them was trained with stochastic. Single one of them was trained with stochastic gradient descent. You take a mini batch of data, you compute a gradient of this mini batch, and you update the weights in the direction of the negative gradient. And you do this for many epochs, 100 epochs, 200 epochs. And this very rudimentary algorithm works better in terms of generalization, in particular, than all embellishments that we can cook up for training neural networks. So SGD is among the most inefficient algorithms on paper in optimization theory. In optimization theory, and yet it is the most performant algorithm in deep learning, in particular, and large-scale machine learning in general, one would have to say. So, one key question that people have tried to ask, and in particular, I have been trying to study is why is SGD so special? So, let me give you one perspective on some peculiar properties of SGD. This is an experiment where we trained a convolutional neural network on an. Trained a convolutional neural network on MNIST with stochastic urine descent. And at the end of training, so you stop the training at some point. And what I'm showing you are the eigenvalues of the Hessian of the neural network. So the Hessian is a large matrix. It defines how the energy landscape is curved locally at the training point. And this is a histogram of the eigenvalues of the Hessian. And what you will immediately notice is that a large number of eigenvalues are essentially zero. So these and there is very So, these uh, and there is very few eigenvalues that are large. So, these are all directions of increasing height in the energy landscape. And those would, and this region does look like a local minimum or a global minimum, one doesn't know. But the most interesting aspect of this picture is that there are so many directions in which the energy landscape is essentially flat. Locally, if you change the weights, the loss does not change. This is not really a local minimum, turns out. This is not really a local minimum, turns out. It is more like a saddle point because there are also a few negative eigenvalues. But the moral of the story is that the Hessian of trained neural networks looks like this, irrespective of the architecture, irrespective of the data set, irrespective of what algorithm you use to train it. It is always a region that is flat in the energy landscape. Now, this is how we were thinking about sorry. I have a quick question. Question about the previous slide. So, did you compare it to kind of like at a random initialization? Because to some extent, if you plot the spectrum kind of randomly, you do have kind of a small number of outlier eigenvalues than something centered at zero. So, I was curious. I think you're correct. I just want to see like how more pronounced is the phenomenon compared to the generic behavior. This is a very nice question, and the answer to. A very nice question, and the answer to this will resolve towards the end of a talk. But to cut to the chase, at initialization, this peak is a little bit shorter. And as you go towards the end of training, the peak increases. So it needs more vibrations. Awesome. Thanks. And we'll see this in much more elaboration. So this is how we were thinking about five years ago at this point of time. At this point of time, we know that training converges to regions that are wide, so maybe we should design algorithms that look for such wide regions explicitly. And here is a trick you can play. So this is X are the weights in this case. You want to find the best weights that minimize the loss. So this is the training error. Instead of minimizing the loss, you could simply rewrite it as the arg max of e to the negative f. You haven't done very much. You simply take like a non-convex rugged landscape. Took like a non-convex rugged landscape f of x, and then make it made it a little worse by exponentiating it. But what you can do now is that you can smooth this rugged landscape using a Gaussian kernel. So G sub gamma is a Gaussian kernel of bandwidth gamma and of variance gamma. And when you convolve this exponentiated loss with a Gaussian like this, what happens is the convolution destroys all regions where f of x t. All regions where f of x changes very quickly, where f of x could be sharp. And what we are left with is essentially all these regions that are white. So, smoothing is the oldest trick in the book if you want to smooth the energy landscape or find white regions. And this is a special kind of smoothing that works well in practice. And we have since then a number of papers that try to implement these algorithms, come up with better ways of training neural networks, etc. One reason why this could work well. Why did this work well comes from the study of binary perceptron? So, binary perceptron is a single-layer neural network, but it is a very hard constraint satisfaction problem because the weights of this binary perceptron are plus one or minus one. So is the data and so is the labels. So it is very different from the perceptron that we study in the machine learning textbook. This is the perceptron that physicists like to study because they can do these replica theoretic calculations because of the binary weights. Binary weights. They have painted a picture of the energy landscape of the binary perceptron, and here is how it looks. So, this circle is Hamming space. If you have n neurons in your perceptron, then this is a Hamming space, n-dimensional Hamming space. And turns out, if I give you a finite number of samples in your training data set, then the red points are all the solutions that fit this training data perfectly. So they interpolate the training data, and they are also isolated solutions of the training data. Isolated solutions of the training data set. By isolated, I mean that the distance between any two solutions of the same training data set is order n, order number of neurons. As the number of neurons increases, this distance, these solutions become further and further away from each other. And this is really where the non-convexity of the problem is showing up. If you initialize yourself somewhere here, it will be quite difficult for you to go to one of the correct solutions. There are exponentially many solutions, but it's very hard to find us. It's like finding a needle in a haystack. Findness. It's like finding a needle in a haystack in a large room. The most peculiar nature of this problem is, however, that even if there are exponentially many isolated solutions, there's a constant number of dense clusters of solutions. And these clusters of solutions are quite special. So if I pick one weight vector here for the perceptron, within a small order one neighborhood of my weight, everything is a solution of the same training set. These are extremely rare clusters. Extremely rare clusters. So, as the problem size grows to infinity, these clusters become vanishingly rare. And in fact, the Gibbs distribution puts essentially zero mass on such clusters. But the beauty of these clusters is that they generalize better on new data, which you can again calculate using things in physics. So according to this picture for a perceptron, wide regions are analogous to dense clusters, and sharp regions in the energy landscape of a deep network are analogous to. Landscape of a deep network are analogous to these isolated solutions. Just like wired regions generalize better for SGD, dense clusters generalize better for the perceptron. So things kind of match up. Now, what I would like to ask is, wide regions, we physicists have shown are a large deviations phenomenon in binary perception. They're extremely rare in the energy landscape, exponentially rare. And what do they look like for neural networks? And that is the kind of theme that I want to. And that is the kind of theme that I wanted to keep in mind across this talk. A tiny bit of a sidebar: we know wide regions exist for neural networks. They may not look like dense clusters because I can perturb the weights and, of course, my loss blows up by a lot. But you can always look at this perspective of smoothing. And we have some work where we start to study methods that take in a rugged landscape, the black line here, and then run. Line here, and then run some PDs on this, some kind of smoothing that comes from Hamilton-Jacobi's type partial differential equations, and cook up a smoother landscape on which we can do gradient descent. And this is kind of one way to say, look, we know that smooth energy landscapes are nicer for optimization. So let us come up with ways of creating smooth landscape, even if the original one weren't smooth to begin with. And these algorithms work fairly well in practice. So this is one variant that we had that is a One variant that we had that is a distributed algorithm. And the purple line is this algorithm called Parley. And it is essentially two to four times faster than distributed gradient descent, stochastic gradient descent across multiple GPUs. So the moral of the story is: we know algorithms typically converge to wide regions. If we go seeking for them, we can cook up new methods that exploit the wide regions in the energy. The wide regions in the energy landscape. And because we are explicitly searching for them, we can also generalize better and train faster with this map. Okay. But the key question still remains, right? So why the heck is SGD so special? SGD does not use these modified loss functions. We cooked up these loss functions, but SGD also works well. So let us try to understand why SGD works well. And there will be two key players in this study. Key players in this study. So, the first one is what we called a diffusion matrix. It is the variance of mini-batch gradients. And you can do a little calculation and come up with a formula like this. So, if there are n samples in your training data set, gradient of FK is the gradient of the neural network on the kth sample. So, x are the weights. So, these are outer products of the per sample gradient. And then this is the outer product of the average gradient across. Outer product of the average gradient across the entire data set. So, roughly speaking, this is a term that looks like the Fisher information matrix, the first term, and the second one is simply zero if you were at a critical point of the original loss function. And turn circuit is all. Hello? Sorry, there's a question from the audience. Let me read the question. What's your definition of the wide region? Or the measure of the sharpness? Yeah, so there is a lot of contention about how to define. About how to define white regions. The way our theory defines white regions is that it is the volume of the sub-epsilon level set. So you pick a particular energy of your loss function and then calculate the volume of all the weight vectors that give you loss below that particular level. Using other quantities like Hessian, the highest eigenvalue of the Hessian, and so on and so forth, is not a very nice way to measure the width. Very nice way to measure the width of a region because you can kind of for neural networks with symmetries, these quantities can change without actually changing the object objective or the input output map. So we define them as volumes of level sets. And this is a sound definition. It doesn't like change in respect to parameterization. Thanks. Okay. So this is the first key player, the diffusion matrix. The diffusion matrix is the Matrix. The diffusion matrix is the noise in SGD. So it is how different SGD is as compared to gradient descent. The second key player is the scalar that sits in front of the variance in your noise. And let us call it the temperature. It turns out to be the ratio of the step size and the batch size. So B is the batch size, eta is the step size. So for now, simply remember that there is a constant beta inverse or temperature that is equal to the ratio of the learning rate and the batch size. Learning rate and the websites. If you try to write down a model for SGD, this is not SGD, mind you, it is simply a theoretical model, you will obtain a stochastic differential equation of the following form. So X are the weights again. Gradient of F is the full gradient on your entire training data set. So if I were to just give you these two terms in the equation, then that is simply gradient descent, nothing special. Stochastic gradient descent is gradient descent with some noise. Descent is cadent descent with some noise added to it. The peculiarity again is that this noise we model using Brownian motion, but not Brownian motion with identity covariance because the covariance of your stochastic gradients is not identity. In particular, it is exactly this matrix D. This is the variance of your minivas gradients. And as I promised, beta inverse is the temperature that sits in front of it. Larger the learning rate, larger the effective noise, smaller the batch size, larger the effective noise. Larger the effective noise, larger the trace of D, that means the larger the diversity between the gradients, larger the effective noise in a this makes intuitive sense. Now, given a stochastic differential equation like this, you can study it in the following way. So instead of looking at one trajectory of SGD and trying to answer where it converges, it's a non-convex landscape. So you will reach the first critical point that you encounter, and then there is nothing more to study. You encounter, and then there is nothing more to study after that. But you can kind of flip the question around and ask yourself: if I were to wait until stochastic rendescent, which is really a continuous time Markov chain, mixes completely on the energy landscape, let's say we assume a bounded weight space for just a theoretical construct, and assume that your dynamics are ergodic and stochastic renders and mixes on this space. What is the steady-state distribution of this? Distribution of this process. And in order to understand this, this is a very classical calculation. One uses what is called a Fokker-Planck equation, which is a partial differential equation, where rho is a probability density on the weight space, and rho sub t is the derivative of rho with respect to time. And this turns out to be equal to the divergence of two terms. The first one is how the drift flows the Drift flows the probability mass. So, if you have drift that is strong in certain directions, the probability flux will also be strong in those directions. That is what the first term is. It is called the drift term. And the second term really comes because of your stochasticity. So, gradient descent is only again these first two terms. It would be what is called a Liobil equation. With stochastic gradient descent, you again have a heat equation-style term, which is where the diffusion really plays a role. And unlike the A row. And unlike the standard heat equation where this term would be identity, it would simply be the Laplacian of rho in a heat equation. For SGD, this is an interesting matrix. It is not identity. This matrix D. So at the end of the day, we would like to find study how the solutions of this PD look like. And this is, turns out is quite non-trivial, but we can use tools from Use tools from optimal transport to give us some insight. So, first, consider the case where we have the heat equation. There should be, yeah, the heat equation where D is identity. And we know from the theory of optimal transport that heat equation performs steepest descent on a directional energy. So, this is the energy that is minimized along trajectories of this equation if you were to take the k. The exact same The exact same dynamics, of course, the exact same PD can be thought of in a slightly different way. It is the steepest descent in the Wasserstein metric. So, this is the steepest descent in the Euclidean metric. This is the steepest descent in the Wasserstein metric. And that really you can write down as a proximal point iteration where row k plus 1 is the next row at time k plus 1. It has some proximal term. This proximal term is measured in terms of a Wasserstein square distance. In terms of a Wasserstein square distance, and the functional that you're minimizing is the entropy. So, moral of the story: you have heat equation, heat equation, trajectories of the heat equation lead to large entropy of the solutions. This is quite intuitive because heat flows away from the source and makes the temperature equal in a particular domain. And this is why this particular entropy will be minimized, negative entropy will be minimized. Okay, in other words, you can say that negative entropy is a Lyobna functional for Brownian motion because Brownian motion is the stochastic differential equation that underlies the heat equation. So this is a functional which does not increase along the trajectories of the heat equation. The heat equation converges monotonically to the solution of the maximum entropy distribution. The same result is true for the Fokker-Planck equation with identity diffusion. With identity diffusion, identity noise, identity variance of the gradients. And this is a very classical equation and it's quite celebrated. So this has a name called the Jordan-Kindleyer auto functional. And in this case, what they showed, and this was really like a breakthrough result at the turn of the century in optimal transport, it says that the solutions of the Fokker-Planck equation, rho, that are. Equation rho that are at steady state so that rho doesn't change anymore can be written down as the solution of a variational optimization problem, which is a sum of two terms. Just like the heat equation, we have an entropic term. So solutions of the Fokker-Blank equation, because they have also this second term that comes from the diffusion, they also like to maximize the entropy. But there is also a first term, which is really the average of the potential that gives you the drift. That gives you the drift. F is the potential, gradient of f is the drift. And really, what Fokker-Blank equation is doing is that it is trying to find a probability distribution that is as entropic as possible while at the same time giving you an average small energy. And this is again a Lyopna functional. This is the functional that is decreasing monotonically along the trajectories of the Fokker-Black equation. So you're Fokker-Blank equation. So you again get an analogous result that says Fokker-Blank equation is the steepest descent on the JKO functional in the waste-stand network. Okay, so this is the story for isotropic gradient noise, D equal to identity. What happens if D is not identity? If we say that D is some full ranked matrix that need not be identity, you get a similar result. This is what we showed in a paper in 2018 where Paper in 2018, where instead of you again have the diffusion term, and peculiarly, like even if this D is now different from identity, the diffusion term here in the function hasn't changed. So you're still like maximizing entropy, which makes sense in some sense, in some way. But the cool thing that happens is that the energy that you're minimizing on an average is this function called phi of x, which is not the same as the potential f. Of as the potential f that is causing, giving you a drift. Okay, so on average, you are minimizing an energy that is really some function phi of x. I haven't told you what this function is. And again, you maximize the entropy while minimizing the average energy of this. Okay, so trajectories of stochastic random descent, when thought of in this variational framework, give you steady set distributions that on average minimize an energy that is not. Minimize an energy that is not the same as the original loss on which you are taking greater steps. And this is a little surprising when you first look at it for the following reasons. So if you are given stochastic red descent, you are taking gradient descent steps, you are given a training loss, you are taking SGD steps on this training loss, you expect that you at least converse the critical points of this loss. To the critical points of this loss. What this result is saying is that you don't converge to the critical points of F, what you really converge to are the critical points of. And turns out this, I'll skip this slide. Turns out the steady set distribution of the original problem. So is not e to the negative f of x, which is what you would expect from if noise were isotropic, but it is really e to the negative beta times t of x. So you are really converging to the critical points of phi. To the critical points of phi, not the critical points of F. And what we have proved is that if and only if D equals identity, then Phi equals F. So in short, if the noise in SGD is exactly isotropic in the sense that if you move in one direction, then you have equal probability of coming back. Noise acts equally powerfully in all dimensions. Only then you optimize the objective that you set out to optimize. Have to optimize. If D is not identity, then C need not be equal to F. Okay. And how does D look? So D is a matrix. It is the large matrix, number of weights times number of weights. But you can compute it certainly for neural networks. D is really very far away from being isotropic. So this is a convolutional neural network, a wide Russian, I think, on CIFAR 10. And what I'm plotting here is an eigen spectrum. Eigen spectrum of the diffusion matrix, which is the noise in HD, and the average eigenvalue is about 0.7. If you give me a richer data set, CIFA 100 has more diversity than SIFA 10, then the eigenvalue, average eigenvalue increases, so the eigen spectrum shifts a little bit to the right. The interesting part is that the rank of B is very small. So really like there's large eigenvalues that are really like non-zero. Are really like non-zero, but then there is a huge number of zero eigenvalues. And it is not, it is extremely small. So even less than 1%. So remember that neural networks are very large models. So you could be training a model of 1 million weights, but only about 0.34% weights in this case is where the diffusion is acting. All the other weights are simply gradient descent. And this is a little surprising. And this is a little surprising, right? So, we always imagine SGD as like this bouncing ball that bounces in all dimensions and then converges somewhere in a valley. But that is not what this theory is saying. This is saying that you have essentially gradient-descent dynamics for the rest of the network, and then there is a diffusion acting in this very, very few dimensions. So, maybe we don't even want to think about diffusion in some sense. And that is kind of the flavor that I want you to take away from these plots. To take away from these plots. How important is diffusion to generalization? Well, maybe not too much. I have a quick question. Which measure of effective rank are you using? Is it like trace divided by largest eigenvalue or are you using squaring them? That's a very good question. And so calculating rank for matrices with very, very small eigenvalues is a little difficult because when you compute the eigenvalues using the power. Compute diagonal whereas using the power method, you get numerical inaccuracies. So, hold on to that thought. This is an older paper where I thought that the rank of D that D is low rank and D is low rank, but we will see how it is not as low rank as this number depicts. Okay, it has a really special shape, but not just low rankness. The second thing, did you mention? Second thing, did you, you may not have run these experiments, did you run experiments where you compare this rank for different depths of network? Because I think I've ran into this phenomenon before and I've wondered if you make the network deeper, does it become less low rank? I don't know if you know anything about that. No, yeah. So I've run experiments on deeper networks and the rank essentially does not go up beyond 1%. It's always quite small. And you will see why very soon. Okay. See why very soon. Okay, cool. Thanks. Okay, so one intuition for why this could be occurring is as follows. So we take natural images, let's say a white cat and a white dog, and we give them names, right? So this is called a cat and this is called a dog. But as far as a neural network is concerned, they are not that different from each other, except for the one logic that separates them. So the gradient that a neural network sees. That a neural network sees for images that are similar is also similar. And this may explain why the diffusion matrix has low rank or why the diffusion matrix has such a few number of large eigenvalues. Okay. And we'll again kind of elaborate upon this in a bit. You also get into some very exotic properties of SGD because of such things. So SGD in this perspective is a markup chain that has broken detail balance. That has broken detail balance. Once you go further in one direction, the probability of coming back in the same direction, same direction is not equal because the noise does not act equally in all directions. And such systems are what physicists call non-equilibrium systems. And for such systems, we can get really like a very weird phenomena. So, in particular, what we studied is that we studied the most likely trajectories of SGD. So, what trajectories of data descent look like. Trajectories of gradient descent look like. And under this theory, we can say that the trajectories of gradient descent with such noise, or such is a trajectory of SGD with some noise, can wander arbitrarily far in the parameter space without really converging. This is an autonomous differential equation. And these trajectories, the weights are free to evolve along these trajectories so long as they keep this function. So long as they keep this functional phi constant. So these trajectories are all trajectories where the function phi is constant, and you're allowed to roam forever along this without really ever converging to any specific place. And we've also found such trajectories in experiments. So this is the FFT of the trajectory of about 100,000 epochs. And you clearly see like low frequency modes, which is really where the weights are roaming around in the parameter space. Roaming around in the parameter space. You also see it in the autocorrelation plot. What I want you to take away from this is that the trajectories of SGD because of the noise in neural networks can really be like very exotic. If there is no, if the noise is only isotropic, then let's say in this double well potential style function, you will converge to whichever local minimum you are close to. In this case, In this case, is exactly equal to f, and there is no such funny behavior of trajectories. But as soon as I cook up an example where I introduce some curl in this vector field, you get these trajectories now that are taking very different shapes than what you would imagine is the vector field corresponding to F. These are the vector lines corresponding to the field gradient of phi. If the current is very Current is very large in magnitude, then you can also converge around saddle points or not converge around saddle points, but really keep rotating around saddle points without really ever going to the local minimum. And you can cook up cases where these things happen for quadratic objectives very easily. So assume that your object is quadratic around a critical point, and you can easily create a situation where the iterates of SGD with such noise revolve around the critical point. Revolve around the critical point, but never actually remain at the critical point. As the temperature goes to zero, as the learning rate goes to zero, they do come to the critical point. So, all this, this entire picture collapses to this picture as learning rate goes to zero. Okay, so very, very weird properties one has to say. And that is really where I personally finished my PhD without really ever understanding why SGD works. SDD works. And when I think about this, there is really like three different things going on in deep learning. We pick certain architectures, neutral architectures. Some of them work better than the others. We don't know really why. We pick different optimization algorithms, ADAM, SDD, all their cousins, etc. They belong to the same family of SDD-like algorithms, but the way they interplay with the architecture is quite different. So some architectures are difficult to train and people don't use them. And people don't use them. Some others are harder, easier to train like ResNet. So people use them a lot. We figured out how to train these architectures. But at the end of the day, what generalization error you get out of this training process is really like a hailmay right now. You pick an architecture, you pick an optimization algorithm, you don't like the generalization you got. There is really nothing that tells you what to do next. You don't know where the generalization is coming from, when it comes. You don't know why it doesn't come, when it doesn't come. Don't know why it doesn't come when it doesn't come. And this entire three objects is traditionally thought in very different ways, very different communities in machine learning, even. So this would be approximation theory, this would be of dimension theory, this would be classical statistical learning theory. These three concepts are completely entangled together. And until a couple of years ago, I used to think that, look, maybe stochastic rendescent is really like what is entangling all these things together. And if you want to understand deeply, And if you want to understand deep learning, that SGD is what we really need to understand. In the next part of this talk, I will tell you how data could be at the heart of this picture. And as usual, I'll begin with an experiment. So here is an experiment where we took SIFAR, SIFAR 10, and a wide residual network on SIFAR. When I say the blue line is the data, what I'm doing is taking the image. What I'm doing is taking the images of Sephard and then arranging them in a big matrix. So every column of the matrix is one image. Sephar has 32 plus 32 pixels, RGB images, so about 3072 length vectors for each image. And this is the eigen spectrum of the data collision matrix. So X transpose, simply the second moment of your data. And as you see, it ends at around 3072. It ends at around 3072. The most peculiar properties of this eigen spectrum are as follows. So you have a very sharp peak at the head of the eigenspectrum where the eigenvalues drop very quickly. These are all the dimensions where data is interesting. And that is really where the most of the action lies. But it's not as if data is low rank. If data were low rank, if data were living on some manifold like we like to imagine, then this eigency. We like to imagine, then this eigen spectrum would have dropped down to zero, right? But it is not low-rank, it is really very special where the tail of the eigenspectrum is linear on a logarithmic scale. So what is really going on is you have an exponentially large range of eigenvalues in the data correlation matrix. And this tail, even if it is such a small, it consists of so many small eigenvalues, the effective Eigenvalues, the effective trace of this tail is still a good fraction of the head. So it is not as if these dimensions are not important, they are important, they are just exponentially less important than the top few, but there is an exponentially large number of them. Okay, so this is how data correlation matrices look. And you can take your favorite data set, the eigen spectrum of that data set looks very, very close to this. Okay, the most shocking part of this picture for us was that we. For us, was that we calculated the feature information matrix. So, this is a large network. We only calculated the top 1000 eigenvalues, and for the Hessian, about 600 or so. And so, this is the full Hessian of the same neural network, of a trained neural network. And the eigenvalues of the Fischer and the Hessian completely mirror this structure in the data. We have done experiments for slightly smaller networks where the eigen spectrum of the Hessian and the Fischer. Now, the Hessian and the Fischer are something like this. So they have the same slope essentially, and they show the same kind of head where you have large eigenvalues. So there is some very strong structure in the data. This is also mirrored very much in the Fischer and the Hessian of a trained neural network. And essentially, yes. There is a question from the audience. Sorry. So, what would this plot look like for the random data? What look like for the random data? I'll show it to you in a few slides. Okay, thanks. Yeah. So in this experiment, essentially every quantity that we could think of to compute in a neural network also has the same shape. So this blue brown curve is the correlation of the activations of different layers. And I've only shown you one line because all the activations are essentially overlapping on top of this blue line of different layers. Of this blue line of different layers. If you look at the gradient of the loss with respect to the activations, it is again a very similar shape. If you look at the Jacobian of one particular logit with respect, so the gradient of that logic with respect to all the weights, again, you get a purple line which has the same shape. And every single which quantity that we can think about is completely dominated by the strong structure in your data. Okay, we call such data matrices and such matrices. And such matrices sloppy. And this is a name that we borrowed from some ideas in physics. And what this really means is that there is a large tail to the eigenspectrum, and the slope of this tail is constant. So the eigenvalues are spread uniformly across exponentially large ranges after this little threshold. And even if they're all very small, together they dominate. Very small together, they dominate performance, they dominate your structure, which you see here. The head of that eigen spectrum is not that relevant or important for technical reasons, which we can talk offline. But what is very peculiar is the tail of the eigen spectrum. A few key takeaways from this plot. So, natural data we would like to imagine is a low rank. Is low rank because it may just live on some manifold, but it's not really low rank. As you can simply see by plotting the picture, it is sloppy. If I were to sample a hypersphere uniformly randomly, if this hypersphere is in large dimensions, then every sample is essentially orthogonal to every other sample. So the correlation matrix of the data would be identity, would be isotropic. It is not identity. It's very different from identity. It is slow. Is very different from identity. It is sloppy. So it is not as if inputs are drawn uniformly randomly from a hyperscale. So this is pretty widely appreciated in machine learning that inputs have some structure, but it has a very strong structure. The Fissure information has a rank that is upper bounded by the number of samples. So for over-parameterized model, the Fissure information is low rank. So if you're taking CIFAR, 1 million weight neuron. 1 million weight neural network trained on 50,000 images, the FIM has a rank of less than 50,000. So it is low rank. But that doesn't mean that the non-zero eigenvalues of that fissure are not interesting. They are sloppy. And this is very interesting to think about because the fish, just like the Hessian determines curvature in the weight space, the fisher determines curvature in the prediction space. So if you think of a neural network as cooking up a probability distribution Pw of Y. Distribution pw of y given x. These are this is your class, x is your input image now for the rest of the talk. W are your weights. Then the fim is the correlation matrix of the gradient of the log likelihood, right? Of the gradient of the score function. So this is really telling you how quickly your predictions change as you change your weights. Eigenvectors of the film that are very small, that correspond to very small eigenvalues, are sets of weights. Values are sets of weights which I can change quite a bit without really changing my predictions much. Just like in the Hessian, I can change those weights by a lot without changing the loss. Here, I won't change the predictions on my training samples. Okay. Or any other samples for that matter. So FIM has an interesting structure that also arises from the data. Hessian is not low rank in general, but the Hessian is also sloppy. And the Hessian and Also sloppy. And the Hessian and FIM are very close to each other. So this is also widely appreciated. What is not appreciated is that the eigenspectrum has this weird tail that decays exponentially quickly with the same rate. And one thing that I wanted to note is that there have been a lot of studies of the eigen density of the Hessian. So because this is very closely related to random matrix theory, but the eigen density that we see But the eigendensity that we study using a random matrix theory is qualitatively very different from the eigendensity of actual Hessians of neural networks. Those things are not sloppy. There you see things like the semicircle distribution, etc. Here, this is very different from a semicircle. Okay, and I wanted to show you this plot. So, turns out sloppiness is not really very unique to deep learning, it exists in essentially all problems. Exists in essentially all problems. So, this is a picture from this particular paper which studies the geometry of non-linear least squares. A very classical ideas, kind of redone in a different way. These are the folks who came up with, started talking about sloppy models in physics, all the way back in 2001, actually. These are very, very different data sets. So, this is some protein system in your body. This is some wave functions. This is some wave functions. There are some like Big Bang Theory style models here. Then these are neural networks, also, all the shallow ones here. All these models fitted on natural data have eigen spectra that decay, that live across exponentially large ranges and are quite uniform across that range. Okay, so sloppiness is a property, I believe, I don't know yet, or I cannot prove yet. Know yet, or I cannot prove yet of natural data. If you were to draw data that is not natural or pathological, then it wouldn't be sloppy, or you could draw it easily to and make it not sloppy. Okay, what we can show is in this paper, the trace of the fission formation and the Hessian are upper bounded by the trace of the data. So, this is the x is the data, xx transpose expected value over all different x's is the actually there should be a trace that is. Actually, there should be a trace that is missing here. The trace of the data matrix is upper bounds, the trace of the fissure. So, if you were to follow me in a very heuristic argument, if I take the width of my network to infinity, but keep the dimensions of my data bounded, and if I also assume that the weights don't blow up as I take this width to infinity, then the eigenvalues of the fissure have to decay because the trace of the fissure is also the sum of the eigenvalues of the fissure. Is also the sum of the eigenvalues of Fischer. And as the number of eigenvalues goes to infinity, for it to be upper bounded by a constant, every eigenvalue either has to be small or things have to decay, at least as one over n. So this gives you one insight into how data controls these two quantities, Fisher and the Hessian. And we can essentially calculate similar quantities in the paper for different similar results for different quantities. So this is the Different quantities. So this is the ith logic, Zi. WK are the weights of the kth layer of your neural network. So this is this outer product is really the kth block of the fissure information matrix. So if you take a block diagonal approximation where cross terms across different layers in the fissure are zero, then this would be the kth block of the fissure. And all the eigenvalues, so every eigenvalue of the kth block is dominated by the eigenvalue of the By the eigenvalue of the activations of that block. So, if the activations are sloppy, which they are in our experiments, then the block of that fissure is also sloppy. And so, notice that this is a bound on the entire spectrum. This is just a bound on the trace. So, here we are saying that every eigenvalue of the block is less than that corresponding index of the eigenvalue of the activations. If the activations are sloppy, the fissure is sloppy. Fissure is sloppy. If data has bounded trace and you take a very wide neural network, the fissure has to be sloppy. What we cannot prove yet is if the data is sloppy, then the activations are sloppy, although we can do some thermodynamic limit style calculations to also say this, but not for finite neural networks. What's the activation functions for these? So here they're LU's. So here there are loose, but you but really like you just need a bounded activation function that is bounded by some constant uniformly Lipschitz constant that will show up here as the Lipsis constant raise to L here. Okay, thank you. Okay, so here is one way to exploit these observations and this is simply like one way to demonstrate that sloppiness has to do something to That sloppiness has to do something to do with generalization. So, folks might have heard about this thing called PAC-based theory. It is a kind of a variant of PAC-learning theory, which tries to bound the generalization error of randomized hypotheses, which you should think of as a distribution on the hypothesis space. So, a standard classical VC dimension result will bound the generalization error of one hypothesis in terms of the training error plus some concentration. Terms of the training error plus some concentration term. In fact-based theory, you think of your hypothesis as really a distribution on your hypothesis space on your models, and E of Q is the average population loss of one model that is sampled from this distribution Q. It is upper bounded by the training loss, average training loss of models sampled from Q plus a term that goes down as the square root of K L of your distribution Q and some prior. Q and some prior that you pick over the hypothesis space divided by 2n plus 1. So, conceptually speaking, think of a prior that is uniform over a some bounded hypothesis space. And K L of Q, P would simply be the entropy of Q in that case. And that is conceptually similar to the counting the number of different hypotheses that you are allowed. And now you are getting into a very similar argument like VC theory, which also counts the number of hypotheses in a hypothesis class. So these kinds of ideas became popular in the late 90s. Popular in the late 90s. And the nice part of pack-based theory is that you can get really a lot of intuition on how generalization might occur. So let's imagine that the distribution Q is a normal distribution with some weights and some covariance, and the distribution P is another normal distribution with some weights initialized at the initialization of your model and some covariance. The KL between these two distributions has this very interesting. Distributions have this very interesting term, which is W minus W0. So, this is how far you travel during training, weighted by the inverse of the prior covariance matrix. So, how far you travel during training, according to the PAC-based bound, is underweighted by the sloppy dimensions of the prior covariance matrix. Allowed to travel as far as you want along the sloppy eigenvectors of sigma p without affecting the KL much if that helps you. The Kale much if that helps your training, if that helps your training loss. Okay. And remember this: this is quite special. This indicates that you can travel in sloppy directions without actually hurting anything. The nice part of pack-based bounds is that you can optimize the right-hand side to compute a Q that is that gives you a tight bound. And this tells you that, look, I can find a distribution of randomized hypothesis or a distribution or randomized hypothesis or a distribution or hypothesis that generalizes that. Okay. And these were some of the first few first theory that was used to compute non-trivial non-vacuous channelization bounds for neural networks by Daniel Roy and Carolina Zikote. And that is really like how we will also exploit this theory. Here is one very cool result where we were able to obtain an analogy. We were able to obtain an analytical expression for the optimal posterior Q in Pact-based theory that still gives a non-vacuous bound. So if Q is a Gaussian distribution centered at your trained weights with this particular covariance, and if you pick a prior centered on your initialized weights with an isotropic covariance, then we can show that the eigenvalues of the posterior intact base have to be equal to the equivalent Posterior in Pachbase have to be the same as the, sorry, the eigenvectors of the posterior of Pachbase have to be the same as the eigenvectors of the Hessian at the drain weights. So the posterior is very closely related to the Hessian. This is not surprising because you can do a Taylor series expansion of the loss and you'll get the Hessian term. The eigenvectors are related directly. The eigenvalues of the posterior are quite special. So these are the eigenvalues. These are the inner and these are the inverse eigenvalues. These are the inner, and these are the inverse eigenvalues. They really are proportional to the eigenvalues of the Hessian plus the inverse prior covariance. And what this is really saying is that if epsilon is very small, that means if your prior has a huge entropy, then the posterior eigenvalues are inversely proportional to the Hessian. So if a large eigenvalue is the Hessian, you have a really, really small probability. Hessian, you have a really, really small probability mass along those directions. For small eigenvalues of the Hessian, the flat dimensions, the energy landscape, your posterior spreads apart quite a bit. And this also scales with the number of data samples, which is telling you that there is something interesting going on in how the number of samples is controlling the posterior entropy. In this case, we were also able to compute, for instance, a generalization bound of about Generalization bound of about 0.32 for a neural network with 600 hidden neurons and two layers. As compared to an optimization-based bound of these authors, it is interesting. It is still non-vacuous. It is more loose than their bound. Their bound is about 0.16. But this is the only bound that exists, that is analytical and yet non-vacuous. And yet non-vacuous for neural networks. I want to give you one perspective on how we think of this. And I'll define what we call an effective dimension for a neural network using this theory. So remember that in these eigenvalues, they have two terms. The first one depends on the Hessian, the second one really depends on the prior. So I can think of some scale for the eigenvalues. Think of some scale for the eigenvalues of the Hessian. If eigenvalues of the Hessian are larger than epsilon divided by 2n plus 1, then my pack-based posterior really cares more about the Hessian. If the eigenvalues are less than epsilon divided by 2n minus 1, then the pack-based posterior says, I don't care what the eigenvalues of the Hessian are because the loss function doesn't change too much. And I care more about reducing the KL with respect to the prior. Okay, so this lets us define an. This lets us define a notion of effective dimensionality of a neural network, which is simply the sum of all eigenvalues in your Hessian that are larger than this bound. So P is the number of weights, P of n, epsilon is the number of eigenvalues that are larger than this particular bound. And you should really think of this as the number of eigenvalues that are to the left of this particular elbow before the nice linear decay regime. A nice linear decay regime begins in the sloppy eigen spectrum. And we can show this in the paper why this is meaningful. Because the PAC-based bound kind of quality, it really changes which term it focuses on, whether it is the training error that it focuses on or the KL divergence term that it focuses on. And if you compute these terms, they are quite much tighter than traditional estimates of the number of parameters. So if you say that So, if you say that my Hessian has a sloppy eigen spectrum, the generalization error of your model is really a function of the square root of the effective dimensionality divided by square root n as compared to the dimensionality divided by the square root of n. And for neural networks, the two can be totally different. So, for the same neural network that we saw before, the effective dimensional IT is about 2000. And now you're again getting into a regime where like VC bounds make sense, but in this slightly Make sense, but in this slightly different notion of effective dimensionality. As compared to this, VC dimension is quite large. It's about 65 million. The number of parameters is close to a million. So we have a much more tighter handle on how many effective degrees of freedom exist in a trained neural network, irrespective of how many weights exist in the neural network. And the way you should think about this is that. Should think about this: is that I have a network with a large number of weights, but not all those weights are seeing anything. The data is concentrated on a very small subspace of what it could be concentrated on because it is sloppy. And the large number of redundant weights in the neural networks, the sloppy weights in Ferressian, they don't even play any part in the trading process because changing them does not change the loss. That is why they also don't get a credit. And these really is the these really is the are the weights that play a role in the training process and that one would call is a notion of effective dimensionality typically this is about 0.25 percent of the total number of weights I will stop here and conclude with this plot so there was a question that said There was a question that said, How do these results look like for non-sloppy data sets? So, the left-hand side is a synthetic data set where we created a sloppy eigen spectrum for the data. And as expected, the fission information is also sloppy and the Hessian is also sloppy. If you create a non-sloppy data set where the eigenvalues are just constant, essentially an isotopic eigen spectrum, then the fissure still has this large drop in the beginning that really comes from how many. In the beginning, that really comes from how many classes you have in your data. This first drop certainly is very starkly resembling that, but then the fissure does not increase too much, does not decrease too much. It is essentially flat. These are what non-slop the eigen spectra look like. And you can do this very simple experiment. You can change the slope of the decay of the eigen spectrum. So, this is what is this factor C is. Larger the value of C, the more sloping the sloppiness, the more sloppy the eigen spectrum. The more sloppy the eigen spectrum, and for data sets that are not sloppy, you can interpolate but you don't get generalization. So, all of these models are every dot here is a different neural network where we created labels using some teacher of 100 neurons, and then we fit it using a student with 10 neurons and a number of different experiments. And if the data is sloppy, then the student can fit the teacher without any problem, so it can do benign overfit. Any problem, so it can do benign overfitting. But if the data is not sloppy, then the student always gets a poor zandization error. So this is really an experiment that says random data do not lead to benign overfitting. Benign overfitting happens because of sloppy data. And this is actually very closely related to Peter Batlet's work on linear regression, which also talks about benign overfitting, or Sasha Rucklin's work, which talks about interpolating kernels. You should really see this as an Kernels, you should really see this as a non-linear experiment of their theorems, and it is interesting because we see the exact same thing happening for neural networks also. And sloppiness is really the heart of all these results. So to close, we began by kind of narrating essentially the many mysterious properties of SGD in deep learning. And we have been investigating them through many different angles, right? So, how do you optimize? Different angles, right? So, how do optimization and generalization interplay with each other? Why is SGD an implicit regularizer, et cetera? But it is clear that all these properties are not really intrinsic to SGD. SGD is, after all, a dumb optimization algorithm. They are really coming from the data and they are really coming from the hierarchical architecture. And sloppiness is an insight into how they come from the data. It is remarkable how strong the structure in data is and how this completely. And how this completely dominates what neural networks learn. So, perhaps the reason we are so good at doing deep learning today is because the inputs that we feed these models are not really complicated. They are simple. After all, they're effectively low-dimensional, if not exactly low-dimensional. They're not low-rank. They are sloppy. And this is an old and ever-green problem: the problem of finding sufficient statistics in the data that are predictive. And people have always thought that sufficient. And people have always thought that sufficient statistics are a very low-dimensional subset of my data. And that is why finding them is difficult. If I have 3,000 pixels in my SIFR, five features are interesting to classify the image. And finding which five pixels are useful is a difficult problem. But that is not how one should think about this, I believe. The sufficient statistic is still full-dimensional, except that the importance of different dimensions is totally different. So it is effectively low-dimensional. Different. So it is effectively low-dimensional, but it does help for you to have the extra parameters in the neural network so that you can fit the tail. And this is why over-parameterization could be so useful because it keeps, does a better and better job of fitting the tail of the sloppiness. You are never forced to make a choice between how many dimensions you fit. So this is how I think now. That is why the question mark at the end in the title of my talk and also the question mark in this picture. And also, the question mark in this picture: the interplay of all these very exotic properties are really caused by data. If you could understand data better, if you could understand how data plays a role in learning, then maybe we will be able to resolve these issues. This paper, the later half of the work, is done by Rubin Yang, who is in a third year of the applied. Who is in the third year of the applied math program at Penn, and Jalin Ma, who is in the fourth year of the applied math program. And these are all the students in our group. So we have a lot of papers in other areas also. Some of them are engineers, some of them are scientists, some of them are too young to decide yet what they want to be. It's a diverse and broad group with many different skills. You can find more. You can find more information on these papers. This is the one that we spent the later half of the talk. These are the first two papers that I had from a few years ago talking about SGD. But I'll take questions if you have them now. Thanks so much for the very nice talk. Because of time, I think we can have maybe one question because we're sort of running out of time. Actually, Gurio, you have a question? Oh, yeah, thanks. Just going back to your slide 38, you mentioned that for random data, the feature information matrix would still have a large initial drop in the spectrum and that this was related to the classes. And so I just wonder if we could expand on this just a bit. Yes, certainly. So you can kind of write down the expression for the Hessian of a softmax neural network. Of a softmax neural network, and you will see that the Jacobians of the different logics, the gradient of one logic with respect to the weights, these things typically concentrate. So, towards the end of training, all images of one class will have the same, will be nicely tightly clustered in terms of the Jacobins. And there are 10 classes, so there will be 10 centroids for these clusters, and they are what really give you. And they are what really give you the dominating eigenvalues typically in practice. But this means that you have this, the classes are clustered then. Yes, so for a well-trained network, the Jacobians of different images will be clustered, will be close to each other for every class. Right, but so when you talk about the random data, so this is still, you know, what kind of random data is this then? Is this then the data is random, but the network is still trained on this data, right? So, uh, he in this case, we sampled data using an isotropic Gaussian and then we labeled it using uh 10 classes using a teacher. But ah, all right, so you had a teacher for labeling this. Okay, got it. Yeah, thanks. A wonderful talk, uh, Pratik. Thank you so much. Yeah, thanks so much. I think due to time, we may have to stop here. So, I Stock here. So I'll