I think that this is a very important point. Yeah, right. So you can click on the next the next slide, right? Because it's the it's this um the the contrast between these two things. And there are many cases where I've seen people asking questions, is it correct? When what they actually should be asking, is it optimal? Have you been able to map out an atlas of how you should be asking the right question? Because it's it's what you just said. In many cases, if you do not use Cases, if you do not use a machine learning function in certain ways, you should really not even worry about correctness because it's purely just a question of are you doing the message level or not. Yeah. So I think that was my attempt with this slide at a very high level. And of course, you have to take every example with a little bit of thought. But roughly speaking, I'd say probably 90% of what we use machine learning for sort of reconstruction and data selection maps onto a question. Maps onto a question of what you know what you're going to get on this x-axis, what variable. We don't take uncertainty on the definition of the mass, just like we're just, it's basically choosing some function you're going to use to summarize your data. So if it's basically mapping into choosing some function you're using to summarize your data, I think you don't need an answer. So yeah, I think that's my best answer to that question. But you know, I think it can be. It matches my prior. So maybe I had one. So yes, I think the statement is true. Yes, I think the statement is true that, okay, if your simulator is correct, you can use any function of observable. And then it's going to be correct, even if it's not optimal. There is a version where, like whether or not the simulator is correct, it's not like a binary variable. So for some region phase space, it might be correct, and for other regions, it might not be correct. So there's kind of like similar to the question on kind of theoretical observables. So there's like a class of admissible functions like your learning. Functions, like your learning objective might move you into a phase space where the simulation is not reliable anymore. And so somehow by optimizing, you drop down to this correctness tier. Definitely, but I definitely want to ignoring systematic uncertainties here and leaving that for Tomasa to talk about right on this. So I still confused how we define this electoral sort of so to imagine yourself you have a simple process in which you train Simple process in which you train your neural network to calculate the mean and the variance of a Gaussian sample. So you are given, I don't know, a thousand of Gaussian samples, the neural network trains to do that, and then you are given a sample for which you really need to calculate, you know, the data sample for which you need to calculate the mean and the variance. How would you quantify the uncertainty on that calculation by neural networking? Okay, so right, so let's if you write Right, so let's, if you roughly speaking, if you have some data that you think has some variation, it's like a Gaussian, the aleatoric techniques are sort of trying to model the fact that it's not a deterministic function, that there's sort of, it's sort of the fact that you're going to put a sigma into the model. Now, the thing you just asked was actually not a question about aleatoric uncertainty, but a question about epistemic uncertainty in this language. It's a question of how well could you actually estimate sigma? I'm just trying to understand how you do statistical inference with small networks. Well, so for the statement, like how would you get the error on your estimate of the variance, when you've got a big neural network involved, there's not a guaranteed procedure that's going to give you like exact coverage. So that was like the second half of the talk we were talking about ensembling or Bayesian methods. You do the best you can, basically. At this point, I don't think there's techniques that are guaranteed to do that. Although I know Anne's been talking about some methods. I know Anne's been talking about some methods to try to do more, some methods with more guaranteed sort of coverage. So I think you can use some of those techniques, like retraining your neural network and seeing how your prediction of the variance changes, and you get some error on your prediction of the variance of that Gaussian, for instance. That would basically be what's done. That would be like ensembling techniques. You could retrain your neural network on your training data many times. Neural network on your training data many times, and you'd get some distribution of predictions over your mean value and distribution of predictions over your variance, and that gives you some idea of with your current data and with your current training procedure, what kind of variation you could get in that prediction. Yeah, I mean uh yes, what I mentioned to you is uh predictive, right? So Waldo does uh all info. Inverse. They're two different things. So my talk was about uncertainty, confidence sets and the internal parameters, but I also didn't work out. I just need to find it up. And I didn't get the difference between predictive uncertainty and inferential uncertainty because it's too many. It can't be done. To talk to you. Maybe in the brain of something. Sure, yeah. Just to say, I think those epistemic uncertainty techniques is what you do, and they're all heuristic. And they're all heuristic. If you sort of want, there are empirical studies of how well that actually works, there's something in the back of someone else doing it. So, I mean, there have been studies of taking a bunch of different models and trying to see if look at the accuracy from the predict these robot classification problems, I think, and looking at basically using a held-out test set what the the empirical coverage looks like. So they vary and as soon as you change the data sample, adding a little noise, it goes everything goes haywider. Everything goes haywider. So, if you want me to tell you how I can guarantee you'll get a great estimate on the variance of your model's prediction, I don't think that that's guaranteed. Could we go to the ABS CD society? I think it was 24. So, this is close enough to what we do without machines. Yeah. So, without machines, we worry about independence of the two variables. Of the two variables a lot. And we worry, again, about the uncertainty of the so-called transfer factor that we get from the control ratio. So how do those things get dealt with when you go to high-dimensional space? I think it's, well, so I think typically speaking here, you're still trying to find something independent so that this formula still works. So, but yeah, that's sort of before you do any of this, you sort of guarantee that. You sort of guarantee that C and B are roughly independent. Can you do that with actual intelligence or with artificial intelligence? Actual intelligence. That's actual intelligence. That's human intelligence. Then you might, so first if you just did this re-waving in one variable, you'd look at the statistical error on the ratio of maybe two histograms. And then what else would you do? Well, you'd go test it on some extra data and see how well it worked. Roughly speaking, the analog of the looking at the statistical error on the ratio of two histograms. Looking at the statistical error on the ratio of two histograms, this is an attempt at doing that, which is retraining the neural network a bunch of times and seeing how the finite statistics of the data sample you trained on affects your predictions. So those plots we have, we can look at those together and that's actually been done. These are close enough, it doesn't look massively different, but this has got some wiggles in it. Those wiggles are roughly corrected, and these error bands roughly correspond to these errors, and you can see they're not. These errors, and you can see they're not perfectly capturing all the variations because this isn't a technique that's guaranteed to give you that. So, in addition to those uncertainties, we still sort of go and check in extra data samples to see. Can I just ask you a question about this slide? Actually, just a comment about the previous slide that you showed. You don't have to go back to it, it's okay. But I'm not sure I like the phrase LL dropout. Come back to unexplained jobs. With respect to this one here, so the different curves here for different starting weights. So, I mean, I don't see that that captures sort of all of what we're interested in. I mean, it it I sort of view it as I sort of view it as you using your machine learning process to find some optimal solution for you. And by starting in different places, you might get stuck in different sort of subsidiary minima or something of a loss function. And you don't get down to the sort of the really best one. But there are a lot of other things as well, right? So that's a sort of in principle thing, but a sort of practical thing. How are the red lines produced and what are they supposed to give you? I'll answer that one first and then I'll answer the first question. So these are just, I think basically the bin by bin, like 50 or 60% into quartile range. Sort of averaging. Well this is normalized by the average set of weights. The average set of weights. Okay, to answer your other question, each trajectory actually is anti-correlated, let's say. It starts below and goes above. Starts below and goes above? Each one of the trajectories, each one of the individual bootstrap is actually below and above or above and below, right? Most likely, yeah. But okay, actually, I think this gets back to Francesca's question, which I maybe didn't answer very well. Okay, so. didn't answer very well. Okay, so if you're talking about these, so one way to sort of think about this, there is a sort of more Bayesian perspective on this. So when you're talking about the weights, you're going to choose some initial starting point, but actually you're going to effectively choose some prior over the initial starting point. And then you're going to run some function here. That function is your optimization procedure that will sort of give you some answer. They can be stochastic, but for the moment, you could imagine that's a deterministic optimization process. It's not actually. Optimization process. It's not actually fit. Okay. So there's all sorts of randomness in the optimization, but you're also sort of propagating forward your initial prior on the weights through the model and optimizing it. What that ends up doing, you can sort of prove, is it looks like a procedure where you're sort of doing Bayesian averaging only over the modes, or only over the most likely values at each of the modes of the distribution. So you're sort of averaging over all the peaks. So, you're sort of averaging over all the peaks without taking into account any of the variation around the peaks. Whereas other methods like variational approximations will basically choose one peak and then look at variations around that. So, this is what you're sort of doing is averaging over a bunch of good models in a sort of Bayesian way by choosing some prior and then running the option. That's one way to think about ensemble, the ensemble methods. Can you go back to slide 13, I think it was? Yeah. Is that up? That's up. I think it was before. It was the slide we were talking about how physicistic are thinking of. Our thinking of an actual process. So, like, even on the space of images of cats, I think there is this concept of you take the space of all images that are possible, there is some sort of density and there are ways of estimating from images to language. I think the point I was trying to get at here was, and again, it's not like a massively contrasting difference here, but the point I was trying to get at is most of the time in like a physics model, that uncertainty about the model we're building is not necessarily coming from fitting something to data. It's coming from our knowledge about the physics. Should we change the way we calculate this thing to do a better job? Right? Whereas, in sort of what you were describing, it's much In sort of what you were describing, it's much more driven by we're going to choose some model and then see how well we can fit the parameters of that model. It's much more data-driven. And so the uncertainty and the thinking about our lack of knowledge is much more in sort of that fitting process rather than the sort of thinking about physics process. What do you mean when you say model? Exactly. The physicist thinks of the physics and the machine learner thinks about the function that you're learning from data. Well I want to come back to more basic Come a little bit back to more basic. I mean, SP for an analyzer. So, you know, there are people, I mean, there are people in our field in public physics that say to me, I don't trust machine learning all for classification. I still prefer cut-based analysis, you know, because once you use machine learning, then you'll have a better understanding of the correlations between with the volume and input validities. Right. Yeah, and there might be some truth in this. I mean, in particular, in the situation, we exploit those correlations, right? Right. Right? Right. But for practical things, I mean, that's something which I think is we do very generally in particle physics, but also time our systematics is often based on some kind of measure of goodness of fit. So I mean like what from the recipe I've heard of is like when you use the neural network classification, you better check for each input of the like of um the the agreement of your data with the model colour simulation for this individual input. For this individual input distribution, right? And if there's no agreement, then maybe you should better discard it. So that's something I just wanted to bring to the attention here. I think the goodest of it is an essential tool for trying to monitor and understand these systematic uncertainties. Exactly, and I think I was going to say that you finally said the key statement at the very end, systematic uncertainties, right? Very end, systematic uncertainties, right? Almost everything you mentioned was about what happens when you're applying a model on simulation, and that doesn't perfectly model the data. And so that was, I was sort of avoiding that discussion here because it's sort of broken up between the two talks. So I completely agree with everything you just said. What we sort of also tried to capture here is what happens if training that model could generate new systematic uncertainties. And there are cases where, even if you had a perfect simulator of the data, that you'd train a machine learning model, you'd That you train a machine learning model, you'd get a bad model fit and it would generate new uncertainties. And that's the sort of case like if you have a surrogate, a fast simulator, and now you train it and it doesn't give you the same answers as your real simulator, what do you do? You have an uncertainty now. I still want to say, I think, just in practice, we are now the phase whereas machine learning is slowly taking over many things and it's used a lot. And I think also our diagnostics is improving. I see now that I can see and list and I guess others it's the same. And I guess others are the same. There are more and more diagnostics, like really, you know, not just studying one input observable alone, but to have two-dimensional plots, one observable with the other, and exactly in order to look at those calculations. I still have a feeling that this is not in a major state. This is just from the logic. Every year I see more advanced diagnostic tools, more checks. More checks different in goodness of fit tests apply, for instance. But I think my first conclusion should really do more of that. I agree. I think maybe, of course, goodness of fit in 500 dimensions is pretty difficult. But maybe I'll use that as a plug for the next Vistat goodness of fit workshop or discussion meeting. June 1st and June the 2nd remote meeting, ViStat meeting on goodness of fit. On goodness of fit in two sample testing in two dimensions or more, where two might extend up to, what was it, ten to the eight. I was going to say that another sort of systematic like you were just describing would be overfitting, right? Yeah. If you like that's not uh you have maybe you have a perfect simulator, but if you take your model Perfect simulator, but if you take your problem, you can easily overfit to those data. Is that something? How does that fit into what you're talking about here? And is easier as a problem to easy to use the typical methods for avoiding corporate fitting, things like cross-validations? So a few points. Yeah, so again, it depends on where you're applying the model. If it's just going to determine basically power and size of your statistical test, what overfitting is going to do is just give you a worse function compared to the opted. You have a worse function compared to the best function you could have gotten for doing your statistical test. Otherwise, overfitting to a simulator, making a surrogate simulator and overfitting to your original simulator could absolutely generate uncertainties there. The second half of your question was, oh, can we use those techniques? Yeah, I mean, we certainly can, but you're in this sort of subtle range. So you have this perfect simulator and lots of data. You want to avoid overfitting it. So you use cross-validation or you use regularization techniques and you You use regularization techniques, and these can also push you away from getting the best fit of your data, but it's sort of the best without overfitting too much. I think it still leaves you in a place where you basically have to go and decide whether there's an uncertainty there. Should you repeat this entire procedure many times? Although sort of cross-validation does that, it sort of does this repeated. So that may give you a sort of uncertainty in and of itself. But I mean, cross-validation and ensembling plus bootstrapping are like. And ensembling plus bootstrapping are like not so far apart, right? They're not exactly the same, but they're a few few. They're sort of pretty similar ideas, I guess. In structure. You said that bootstrapping doesn't give you good coverage properties. That's not a surprise to the statistician. It often doesn't. But there are a little more advanced ways to do calculation intervals, so ABC intervals. Intervals or ABC intervals and such things. Have they been tried? Not that I know of. I think there's a lot of, I mean, I think it's sort of been said, at least in some of the barroom discussions, there's a lot of statistics knowledge that could still be injected into these, especially uncertainty quantification literature, that can be very helpful. There are tweaks on how to calculate the confidence intervals from the bootstrap that tend to make it much, much better. Yeah, exactly. I think that the home is called ABC. We don't have a D. Yeah. I. Yeah. Is it different from the South Foundation for another ABC? It's another ABC. I don't know of any papers on that, but it doesn't mean they don't exist. It's described in Efron's book, the beta classic book on describing them. Yeah, I don't know of one, but it could exist because there's so many papers today. It's hard to follow. Yeah, by the way, this whole question of application of what stuff, of neural networks, I mean if you calculate I mean, if you calculate mean and variance in the fusion bookstore, there are some very subtle cancellations which happen due to the basically you get certain between sample correlations and in sample correlations, which you cancel out. Roger could tell you a lot more about it because he wrote an article about it. So it's the subtle correlations due to which The subtle correlations due to which the bootstrap method works for simple things like means and variances. But for things like neural networks, it's completely not obvious that this stuff works. Have you guys studied this? Well, no, and I think the reason is empirically it didn't help at all. Like, when they started studying this kind of stuff, in fact, there's an entire paper called Why Why doesn't bootstrapping help? There's a paper called that. It's all empirical, and it basically just shows you could spend your time trying to study that, but at the same time, it doesn't really help your uncertainty estimates why do that. I think that was sort of what happened. People saw that it didn't really, like for these sort of empirical coverage tests, didn't give you any benefits, and they stopped completely stopped using it. Okay, Daniel's asking. So, when they do this bullshit, like do they bullshit the Bullshit, like do they bullshack the training part or the evaluation part? Because these are two different things. Like, you train the neural network and you might do some things in there. Now you have this function, right, that is trained, and then you could bootstrap the input function as the output function, various functions, the input. So which bootstrap are we talking about? This case was bootstrapping the training set. Okay, so that's not a surprise. I mean, yeah, I mean, that's about the training, but really, if you want to develop answers, it's that second type of bootstrapping if you want to do it. Second baroque if you want to data. And there may be studies of that too. I can't guarantee they're not there. But certainly, if you look at that example I gave in the backup, they may not have bootstrapped the they use multiple test sets, yeah, which I think is effectively sort of bootstrapping the test set to sort of look at what the variations look like. So certainly there are examples of that, taking sub-samples of the test set and seeing how things vary. But yeah, so but I think. But yeah, so but I I think when they were talking about bootstrapping in the original Deep Ensembles paper, it was bootstrapping the training set. Yeah, so this may get back to Chats. So maybe my statement that ensembling and cross-validation have a lot in common are a little bit different, because cross-validation, you're very careful about how you choose your test sets, right? Yeah. So maybe I was a little too quick to draw So maybe I was a little too quick to draw too much similarity there. But you could use those techniques to try to draw conclusions about quality of your training. Can you go back to your epistemic answer? Maybe like slide eight or something. So, I mean, to me, somehow the green is not as relevant as the red arrow, right? Because let me take a limit there. You have a lot of data about Arde. A limit where you have a lot of data about a model, so you can get the perfect point which is closest to your two, right? But within the model, you can never get to two, right? As far as I understand. So let me compare this to the case where I just take a Chebi-Chat expansion of that two. So in that case, I have a well-defined way of knowing what the truncation error is of my polynomial approximation, right? And I can systematically expand to higher order, and there is a formula. To a higher order, and there is a formal limit where I know the thing becomes exact. Now, the question is: maybe this is naive because I demonstrate the lack of my understanding, but when you have a neural network, is there a way where I can formally see that I eventually reach F two by somehow adding more nodes or like doing something? Or is the only way to estimate that true error I take a different model, take the difference, and then I still don't know what it means? All this means? Well, uh. Okay, right. So, a few things to keep in mind. One, there's no expansion. We don't know the function that maps megapixel images to cats. There's no way to do an expansion. And this is, you know, this is, if you're talking about the weight space, this can be nowadays a trillion dimensional. So one of the things to keep in mind is this function space is enormous, right? Is enormous, right? I mean, massively enormous. So, and actually, optimize, so, and that is one thing to keep in mind: that you cannot, when we're talking about deep neural networks, you cannot separate the model and the optimization procedure. The optimization procedure actually helps determine what model you're going to fit. I mean, sort of, it even gives you a form of regularization. So, it's not that it's so easy to just say, I had a lot of data, I fit this thing, and it's great. Actually, it's okay. There's certainly some. Actually, it's okay. There's certainly still estimation error. That all being said, there's some really cool work actually originally driven by some physicists who went to go work for OpenAI, at least part-time, to look at sort of infinite limits, either of like training set size or model width or model depth. And you can start to derive scaling laws on basically your approximation error. We're able to do sort of finite sample size approximation theory. So, all of the sort of statistical learning theory. So, all the sort of statistical learning theory is trying to answer your question effectively. And now, there's some nice physics things like in the limit of infinitely large neural networks that, in principle, should be able to capture your function. What sort of approximation there is? Does there have to be a true function? I suppose there does not have to be a true function, yeah. But let's say there is a true function, okay? The question is, is you know, and without having infinite And without having infinite computing resources to get there, right? Like, I guess the question is: is there a way for me to estimate length of the red double? So, without beyond literally taking, you know, this guy and this guy to the network and taking the difference. Cross-validation, statistical learning theory, you know, I don't know at the end. You know, is it always my category three or can I go to my category one, right? Category three. To my category one, right? I mean, in my talk, I had the three categories of, and I'm thinking, okay, what I have in mind is f is a quantum distribution function, right? Because people have used neural networks to just provide some very generous random model. And there, f really is just a one-dimensional function. So, my comparison: do I just do a temperature polynomial expansion is a valid one, right? And the question is: the question I really have is, you know. Have is there a way for me to estimate this error that you have intrinsic to the model in some more sort of quantitative way other than just comparing model one with model two, taking the difference, and I don't know what that discons will be. So there are some guarantees and it really depends. I mean, there's things like cross-validation, but like let's if you start with low dimensions, the low dimensions and low number of parameters is sort of a different story. Of parameters is sort of a different story. Not massively over-parametrized models, which is the regime we're in now, but your one-dimensional picture. There's an entire statistical learning theory where you can get bounds on how well your predicted model matches the true function with that, and that bound or that error depends on the number of parameters, the number of training points. There are approximations for that. Those all break down in the limit of over-parameterized models. So, an entire new theory is being developed to determine. New theory is being developed to determine what's called this sort of generalization error, or roughly speaking, how well the model you fit will compare to the true function. But maybe Chad or Anne have other comments on that. Hey, I would just say that I think in general, neural networks have a huge capacity to fit in complex relationships. I would be much more concerned about overfitting than we are. The approximation error is probably more important, and you have a trillion parameter model. So the true function, do you have a problem? So the true function, do you have a like a you have calibration data, right? Like data that you can that defines the true function in the situations you're talking about? You can fit it, right? In the situations you're interested in. With that distribution F2, do you have something like observable data? Sometimes. I mean, but it's not, yeah. Sometimes, I mean, but it's not, yeah, but it's not, we don't know F true. We might be able to have data which is likely sampled from the relationship you care about. So, I feel like calibration data may be a good example. Like, if you want to learn how to classify electrons, you can go look for Z bosons and then sort of look at how well you did on a very, very pure sample of true electrons. But we don't know what the F true is. Yeah, yeah, well, I'm not sure how the discussion will find this further. But what we're working on is basically have a family. Working on it, basically, you have a family of functions, and it could be a physical model, and you have your best model, and then we can have a map calibration data to get onto that. Yeah, and that's sort of what I wanted to talk to you during the break. But you need your calibration set. And it's kind of one map, so it could be the family functions could be your physical meaningful and then your Yeah, I guess the models, but I think there's the question of also like it's a calibration set, right? So you still have the effect of like a finite sample size. Right, it's just that nothing in machine learning guarantees calibration. Like normalizing flow or the strata, it doesn't give you calibrate the distribution of germs. Yeah, I agree. Okay, so we're bumping against the coffee break, so thanks a lot for the uh nice discussion. 