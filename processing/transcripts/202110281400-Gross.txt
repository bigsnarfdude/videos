Well, thank you very much, everyone, for being here, and thank you, Elizabeth, for accepting our invitation. Professor Elizabeth Gross, she is currently an assistant professor at the University of Hawaii at Manoa. She did her PhD at the University of Illinois at Chicago, and she has been awarded the very prestigious Career Development Program Grant Career. So, we're very happy to. So we're very happy to have her here talking about learning on phylogenetic networks using invariants. Thank you very much, Elizabeth. Awesome. Well, thanks. Welcome to everyone who's joining us today. And thank you for that very kind introduction. I'll have to say all of your introductions have been so kind and warm for all of the speakers. And I've been really enjoying that. And so today I'm going to talk about And so, today I'm going to talk about looking at the problem of inferring phylogenetic trees and phylogenetic networks and look at how the underlying algebra and geometry of these models can inform us about inference. And particularly, we're going to combine some algebraic techniques and some statistical learning techniques. And so, this is joint work with Colby Long and Joe Rucinko. And it was started with one of my students, Travis Bartinette, when I was at San Jose. When I was at San Jose State. And so I'll show some figures that he has done as well. So, oh, hold on, give me one second. There we go. All right, so we've seen some phylogenetic trees already. So actually in David's talk in the last hour, we saw a couple of phylogenetic trees pop up, and we've seen phylogenetic trees pop up in Rudy's talk as well. So to give you guys an So, to give you guys an outline of the talk, I'm really going to focus first on the problem of tree reconstruction because that's going to lay this foundation for networks. So networks, we can just think about a generalization of trees where I allow some edges and I get cycles. But I really want to focus on the underlying geometry. And so, if we can understand the geometry of the tree models, then the geometry of the network models. Work models becomes a little bit easier for us. And so it might be a little bit heavy at first on the tree side, but again, that's on purpose. And then once I introduce phylogenetic trees, we'll move on to networks. And then I'll talk about some of the statistical problems for inferring phylogenetic networks, talk a little bit about the identifiability problem, and then move into using sort of support vector machines in order to classify. To classify certain coordinates. So, coordinates are going to be four leaf phylogenetic networks. So, to motivate this talk, so Rudy had a very nice, the motivation that she was giving for this phylogenetic reconstruction was on lungfish. I have on the right over here the phylogenetic tree of the Hawaiian honey creepers. And I just wanted to talk about how. and I just wanted to talk about how this reconstruction can give us not just is not just giving us a visual history of a collection of species or taxa but it can also give us a little bit more insight on how the stories connect with other things like for example here we will see how Hawaiian geology plays a role so these are the Hawaiian honeycreepers and so their nearest ancestors are believed to be from Ancestors are believed to be from Asia and they're most closely related to rose finches. And so it's believed that maybe like 5 million to 7 million years ago, because rose finches will find try and find new wintering grounds. And so maybe a large collection went to go find new wintering grounds. They got caught up in a windstorm and ended up in the middle of the Pacific. And so right around that time is when, so the Hawaiian, you know, the main Hawaiian islands are rather young. The main Hawaiian islands are rather young. So, around 5 million, 7 million years ago is when Kauai and Nihau were forming, which are sort of the oldest of the main Hawaiian islands. And so they settled there. And then, you know, as time went on, what would happen is, so they're this example of basically evolving in a very insular environment, right? Because they're just in this archipelago in the middle of the Pacific. Archipelago in the middle of the Pacific, but there's such a diverse landscape. And so as time went on, maybe they started, you know, a collection of birds started to evolve according to Maui, some of the, and find a niche there. And then they would move to another island, maybe Oahu, so Oahu, maybe 2 million years ago. And then they would do some evolution there and then go back to the other island. And so when you have this process, you get like this great diversity very quickly. Diversity very quickly. And so, in fact, if we look at these birds, some of them, you know, we see the morphological differences. So, some evolved so that they were able to feed, get nectar from very tubular sort of petals. Others were able to get insect out of barks. And so, they're like this prime example. Now, one of the interesting things about this phylogenetic tree that I have on the right is not only is it The right is not only is it giving us the history of the species, but what we see here is a lot of the speciation events that happen here, and that's where the speciation event is where it branches, actually occurs with the formation, the geological formation of Oahu. And so this tree is telling us multiple stories at the same time. And so this is my motivating example for why we want to do this tree reconstruction or this network reconstruction is that it can tell a story. Is that it can tell us stories. And then, further on, furthermore, it can aid in conservation and ecological restoration. So, sometimes we want to, there's been some studies that say that if you restore, keeping in mind phylogenetic diversity, then the restoration takes hold a little bit better. And also, you know, another example comes from epidemiology, right? And so, I think we saw. Right. And so I think we saw this with COVID. Rudy had mentioned that, okay, we can actually construct these trees really fast nowadays. And so we can actually construct them real time if we're trying to track a pathogen. And so when we're, you know, when we're able to do that real, real-time construction, it allows us to understand how the pathogen is evolving. And for example, that's what gives us a really good idea of when COVID-19 probably COVID-19 probably jumped from animals to humans because we can estimate these branch lengths pretty well nowadays. So how do I reconstruct this phylogenetic tree? And I'm going to first focus on, okay, what's going to be my data? And we're going to start with aligned DNA sequences from a collection of species. And in this particular talk, what our data is going to be is, so here's Going to be is so here's three species, and I have three aligned DNA sequences, and then each of these columns here is called a site. And so, if I look at each of these columns, it's a three-tuple of DNA bases. So, this column here or this site here, I get GTG. And what I'm going to do is, so I have four bases, A, C, G, T, and so I can have Sorry, four to the three, so I can have 64 possible three tuples that appear. And what I'm going to do is I'm going to count, right? So maybe my sequence here is length a thousand, and I'm going to count how many times I see GTG. And then I'm going to count how many times I see C C G C, how many times I see G A A, and basically all of the 64 three tuples. And then I'm going to divide by a thousand. And then I'm going to divide by a thousand, and that's going to give me my relative frequency. And so when I have this table, this is actually going to be an empirical distribution, an empirical distribution for us. And one thing that I would like to point out, the reason why I really want to pay attention to our data is because in Rudy's talk, her data was a little bit different. Her data was actually, you were taking in a collection of trees and then doing some statistical analysis. And then doing some statistical analysis on here. Here we have a collection of three tuples or n-tuples if I had n leaves. And so now I want to take this empirical distribution and I want to say, okay, you know, is it more probable that that empirical distribution came from this tree or this tree or this tree? And so there's a couple of ways that we can do this. And so in reconstructing phylogenetic trees, there's parsimony-based methods. Parsimony-based methods, there's distance-based methods, and there's model-based methods. And so I'm going to focus on model-based methods. And there we're going to assume that evolution proceeds along an unleafed tree. And like I said, the data are the observed frequencies of the DNA species. One of the important things about these tree-based Markov models is that they assume site independence. And so that's something that we should keep in mind that we're making that assumption. And then, one of the, you know, one of the things, what makes this so even though we can do it fast, there's still some mathematical challenges that come along with inferring these phylogenetic trees. And where that comes from is actually is the latent variable problem. And so we actually is a graphical model with hidden variables. So the leaves are going to be our living species. And those are the ones that I can sample DNA from. Sample DNA from. And then the internal nodes are going to be, you know, are usually extinct species, and I don't have information on. And so we put a model on each of the trees, and then that allows us to do things like maximum likelihood estimation. And so in my previous picture, you know, my picture that I had, the trees seem very confident with each of the edges. But in fact, right, there, you know, there's We're confident about more edges than other edges. And so this particular graph sort of weights. The like more sure colors tell you that they're more sure about those particular edges. And then here, the more fuzzy where we're less confident. So now what I want to do is I want to take just a short interlude into the mathematics of these models because I'm really coming from the computational. Coming from the computational algebraic geometry side. And so, in computational algebraic geometry, we're sort of usually working with polynomials. And I want to show you how the polynomials are coming up in these models. And then that's going to basically give us the motivation to move to a geometric perspective or algebraic geometric perspective. So, let's start with a very simple example. So, this is a three-leaf claw tree. And so Tree. And so in a tree-based Markov model, I have two types of parameters. I have the combinatorial parameter, which is the tree or the network as we move on. And then I have numerical parameters. And these numeric parameters for each edge, I'm going to get a transition matrix. And the transition matrix is going to tell me, okay, the i-jth entry of the transition matrix is going to tell me the probability. So, like, if I'm going along this edge. If I'm going along this edge, this first edge here, the i-g entry of M1 is going to tell me the probability that X1 is in state J with Y beginning in state I. Okay, so it just tells me the probability of moving between each of the states. And that's really all that I need to define this model. And then I have some, this is going to be some hidden variable and it's going to have some root. And it's going to have some root distribution. In the models that we look at, we tend to make that there are certain things going on that we get to assume that this root distribution is the uniform distribution. So these particular models can be quite complicated because if I just allow these to be stochastic, the only requirement that I'm making is that these are stochastic matrices, then I actually have a lot of parameters for just this very simple. Parameters for just this very simple claw tree model. And so a lot of times we make a simplification and we move to different simpler models where you're reducing your number of parameters. And so the one that I'm going to look at today is going to be the Jukes-Cantor model. And that's probably one of the simplest models because you're just putting for each of these transition matrices, what they look like is you're going to have one probability of staying in the same state and then another probability for changing. Another probability for changing. But this really reduces the parameters of the model, which allows for easier inference, honestly. So now that I have this three-state Markov model, what I can do is I can actually write, you know, I can go back, you know, I can do this with basically at undergrad, write down the probability of observing any of the three tuples. Any of the three tuples. And so let's say I wanted to figure out the probability of observing the three tuple AGT. All right, all I'm going to do is I'm going to marginalize over that y variable. And so I'm going to look at the probability that, you know, y is in state A. And then what's the probability of staying the same here times the probability of changing to G times the probability of changing to T. And so the main thing that I want written down here. That I want written down here is if we notice, okay, what I get is this is actually a polynomial in the entries of those transition matrices. And that's really the foundation, like this, noticing this is really the foundation of the work that we do in algebraic statistics for these particular models. Okay, so I'm going to think about each of the entries of M1, M2, M3 as indeterminates or as As indeterminates or as variables. And so then this has this particular polynomial has four terms. It's degree three. If I think about pi as a constant. So now we're going to bring in the algebra and the geometry here. So what I have over here is, you know, so what I've convinced you is I have this parameterization. I have this parameterization of this model. So, for a model for us, so if I fix the tree, so I'm fixing the tree to be the three-leaf clot tree, a model is going to be a collection of probability distributions. And so, and in this particular case, this collection of probability distributions is going to be parametrized by polynomials. So, this is that map, but me writing it in a different form, in tensor form. And now, in that model, I'm going to think about each of the distributions. So, these are discrete distributions. And so, over here in the image space, each element is going to be a 4x4x4 table where the ith jth kth entry tells me the probability that x1 is in, I guess I'm using jkl here. Okay, where the jkth elf entry tells me the probability that x1 Me the probability that x1 is in state J, X2 is in state K, and X3 is in state L. Okay, so I'm thinking about these as tables. I have a collection of tables. And now if I vary over, let's say all stochastic, I mean, all matrices, if I'm in the chickscanner case that have that form, that alpha is along the diagonal and the betas not on the diagonals. And I vary all possible parameters, then I'm possible parameters, then I'm going to get a whole collection of these four by four by four tables where each of these tables is a distribution. And that is going to be the model that corresponds to this particular tree and the juice, you know, and that also is associated to the juice-Cantor DNA substitution model. Okay, so our models are these four by four by four tables. So now what I can do is I can look at those tables. And so I'm thinking about each of those entries. So I'm thinking about each of those entries of tables as a variable. Oops. Hold on a second. I just all right. So I'm thinking about each of those entries of those tables as a variable. I'm going to give them, you know, a label of P and I'm going to index them by JK L here. And then I'm going to ask the question, what are the algebraic relationships on these P's? So for example, if I went through that process that I just did, I would find That I just did, I would find that the probability of AAA is the same as the probability of absorbing CCC in the Jux-Cantor model. And so an algebraic relationship on that table might be that PAA minus PCC is equal to zero. And then so I'm going to find all of the polynomial relationships among those variables. And so when I find all of those algebraic relationships, All of those algebraic relationships, they form what is called an ideal. And so, this ideal is this collection of polynomials or algebraic relationships. Now, a lot of the times they're not going to be very nice like that in the linear sense. They're going to be much more complicated. And so, for example, if I didn't have any restrictions, if I didn't have any restrictions on these transition matrices, then I get what's the general Markov model, and then this ideal has. Model and then this ideal has elements of degree five, degree six, degree nine that get complicated rather quickly. All right, but coming back to the Jukes Cantor, I'm in this simpler case. And I have this ideal, and then what I'm going to do is, right, this model is about those set of tables is a subset of the probability simplex because all of the entries are going to sum to one. But if I then look at all of major if I then look at all of maybe the elements of you know all of the elements of R444 that also satisfy that those polynomials and I get a larger set and even better if I go to the complex numbers then I get you know I like that even better and I'm going to get this algebraic variety and so when I do that process right taking all the points so I Taking all the points. So I took a set, I found all of the polynomials, all of the relationships, and then I take all of the points in the complex numbers that satisfy those same relationships, and that's how I get the variety. And so this process is called taking the Zariski closure. And so I've gone from ideal to a variety. And so now the whole idea of algebraic statistics, or one of the ideas of algebraic statistics, is this particular variety, which contains my Which contains my model, right? It was a superset of my model. We want to understand the geometry of that variety to help us inform methods about parameter estimation and model selection. And so let's see how we can do this in the case of phylogenetics. There we go. And so for phylogenetic And so for phylogenetic trees, so this idea, so how do I use this algebra and geometry? So one of the first things that I can do is I can take that ideal and I can take polynomials and I can take my empirical distribution and I can plug them into those polynomials. So if I fix a tree and I take a polynomial and the ideal corresponding to that tree and I plug in the empirical distribution, then it should be close to zero. And so this particular method is called an algebraic. Method is called an algebraic method, and the polynomials are called, you know, are called invariants. And so these methods were first proposed in the 80s by Cavender and Felsenstein and Lake independently. And it actually worked pretty well, except they only used linear invariants. So things, you know, where the degree, the polynomials were of degree one. And then more recently, Marta Casanelles and Jesus Fernandez Sanchez actually Fernandez Sanchez actually looked at that same method but using the full set of invariants or using a lot of full generating set of invariants. And their process worked as well as other methods and inference. And then in fact, there's been a lot more work. And so even in Popstar, which is one of the, yeah, which is a large software comp You know, software compilation in order to do phylogenetic tree inference, there's some algebraic methods that are implemented. So, another very popular one is Laura Cupatko's SVD cortex. So, these methods are out here, and the main idea is you plug in your data into a polynomial, you see whether it's zero or not. Now, why does this work? So, there's a fundamental thing that's going on here with trees. thing that's going on here with trees that makes this method work. And so in order for me to say, okay, let's say I have a tree one and a tree two. And so they're going to have a corresponding variety, variety one and variety two. And I'm going to have some sort of data point. And I want to be able to tell, you know, I'm going to plug it into some polynomials. And I want those polynomials to. want those polynomials to be zero if it belongs to v1, but not zero if it belongs to v2. And this is only going to work, you know, you know, with probability one if I actually have some separation between these varieties. And so what do I mean by some separation in these varieties? I mean I want where they intersect. They're going to be for trees, they have the same dimension. And so when they intersect, I want that intersection to be lower dimensional. intersection to be lower dimensional. And then I would say that they're generically identifiable. And if they're generically identifiable, then I get to use these algebraic techniques. And so in this, you know, this might be a worse case if I have something like this happening, where one of the varieties is sitting within one of the other ones, because then I'll have polynomials that will vanish for both of them. Any, yeah, I'll have polynomials that vanish for everyone, and I won't be able to distinguish between the two. Another thing, I could have the two varieties be the same. And so, in fact, that does happen. So for trees, if they have the same unrooted topology, they have the same model and they have the same varieties. And so, the tree is, I mean, the root of a tree is never identifiable. A tree is never identifiable. So, this works for unrooted trees. And so, in fact, rest of our conversation will be in the unrooted case. So, now I want to take these ideas and now I want to apply them to phylogenetic networks. So, what is a phylogenetic network? All we're going to do is we're going to think about it as a generalization of a phylogenetic tree. So, the problem with trees is that they can't. Trees is that they can't model reticulation events such as hybridization or horizontal gene transfer. And so, in both of those cases, I might get these edges that are going from one branch to another. And what do I get? I get cycles in my graph. And so, I want to expand the types of graphs that are outputted from this phylogenetic inference process to also include graphs that can have this shape. And this occurs all over. And this occurs all over in nature. And in fact, these types of models are becoming more and more and more popular because we're seeing this sort of behavior happening more and more often. Now that we, now that we're, you know, now that biologists are searching for it. And so there's a real need to have these tools and techniques. Okay, give me a second. Give me a second. All right, so now that I've explained the map for trees and the algebra and the geometry that's going on for trees, let me explain what a phylogenetic network model is. And so there's different models. We're working with a network-based Markov model. And so it's just this generalization of a tree model. And so if I think about, again, the network model as a Again, the network model as a collection of probability distributions, then each of the distributions in that network model is going to be the mixture of two tree distributions. And so, in fact, how I get it is, so let's say I have this network over here. This vertex where I have two edges coming into is called a reticulation vertex, and these two dotted edges are called reticulation edges. And so, if I delete each of those reticulation edges, then I get Reticulation edges, then I get two different trees. And so what I'm going to do is I'm going to take the parameterization for this tree and take the parameterization for this tree, and then I mix them with a delta parameter, which, you know, I think I call it gamma later on in the chat, but gamma delta. And so now the main difference is here though. Is here though. Well, like one thing, I think one thing that I would like to highlight is in phylogenetics, you also have what are called mixture models. And so in a general phylogenetic mixture model, these parameters, what I would have is for this tree one and this tree two, these parameters would be independent of each other. They wouldn't match. So in the network model, you have the identification of the models. And so you get this sub-model of basic. Get this sub-model of basically the mixture model. I can do the same thing as I did before, where I take the image of this particular map. You know, I convince you that this is polynomial, this is polynomial. So I take the mixture of this, I get a polynomial map. I look at the image of that map, and then I take the Zariski closure the way that I just described it before. So, Elizabeth, I have a quick question. Yeah. What is Yeah. What is the? I mean, the mixture is not unique, right? I mean, it can happen that you have two different ways to express a phylogenetic network as a mixture, two different mixtures. Yes, it's not, it's not. How do you decide? Oh, okay. So, but then, okay, so that is like a little bit different of a question because you're asking, oh, can I infer that delta parameter basically for example, yeah, right? And so we're not really looking at. And so we're not really looking at that, that particular question. We're looking at: can I infer the actual combinatorial object? And so we're just going to think about it all like, you know, yes, there's multiple ways that I can get a different distribution, like depending on the different numerical parameters that I put in, right? But they're all just going to be in the same set. And I'm only like looking at it as a set. But the trees involved can be different as well, right? Once you have more, more. As well, right? Once you have more, more, I mean, if you have more connections, more of these dotted lines, you might actually that's what well, no, actually, that's what we show. So I'll show that a little bit, you know, a little bit later is actually you only have like a limited number. It's up to what's called like the semi-directed network topology. So so if I unroot things, so so if I take, oops, give me a second. Give me a second. Okay, so I'm actually going to look at this one real quick. So, what can happen is: so, if I have this, these two networks, I'll get two different trees, I believe, in this one. Oh, no, no, I'll get the same two trees, but they might have like different numerical parameters, different edge lengths. And so, these will actually give me all of the distributions that I can get from here. I can also get. Can get from here, I can also get from here, and vice versa. And so, right, so you know, another way to say that is you give me a set of parameters here, I can find a set of parameters here that will give me that same distribution. And so, we do have this identifiable, so that would be like an identifiability issue. And so, because those models are exactly the same, the best that we can recover is this semi-directed network where I lose the root. And in fact, when I have trying. And in fact, when I have triangles, I actually lose which ones are the reticulation edges. And so I also lose some information about the composite trees there. Well, not actually for three cycles, not the composite trees. But yeah, does that clarify? This is my question. Thank you. Awesome. Okay. So I think that that was a really great interlude into really the problems that come up with identifiability here. With identifiability here. So we're going to, you know, I have this network variety. And now if I have two networks, then I'm going to say that the networks are distinguishable if where they intersect the sub-variety that they intersect is of strictly lower dimension. And this means that, okay, you know, with, yeah, so with probability one. With probability one, if you give me some true data point, so some true distribution, I can tell you exactly whether it came from network one or it came from network two. And so if I'm working with a class of networks where everything is where all pairs of networks are distinguishable, then we call the network parameter generically identified. Network parameter generically identifiable. And then, you know, I've gotten into this little conversation about, okay, yeah, you know, because of this issue of like brief parameterization, there are just some things that I'm just not going to be able to identify just from the setup of the problem. And so just like with trees, I'm not going to be able to identify the root. And then we'll see when I have a three cycle like this. Three cycles like this. So, this is like the best hope that I can identify. And then we'll see with three cycles for the Jukes Cantor. I actually won't be able to tell whether or not like these, the reticulation edges are going towards this point or whether they're going to this point. All I can tell is that there's a triangle there. And so this identifiability problem I've actually looked at quite a bit. I've actually looked at quite a bit. And so with Colby Long, we looked at basically we started off just looking at these networks with a single cycle and looked at this identifiability problem, just really restricted with the Jukes-Cantor model. And then there was a very nice paper with Benjamin Hollering and Seth Sullivan, then extended those results for the Chimura 2 parameter and the Chimura 3 parameter. And that really Three parameter and that really set this foundation for this bigger work that sort of subsumes all of this with Leo Van Ersel, Remy Hansen, Mark Jones, and Yuti Marikami. And so what we were able to show in that one, again, just using these geometric arguments, is that let me first define a level one network. So a level one network is going to be One network is going to be any network where for every cycle only contains a single reticulation edge. I mean, a single reticulation vertex. So when I have a large cycle like this, I can't have two reticulation vertices. I can only have one. And so if it satisfies that criteria, it's called a level one network. And so what we were able to show was that, hey, if you give me any two level one networks. Level one networks on N leaves, and they're triangle-free, then those two varieties are distinguishable. Those two networks are distinguishable. And so this actually gives us some identifiability results. So, but it also gives us a problem that we have this problem with triangles that makes our work a little bit harder. So, if I have no triangles, then I'm great. I'm in that world where everything. I'm in that world where everything is distinguishable. I can, if I have a perfect, you know, distribution, I can tell you which network it came from. But what about these issues about these triangles? So they're going to cause a problem for us. And so in fact, with Colby-Long, we did a bunch of computations and just computed these for the Jukes-Cantor model. And we see some nested behavior. And so for the three cycles, we So for the three cycles, we get, well, for trees, there's three distinct varieties. Those are going to correspond to all of your three possible quartets that you can get. And so those are six dimensional, and these sit inside six seven dimensional varieties that correspond to the three cycles. And then the four cycles are eight dimensional. And they contain the three cycles and the four cycles. Four cycles. And in fact, we can put this all into one sort of graphic in the following form, where, you know, if I have a line that goes between the coordinate here, and we call these coordinates, there's a line going upward, it means that the variety is contained. So this is going to make the inference problem a little bit harder for us because we have this stratified space. But now, what I'm going to do is, I'm now going to turn my attention to: all right, here are, you know, I've talked about some of the geometry for these phylogenetic networks, some of the algebra. And now what I want to do is, okay, how can I actually infer these? And what are some techniques that I can use? So we want to use those algebraic approaches. So, but we have a couple of challenges. So the first challenge is we have this nested behavior, which you don't have for trees. Which you don't have for trees, right? Because for trees, they're all the same dimension. And we know that if they have different unrooted topologies, then they're different varieties. So that's going to, you know, cause one problem. And the other problem is actually something that came up in the tree setting as well, is that let's say I have a perfect distribution. Okay, so it belongs to one of these network models, say this network model. Now, when I take all Model. Now, when I take all of those polynomials and I evaluate it on that distribution, all of those polynomials are going to be zero. But I'm not working with a perfect distribution. I'm working with an empirical distribution. So I have some noise. And so when I evaluate, so even if it's generated, even if the data is generated from this model, right, when I construct an empirical distribution, when I plug them into the polynomials, they're not going to be perfectly zero. Be perfectly zero. They're going to be off. And because these models are, you know, because like these models are hard to analyze, right, there are no statistical methodologies to tell me like what is close enough to zero. So the only thing that we really, you know, have done so far is just sort of by trial and error, right? Like, okay, this seems close enough to zero, right? And so that's the other thing that makes this problem particularly difficult. And so. Difficult. And so we want to bring in some statistical learning in order to address some of those challenges. Okay, so this is going to be like, we're going to stare at these mess of points for a moment while I set it up. At least the color for the mess of points should be a little bit calming. So, you know, there's lots of different classification methods, and we wanted a classification method, one that worked. method, one, that worked, and two, that we had some sort of, you know, we could develop some sort of geometric intuition about. And so what we ended up, you know, after some trial and error, what we ended up going with is support vector machines. So support vector machines, you know, on a very high level, you're just creating a bunch of separating hyperplanes and you have some soft margins. So you're allowed to go a little bit on either side with some cost. And so we're actually going to And so we're actually going to do it plugging in the values of the polynomials or values of the invariants. But I first want to show: oh, if I just work in probability space or Fourier space, I'm going to have problems in this separation. And so each of these points, so the yellow points here are correspond to trees. The magenta are going to be three cycles. The blue or the cyan points are four cycles. And the orange are going to be the double three cycles. Double three cycles. And so, how do I get each of these points? Well, for each of these points, I picked one of the network topologies. So, if I'm looking at cyan, I'm taking one of my 12 four-cycle networks, and then I'm picking some random branch links between 0.1 and 0.2. And those are sort of biologically motivated numbers. And then I'm running sort of the model with those parameters. And so then I'm going to get. And so then I'm going to get, you know, in this particular case, I have four leaves. So I'm going to get four aligned DNA sequences, and maybe they have length. I think probably in this, they probably have length a thousand. And then I'm going to come up with that empirical distribution. And then this particular plot here is a projection on two of those coordinates. Now, but I'm misspeaking in a particular way. Speaking in a particular way. So that would be the probability coordinates. And so in probability coordinates, I would have 256 of these coordinates because I have a 4x4 by 4x4 table. And so what we're going to do is we do have some linear relationships. So just like in the three leaf case, like the, you know, the probability of observing AAAA is the same as observing the probability of CCCC. So I'm going to. Of CCCC. So I'm going to mod out by those particular identifications. And then I'm going to move to a Fourier space. So one thing that I sort of brushed under the rug is even though, you know, when I was first talking, we have these complicated polynomials, in the Jukes-Canner setting, I can actually do what's called a Fourier-Hadamar transform, and I get into a monomial parameterization. And so in those particular coordinates are sort of nice for us to work with. Nice for us to work with. And those are going to be the Fourier coordinates. And so after I do this transform from the probability space to the Fourier space, and then I mod out my linear invariance, I'm left with like 15 coordinates. Okay. And so this is a projection on two of the coordinate, on two of those 15 coordinates. And this is another one. And then if we, you know, do all these pair wise, they all look the same. They're just like these blobs. Okay. And so I can't get. So, I can't, you know, this is giving me the sense that I'm probably not going to be able to get separation either in the probability space or in the Fourier space. One interesting thing, though, is that you can see like the, you know, you can see some of the geometry coming here because the yellow, for example, are the trees. And so I'm thinking about it, you know, I have three varieties and they're coming in together and then they're coming here. And this is sort of the intersection of those three. Of the intersection of those three. Okay, but this is just mainly to motivate: is like, okay, I can't do classification here. So, what we end up doing then is we end up, you know, looking at each of these, you know, each of the models. I compute each of the ideals for each of these four leaf coordinates, and I get a whole bunch of polynomials. And so, these are going to be my invariants. And I'm going to look at And I'm going to look at one particular invariant here. Oops, I'm so sorry. I have to be more careful with my mouse. Okay, so here I'm going to actually look at two invariants that distinguish. Yeah, two invariants that distinguish between the tree and the three cycle. And so I'm going to pick. cycle and so i'm going to pick so the tree sits within you know the tree variety sits within the three vi three cycle variety and so that means the ideal corresponding to this three cycle sits inside the ideal corresponding to the tree so i'm going to pick some invariant some polynomial that vanishes for the tree but doesn't vanish for the three cycle and so what we see here is okay that we can actually see some separation when we do this See some separation when we do this. And so, this is the main motivating idea, except we're going to take this farther and very high dimension. And then I can compare, okay, now I'm going to, you know, train an SVM on an invariant that vanishes on both, right? And I am not able to classify, but if I chew a polynomial that does distinguish, then I am able to. And so this is the main idea, right? This is the main idea, right? I'm going to take some, I'm going to generate a whole bunch of these according, you know, I'm going to decide on some range for my branch link, some range for my mixing parameter. I'm going to get all of these empirical distributions from each of these coordinates, and then I'm going to train. Can I ask you another question? Yeah, yeah. So, support vector machines are nice. I mean, I love them, but normally when But normally one nice alternative to support vector machines because the interpretability of, I mean, there's a more probabilistic interpretation of things is logistic regression, for example. I mean, like multivariate, multi-multi-class logistic regression. Did you give it a try? I mean, did you give it a try? We did give it a try. So how this project started is so like, so Travis is much more on the statistical side. And so the first thing that he did was just sort of go through. That he did was just sort of go through each thing. And so, logistic regression was one of these, and then see how it performed. And so, that's how we sort of narrowed down to the support vector machines because it was giving us like the initial good results. Okay. Yeah. Okay. So, yeah, and we played, we played around with other things like K Nearest Neighbors was also giving us good results, but then there were problems with that. And I think it was more pulling. Pulling, it was more pulling out like our how we were sampling than anything else. Thank you. And so, you know, the main idea is we're going to have this training set. And then once we have this training set, what we can do is we can take in an aligned DNA sequence on for taxa. And then we're going to compute the estimated Fourier coordinates. That was the process that I talked about before for each of these end-to-end. N tuples. And then what I end up getting is I get 1126 polynomials that I want to check. And how do we get these? Well, I find all of the distinguishing polynomials. So I see like all of the polynomials that are one ideal, but not the other. And I get like maybe four in the range of 400 for that. And then I have to do like this symmetrization process because in early Because in earlier work that Joe Rosinko and Brian Hip did, is that they found: okay, this process for algebraic methods for trees is really sensitive to the generators that you can choose. And so you want to choose symmetric generators. And so that's going to give you less bias. And so that's how we end up with all of these invariants. And then we use a support vector machine. And then I'll just end super quick with just showing you some of the results and some of the challenges. Okay, so this is a confusion matrix on this process. I have numbers for each of the trees, but I mean, for each of the coordinates, but one, two, three will be trees. Well, I always do that with my mouse. Okay. Four to nine are the three cycles, 10 to 21 are the four cycles. 22 to 24 are the double three cycles. And so this is just telling you, okay, where, you know, how did we generate? Where, you know, how did we generate this empirical distribution from which model? And then where was it classified? And so we have a lot of red around the diagonal, which is great, but I think this particular one was like 85% or 84%. But I really want to pay attention to where we're getting it wrong, because we're getting things wrong, but we're getting things wrong sort of in a good way. So here we have some misclassification going on. And so these are going to be the two Be the two. What happens? Maybe I need to go back real quick to this picture. Okay, so each of these trees, if I follow down these lines, so each of the four cycles, if I follow down the lines, they're going to meet at two different trees. So I have two lines going down and they'll stop at two trees. And so I'm going to have, I can group these. Group these 12 four cycles into the trees that they display. And so, where I'm getting it wrong are being misclassified into another four cycle that displays the same two trees. And so that's sort of nice. So that also gives us a little bit of confidence in this method. Same with three cycles. We get misclassified towards the other. Classified towards the other three cycles that contains the same tree. One thing that we want to check is: does it work well in actually distinguishing trees? Because there's already great methods for distinguishing trees, so we need to work as well as there. And so this is like a Hulsenbeck diagram. And so it's basically taking tree space and you're putting two param is your as a two parameterization of tree space. Parameterization of tree space. And so the dark blue means that we're doing really good. And then this is where it's going to be hard no matter what method you're using, because this is where the internal branch length is going to zero. And so you're just going to have difficulty. And then you also have like this difficulty up here when everything gets super large because there's been, you know, the branch lengths are super long and everything starts just going to the uniform distribution. And then we played around with. And then we played around with like different ways that we could break this. We do have challenges on the range of the gammas or the mixing parameter that we can use. If we allow that mixing parameter to get close to zero, we have problems, obviously, because if it's, you know, if one of the parameters is zero, then it collapses to a tree. So we've been playing around with how large of a range we want, a large enough range that's useful. But, you know. But also not too large that we get misleading results. Also, the range that we choose our branch links is important. We need to bound a little bit away from zero to get good results. We played around, okay, what's the correct number of sites that we need? This is like only 100 sites, which would be very, very, very low. So it starts to go awry there. We find that you just need. Awry there. We find that you just need about, you know, for your training set, about a thousand, two thousand, three thousand sites. We can go a lot larger. You know, we can go to a million sites and basically people who are running this usually have a lot of sites. And then lowering the training data size. And so even with like, I think this is like 100 for each of the networks, 76 isn't so bad. We're more like. We're more like doing 2,000, 3,000 for each of the networks. And then this particular picture, this is my last picture, and then I'll get to the final slide, can show you like really the challenges that we're facing here. So if I think about that mixing parameter, this is for one particular three cycle, and I fix the branch length. So depending on what my branch lengths are, and I plot the value of the invariant, I can see that, okay, I can go pretty. I can see that okay. I can go pretty far from zero, or I can stay very, very, very close to zero. And this is going to be, you know, just the fundamental challenge because you're working with this stratified space. We don't, you know, this is a very interesting picture to us that we don't understand. So, you know, that's how we can use statistical learning to, you know, infer these coordinates. There's a lot more to be done in this regard, but like. To be done in this regard, but like a lot of the theory needs to be there, and so you know, there's this question about identifiability if we want to move beyond level one networks, level K networks, and there's some people doing some really great work in this regard, going to like level two networks and seeing what happens. And then I need these polynomials. And so, this is the question about the invariants. And so, Cummings, Hollering, and Mannon have, I think this is the only paper I know that really gets into these phylogenetic invariants. gets into these phylogenetic invariants. But there, you know, we would love to be able to have, you know, we've been computing them using a computer algebra system. If I want to go to larger networks, I'm going to need some other method because the computer algebra method will eventually just be too hard for us. And then, you know, the idea is, okay, if I can infer coordinates, can I then build those inferences into a larger network? Into a larger network. And so I'll end there. There's a couple of references here. And if you're interested in outbreak statistics and sort of this idea looking at varieties to inform parameter estimation and model selection, I have two references. These are two good references on the matter. All right. Well, thank you very much. Well thank you very much Jeremy and thanking Elizabeth for such an interesting presentation covering a lot of very very fascinating topics. So does anyone from the audience have a question that they would like to start with? Jesus doesn't have any more questions. No, I ask all my questions. Very nice stuff. I ask all my questions. Very nice stuff. That's fine. People can email me if they have any questions too, of course, right? Thank you, Elizabeth. I'm sorry. I had to step out for a moment. There is a Gans installation going on in my place. Oh, no. Okay. Yeah, but it should be fine. But I'll catch up with you later. Thank you. You're welcome. Okay, so maybe one last chance for someone else to ask a question. No? If not, well, thank you very much, Elizabeth. It's been wonderful to listen to all of these. I'm not an expert on this, so I don't really have any. Yeah, it's a little bit, yeah, but I think that this has been what's so been so great about the workshop is bringing in people from different Workshop is bringing in people from different, yeah, different viewpoints. So, all right. Well, you guys have a wonderful day. Thank you very much. Bye. All right. Bye. See you. Thank you. You're welcome.