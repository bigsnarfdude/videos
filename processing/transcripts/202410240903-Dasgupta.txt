About interpretability and clustering. Okay, so let me just start with a quick overview. So, of course, interpretability, explainability are big issues in machine learning right now, but most of the work has been on supervised learning. But unsupervised learning is also an arena in which interpretable models would be very helpful. For example, you know, clustering is the first line of defense for understanding a lot of very large data sets, a large set of customers. A large set of customers and you want to understand them, cluster them, and hopefully you'll get a few coherent groups. Now, the utility of these models would be increased if they were interpretable. And so that's what I'll be talking about. So for clustering, probably the most popular approach is gaming. Okay, so I stop with that. I think everybody probably knows what it is, but just to get the same terminology, let me just quickly go over. Let me just quickly go over the problem. So, you have n data points, say, in three-dimensional space, and you're also given k, which is the desired number of clusters. And the output is going to be k cluster centers that I'm going to call mu1 through mu k. And these implicitly specify the cluster. Every point is associated with its closest representative or cluster center. And the goal is to minimize the total squared distance from each point to its nearest representative. That's the gay-means call. That's the k-means cost function. We want to find k-centers that minimize this distortion. This is empty-hard, like just about everything in unsupervised learning. But there are many reasonable approaches. There's Lloyd's algorithm, there's QBS ‚Åá, there are all sorts of approximation algorithms, lots of good approaches. And so this is a very popular method. Okay. So how does interpretability come into this? Interpretability comes into this? So Rk means clustered, interpretable. So let's say we have this 2D data set, and so the data points are in black, and the cluster centers are these red things. So in this case, we ask for K equals 6 clusters. The clusters are these Voronoi cells, the part of the space for which that center is the nearest neighbor. And so in general, the clusters will be these arbitrary convex regions. Convex regions. Is this understandable? Yes, it is, because it's in 2D. In 2D, everything's understandable. But what about in higher dimension? Are these clusters understandable? And I'll argue that it's actually a little bit hard to understand them for two reasons. First of all, each cluster is not just specified by a single center, but by its interaction with all the other centers. So it's not enough to know just one center. You have to know. You have to know the rest of that. And the second thing is, when there are many features, computing distance involving all these features is also a little hard to understand because different features might have different impact, they could have different ranges, etc. So for these reasons, these are not really very understandable, and it would be nice if we had simpler kinds of clusters. So I'm going to talk about, I'm sort of going to give a survey of a whole line of work. A survey of a whole line of work. But the first paper in this line of work was joint work with these three people: Merve Frost, Michal Moshkovitz, and Cyrus Rashchiran. And this appeared in 2020. And it had a certain proposal for interpretable k-means clusters. So, this is the idea. You have this data set, you do your normal k-means, and now you try. And now you try to approximate those clusters with the decision tree. Okay, so over here you get the usual convex regions. And now you approximate it with this decision tree where you start with this split. Okay, that gives you cluster number two and so on. And so you can see that actually in this case, you can get a very good approximation to the original clustering. And the clusters are super simple. Okay, there's one cluster that's specified just with one feature, and the rest of them. One feature, and the rest of them are specified with just three features. Okay, they're these sparse hyperrect angles. So, this is the proposal to actually replace k-means clusters by these sparse hyperrectangles, to replace the clusters with essentially a decision curve. And now, in this case, you know, you can do it really well, but the question is, what happens in general? Okay, so in general, we have this data set, and this would be, if we were solving the k-means problem, we would Solving the k-means problem, we would get a certain cost. But now we're restricting the clustering that we get. We said it can't just be any old convex region, it has to correspond to a decision tree. So we have to get these nice sparse hyperrectangles. And because we're tying our hands behind our back, the resulting cost could be a whole lot higher. And the question is, how much higher? And so we're just going to call that the price of interpretability. Okay, how much more do you pay? How much more do you pay in order to get these nice box-shaped clusters? Yeah. You said they need to be sparse. Definition for what exactly is required? Well, the thing is, because it's specified by a decision tree with just k leaves, each cluster itself is a conjunction of at most k features, probably more like mod k. So I mean, another possible definition that we crossed would be. That you cost would be that you were given a clustering, now you compare yourself to that clustering that you were given rather than the optimal cost. That's what we'll be doing. That's what we'll be doing, exactly. Presumably, this might also be lower in the well-separated case, right? That's right, yeah. So the price of interpretability could be a lot lower if the data actually. Or if the data actually has nice cluster structure to start with. But we were just interested in general: like, what does it look like? Another question. If the data is sparse, then we know that if we do a random projection to load the nature, we will still retain singular clustering. So that would be another, it overcomes the issue if I use cluster centers in the implementation. I say one issue is that there are many dimensions. But if I use a projection, if there are sparse, there's If they're sparse, if they're separated, I can use a projection and have many fewer direction coordinates. So then the centers could be a good projection. Yeah, so the question is that, look, let's say you have clusters that are relatively well separated, and they're k clusters, then we know that a random projection to all of log k dimensions would actually maintain a certain amount of separation. The problem is that it's right. Yeah, but the issue over there is that the resulting dimensions are themselves linear combinations of the original features. And so each new dimension is not necessarily something that is interpretable in its own way. Okay, so this is the definition of the cost of interpretability. And this is for the k-means cost function because that's probably the most important cost function in clustering. But we can equally ask. But we can equally ask it for the k-medians cost function, okay? Where it's exactly the same thing, but instead of squared Euclidean distance, you use L1 distance. And this is popular in theoretical computer science because of nice properties of the L1 distance. Again, it's called K-medians because in this case, the optimal placement for the center of the cluster is the mean. Okay, whereas over here, if you're using the L1 distance, the optimal place for the center of the cluster is the median of those points. Of the cluster is the median of those points. Okay, so it's called three minutes. So there's been a long line of work since this paper in 2020, and some of it has been in ML conferences, and some of it has been in theory conferences. So what I'll be doing is to just give a quick survey of all of this work. And I think that this line of work has actually worked out quite well. And let me explain what is actually. And let me explain what has actually emerged from it before getting into the details. So, what has come out of it? The first thing is, I think that this is a pretty good. Oh, yeah, go ahead. Maybe as I've seen before, they tend to define Q media as V or two distance up to the C squared, but not a one distance at least as a spider. Right, yeah. So the deal is that if one were to choose one k-median definition, K-median definition, it would probably be this one. But once you have this, you can replace this often with any metric. And it might be L2, so sometimes that's of interest. But I think this would probably be the sort of the simplest or the most vanilla k-medians version of it. But there are versions where you put in any metric, and then the distinction is whether it's just squared or not. Okay, so the first thing is a model of interpretable clustering. The second thing is. Clustering. The second thing is it turns out that the price of interpretability is now well characterized. For k medians, it is log k, and for k means it's k. And there are lower bounds that almost match that. So it's log k in this case and k in this case. So not too bad. It doesn't depend on the number of points. And I'll show some experiments later where, you know, as is typical with approximation algorithms, in practice, the factors are much lower, actually, very close to one. So one thing is missing here. So one thing is missing here is what is the size of a decision? K here is the number of clusters. And also the number of leads of the tree. One row cluster. So there aren't multiple reasons individual data points might belong to a given cluster. They all follow the same path in the tree. They all follow the same path, yeah. But I'll talk about cases later actually where you can grow it further. Okay. Okay, another good thing that's come out is some really simple algorithms for generating these interpretable clusters. So I'll talk about the initial algorithm that we proposed. And then there's this really intriguing new algorithm that's been the basis of a lot of the more recent work, which is a random cut tree. And it's a data structure that has some very remarkable properties. It can probably be used for other stuff as well. So I wanted to check that. Yeah. Yeah. Is there some some way to see why the mean is from? Oh, is there some sort of intuitive reason for that? Yeah, but what's the reason for it? Right, yeah. No, that's a great question. Um yeah, so I mean yeah, we have these lower bounds. Um yeah, I I don't Yeah, I don't have a nice intuition to. That's a really good question, yeah. The sort of like the types of clusters, like types of types of partitioning that you can get from the K-medians, alpha clusters. I mean, one way to think about it is that these, you know, the clusters that we get at the end are axis-aligned hyperrectangles, and in a sense, that's more that's better synchronized with L1 distance than with. With L1 distance than with L2 squared. But that's, you know, that's just a very hand-wavy thing. But, yeah. Is it clear why there is no dependence on D? This is a nuclear space, right? Is there an intuition as to why nothing depends on D here? On the dimension? Yeah. So what we'll yeah, so we'll we'll actually go through the argument, okay? And so ultimately we just have k points that we're trying to split. Have k points that we're trying to split. And when you have k points, you know, intuitively the dimension is k, log k, something like that. And so ultimately, we're just trying to separate those centers. So there's certainly some reason to hope that the dimension would not actually come into it at all. One thing about the question we get is, I guess with like L1 is sort of starting to altering the feature selection. I'm wondering if that sort of naturally runs the I wonder if that sort of naturally lends the side to the tree that is sort of tied to the tree. I think that's right, yeah. And I think, yeah. So the question was that, you know, in terms of this discrepancy between the price of interpretability, log k over here and k here, you know, so the suggestion was that, you know, L1 is in a sense already sort of axis aligned and therefore, you know, more amenable to C2. You know, more amenable to the simple strength, yeah. Yeah, and so it it yeah, so so I think that's actually why I could be other simple notes in the tree, like that will be more aligned with here we're just decision steps, but we want to think of different decision nodes, of different simple functions. Yeah, so Shai's comment is that we're just looking at splits based on single features, like I Based on single features, like are you to the left or the right? But if you wanted to come up with a decision tree that was really geared towards L2 distance, maybe you could have nodes that are, say, based on distance from a certain point or something like that. Other types of splits. And that's actually, yeah, that would be an interesting thing where maybe the split is just based on two points and you see which one you're closer to. And that might work. Okay. So. So, let me start by briefly just describing the first algorithm, which is iterative mistake minimization. So, what you do is you've got your data set, and you run your normal k-means, whichever one you like, okay? And you get those normal convex clusters, and each cluster has an associated center, and we're going to hold on to those centers. We're going to stick with those centers, we're not going to change them, okay? In order to get a decision tree, what we do is that we first What we do is that we first look at all possible splits. There are not many of them, like D times N or whatever. And we pick the one that makes the fewest mistakes. And a mistake is that this guy should have been with center number four and now is aligned with center number one. Okay, so that split made one mistake, that was the best possible, so that's the one we go with. So it seems like there are two different ways of asking what is the error of switching from list k means. K means clustering to the decision tree. You can say how many points maybe are wrong, but you can also just ask in terms of the space. Or the cost. Yeah, the cost. The cost cost. So the measure that we care about is the cost, is the multiplicative increase in the cost. But the algorithm itself is just looking at the number of points that get messed up. Okay, and then we'll see how they relate. The problem is that quick learning with decisions. Weekly earnings with decision steps. Use decision steps as weekly earners. That's what you're doing. Minimum 01 loss. Right, right, exactly. Yeah. So we look at the class as defined by these centers. That's right, yeah. So now we don't need to split on the right anymore because there's only one center here. Okay, so we're just holding onto the original centers, and now we split. Original centers, and now we split this one, we make two mistakes over here. This point should have gone with that cluster, this point should have gone with that cluster, and so on. And at the end of the day, we have this decision tree, and these are the mistakes we made. Okay, so it's a nice simple algorithm. So here's just a summary of the procedure. And so this is what you can show about it. So let these be the centers you found. These be the centers you found. Maybe they were the optimal centers, maybe they weren't. In general, you cannot find the optimal centers. Okay, so this is whatever you have. This was the normal k-means cost. This is the tree you have. The additional multiplicative cost is 2h plus 1 in the case of k-medians, where h is the height of the tree. And it's 2hk plus 2 in the case of k-medians. So the height of the tree is going to be at most k. So this is a factor of k. k. So this is a factor of k. But you can also see that, you know, intuitively we'd hope it would be something more like log k. Okay, so in this original paper, this is what we showed, and a lot of the more recent work has been able to actually get this down to log k rather than just the height. And it's been surprisingly tricky to do that. So let me show you the argument for this. How much longer do I have? About 10 minutes? Let me show you the argument because it's actually Let me show you the argument because it's actually very elementary. So, what happens is that we're building this tree, and at any given point, we have some cell of the tree that's got a bunch of data points, and it's got a bunch of centers. Let's say it's got four centers. If it's got four centers, we need to split it somewhere. And we're going to split it. We're going to split this box. This is the bounding box, not of the data, but of the centers themselves. We're going to split the boundary box something like this. Now, we're going to choose the split that makes the smallest number of mistakes. At this cell U, let that number of mistakes be T sub U. And what's going to matter is the L1 diameter of this bounding box. In this case, this was this, the sum of the side lines. So one thing that's very easy to show is that the amount by which the cost increases, so this would be original k-means cost, is the cost of the tree, is simply this factor. Cost of the tree is simply this factor. It's the sum over all internal nodes of the number of mistakes at that node times the L1 diameter of that founding box. And so let me give you the argument. It's literally a one-liner. So the argument is as follows. So at the end of the day, you've got this tree, and each leaf of the tree has one of the original centers. If a data point is with its correct center, you're all saying. Is with its correct center, you're all set. You paid no extra for it. If it's not with its correct center, you look at where you split off from that correct center. That happened at some cell U. So there was some cell in which this was the data point, this was the center, and it got split off. Now, the additional cost for that center, so this point should have been there. Instead, it's with one of these guys. The additional cost for it is at most this diameter. And how many mistakes did you make? And how many mistakes did you make? This many in that cell. And so, this is the overall additional cost associated with that, with the treatment is the bore number of mistakes they have to make. Okay, so that's the upper bound on the cost. Okay, so that's what we got over here. And now, what we'll see is that actually, if you look at any node of the tree, if this quantum. Node of the tree, if this quantity is large, this Tu times the diameter, then it means that actually the K-medians cost for that node must also be large. And we'll show that the K-median's cost restricted to the points in that node is at least this much. And that when you rearrange it from each level of the tree, you see that the K-medians cost is at least the sum of these guys over that level of the tree. And then when you sum over levels of the tree, And then, when you sum all the levels of the tree, you get the height of the tree. Okay, so that's the whole. And so now, this is the last thing we need to see. Why is this the case? Why is it the case that if you have a cell in which you're inevitably making lots of mistakes and the diameter of the bounding box is large, the inherent k-medians cost of that cell must be high. So, this is the way we see it. Here we have a cell, and for each point, we have its association. And for each point, we have its associated center. And we are interested in the k-medians cost for this, so the sum of these distances. So for this one, we'll look at this distance. Now, the L1 distance is decomposable into coordinates. So let's look at the distance along one coordinate and then along another. Let's just focus on this coordinate. So now we're just interested in this distance. We have to show it's significant. We'll divide it into chunks. We'll say it's equal to this plus this plus this. It's equal to this plus this plus this. So, what are these segments? So, we have the centers. Let's look at the centers along this coordinate. And let's look at the midpoints between consecutive centers. Now, we know that no matter where we cut, we make at least T sub U mistakes because we chose the minimum number of mistakes. So, if we cut here, for example, we make T sub U mistakes, and for each mistake, there's some point on this side and the center on the other side. And so, the distance. The other side. And so the distance from that point to its center must be at least this much. And so just by adding together these segments, we get the result we need. So it's a very elementary argument, and this is how you get the 2h plus 1 for k medians. And for k means, it's exactly the same thing, except that you use the factor to triangle inequality and Cauchy Schmidt's. So nothing just. Okay, so that's the first algorithm. So, the second algorithm is this thing that I really like. And this was introduced by Makari, Chev, and Shen. And this is the random country. So their way of building this interpretable clustering is saying, let's just look at the case centers we get. And let's say that all the data lies in some bounding box. So we start by putting a bounding box around all the data. Let's say it's minus a to a raised to a. Minus A to A, raised in D dimensions. And let's look at where the centers lie in there. And let's forget about the data points. Okay, throw them away. We don't want them anymore. We just have the centers. What we're going to do is just pick a random coordinate and a random split value along that coordinate. And if it happens to split centers at any, at our current leaf cells, go ahead and use it. And keep doing it until you end up with just K leaves with one center. Okay, so it's literally random cuts. Pick a random coordinate, pick a random split point. By a random split, you mean uniformly at random points? Uniformly at random. So uniformly at random. Okay, and keep going until you get k leaves with one center. You pick a random, a random coordinate random, and then how do you decide whether this is good random? So, um, yeah, so you pick it. So initially you have all case sentence. Initially, you have all key sentence. You pick any split that actually separates two of the sentences. Only two is enough. Yeah, as long as it separates some sentence, that's good. It doesn't matter how many. Because eventually that way you'll get a tree with k leaves, one with each rest. Each with a center. It seems like there's a big sacrifice in interpretability here, right? Because if you ask a person. A split that is very close to one of these reference points would not be. These reference points would not be considered as explanatory as they are keeping the decision to structure, but maybe they're losing and making the argument meaning. Yeah, so that's a good point that apart from having this nice tree structure, which gives you these sparse hyperrectangles, the clusters would be even more interpretable if the splits were in regions of low density, for example. Low density, for example. And that's not something that we've considered. Yeah, so this and so this algorithm, the amazing thing about it is actually the guarantees associated with it, which we'll see in a second. But in practice, it's not as good as the other. It's a good question. What is an interpretable split? Yeah, yeah. Yeah. So it's not clear that there will always be these low density things, but maybe in a relative sense, that would be a cool thing to look at. That would be a cool thing to look at. Okay. So this gives you the log k in terms of k medians. This does not solve the problem for k medians, by the way, is for k medians. And it gives you the log k. But I'm not going to go into the argument for this because I don't have time. But I just want to give you the guarantee, which is actually really remarkable. So what it says is the following. You just have these centers, and the data don't matter. So you get these k centers. So you get these k centers, later bring in a data point. Okay? But it didn't need to have existed before. Later, bring in a data point and look at its closest center. When you put it down this tree, it gets associated with a center that's at most log k times further away in expectation. So it's a pretty strong guarantee. And what it means is that this method, so this is a guarantee for individual data points, not for the overall sum. Not for the overall sum, okay? And this means that this can be used for all sorts of other purposes, including nearest neighbor search and so on. Which of the two operators is the most important? I think it's costia. Okay. How large is the concept on the data? It's big. Concept is big. But it's uh, yeah. Yeah, yeah. So I won't go through the argument, but it's um. The argument for this. I'll show some experimental results just for a second since I'm almost done. I just wanted, the whole point of the experimental results is just to say that the factors in practice are much less than k. In fact, they're close to one. But that's a common thing. Is there a sense in which you can use that expected result to backtrack similar results to, say, the original approach you proposed or something more? The original approach you proposed, or something more explanatory using some sort of like probabilist, or there must exist a cut that's most interpretable. Yeah, so the thing is, right, so that would be great. But it's turned out that random cut method, it's been hard to even do it for k-means. So even for that, so even so it would be nice to be able to add further constraints to it, right? But even doing it for k-means, But even doing it for k-means has been hard, and so I didn't present that. I mean, there are versions for it for k-means, but they're basically a nest. So that's a consideration. How efficient is the algorithm that takes this thing? That was something you worked for. Things like the reason issue here. Yeah, the algorithm is in is oh, you mean how many tries before you make it? No, just what's the complexity of the algorithm? Like if you What's the complexity of the algorithm? I give you a k of points in L D, and you'll give the decision tree. Rotation time would require that. Like if you have adversarial points, it could be very bad because they can be very close together and then you'll never sample a cut. Ah yeah yeah, but no. Right, so the way I presented the algorithm, you are sampling from the entire space, but in reality you would not do that. But in reality, you would not do that. You would just restrict attention to the bounding bonds of the side. That makes sense. And so it will be a good condition. Yeah, that was just sort of the mathematical convenience. You said before it was very hard to do for the chains. Hard in what sense? You actually had to sample from a much more complex distribution. And, you know, random isn't enough. There are other constraints. But I think basically it just hasn't been figured out properly. Basically, it just hasn't been figured out properly yet. So, I think that's an open problem. So, okay, practice, this is great. I should probably wrap up. So, I just want to say that there is a very nice open problem here. There are actually two. The first is to do the k-means properly. And the second is to do this Lagausian nature models. The same thing. We want these boxes, but now our clusters look like these footballs. How do you do it? 