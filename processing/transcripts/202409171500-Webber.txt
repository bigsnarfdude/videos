Hello. So I'm going to talk kind of loud because it's raining furiously. So my name is Rob. I'm a newly minted assistant professor of math at UCSD. And today I'll tell you about robust randomized preconditioning. Randomized preconditioning for kernel ridge regression. So let's get into it. So I'm going to start with the question, okay? What is the best numerical method for solving a least squares problem where we're looking for a vector beta? And where are we going to find it? I don't know. I don't know. That's the question. Oh, I found it back a slide. We're going to find a vector beta that minimizes the square norm of a beta minus y, where a is going to be an n by k matrix and y is an n vector. And there's two cases, either n equals k, in which case a is square, or n is greater than k, in which case a is a tall matrix. So consider two very classical numerical approaches. The first I would call The first I would call dense factorization. We will calculate a factorization of A in order n k squared operations, which quickly determines beta. So for example, a QR factorization will do it. The computed solution beta is exact up to floating point errors that might occur on your computer. The second option to consider is conjugate gradient, often abbreviated CG. Assuming that A is well conditioned, we iteratively compute beta. We iteratively compute beta in a number of operations that's proportional to the number of non-zeros in A. And it converges exponentially fast. So the computed solution beta is as accurate as you or I need. If you want four digits, you get four. If you want eight, you get eight. From this comparison, we can see that when applicable, CG is much faster than dense factorization. So in this talk, seeing as CG is fast, let's push the limits of applicability for CG. Let's push the limits of applicability for CG together using examples that are drawn from the field of kernel machine learning. This is the plan for the talk. And I'm going to provide some background. I actually don't know how many people are really well versed in CG or PCG. So I'm going to provide some notation at the very least that we can kind of settle on. We can conceptually reduce any least squares problem, a beta minus y, minimize the norm squared, to a Y minimize the norm squared to a positive semi-definite system in one of two ways. Either it's going to be really easy because n equals k, it's a square system, a is positive semi-definite, that means it's symmetric with non-negative eigenvalues and y is in the range of a. And then immediately the solution is a beta equals y. Okay? That's one option. The other option has reappeared. Other times the problem is still equivalent. The problem is still equivalent to a PSD linear system through the normal equation. So if you left multiply by A transpose, you see that A transpose A is a PSD matrix, and the solution for beta is A T A, beta is A T Y. For solving any least squares problem reduced to a PSD system, Mx equals B, we can use this amazing conjugate gradient algorithm, which is given to the right. And I don't expect you to like parse it really quick. Like, parse it really quick. I just want to enunciate what are a few key features. The biggest feature is that at each step, CG is going to apply a matrix vector product with this M matrix. And you can see this occurring here. It's just occurring once per step. This is the dominant cost. The error will decrease exponentially. So, in the proper norm, it's going to be less than or equal to two times some exponential factor, where you get the number of iterations times two divided by. times two divided by the square root of kappa m. And here we're seeing the exponential speed is determined by kappa m, the condition number, the ratio of the biggest to the smallest singular values. And as a rule that I've found useful in all of my numerical tests, if it's less than or equal to 10 to the four, 10 to the four is the golden number I found, high accuracy is achieved in one, 10, or 100 iterations. However, if it's much bigger than 10 to the four, see Much bigger than 10 to the 4, CG is slow. So 10 to the 8, no good. 10 to the 10, no good. 10 to the 16, definitely no good. There is a fix. Sometimes we can instead solve a PSD system mx equals B using preconditioned conjugate gradient given to the right. And again, I don't expect you to parse this, but there's two features now per step. There is a matrix vector product with M, and there is also a linear solve with a preconditioner matrix P that. With a preconditioner matrix P that approximates F. So we can see this linear solve is being applied to R sub I at each iteration, and then it's being kept in memory for the next iteration. There's only one linear solve per iteration. Now, PCG is mathematically equivalent to applying CG to a slightly transformed system. There's a left and right preconditioning that goes on, and that's a little bit subtle, but this is equivalent to the system p to the negative of a half m, p to the negative a half y is p to the negative. P to the negative of a half y is p to the negative half p. And this means that for fast convergence, we need p to approximate m in this very specific sense, where the condition number of p to the negative 1 half m, p to the negative 1 half is less than or equal to this golden number 10 to the 4. So this is the basic background. The question that folks like me are obsessed with right now is: how can we find an effective precondition or P for least squares problems in general? In general. Or maybe that's too hard. So, when can we do it in specific? This talk will give a partial answer. It will cover randomized approaches that have emerged over the past decade. Randomized low-rank approximation can provide a preconditioner P that approximates A and is really accurate if A is well conditioned apart from a small subset of large eigenvalues. A different approach we'll see is randomized dimensionality. We'll see, is randomized dimensionality reduction, which provides a preconditioner now to A transpose A. It's approximating A transpose A, and it runs fast if A is a skinny matrix. That's the technical term, a skinny matrix. For example, a million by a thousand. Okay, so you need this aspect ratio for the second method to work. In our recent paper, we tested these randomized approaches in a lot of applications that I'll show you and identified the best empirical and theoretical methods. Best empirical and theoretical methods. So I'll position this work as maybe other people invented these strategies 10 years ago. There has not been any systematic comparison. I will present to you the systematic comparison and what works best. Okay? So let's jump into it through this amazing kernel ridge regression application, where we are given n input-output data points, and the input is X, the output is Y, the input is a D vector, the output is a scalar. We want to approximate. A scalar, we want to approximate the mapping from x to y, which would be f. So we build a prediction function, f hat, as a linear combination of kernel functions that are centered on all of the data points. So these are the linear coefficients. These are the kernel functions centered on the data points. And a kernel function is going to be a bivariate function. Some great examples are the Gaussian or the Laplace kernel. I'm unable to use my laser pointer, but that's okay. I'm unable to use my laser pointer, but that's okay. So, is the setup of kernel regression clear? Cool, cool, cool, cool. So, the question is: how do you get the coefficients, right? You want this model to be trained. You want the best coefficients. And often we optimize with least squares. Here's the least squares connection. We choose the coefficients beta to minimize this loss function. A beta minus y norm squared is going to be the sum of square residuals plus mu beta transpose a beta. plus mu beta transpose A beta. That is a regularization term. Mu greater than A is a regularization parameter. A is what's called the kernel matrix. It is given by evaluating entry-wise the kernel evaluated on points I and J. That'll determine the IJ entry. So to find beta, we can equivalently use this PSD reduction and we can solve this linear system A plus mu identity beta equals Y. This is the linear system. This is the linear system that we're going to examine. And the problem in practice is the condition number can be so bad. So, really, the best predictors would result in conditioning of 10 to the 10 sometimes, or even higher. So, many groups have tried to fix the conditioning, and there's been a lot of smart work, you know, Niestrom approximation, random Fourier features, ridge leverage scores. There wasn't much of an empirical or theoretical comparison, so we decided to do it. And the best performing. It and the best performing method we found would be this approach, this paradigm that I'll call low-rank precondition. So, specifically, to solve this problem, A plus mu identity beta equals Y, and you can apply a preconditioner, P is A hat plus mu identity. And the best A hat we found was a rank R approximation of A from RP Schalesa, which is an algorithm that I've been working on developing over the past years. It's given in pseudocode to the right. Code to the right. And intuitively, you could think of it as a partial Chalesky decomposition where you stop and you choose the pivots really smart. So, where do you stop? You stop when the rank is about 10 square root of n. That's the best effective parameter choice we found. The pivots are chosen from an evolving probability distribution that emphasizes novelty. And I'm not going to explain all of the details because I want to get to the empirical results. But the big thing here going on is this is fast. The big thing here going on is this is fast. It is approximating your matrix really fast in the sense that it only even reads R plus 1n kernel matrix entrance. It completely ignores the others. They never need to be computed. It only expands order R square N arithmetic operations. So there's this quadratic in R cost for processing it. The three questions we answered are all subtly different and they all have really different answers, okay? So how does RP Chaleska compare to other How does R.P. Schaleski compare to other practical, low-rank preconditioners people are using? How does R.P. Schaleski compare to the theoretical, best-rank R preconditioner that you would ever hope for? And how does R. P. Shaleski compare to the ideal preconditioner that scientists want? So these are hard questions. Maybe the easiest one is the empirical comparison to other low-rank preconditioners. So all we did was we showed that it solves more problems than other pro. Problems than other practical low-rank preconditioners. So we used 20 standard problems, mostly downloaded from lib SVM, and ran all these algorithms for 20 problems. We declared the problem solved when the test error saturates, which occurs pretty much when the relative residual is less than 10 to the negative 3. So we made this our precise cutoff criterion. And you can see in pink, randomly pivoted Chalesky is solving more problems than the other methods. Problems than the other methods. It's solving 100% of problems when you have a kind of small regularization. It's actually only solving 55% of the problems when you have an even smaller regularization. This regularization parameter really is the devil. I mean, you want it small for test accuracy, but it makes the problem so hard to precondition. The second best method is ridge leverage score sampling that's in black, but our PCLS gets consistently a little bit better. Rhythm-oriented features do not do well empirically on standard problems, so that might be surprising to some people, but it's actually not competitive. Yeah, for sure. Yeah, so there was not a comparison to quasi-Newton methods. I feel like you're going to be getting the low-rank preconditioner at a much higher computational expense. So the amazing thing is you're getting a square root n or 10 square root of n rank preconditioner at order n squared operations because of the n times r square scaling. So, if you're using a method where successive multiplications with the matrix would be furnishing the search directions, then to build a rank R preconditioner, you would need order Rn square operations. Yeah. Yeah, yeah. So, I believe that it wouldn't be compatible. It wouldn't be competitive. That is my belief. But we should talk about it and we can highlight exactly what's the algorithm being proposed because it's interesting. I mean, for sure. Yeah, for sure. You don't want to be looking forward to the idea. So what's that like your objective? So I would call to me it seems like just uh power colours. Yeah. Like matrix and the nice thing with that. That's this one. Uniform. Yes, the uniform is nice drum. Yeah, third best is the third best, actually. Isn't that crazy? You could just pick them uniformly at random, and it's not quite the best you could do, but it's somewhat close. Yeah, it's sort of surprising to me how well it does. This is quantitative eigenvalue control. So we're trying to precondition a matrix whose eigenvalues are given in the 20 instances by the yellow curves. So we're seeing that this is. So we're seeing that this is a regularization, 10 to the negative 7 times n. And so you can have a condition number as bad as 10 to the 7. That is, the ratio of the biggest to the smallest eigenvalue can be as big as 10 to the 7. This is what empirically looks like for 20 problems that we viewed as very standard or indicative. And then after you've preconditioned, you look at this preconditioned eigenvalues, and you're really grabbing the top ones and accounting for them in the approximation. Accounting for them in the approximation, you're bringing them down. And in most cases, you are causing the condition number to reach this golden level, 10 to the four, or smaller. So you're really doing it for this regularization parameter of 10 to the negative seven. You can think about what would be the best rank R preconditioner. That would be an R truncated eigen decomposition. That would be the theoretical best preconditioner. And it's like pretty close, right? This algorithm is actually very practical. Actually, very practical is empirically doing about the same job as the theoretical best preconditioner. If you believe the blue and pink are about the same, I tend to think looking at this plot, the blue and pink are about the same and yellow looks really bad. So that's how I interpret this plot. I'm running a little short on time. We did a theoretical comparison. The basic idea is the best rank R preconditioner controls the condition number if the R plus first eigen. If the R plus first eigenvalue is on the order of the regularization. So you need to get rid of the big eigenvalues, and the R plus is on the order of regularization. We were able to prove slightly weaker results for R. P. Schlefki. That one's not coming back. Okay. That's too bad. Yeah. You know, I uh You know, I would find that really hard. Yeah. Yeah, for sure, for sure. Yeah. Yeah. So presumably it is better than just you follow me at Trendor. Like at first, like just organize and learn the data and so like the location makes sense. Yeah, so that can be compatible. So, that can be competitive. What I think about when you tell me that is k-means or maybe a clustering algorithm, and grab one representative point from each cluster. Yeah, it was not included in these comparisons because it's not often applied in kernel machine learning nowadays. And there are some scalability challenges. And I haven't seen recent papers doing it, but I think it's a fabulous idea. There's some early work that shows that can be indeed very. Be indeed very competitive. You know, there is a bit of a problem with sort of representativeness and high dimensions. Oh, yeah. Yeah, seeking out novelty is achieved in both methods. Novel data points. What's up? What's that? Yeah. Yeah. But here you're adapting your distribution. Yeah. Well, I'm not saying that you could do better than uniform. And he's suggesting we are, we are, we are. Yeah, I mean, maybe I'm like just too, I don't know, persnickety about details, but but you know, from a big picture, yes, these are all doing the same thing. From a small picture, I'm kind of like obsessed with which one's exactly the best, right? So k-means plus plus is quite similar in its seeding rules to our R.P. Shalewski decision for which landmark. Pichalewski decision for which landmarks to pick, but there are differences. Well, that's kind of you. I worked hard on that one. Oh, okay. We have it back. We have it back. Okay. So I'm probably going to end like 12 minutes late. I know that's terrible, but I'll just. Yeah, that's great. That's great. Yeah, that's great. That's great. So, you know, we basically said we can't quite prove something that strong. In fact, something that strong is probably not quite true. We can certainly prove that this low-rank preconditioner controls the condition number when the sum of the tail eigenvalues, those with indices r plus one or larger, are on the level of the regularization. And so we just introduced the mu tail rank, the smallest number r, such that the sum of the tail eigenvalues is less than or equal to mu. Values is less than or equal to mu. And we proved exponential convergence of CG with RP-SHAWSC preconditioning when R slightly exceeds the tailwind. So here's what these theorems kind of look like. You need a failure probability, you need an error tolerance, you need to construct a random approximation, slightly larger rank than the new tail rank, and you're controlling the condition number at a level that's less than or equal to three over delta. The over delta here is coming from Markov's inequality. From Markov's inequality, in which case you get exponential convergence. Revisiting the three questions. How does R. P. Scholeski compare to other practical low-rank preconditioners? It solves more problems empirically. How does R.P. Schalewski compare to the theoretical best-rankar preconditioner? It controls the eigenvalues nearly as well, with some theory to support this point. How does R.P. Schaleski compare to the ideal preconditioner that scientists want? Well, it solves a hundred. Well, it solves 100% of problems if the regularization is 10 to the negative 7, but only 55% of problems if the regularization is 10 to the negative 10. So our PCLSD is definitely helpful, but I do believe that we fundamentally need to go beyond low-rank preconditioning to give scientists what they want for kernel ridge regression in this N by N PSD setting in general. Now, moving on to the second part of the talk, kernel ridge regression isn't always like this. Kernel ridge regression isn't always like this, right? So sometimes the data size n is too large. We cannot build a 1 million by a million matrix that is dense and do linear solves on my laptop. I can get it up to 100k by 100k. So what do you do? You can approximate f of x equals y by building a cheaper prediction function that is a linear combination of kernels evaluated on a subset of what are called inducing points, here indicated by xs1, xs2. Indicated by XS1, XS2, all the way to XSK. The inducing points indexed by S1 through SK could be chosen uniformly. They could be chosen by Arpi Schaleski. These are indicative points. Amit suggested maybe a k-means approach could be cool. But then you get a different linear system. And you need to choose the ideal coefficients beta hat to minimize least squares problem. This is the sum of square residuals plus a regularization term. Now it's mu. Regularization term. Now it's mu beta hat transpose ASF beta hat, where F is just this index that's picking out the selected rows or columns. This is MATLAB location, I believe. So to find beta hat, you can do the PSD reduction. You can equivalently solve the PSD system. AS colon A colon S plus mu A S S. Beta hat is A S colon Y. Recall that A is the kernel matrix defined entrywise. S picks out the Defined entry-wise, S picks out the selected rows of columns. And the conditioning in this case can be arbitrarily often and is. So actually, 10 to the 16 occurs quite often in applications. Many groups have used the Falcon preconditioner, Vrosasasco et al., based on a Monte Carlo approximation of this gram matrix term. We compared Falcon to a different approach that we called CRIP, which we hadn't actually seen in the literature before. And the best performing method to solve this linear system. Method to solve this linear system turned out to be Krill. And I think the Krill is going to be incredibly natural to many people in the audience. We just hadn't seen it applied to kernel ridge regression. What it uses is it uses this preconditioner, P is AS colon, V transpose P A colon S plus mu A S. So it's approximating this gram matrix by inserting V transpose Phi, which is a sparse random sign embedding. Sparse random sign embedding. And I've given the explicit formula we use right here. It's made up of n random columns. They're sparse. They possess uniform plus or one values in zeta equals log k uniformly chosen positions. And then to make it scale properly, you need to divide by the square root of zeta. And so this is the random, they're independent columns. The sparse random sign embedding is a key tool in randomized numerical linear algebra. In randomized numerical linear algebra, and it works great, it works really great in kernel-ridge regression. So, we asked and answered: how does Krill compare to other restricted KRR preconditioners? And how does Krill compare to the ideal preconditioner that scientists want? Compared to other preconditioners, it solves all 20 problems regardless of the regularization. The regularization, in fact, doesn't seem to matter in this case. You can take it down to 12, 10 to the negative 12. 12, 10 to the negative 12. Okay, it solved all 20 problems, outperforming other methods. Again, each problem is solved when the test set error saturates, which is basically equivalent to this relative residual being 10 to the negative 4. And then what is the quantitative eigenvalue control? What's going on at the level of eigenvalues? Yes. I thought you were always the trinity. I can't train as much. Oh, right. I boiled too much information into the slide. I wanted to kind of quickly justify to you all why I am running the method and declaring it solved when the relative residual achieves a level 10 to the negative four. This is the actual criterion I'm using. The reason 10 to the negative 4 makes sense is because these are actually 20 prediction problems where we separated into a training data and a test data set. Training data and a test data set. And once you get down to 10 to the negative four in the computed accuracy of beta, your test error is totally flat. So there's not much reason going further. You can get it better in floating point precision, but if your test error doesn't improve, then I don't see much of a use case, although there could be one. People use these methods a lot. The quantitative eigenvalue control is so cool. So in the left, we can see that before preconditioning, we are really getting like 10 to the 16 conditions. Really, getting like 10 to the 16 condition numbers, arbitrarily awful. There's nothing that keeps the condition number from blowing up. But look at the different vertical axis scalings, right? So I've changed it from 12 orders of magnitude to two. The Krill eigenvalues exhibit this crazy pattern where they concentrate, right? They concentrate around this curve, this empirical spectral distribution, which is exactly the empirical spectral distribution for G transpose G to the. For g transpose g to the negative one, where g is a 2k by k Gaussian. Okay? So that's a lot if you've not seen this before. But I can ask this question and I can start to answer it for people. Why does Prill control eigenvalues similarly to an inverse Wishart matrix? That's a Gaussian times a Gaussian transpose inverted. The partial answer is if we didn't have any regularization, if mu is zero, then you could do a calculation to say that the real precondition. To say that the real precondition eigenvalues are explicitly the singular values of phiq raised to the negative two power, where phi is the random sign embedding that we generated, and q is an orthonormal basis for the range of a colon s. This is a linear algebra and S V D calculation you can do. It is exact. There is no approximation. And then there's this amazing phenomenon of Gaussian universality, right? Where we see that the singular values, the I-thing. That the singular values, the i singular value of pq is really really close in distribution to the i singular value of a Gaussian matrix that has dimensions d by k where d equals 2k is our embedding dimension. And I'm just calling it d because there's big gaps in the theory. You saw empirically that this works, but really smart people have worked for decades to try to prove it works. And there actually are problems here that maybe are interesting. Are problems here that maybe are interesting to some people in the audience? Universality holds empirically when the embedding dimension is 2k and the number of non-zeros is log k. You saw it, it works. Mathematicians have so far proved it only when d is way, way bigger than 2k, like 1000k log k, or when zeta is way, way bigger than log k, like 1000 log k to the cubed. Something about the sparsity makes it very, very difficult to prove universality results. Difficult to prove universality results. So, open problem if anyone's excited. I would love to cite your paper if you could ever prove better error bounds. I didn't. I cited the available papers and took a sparsity parameter and a betting dimension that were too large, way larger than you need, but it gets you the results you want. You quantitatively control the condition number with high probability and conditional on this event, you converge rapidly. Now, finishing up, slightly late, let's revisit the two questions. Let's revisit the two questions. How does CRIL compare to other restricted KRR preconditioners? It solves 100% of problems. How does CRIL compare to the ideal preconditioner that scientists want? Well, it solves 100% of problems and runs fast if K is order square root of N. The number of inducing points is the square root of the dimension. It's slower. It's much slower if K is a lot bigger than the square root of N. A lot bigger than the square root of n. Because, in general, there's going to be a k-cubed cost. Okay, there's going to be a k-cubed cost associated with the algorithm, and that can get pretty bad. That can get a lot worse than as k gets much bigger than square root of n. Summarizing, I asked, what is the best method for solving a least squares problem? A is n by k and y is the n vector. Many scientists do rely on dense factorizations. They cost order n k square operations. They cost an order n k square operations. Conjugate gradient can be five orders of magnitude faster since it only needs matrix vector products with A, but you do need an effective preconditioner. Two effective preconditioners for CG are RP-Schleski and CRIP. And then I position this paper as basically reviewing all of the best theoretical work that came before and finding what works in practice, right? And maybe providing some auxiliary theory. What I'm actually Auxiliary theory. What I'm actually interested in in the future, and what we can talk about, I do have some ideas or concepts of ideas, is how can we systematically expand the class of least squares problems that can be effectively preconditioned or solved? I had some bonus slides if I get technical questions, but thank you for your attention. Yes. So in fact, the choice of new look at the data is correct, but there are two choices that scale it because it makes right. So get number one. And for example, if you mentioned five and it's done, they make sort of the reason and say, well, you know, usually chosen this way, you know, for my event. Usually it doesn't sway, therefore, my ranking doesn't sway. I'm not going to try to compute any rank higher than that and it's sort of path and your experiment that is necessary. And you think it's because, you know, here is a theory that practice is different. Because, you know, that will raise questions about relationships between the size of the matrix, the rank that you want to reach, and the condition number that you care about. There's sort of a little bit free parameter, et cetera. Yeah, you know, would I like talks about those? Yeah, well, I mean, it's really handy that Falcon works regardless of the Krill works regardless of the regularization, right? We saw that Krill theoretically and empirically is going to work for any regularization. So that's great. It leaves scientists free to tune the regularization and get the best test set error. However, R.P. Schalewski's big failing is it's really dependent. Really dependent. It's really dependent. No, that was not for a fixed test error. Minus four was nice to have that residual. This is a little bit different, unfortunately. This is the relative residual. So this is, and it's actually a little bit easier if we look at it. It's exactly analogous here. It's exactly analogous here. You're looking at the right-hand side, right? So you're trying to solve a plus mu identity beta equals y. And you're gauging the convergence of the residual, that's the right-hand side minus the left-hand side, rescaled by the norm of the right-hand side. So this is simply an a posteriori calculation. No, this is currently the training data. The training data. No, no, no, no. The test was just an offhand remark I made. That was just an offhand remark with no slides to back it up. Yeah. Maybe there is a test error interesting there, or maybe not. Yeah, yeah, we probably could have added another slide of test error. That's definitely true. Empirically, the test error goes down as you decrease mu. It stabilizes at a certain point. I've had it hard to figure out exactly what is the mu at which it stabilizes. I found the theory to be falling short. Falling short in the sense that it doesn't provide very quantitative predictions. So I always use cross-validation. Yeah. Yeah. Yeah. Yeah. I think that's like a really easy test that I should probably run. Just, you know, find some controlled problem, increase and see how the best mu where the test error saturates should increase. Rate should increase. Yeah, I'm pretty over. Yeah. Yeah, I think we just find. Yeah. Oh, geez. Yeah. Cool, thanks. It's not very specific relative hardware.