I'm working too much in this conference so far. Also, thanks for the invite. So, model discovery just a brief overview. The paradigm kind of wants to seek an underlying model in a human interpretable way, which is kind of opposite to the current black box models and neural networks. So, in essence, if we have some kind of physical system, are we able to extract something that we can, like a model of it, that we can actually use for later? Can actually use for later analysis or even just verification of current theories, such like that. One pretty cool application of this, specifically with Sydney, was I think they applied to dark matter or black holes, one of the two, and they were able to confirm and deny two existing theories that discuss the dynamics of them. So that's one potential usage that this can be seen in. We're specifically interested in PDE. We're specifically interested in PDEs. We've seen them quite a lot this week, so I won't go too much into why they're important. We're considering specifically this, let's say, format of them. So we're looking at time-dependent spatially temporal V's. We define L as a governing model. I think the notation should be pretty consistent with these slides. And we have U of X and T being the hopefully smooth. Hopefully, smooth realization of the phenomenon. The three tools that we use within the work will be symbolic regression, SIMDI, and the Neural ODE. Symbolic regression hasn't been discussed too much yet. SIMDI has been touched on, and Neural ODE has been extensively applied. Just a bit. Okay, so because of that, I'll quickly go through symbolic regression. The premise. The premise is that any equation can be modeled as an acyclic graph. So, this is basically what happens under a hood of a symbolic regression algorithm: it's a genetic algorithm which uses genetic programming to basically evolve a tree down its leaves and hopefully basically by taking the loss with respect to some kind of data. So, the predicted model versus the data, you want to come up with some kind of correct expression. With some kind of correct expression versus whatever you're trying to model. We kind of formulate it like this: the eval, in practice, it takes an individual data point. So if it's an image, it takes a single pixel and basically traverses the tree to get a predicted output and then compares that to the true data or whatever you're capturing as a true model. Yeah, theta SR is the abstract composition structure. SR is the abduct composition structure, which is exactly this tree. You can define your own operators, and from there, as I said, genetic algorithm, there's mutations of this whole list of parameters that you can play with. So I won't go too much into that now. Cindy, yeah, it's not been, it's been mentioned already. So the initial paper was by Steve Brumpton and Nathan Kaus, who I think is an organizer for this. Right, so it's basically a sparse identification of nonlinear dynamical systems. So the premise is to discover a parsimonious governing equation, kind of follows Occam's razor. You want to get a linear combination of potentially non-linear functions, and those functions come from basically a very, very large library. Or it could be a less large library depending on how you specify. But again, that's kind of a user-defined choice. Again, we've just kind of formalized it as such, where basically theta SIMB here, we take the sparse regression. Depending on what kind of algorithm you want to use, you might get different results. But yeah, simply sparse regression over a large library of functions to identify what functions contribute. There we go. Right, neural ODE has been Right, neural ODE has been thankfully covered right before me, so I won't go too much into that. Essentially, you have an unknown ODE that's parametrized by a neural network, and then you call ODE int, which is some kind of solver, which integrates back over that and produces an output. We kind of define our ODE as such for the context of PDEs. In practice, we kind of use it very similarly to how we've seen. Very similarly to how we've seen in the rest of the week. One interesting thing that we've kind of played around with was we were considering basically the most shallow neural ODE, i.e. a one-layer ODE, and what we can say about that, because eventually it becomes basically like a linear regression, but with a time traversal, which is the ODE integrator. With a bit of thinking, With a bit of thinking, Simon was the mastermind behind this acronym Trindi, which he was very proud of. And so we suppose like a very simple linear model as a neural network, which is what I've just mentioned. You eventually get this, where you have the inner product. And if you take sparse regression over this, you basically get SIMD, but with some kind of time dimension, which is why we've denoted this as. Denoted this as time reversal Indy, which is basically casting Cindy as a very special case of a neural operator, oh, sorry, a neural ODE. Yeah, I've just mentioned this because this is one of our test cases. Considering it's a very, very shallow network, we were able to still get a lot of interpretability out of that. And so it was a kind of useful intermediate between CINVI, for example, or symbolic regression and deep neural network-based. Deep neural network-based neural D. Okay, so why are we interested in variants? Again, this is being talked about in an extent in this workshop. Just a couple highlights. So we want robustness against transformations. I mean, the common example would be CNNs, and we want translational invariants across images. Identification of fundamental physical laws. Of fundamental physical laws. Yeah, if I drop an apple here and I drop an apple on the moon, it might move a little bit slower, but should still work. And then we want to reduce model complexity by focusing on invariant features. So when we know that a model should be invariant, so if it's a physical model, again, if I drop a link here and I drop a link in China, it should fundamentally look the same. Modern approaches, we've actually seen quite a few of them this workshop as well. Data augmentation, equivariant neural networks. Equivarian neural networks. So very sophisticated work again done here. We approached this problem inspired by a multi-scale image feature analysis. There was a series of papers introduced by Florak Koenderink and Terhara Romani, which will be somewhere referenced down here. Full references are at the end. This is kind of a more classical approach, which we quite liked. And also in contrast to the To the more extensively worked-on approaches that we heard earlier this week, Iris kind of falls into the realm of data augmentation, where, again, if we look at the data, if we're able to modify the data first, you don't have to think too much about overcomplicating the architecture just yet. And so we approach this basically as data augmentation with a classically inspired approach. With a classically inspired approach. So, how do we do that? Again, if we're looking at invariance, we want to be able to define some kind of system that we can index without any respect to a specific coordinate system or any other constraint like that. And so, this is from the Terhar-Romani paper that I just mentioned. They introduced a setup notation, which is what we use here. Multi-index tensors, I won't read this up to you. I won't read this up to you directly. Basically, we want to be able to, we use these multi-intensive tensors such that we can be coordinate system invariant when we're describing our PDEs. What I will highlight is just our notation for derivatives is just going to be as the subscripts. We have n tensors for multi-indices, and again, more of a notation thing here. This with With also with these special tensor definitions, we've seen actually these come up again this week. The Kanaka delta tensor is defined as such, and the Liebesibuta epsilon tensor. We use these two because of the usage in tensor contractions, and as we'll see later, this helps us identify a subset of polynomial combinations of PDEs. The following are the previous definitions, these three were all. The previous definitions, these three were all really important in being able to construct systematically a set of polynomial PDEs which are all invariant. So these kind of building blocks are what we use to define how we construct PDEs that we can confirm are invariant. This is basically our system. Again, we basically have a polynomial combination of epsilon and delta tensors. Delta tensors. I think this is a little bit better described once I show you the actual equations that we get out of this. But yeah, as I've mentioned, the building blocks here, if C is a product of Crown-Co-Delta tensors and the Civita epsilon tensors, then we could say that L is invariant. And that's basically our end goal. We want to be able to construct systematically a large library of invariant features, which actually we then factored down. Then factored down into the, let's say, least common denominators that are able to represent a larger space. So, this is kind of an example where we have spatial dimensions equals two, and we kind of constrain our size of the library for practical purposes, really. Okay, so this kind of defines like our indices. C is as defined before, and then we have. C is as defined before, and then we have the actual combinations, which we get here. These are more models that we come up with, and this extends to the next page, I believe. And now, as I've mentioned, we basically factor out these models and try to obtain kind of a minimal set that we can use, for example, in symbolic regression, where you would be able to just combine this set in terms of polynomial combinations to be able to express a much wider span of functions. Span of functions. Right, so nominally this chart is actually what we use as our feature set for the following experiments. So it's kind of how we've organized. And more succinctly, these are the six functions that we basically factored out of the previous table. This can be done programmatically. We used SymPy to help us with the computation and verify that our Help us with the computation and verify that our paper workings are correct. Again, so we focus on the irreducible set of invariant polynomial PDEs. This way it's a very succinct feature set of just six. And that forms the basis of our experiments for the following. I'm not going to go too much into implementation. We use symbolic regression. We've kept most of the parameters according to the guidance that is given in the actual PyS. Guidance that is given in the actual PISER package. Cindy, again, we don't play around with too much. The key parameter choice would be alpha, which is actually like a thresholding for how correct you want the model to get. Neural ODE, including Trinity. I will focus mostly on Trinity. Neural ODE is still a bit of a work in progress. And we focus on the heat equation and curvature equation. So again, we're looking at models that we assume to already. We're looking at models that we assume to already be invariant, and then we focus specifically on these two as they're already very well explored in image analysis. And so we can potentially derive more, at least to understand whether our technical solar based on these. So this is what I've just mentioned. We take examples in two dimensions because we can actually visualize them. This is more of a practical limitation. We take up to the second order derivatives. They should be expressive enough for the examples. They should be expressive enough for the examples that we're taking. And just as a side note, the sixth feature is the only pseudo-invariant feature. It signs upon reflection, but otherwise the rest are your standard orthogonal orderings. The data, we've taken STL10 images from that image data set. This is just an example. We take 12 frames. I think for the heat equation, we take 94 oil steps per frame. For Euler steps per frame, and for curvature, there's 30. Generally, we just try to stay within the stability bound for heat equation. Curvature has no such, but visually looked about correct. We define the training data. We have U as kind of our true model, and VJX is the finite difference we solve. And so, because, again, we chose heat equation and curvature, which have a lot of existing solvers, we can have both the data true. Both a data true and like the analytical true version. Right. So the feature set, this comes directly from the feature set that I just presented. We have like a baseline feature dictionary. So this just takes like, for example, u by x, u by xy, u by y, u by xx, and u by y. That would be what we would consider the baseline feature library. And then the invariant is exactly what I presented earlier. invariant is exactly what I presented earlier. And so this is what I've mentioned. The span of the invariant is the exact invariant subspace of the span of the non-invariant. And this, basically, the non-invariant subspace has 923 functions, which with the formulation that we have here. And the invariant has 166 in total, which are all generated from those six. Our quantitative measures, we have both a loss and an error. We differentiate because the loss is basically the pixel. Because the loss is basically the pixel-wise, this is our predictive model and this is our data. And the error is actually with respect to our analytical model. So we calculate both. Error 45 is exactly what it sounds like. We take a 45 rotation of the image and we see if the same model without any extra training can accommodate for the rotation. And so this is kind of a quick test for the invariant properties. Right. So across the images, we take a subset of 5,000 pixels. And the 5,000 pixels are taken over the time steps as well. And so that covers the full time evolution. Sorry, I should also note that we do weight this to be proportional to the, let's say, signal strength. So we try to ignore any white space out of practical choice. Out of practical choice. Heat equation for quantitative results. You're going to notice that for the most part, the error metrics all here will be roughly comparable. Where we see the nice gains is if you look at, and this is quite expected, if you have a smaller feature library, then time should be smaller. We decrease this by a factor of eight. And then symbolic regression can be notoriously lengthy. And so this is where here it's not. So this is where here it's not so noticeable. Later in the curvature case, the time decrease will drop quite dramatically and with a similar expensivity as measured by the loss error and the five. I think I'm moving a bit too fast now. Okay, so I've made these plots because it's kind of hard to compare equations. I actually struggled with that, and I couldn't figure out how to compare, I think, 2024. 2024 equations in some reasonable way. So, what I've done is I break up or I parse the equations into the component factors and see what it comes up with. This is just the plain symbolic regression, and as you might expect for basically a Laplacian with no diffusivity, the results are typically quite good. Even with 5% noise injected, gaussian noise, you get. You get a bit of an extra factor here, but reasonably it's still identifying the Laplacian. And this is the same, but with Cindy. You can see Cindy performs a little bit worse than symbolic regression, but symbolic regression tends to be quite good for very, very sparse equations. You get the same extra second derivative by y. And with the invariants, you still get quite a big error bar, but I would attribute that to basically the noise. And yeah, it's able to retrieve moplacity quite easily. And same with SIMD. So in general, although the error metrics were not surprisingly different from the non-invariant case, we see the model is more consistently accurate to the true. You'll also notice though, so I've got the mean noted across the plots. You've got a 0.96 here and a 0.96 across the symbolic regression as well. So this was actually frustrating us a lot because we weren't sure why we weren't getting just a, because we have no diffusivity, it should be just a one. I think this was touched upon a little bit before as well, but depending on what method you have. Depending on what method you use for the data, this will impact how you end up predicting or what you end up predicting as a model. We realize that we're not actually modeling the PDE, we're modeling the time-step operator of the PDE, and that can be formalized as follows. Considering our regression problem, this is basically how we can actually derive that 96 value expansion. Expansion and basically end up with this. If you evaluate this for our given data, you come up with 0.96. So we're fairly confident that the invariants are working. This is just a quick side note explanation of why we're getting strange testing values. And this is actually just a plot of this. So we're plotting the analytical model versus the numerical model. Again, you get the same slope. So good visualization. Good visualization of what I just mentioned. For curvature, I'm not sure why that just skipped pages. Sorry, I think that was supposed to be below. Anyways, I've just realized the image is not quite large enough for you to see the rounding of all the edges, but that is what happens after you get to the twelfth frame. Curvature is basically defined as the cube root of the fifth feature. Again, this is a bit contrived because we're generating data. This is a bit contrived because we're generating data, so we get to choose equations that work nicely within what we're trying to test, but not too nicely. So the q root appears to be notoriously hard to try to get a model to work. Uh the plot is Yes, so this is the actual curvature plot that I was trying to show afterwards. This is where I mentioned that the time can get, you see more significant improvements in time. Apologies, the table is not the same format as the previous, and so there's not a great The previous, and so there's not a great flow there. But you can see a noisy case with symbolic regression can take, I think this is around 20 minutes, whereas with invariants and in the noisy case, you cut that by half. And so the time speedups for symbolic regression are most notable. You still have the same speedups slightly with SIMD, but they're not as compassive. And then here, again, And then here, again, what I can say is that the loss and error tend to be actually slightly better with the invariants, which is always good. It means that we're expressive enough to compete with the larger libraries. I wouldn't say that they're extremely significant, and so I wouldn't necessarily sell them as the main selling point, but without the time decrease without the loss of expressivity, it's kind of what we saw here. I was going to show you the bar plots. I was going to show you the bar plots, except they get very, very long because sometimes with symbolic regression you can get an equation with 25 terms, and that just gets a bit messy in terms of bar plots. I show just a human-selected biased case of the equations that we might get out of for curvature flow. What's interesting is you do get this kind of feature five with a fractional power, which accurately captures the cube drive. Which accurately captures the cube root, which is really nice. In general, the invariant cases try to, they tend to capture that the feature five was important to varying degrees. And the non-invariant case, for example, this term here is a key term in the curriculum flow. And so it does capture what we would consider important. But these just tend to be a little bit difficult to parse. And for example, something that we wanted to explore at some point was. Something that we wanted to explore at some point was to actually solve these equations to see how good of a representation they would be as a surrogate model, let's say, of Curvature's quote. But again, some of these that we get out of the model, I don't think, have analytical solutions. So of course that gets a bit more messy. Trinity implementation note: just because we haven't seen it, or this is kind of us-defined, we have what we call a We have what we call a feature transformer, which, let's say, expands out the dictionary, or expands out the data to include our invariant terms. This just passes through a linear layer, and then we take the LDE integrator. We only really played around with the learning rate, and this was by hand, so consider it very little hyperparameter tuning. We used L1 loss. We did have a regularization parameter. Parameter. And then our loss was calculated over the full generated trajectory versus the trajectory. In lieu of the loss, I think this was a bit more exemplified by if we plot the weights of the features that we identified. This is for the heat equation without invariance. I've only plotted the noise just for so we don't have to look through too many plots. To look through too many plots. Yeah, so this pretty accurately identifies the x and y terms. That's why there's two of them. You'll see here we converge, let's say, to the appropriate coefficient much more slowly. But we avoid this kind of disparity because these two should be exactly equal to be actually invariant, whereas this will always. Whereas this will always be invariant. And so there's a small trade-off between the convergence time and the invariance of the resultant model. With the isophote curvature, this actually results quite nicely into the attempts to model curvature with, let's say, pseudo-Laplacian, or an attempt at Laplacian, which it also converges quite quickly. This one, again, it does. This one again, it actually doesn't converge, it doesn't plateau out. I assume that if I run this for long enough, it will eventually. But we see that it is able to actually capture that feature five term quite nicely. And this is just an example of that same run. This is the evolution, and again, I've just realized I should probably focus in on like a quarter of this, so you can see the actual curvature flow happening. See the actual curvature flow happening. But the resulting forward model is something that we could actually feasibly use to remake the same type of code. Yeah. Okay, so I'm ahead of time for sure, but anyways, my caveat is, again, we're looking at models which themselves are invariant. Probably a good test case would be to check something that isn't, and then Check something that isn't, and then see if it's still expressive enough to also account for that side of functions. This was entirely expected. You have the invariant features. They improve the learning team without learning time, without sacrificing expressivity. The learned models are more consistent, which is what I mentioned with the delta terms. Across all of the invariants, they were able to only capture a delta term. They improved the generalizability. Again, this was a bit expected considering we're looking. Was a bit expected considering we're looking at invariance. The resulting models were more interpretable in general. This was mostly exemplified by here where in general, I mean, might be still a bit difficult to parse, but they tend to be a little bit more concise or shorter. And then, right, the Trinity, I think, was just a kind of interesting I think it was just a kind of interesting theoretical work just to put SIMD into a neural ODE context. But again, I won't say too much about that because I think we need to be able to show the full deep neural network neural ODE for a better comparison there. Yeah, Simon is my supervisor. Thank you for sending me here. Antonio has been a really, really great resource for actually implementing some of the finer details there and a great bouncer for ideas. And a great bouncer for ideas. And again, Rob Toby was kind of the originator of the idea. Yeah. Sadly missed. Sadly missed. Yeah, and then once we are able to run the neural ODE at scale, we want to be able to put it on that five. So it's TP. Any questions from the screen? I don't. The coefficients you would say they fit well solutions or not? Um what do you mean by the coefficients? So the solutions the solutions you get um is this is a good solution to the heat equation that you're trying to remote? Um in general, so like Um in general, so like let me take this. Um the reason why we did the analysis of the coefficient to the term was precisely because this was initially in our eyes like not a good match. The heat equation is so simple that it should theoretically just get like delta u. That should be it. But again, because we realize that, yeah, if we're using this data, then we end up, you can actually calculate this for a variety of different numerical schemes. So in our case, we used the forward Euler. So in our case, we used the forward Euler with central differencing. You result in a 0.96 coefficient. So in that sense, like yes and no, ideally, and this is actually what we mentioned in the open questions, if you can be, let's say, generating method invariant or agnostic to the way that you solve for the initial training data, then that would be ideal. In our case, I would say the coefficients are good enough, considering that we're still learning from the data. Considering that we're still learning from the data and it's accurate to the data that's presented, whether that's accurate enough for a specific situation, I think would not, it would depend on whether it matters, let's say. Oh, one thing you will note in the trendy though is that in general, for both the invariant and non-invariant case, it doesn't actually even come close to the one. Close to the one. So, what I'm saying is basically while we identify the correct features, the magnitude of the coefficient to these features is not very well matched, let's say. And so, that was actually something that we struggled with. Because we would always get the correct features, but somehow the value was always low. And that's, again, this is why I mentioned we still need to text deeper neural outlook as well and figure out why. If anyone has an idea of why neural ODE, but kind of shrink down the kind of shrink down the or over smooth I guess in some sense I did have a question uh you mean uh you showed a numeric result comparing the question with SIMI and it's more costly for different symbols find that it's more robust to noise maybe it seemed like for the curvature to discover a more simple compact form so yeah I think across the board what I've noticed for symbolic regression is Board, what I've noticed for symbolic regression is that in general it's more robust to noise, as we mentioned. I will also mention that Cindy has different libraries, let's say, but also the parameter list is quite short. There's simple thresholding for like, for example, the trade-off between how swice you want and how expressive you want. But symbolic regression has, I think, some 20-odd parameters which you can very carefully fine-tune for the For the expressivity along quite a few different veins. And so, for example, you can define how many nested expressions you want in symbolic regression, whereas Cindy will just, again, define a sparse combination. That's it. So, I think symbolic regression in general is a much more expressive framework to use at the cost of the runtime. Which here, which you can't see. In our curvature case, was the 20 minutes. If I run that on my laptop and not on the CPUs on the cluster, it takes like 40 minutes for a single model. So altogether, if I ran with noise, without noise, with invariance for blockcases noise, it takes like three, four hours because it slows down at the end as well. So yeah. And costly too, too, but maybe. Yeah, despite it not being too I wouldn't. To, I wouldn't call curvature flow extremely complex in the grand scheme of complexity of equations. And so, this is also why we were looking at: like, if you already know the invariants, can we basically compute them or use this to compute them fast? Questions and thanks, speaker. Um we have one more time. Um we have one more time.