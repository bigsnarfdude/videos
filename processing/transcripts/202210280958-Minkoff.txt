Sure. Let's move on. So, the next speaker is Susan Piccolo from UP DAOs, and she's going to talk about the use of extended source inverter for estimating the noise level in setting the turn, Suzanne. Okay, well, thank you very much for the invitation. I'm, like Crystal, very sad that I didn't manage. Thanks so much for inviting me. So, I'm going to talk about something that is sort of building on a talk I gave at a fully remote BAMPF workshop, the Women in Inverse Problems Workshop last year. And this is using extended source inversion to solve the seismic inverse problem and get around the cycle skipping problem, but also in the context of doing Context of doing that to estimate the noise level in the data because we actually need to do that to use the algorithm that we're going to talk about, the discrepancy algorithm. So this is joint work with my graduate student at UT Dallas, Hui Yi Chen, and Bill Symes, formerly of Rice University. And I just want to acknowledge support from the sponsors of our industrial research consortium at UT Dallas and And all right, so see if I can make this work. Whoops. Okay, so an outline, I'm going to sort of repeat things that I think several people now at the end of the week, several of you have already talked about, that is the cycle skipping problem with FWI or least squares inversion for the seismic inverse problem. And I'm going to put everything that I discuss into the context of a really Into the context of a really simple inverse problem. And this simple inverse problem we looked at because it still exhibits this cycle skipping problem, but it's simple enough that you can actually prove theorems about how well you do and up to what level of noise things to work, et cetera. So I'm not going to focus so much on the theory. I talked a little bit about that a year ago in the women in inverse problems talk, but I'm really going to focus a little bit more on. To focus a little bit more on why we need the noise level and how you can get it. So, before I do that, I'll talk about extended source inversion and then the noise estimation algorithm and try to give a little bit of intuition about how this works. And then I'm going to just show numerical examples. So, I'm not really going to have any theoretical results in this talk. So, I apologize at this point. A lot of this might be review, but I'm going to. Review, but I'm going to go over it again just to tell a complete story. So, what's now called FWI, I just think of it as least squares inversion, full waveform inversion is something that people now are fully invested in considering for the seismic inverse problem. But it suffers from this well-known issue that if you look at this plot from the paper by Jean-Verrieux and Stephanu Perto, that is well-known in the middle picture. Known in the middle picture here, we have this dark black curve. That is the seismogram. So that's your seismic data. And we see three periods here. If your initial guess for the inversion is off by more than half a period, your predicted data might look like the dashed curve in the top. And then when you go to solve the least squares problem, what will happen is rather than Problem, what will happen is rather than the nth period of the true data lining up with the nth period of your predicted data, the n plus first period of the predicted data will line up with the nth period. So you'll end up in a local minimum. If instead you have an initial guess that's good enough that you're within half a period of the true data, then hopefully the nth period of your predicted data will actually line up with the nth period of the seismogram. nth period of the seismogram. So this is the cycle skipping problem and it amounts to the fact that if we're going to use local gradient based optimization, which is the most feasible thing to use to solve the seismic inverse problem, we have to have a good initial guess. So this isn't probably news to anybody here. So there have been a lot of different methods proposed in the last 10 or so years that extend your objective. That extends your objective function by adding degrees of freedom to try to convexify the objective function. This was discussed a little bit, I think it was Monday, by Bjorn Enquist, Yunin Yang on their Vasserstein metric and optimal transport scheme. I'm going to look at something slightly different, but it's going to show very similar pictures. And so there are different extension methods. There are different extension methods. There are a lot of different extension methods. I'm only going to focus on one, which is source extension. And what we're going to do, you'll see, is we're going to relax the requirement that the source have compact support. And that will allow us to fit the data basically with any model parameter we want. In our case, the interesting model parameter that we want to invert for is going to be the seismic velocity or the reciprocal velocity, the slowness. Be the slowness. Eventually, as you get close to the global minimum or the geologically informative Earth model, you'd like to suppress these additional degrees of freedom so you end up with something that makes sense both for the model perimeter you're interested in, the velocity, and also the wavelet, the source. I'm going to show you a very simple model problem where a lot of the computations can be done by. Of the computations can be done by hand on paper, but this simple problem still illustrates the cycle skipping problem that you see in FWI for more realistic problems. So here's the simple setup. Here, the Earth is a cube, and it's a cube of one material. So rather than having different rock and fluid layers, we've got a homogeneous medium. The velocity in that medium is 2.5 kilometers per second. The slowness, which is reciprocal. The slowness, which is reciprocal velocity, is then 0.4 seconds per kilometer. So that's a huge simplifying assumption with about one material, it's homogeneous. And we're also going to assume that we have one source that generates a wave that propagates to one receiver, and the source and receiver are going to be assumed to be one kilometer apart. So, those are the simplifying assumptions. Clearly, if you have a homogeneous Earth model, then we can write down the solution. Then we can write down the solution via the Green's function. So, in the top picture on the right here, we've got a typical seismic source, a Ricker wavelet. And you notice that it's only non-zero very close to time zero. So outside of time zero, the wavelet is zero. And in the case of this simple problem, the solution, where I'm going to have it on the next slide, it's just a scaled shifted version of the source. Of the source. So the data, the seismogram, is a scaled-shifted version of the source. There it is on the bottom there. And this was a 20 Hertz Ricker wavelet. So mathematically, you've seen a lot of this in other talks. We're going to look at the acoustic wave equation in three dimensions. That cube was three-dimensional. We're going to solve for the acoustic pressure, capital P. We have a Capital P. We have assumed the density here is constant at one. And then rather than solve for sound velocity, we'll solve for reciprocal velocity or slowness and, which you've also seen a lot this week. And we're going to assume the source is a point source in space and some wavelet in time. So this was our wavelet in time, this Ricker. So as I said, now the data, the seismogram, is just a scaled time. A scaled time-shifted version of the wavelet, and it shifted by mr in time. M is the slowness, r was the distance between the source and the receiver, so it was one in this case. And then the, I'm sorry. Okay, then the inverse problem is most realistic inverse problems have noise in the data, and so you don't usually know that noise load. You don't usually know that noise level. We'll call it e-tar for target noise level. But imagine you have some target noise level. And I'll show you examples where we don't know the target noise level, but imagine you have some understanding of what it is. And also imagine that you have some information about the support of the source. So we're going to assume that the source is only non-zero between some minus pi to pi, not minus pi, minus lambda to lambda. minus lambda to lambda interval. And so that's the maximum lag constraint. Then we want to find in this case both the slowness and the wavelet. So the wavelet has compact support around zero and we solve the FWI problem up to the noise level e tar. Okay, so that's our inverse problem. And the full waveform inversion objective function is just this least squares. The objective function is just this least squares midfield, which minimizes the difference between the data that we predict from our estimate of M and W and the data we record or observe, D. And so here I'm showing you a version of the data, which is that shifted scaled Ricker wavelet, but we've added coherent noise. So here we've added 30% coherent noise. And on the right, you see the FWI objective function. Bjorn Enquest showed a very similar picture. And Enquest showed a very similar picture without the noise on Monday. You see, there's a global minimizer at 0.4 seconds per kilometer, but there are lots of other local minima. There's a local min here at 0.5. There are these other lobes that are local min, but the bigger issue is that all the flat places in the objective function are places where the gradient would be zero. And so if you start far from the global minimizer, you'll get trapped in one of these, you know, low. In one of these local mins. And so this is now something you've seen a couple of times this week. But there are huge numbers of local minimizers, and this is the cycle skipping problem. You have to start within this region. Remember, lambda is the maximum lag of the source, and R is the distance between the source and the receiver in order to solve the inverse problem using FWI. So there were a couple of papers that came out in September. There were a couple of papers that came out in September and Inverse Problems. It was sort of a two-part set of papers. The first part you'll see referenced on the next slide, or a subsequent slide, had a summary of the theory and some numerical experiments and discussion. And the second part was written just by Bill and is proof of a bunch of theorems in this simple context. So we're not too happy with FWI, and we want to do a little bit better. And so we're going to And we want to do a little bit better, and so we're going to look at this extended source inversion idea where we're going to minimize over both the slowness m and the wavelet w in the following fashion. We've still got the least squares objective function here. We're calling this e squared because remember we're going to try to solve that up to this target noise level. And then we've got an added regularization term, a penalty term. This penalty term has a penalty weight alpha that you have to somehow choose. Alpha that you have to somehow tune, and then it's got something multiplying the wavelet that we're inverting for, and the something that capital A will called an annihilator. And basically, it penalizes energy away from t equals zero. Remember, the source was centered at time zero. And so, one, you know, easy annihilator is just multiplication by t. And then, you know, things will. You know, things will work out if you multiply by t and you're hoping that your source, the physical source, has support near zero, then that works. We have shown that there is a global minimum of this extended source inversion objective function that is close to the global min, the geologically informative minimum of the FWI problem. Problem. And I'm not going to talk about that, but up to about 60% noise, you can show what the bound is on the difference between how well you do with ESI versus the true minimum of FWI. So one question is, now we've got this new objective function, let's look at how it does. And we're going to have to think about how to solve the problem with this combination of looking for both the source and the slowness. So here I'm sure. So, here I'm showing you that same coherent data on the left. I'm sorry, that same data with the 30% coherent noise. So, the data generated by that Ricker source where we've added 30% coherent noise. And on the right, you see in blue, the same FWI objective function that I showed a minute ago. It's still got lots of local minima. And then red is the ESI objective function, and it looks a lot. Objective function and it looks a lot better. It looks like it's got one minimum at the true geologically informative minimum of 0.4 seconds per kilometer. And anywhere you start, you'll end up in that minimum. So this looks like we've definitely improved things with this extension method. One of the issues is thinking about how you choose the penalty parameter alpha and also Alpha, and also how we solve this problem in general because the objective function has very different sensitivity to the wavelet w and the slowness m. And so we actually are going to use the variable projection method, which Milena talked about yesterday in a lot of depth, in order to solve this problem in a nested way. So we're going to, do I say that somewhere? Yeah, so we're going to solve for the wavelet. So, we're going to solve for the wavelet in an inner loop, and then once we have that, we'll solve in an outer loop where we fix the wavelet. We'll solve just for the slowness m. In the case of this very simple problem that I've set up, you can actually write down the solution to the normal equations analytically. You don't need to do something computational to solve for the wavelet W. So, that's kind of nice. All right. So, going back to discussing the penalty parameter, the penalty parameter is really important to the convergence of the algorithm using variable projection for both the slowness and the wavelet. If you take a very small penalty parameter alpha, we get this blue curve. And the blue curve is great because it doesn't have local mins, but it's almost flat. But it's almost flat. And so you often look at the gradient of the objective function and try to, you know, drive that towards zero to solve the inverse problem. And it would be very hard to tell which of these values for slowness are the right one if you're just looking at the gradient being small, because the gradient's small almost all the way along here. So there's not very good resolution of the slowness. Of the slowness with this very small alpha value of 0.1. As alpha increases, the red curve is for a value of alpha equals 1, then things look better. We still have a global, you know, a single minimum near the global minimum for the FWI problem, but you can start wherever you want, and the resolution for m is getting better. If alpha is bigger, say alpha equals 10 or alpha equals 100, then you start looking. 100, then you start looking a lot more like the FWI problem. Now we've got other local mins. Specifically, there's one at about 0.5 seconds per kilometer. And the purple curve has lots of local minima. So the purple curve where alpha is 100, you're basically back up at FWI. And so you have to be careful. But the resolution for the slowness is really good. If you can get close to that minimum. That minimum, you know, somehow, then you're going to get much better resolution for the slowness m. So, as alpha goes to zero, the error, which is almost what we're plotting here, we're actually plotting the objective function, which has more than just this term, but it's dominated by this fwi term. That error goes to zero as Goes to zero. As alpha increases, the region of convexity gets smaller, the resolution of m gets better, but the error also increases. And so the way we solve this problem is we consider the fact that you wouldn't know the noise level in the data initially. You wouldn't know a good choice for the penalty parameter alpha. So we're going to start with an alpha of zero, which is sort of knowing nothing, and dynamically increase alpha. Increase alpha as the algorithm proceeds. And so we have to think about why we'd want to do that. Remember, there's this target noise level that you don't generally know. That target noise level, I didn't put the formula on the slide because it's kind of ugly, but it has an impact on what the value of alpha is. So we're going to define noise just as unmodeled signal. So it's anything that's not in your model. And in this case, Model. And in this case, that's the same as the minimum of the FWI objective function over all the sources that have that constraint that they're compactly supported around zero. So we're calling those the physical models. So to use this ESI BPM approach, you've got to choose an alpha, and you generally don't know what to choose. We're going to start from alpha equals zero, and we're going to increase alpha dynamically throughout the process. Dynamically throughout the process, hoping that when we get close to the global min, then if alpha is big, we'll actually get there because we'll be close enough that we won't get stuck in a local minimum. So I'm going to show you first an experiment where we don't update. We update alpha, but we have the wrong target noise level, and we don't update our estimate of the noise level. We just assume the target noise level is correct. Assume the target noise level is correct, and we're going to see how we do. And then I'll show you some experiments where we do update the target noise level. Even if we had the wrong estimate initially for the noise level, we're going to update it and we'll see how that goes. So just one more thing to say about noise. If you think about this problem, if you were to choose a target noise level that's, well, maybe I'll show you that experiment. Yeah, okay. So if you choose too small of a target noise level, then you're going to Small of a target noise level, then you're going to overfit because you're going to try to fit some of the noise. If you choose too big of a target noise level, then you're going to underfit. And so we're sort of balancing those two extremes. And so here's the actual algorithm. It includes at its heart this discrepancy algorithm. And then wrapped around that is a noise estimation algorithm. So let me just discuss the discrepancy algorithm first. The discrepancy algorithm is going to. Algorithm first. The discrepancy algorithm is going to alternate between updating our estimate of the slowness m and this penalty parameter alpha. So the first thing we're going to do is we're going to assume that we have some initial m. In all the experiments I'm going to show, my target m is 0.4 seconds per kilometer. My initial slowness is going to be taken to be about a third, so 0.34, I think. And we're going to start with an alpha of zero. And then you need. Zero. And then you need some tolerances for when to stop the optimization and so forth. We're also going to assume that we don't want to hit an exact noise level, that we don't have, we have sort of fuzzy information about the noise level in the data. So we're going to have some range of noise levels that would be acceptable, sort of a minimum and a maximum. And so the first thing we do is we start with that fixed initial slowness M, our estimate, and we're going to update alpha. And we're going to update alpha so that we have an alpha that gives an error in our problem between our max and min target noise level range. Then we're going to fix alpha and we're going to update the slowness so that we get the gradient to be small. And you alternate back and forth in this discrepancy algorithm between these two updates. First, for First for alpha, then for M, and back and forth. And when both the range of, when the error is in the range of acceptable errors and the gradient of my objective function is small, then you get out of this algorithm. You're done. And what I'm going to talk about first with my experiments is just the discrepancy algorithm where you assume that you know the target noise level and alpha is updated based on that target noise level. Updated based on that target noise level. And then I'll show experiments where we may have an incorrect target noise level, and we're going to wrap around this a noise estimation algorithm. And all the noise estimation algorithm is going to do is it's going to estimate the noise given your estimate of the slowness M by minimizing the FWI problem with that M over all acceptable wavelets W in the Wavelets W in the physical space. In other words, wavelets that have compact support around zero. So they have that maximum lag constraint. So that's the goal. The goal is to estimate the noise looking at the real data, which may have some wiggles outside of where the wavelet has compact support, and what we get by predicting the data with that target M and the wavelet with the physical lag constraint. With the physical lag constraint. And so that's going to give us our estimate of the noise. And then we look and say, well, did our initial guess at the noise level, what we called the target noise level, you know, was it close to the estimated noise level? If it isn't, then we're going to update our target noise level and repeat. We'll go back up and redo the discrepancy algorithm with that new target noise level. And eventually, you'll get to the point where hopefully our target and our new estimated noise level. And our new estimated noise level are close, and then we're done. And so let's sort of see how this works. First, going to show, as I said, an example problem where we still have 30% coherent noise. So it's going to go with this data picture. And we're not going to update the noise level. So we're going to assume the noise level is one of these six values when we start. We're going to assume it's 10% noise, 20% noise, up to noise. Noise, 20% noise, up to 60% noise. And we're going to see how we do. And again, we're going to start from an alpha equals zero. Alpha will increase dynamically as the algorithm proceeds. I'm not showing those iterations here. And we're going to start with an initial slowness estimate of about a third. And let's look at how we do. So the best estimated noise level from this formula here is about 0.29. So it's close to 30%. So it's close to 30 percent. It's not exactly 30 percent because of variance tolerances we have, and some you know, the fact that we don't exactly know the maximum lag for the wavelet and all of that stuff. So, but it's close. And here's how you do. If you have too small of an estimate of the noise level, say 10%, 20%, 30%, we actually get that best estimated noise level. Level 0.287 essentially. But if your noise level was too big that you thought initially was the noise level in the data, then you don't estimate the noise level correctly. And more importantly, for those small levels of noise that you started with, noise levels in the data, you solve the inverse problem. You get the geologically informative slowness. But for the noise levels that you start with that are too big, You start with that are too big, you land in a local minimum. And the reason is that the penalty weight gets big, the alpha gets big, and because the noise level is incorrect, and you end up in this local min at 0.5 seconds per kilometer. So the upshot of this example is if you don't know the initial target noise level and you don't allow the algorithm to update it, you do update alpha, but you don't. Alpha, but you don't have the correct noise level, then you will still have the cycle skipping problem. You'll still land in local minimum. In the next three examples, I'm going to show you, we do update the target noise levels. So here we have the exact same data. It's got 30% coherent noise. We start at alph equals zero. And we again assume these are the target noise levels. So we assume there's 10% noise in the data. We run the whole. Noise in the data, we run the whole process and we do solve the inverse problem. And this time we, you know, we get the right estimated noise level. And that occurs for all of our initial guesses at the target noise level. So in this case, if we assume the target noise level was 60% and it's really 30%, then we update the noise level. And in all these different cases, for all these different experiments, we do get the correct estimated noise level. Estimated noise level. We also solve the inverse problem, getting the global minimum for the FWI problem in all cases. So, this seems like a clear win. If you use the discrepancy principle, you can start from having no information about the penalty weight, starting from alpha equals zero, and then alpha increases to get you to that correct global minimizer with pretty good resolution. But you have to update what your initial. Have to update what your initial estimate of noise in the data was to correctly solve this problem. And here's kind of a funny picture showing you the extended source inversion objective function as a function of those, you know, alpha values for the six different experiments we tried. So we started initially with a 10% estimate of the noise level and then 20%. And so those were all wrong, right? The noise level was about 30%. Right, the noise level was about 30%. And you see that alpha is basically about the same size, except for the 30% noise level. There it was slightly smaller. That's this yellow curve. But the important thing is all of these problems had a convex objective function. So we did okay in all cases. So that was the coherent noise case. In the case of random filtered noise, we might have data that looks like this. We might have data that looks like this. Again, it's 30% noise, but it's not coherent. It's kind of all over the place. And again, we do pretty well. If we don't know our initial noise level and we try different noise levels, say starting from 10% or 20% up to 60%, we solve the problem, get the right slowness, which is our goal of about 0.4 in all cases. And the best estimated noise level in that. Best estimated noise level in that case is about 0.26, and we get that in all cases. Of course, you don't really care about this, but you only care about it because it's important and used to update alpha. And if you don't get the right updating of alpha, you can land in an FWI cycle skipping-ish situation. Again, the objective functions here look really nice. They're all convex, similar to the previous picture. And I'll show you one. Similar to the previous picture, and I'll show you one more example. We also looked at what if you assumed the noise level was zero, and the algorithm seems to work in the simple setting in that case, but I won't show that. I'll just show an example where you actually have noise-free data. What happens if there actually is no noise in the data, but you assume there is? What happens if you assume there's 10% noise or 60% noise? How do you do? And we were kind of surprised. We had to tweak the stopping criterion. But other than that, we solve the problem. We get the true slowness of 0.4. We estimate the noise correctly to be zero, whatever starting value we use for it. Alpha can get pretty big, but we still solve the problem. So that seemed encouraging. So, in conclusion, we're looking at a super simple problem here, which is a homogeneous Earth model with one source and one receiver. Source and one receiver. But it's okay to start with that because that problem still illustrates cycle skipping as an issue for FWI and the fact that you really need a good initial guess in order to not have the FWI algorithm fail. We discussed extended source inversion and how that convexifies the objective function. The algorithm that we used is this discrepancy algorithm, which sort of has a nested loop. Has a nested loop for the wavelet and the slowness. ESI allows us to avoid cycle skipping so we can solve the inverse problem, but the discrepancy algorithm then requires us to update this alpha. And so the alpha has to be chosen appropriately. And it depends on the noise level in the data. So, what the algorithm I described today does is it updates both alpha and the noise level to help us solve the inverse problem. To help us solve the inverse problem and not get stuck in local minima. And obviously, our goal is to go to something more interesting than a homogeneous medium. And that's what we're trying to work on now. But this problem was nice because there were a lot of things you could prove in this context, and you could write some things down on paper. So that's my talk. Thank you. We may have a pipe for a moment.