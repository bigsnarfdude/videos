Okay, so um these three projects. The first project I will talk about is a so-called T2 magnetic, so it's MRR. So essentially that's spectroscopy. Basically that's a scan of your brain to track whether people's brain, say people have Alzheimer, how does that deteriorate? Spatroscopy. So there are two methods I'm going to show you. I'm going to show you. And then the second part, the last part of my talk, will be showing you the poor elastic wave equations and how, and the FDA four-way form inversion of that, the formulation, not the solution. Okay, so the first part. So usually we see this very often. So this is a typical spectroscopy. So this kind of thing can only be done through the inversion. If you cut it open, you still will not be able to see it because this is about the relaxation time. Because this is about the relaxation time. So, different material has different spins, and then each different relaxation time called T2 signifies what kind of material it is in that pixel. So, different colors mean different material, and then medical doctors, they know that in which range of this T2 are so-called good material and which range is bad. So, they want to see that to track the deviation of our of people's brain. Of our people's brain. So, what is the problem? So, let's see, going down, down. Yes. So, what is the model? So, the model is the model is this. So, you get the observation. Yes, so this looks a lot like what Laplace transform. So what is that? So you have the observation data, and the observation data goes from a function which we call phi, which is a function of T2 relaxation time. So you want to recover this. Time. So you want to recover this one. However, your signal you receive is this truncated Laplace transform of the signal you try to recover, plus some noise. And later we'll see how your post it is. So you are given this data, you want to construct this. And if you discretize it, then the A here is a discretization model. And for this experiment, medical doctor will tell, they can measure this, they can estimate this. This they can estimate this noise. So, in this case, it's a Gaussian noise with a given variance. So, this is a typical signal which will be given to you. So it looks like exponential decay, but it's not perfect because it's polluted by all the noise you have. So, you are given this one, and how much the noise is is given by this signal-to-noise ratio, what I wrote here. What I wrote here. So this should be F observation, and this is the IMS of the noise. So as you can see, so this gives some idea about noisy data. So the first time we deal with it, there are a lot of some information from the experimental list. So our constraint is they want to find a solution, phi, which is a finite sum of Gaussian. So this mu here tells you where the material. Tell you where the material is, and they also want to find. So, these are given to us. So, this is a prior information we have. So, we learned, we did something. So, my former student and my collaborator, so we did something. So, we borrowed some idea from image processing. So, instead of reconstructing these nonlinear, solve the nonlinear problem, we assume we have a library. We use a library of truncated Gaussian mixture. It's truncated because we know that T. It's truncated because we know the T2 range of the brain material. And then we try to recover this CI with some noisy data. So this is a typical technologization. We all see this many, many times. However, so the constraint here is it has to be positive, and this is a two-norm. So what is the problem here? The problem here is: I will show you in the next slide, this is a highly Yopos problem because of this. Highly Yopos problem because of this exponentially decay kernel. So it's not clear how you find this lambda. So the way we solve it is actually we use different level of regularization and then we find a linear combination of them which actually give us a stable approximation. Okay, so the first part is about how to choose this one. So it's multi-reg means we use a lot of different lambda in a range. Use a lot of different lambda in a range, which is given to us. It's a range, so we construct a lot of solutions with respect to lambda. And then, and also we try to use the noise. So, this part, I just want to give you some idea about this. So, we combine this different level of glorization. For those of you familiar with things like this, this might remind you of the aggregation model in statistics. But this one is different from that. Different from that. So let's show you the result. So this one is we test, so we combine that with, we compare that, our method, multi-regularization with discrepancy principle. So when we do that, so as you can see, the true signal is the black one. So we go from, so they have different, so the true signal, when you go from here to this next column, the true signals are different. The peak separation are different. The peak separation are different. So, this picture tells us that the multi-reg actually outperforms discrepancy principle. So, when we try to do, let's skip this, so now this is a brain image with high SNR and low SNR, is what we see. So, what is plotted here? One way to, so for each pixel, you construct the peak, and what is of interest is you compute the mass. You compute the mass of this T2 between this range, it's the so-called good material. And for each pixel, you do that. So you construct this brain image. So the more blue, the better. So now, okay, so I'm going to switch here to another one. So now we, even though our method outperforms the current method, but we try to understand what's going on here. So this is about the work. So, this is about the work with Wu Fe. So, what to see? So, what is the problem here? So, you can see here, this is the same as the previous problem. The previous problem was e to the minus T over T2. So, this is just a change of variable. When we change it to the usual Laplace transform, we know. You pick up this kernel here. So you just use some, so this is your kernel, which decay very fast when S is going. And then you try to kind of reconstruct this one. And experimentally, we are given this level here. So here, this talk, we want to see the reproducing kernel Hilbert space here. But how do we see that? So this is how we borrow from this kernel learning literature. So as you can see, if you look at this one, you can see If you look at this one, you can see this kernel decays very fast. So, this kernel here is probing the unknown. Therefore, when S is getting larger and larger, that part of your solution is getting less and less information. So, this kind of idea prompted the definition of exploration measure. So, this measure is exactly this kernel here. It tells you how much the kernel is probing the solution. So, with this definition, and then you know. And then you normalize it with number. So then here, I'm just trying to rewrite the problem we have, right? So this is the loss function defined to be the, okay, this is the objective function, which is, we try to look for solution in L2. And then this is a loss function. So I haven't put any regularization term there. I haven't yet. So from this structure, how do we see the reproducing terminal here first? How do we see the reproducing turn of Hilbert's space? You expand this quadratic term, you see these terms. So you have the red term, this is the operator L, and then so you are taking this inner product in L2, and you have this. So F is your data, and then you have this term. So what happened is we first define a kernel G. So to see the reproducing killer RKHS. RKHS, we define a new inner product of 2 L2 function in AB, which is defined as this. You let L operate on that, and you take this. And then I just plug in the definition, I see this is what it is here. Once you see this one, you can define, so now you can define, so what is the G here? From the definition here, I know this is a kernel G, and then I normalize this. I normalize this kernel with the exploration measure, which is defined before, then I can write this one. And then with this kernel, I define an operator called LGBTR. So now, and then with this, you can see that this basically tells us if you want to do this, the most natural space for your solution is this weighted L2. So this is a weighted L2. Weighted L2 inner product, and it's weighted by this exploration measure, which we computed before. So you can say, well, what? Well, you define that LG, and then you can prove these four properties. The LG, map this one to this one, and then you can show this property that the inner product you defined before can be written like this. And it is self-adjoint with respect. And it is self-adjoint with respect to this inner product, the LG. And this property is important, then you can easily check that this G-bar just defined is symmetric and positive to semi-definite. This is the meaning here. And then another important thing is the operator is compact operator because the kernel is finite. And with all of this, you know that the G-bar, which I just compute from the operator, is Operator is the region reproducing kernel of Hilbert space. But what is that reproducing kernel of Hilbert space? So, with all the four properties from theory, I know this is true. So, the reproducing kernel Hilbert space is this one. You take the square root of that, you let it act on your original natural space, L2 weighted L2. So, this is the subspace of the bigger one. So, now I'm going to look for the solution in this smaller space with all the Solution in this smaller space with all the structure. And then you can see that the okay, so this is a reproducing Hilbert space. What is the inner product? The inner product is defined this way. So when you see this, you say, well, but Lg is a compact operator. What do you mean inverse? Rather, inverse taken in the space which corresponds to non-zero eigenvector. So that's a space. Space, and that's all, and then here just remind you what reproducing kernel Hubert space is. And then you do, you use all these two properties. You can prove this, that if you take one function from hg, which is smaller than the weighted space, if you take that, you do the L2 weighted product, then you end up getting this one. So you have a filtering process. So when you do L2 product, where's a file? Do L2 product with a function in hg, you actually get into the hg space by doing this. Okay, and why is this important? Here, I'm just trying to analyze the quadratic form, I just expand it, right? So then that quadratic term is given this way. So that quadratic term in my objective function can be written as the inner product in HG, which is a reproducing permit over space. Okay, and then, and what about your data? And then the second term, About your data, and then the second term in the expansion is this one. I can use representation to find the representation for my data. Yes? So in this way, I write everything as inner product. Why are we doing this? Because we want to see what is the Foucher derivative. So with all the calculation, before I come here, so this space is important. So it's a function space of identifiability. Identifiability means that my inverse problem has a unique solution in this space. So that's a definite thing I can say. As you can see, what does the non-uniqueness come from? The non-uniquen comes from the zero eigen space of LG. Okay, so the reconstruction later I show you everything is in this FSOI. But what is the advantage of using all of this machinery? The advantage is depends on how you write. If I restrict myself, If I restrict myself in the L to weighted space, the natural space, then for cheer derivative of the loss function is this guy here. And remember, this phi f is a risk representation of my data. And then what if I want to search in Hg? If I want to search in Hg, this is your for shared derivative. So what is the significance here? The significance here is when you invert this one, actually you don't just invert on your data. Don't just invert on your data. You invert on LG operating on your data. So, through this process, it removes the component from the zero eigen space already. So, that's a thing. So, suppose the true solution is in this FSOI, and suppose the F is complete and noise-free, then, of course, either way will give me the same answer. There is no difficulty. Answer. There's no difficulty here. However, a lot of times we discretize that. So we have discretization error and there's a noise. Therefore, the construction of these two are going to be different. But how different? So here, so there are two ways. So that's what I said. So this loss function is the same, but depends on where you look. So this is a tickling regularization parameter lambda. And we can easily write down the normal solution. Okay, so what is the Okay, so what is the catch? The catch here is a lot of time your data comes from, of course, of the true data, but with some noise which comes from your producing kernel Hilbert space. The other one comes from the orthogonal space. So usually you have this too. So you write, however, when you do this Hg, what happened is because you do the Lg acting on this. So that process. This. So that process killed this, eliminated this noise already. So when you do that, you can see, so this is the error estimate. So what we can see here is I'll show you the numerical result. However, what we see here is from this bias here, you can see the error. So a lot of time when you do this, once you are in this HG, you can easily use L curve. Because somehow a lot of time when you use the L curve, Because somehow a lot of time when you use the L curve, you don't really see the sharp corner for a lot of time. However, for this Hg, you see a very sharp corner. And the delta you choose is small. Why should that be small? You see, for this one, you want your delta to be small, so these terms are inevitable. But however, when delta is small, you can estimate what these two are. So these are the bias. The bias, if you do this one, the bias will be this plus this. Your lender is small. So this. Lambda is small. So this noise gets magnified very, very largely. If you do the Hg, this is what you have. So let me show you the result. So this is a signal you get. So this is just one realization. This is noise NSI is one. So it's the same magnitude. So this is a discretized version of that. And here you have this true signal, but you add a lot of random noise with a given variance. So then you recover. Variance. So then you recover this. So the way to read about this is remember: so the real signal, the real solution is this one, which is in FSLI, which is the true one, black. And then the dotted RKHS reconstruction is this dashed line. As you can see, the L2 reconstruction is everywhere. So another thing is I also plotted the exploration measure. So the way we know is the RKHS reconstruction is good. Reconstruction is good when the exploration measure is high. So, from here, even though I don't know the true solution, I will know that here it's very trustworthy. Another way to look at this one is also, so we did this one realization. So, we do 100 realization and we plot the difference. One more thing. Here, also show you how your posings are. You see this black, this blue, and this red, they look all different, right? They look all different, right? But when you use them to generate a signal, the signals are almost identical. So, this is a very, very, very highly open. And here he's trying to show you that the error, because here is a testing, the error of RKHS not only is lower, but within his 100 different realization, the standard deviation of your result is also much, much smaller. And the bar that here is the NSR. And the bar here is the NSR. So when you go NSR, noise getting higher and higher, the result deteriorate. And here is to show you the overfitting of the L2. As you can see, the loss function, loss value is very low for this bad reconstruction. So this obvious is overfitting. So then let me switch gear. I probably still have eight minutes. So I'm going to switch to the next one. To the next one. The next one is about the poor elastic materials. So, in a lot of, so this is poor elastic waves happens when you're trying to probe materials made of elastic matrix with pore fluid inside. And so it's from Beals. So, what difficult to give a talk like this because this model is very ugly. But this is a homogeneous. But this is a homogenized model. Just trying to imagine you send a wave in, say, to the bone. It's a long wave range. So both the fluid and solid will start to oscillate. But this is a homogenized model. Therefore, at each point, you can talk about the fluid displacement and solid displacement. That's why. Okay, so the two are like here and here. And then I'm going to show you the very first polarized wave equation. For low frequency range, it looks like this. And the unknowns are. This and the unknowns are the solid displacement and fluid displacement, second order. That's the way we are used to. And because the pore fluid is viscous, the wave would, the energy will dissipate. How fast it dissipates depends on, okay, this is a porosity. This is a pore fluid viscosity. And this is the permeability. It tells you how porous things are. So, how do you? This is a low frequency. So how do you, this is a low frequency model. But what about the general frequency? Okay, so in computation, usually we rewrite the second order equation to be first order. It looks much cleaner this way. So as you can see here, now it's in velocity. And this Q here is a relative motion between fluid and solid. Okay. So let's forget about the meaning of it. So suppose I want to solve this problem. What what is the third thing which will catch my eye? thing which will catch my eye. You want to solve this time domain problem. The first thing which will catch your eye is actually this time convolution term. This is what there is for the Pore Elastic model. Yes? So this kernel here, alpha J, which encodes all the microstructure information of this polarizing material. So suppose now I want to do inverse problem, or forward problem, I have to handle this first. Forward problem, I have to handle this first. So, this one, to make the story short, it was proved in 2014 that you indeed, because when you try to compute this, you don't want to really save the time steps if I want to compute the memory return. This is the problem. However, in 2014, it was proved that this kernel actually has its own structure. Actually, it has its own structure. So the theory goes like this: in the frequency domain, this kernel can be decomposed like this. This is not an unsource. This is the proof because of the causality. So it can prove this way. So this tells you, what does it tell you? It tells you that there are several relaxations scale of your property. So the P, when we convert it back to tie domain, you will see. So essentially, So essentially, this tortuosity function can be approximated like this. Of course, alpha infinity, this Rk, Pk, and this are unknown. I mean, either known, if you solve forward problems, no. Inverse one sum, no. So because of this, and also this P, the pole here, they are negative. When you go back to time domain, what happens is that memory term can be written like this, just by inverse of LausTransform. And then you can use the usual technique of defining the auxiliary variable. That's the usual way. If you have this kind of representation, you want to handle a memory turn. One very efficient way is to define, okay, so why are we doing this? The thing is, we don't know what it is because this QJ is an unknown. This is a fluid velocity. However, we know the forward problem, we can compute the pK. The forward problem, we can compute the PK and RK. Yes? All right. So now. So the forward problem for poor elastic equation looks like this. So if I want to do full-wave inversion, this is a forward problem. Okay. Yes? So this is a forward problem. Of course, in reality, the first question you ask is, okay, so now you have this auxiliary very This auxiliary variable, right? Obviously, you cannot measure it. You measure something. So, okay, anyway, so this is the forward problem. But the good thing is, it doesn't have memory term anymore. It has this auxiliary value. So, this is how to show you numerical result by using this. So, a signature of poor elastic material is for elasticity, you have P wave and S wave. Pore elastic, you have 1S and 2Ps. 1s and 2 p's. P1 and P1P. But okay. So now this is almost almost here. So now misfit function. Suppose I want to do FWI for this. Yeah? So the first thing I, okay, so now I have to do, of course I put the sensor at different, on different spots. So I call them YR, right? I record things there. So what is this R here? This R is to take care of R here. This R is to take care of the fact that there are certain quantities you cannot measure. For those, you don't care. You can just put zero there and you don't measure. So those will not contribute into this loss function. Okay, so this is the R here. And to write, I showed you before, you can write this way. Okay, so this to show you that the forward PDE, so it's a constraint, PDE minimization problem. So this thing here. So, this thing here is what I showed you that system before. You can put source here. So, H2 is about the source. And this W is your solution, M is material property. And the last slide here, I just show you the adjoint problem is this one. As we expect that, the adjoint problem has similar structure as a forward problem here. This is a forward problem. So by doing this, then you can Then you can