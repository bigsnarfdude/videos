Of course, thanks to organizers for inviting me to this beautiful place. Buildings are beautiful, the landscape is beautiful. Even the weather was beautiful for me yesterday because I had to finalize my sites. So maybe not for you. And yeah, of course I would like to thank all of the organizers, Tony, Sudong, Ali, Monique and of course Observer Lair. Observer there. So anyway, I would like to thank Frederick. We are an observable state for the funding of this beautiful workshop. Once Ali sent me the schedule, I was a little bit shocked when I saw that I have to do our presentation of ensembles. And then I realized there are a couple of people in the audience which The audience, which are also specialists in ensembles, and they could maybe even do this better than I. So, I hope that I'm not too boring to these experts, and I hope I can entertain all the other people during the next two hours. If I missed some important literature, something like that, please feel free to point out that. That. So I collect some literature for the first and the second part, the first part. First part will be on linear ensembles, then there's this important copy prayer. And then there will be the second part on bilinear ensembles, and I've lectured some literature for linear ensembles, and some literature for the bilinear case, and a little bit beyond, maybe a I'm sure that I need something which might be important. So, people feel free to talk about that. And yeah, so let me start with some kind of motivation. Since I'm a mathematician, actually I don't need any real words, so to say, motivation, because for me these osformers are a beautiful playground because they are somewhere in between Somewhere in between finite-dimensional systems and infinite-dimensional systems. Of course, if the parameter set is infinite, then they are infinite-dimensional, but the operators which show up are usually bounded multiplication operators. And so they have much better properties as if you go into, let's say, partial differential or other type of distributed systems. What I mean is that so this is somewhere in between this more difficult. More difficult the stuff where you have unbounded operators and strongly constricted semi groups and all these things. And here the operators, at least in my setting, they will be always bounded. But nevertheless, there are some neat applications. At least people told me that there should be some neat applications. And the first part here, this, I'm not sure, but maybe Roger casted this term broadcast control. So think of Control. So think of a huge amount of systems, almost identical systems, and you are not able to address the individual systems. All you can do, you have to address all these systems with the same control. And I would say the probably most well-known example, or maybe this was the area where this ensemble control started to grow up. Control started to grow up is in MR spectroscopy. Because there you have a huge amount of molecules, almost identical molecules, in your liquid tube. And then you have two or three magnetic fields, a very strong one which is static, and two others which you can use to control all these molecules, but you will be never able to address these molecules individually. So you control with control with from the outside with one control you try to try you have to control all these molecules simultaneously and you can think of some some other scenarios which fit into this form of micro satellites or something like that you want to control and simply due to kind of resolution problems you are not able to address them individually or something like that and there's another And there's another application area which I call open robust controlling. That's a stupid terminology, but that's the only one which I could think of. And here the point of view is a little bit different. Here you have maybe an individual system, but there are some parameter variations. You do not know exactly the parameters of the system. So now you want to control this system. You want to do this by an open loop, which is, in principle, not a Is in principle not a good idea. At least engineers won't do that. They would prefer some feedback. But again, in problems which pop up in quantum mechanics, you know there is always this problem. If you want to apply feedback, you have to measure some state or part of the state, and then you are always running in this measure problem that measuring a quantum system immediately affects the state itself. So sometimes you So sometimes you simply cannot or you do not want to do this. And then you have to think how open loop controls are usually very sensitive. And then you have to think of the idea how can it be, how can I make these open loop controls maybe up to some extent robust? Wrong direction. Okay. Oh, yeah, okay, yeah. I mean, just for the terminology, I usually use. I usually use this term ensemble control, but we can also find other terminologies like simultaneous control or controlling families of systems and so on. So now a prime example from the Pani Mars temposcope. And the first part is about linear systems, but this prime example is not linear, it's a bilinear system. And I took some snapshots of I took some snapshots of a beautiful movie of Stefan Derhasser and the movie this, I would call it the dancing arrows. You will see in a moment why maybe some of you know. So you start with the plot equation. Everything is put together, but of course you can separate this so that you see, let's call it C5x, so that is of the form some. For some uh there is actually no drift in that case. So you have a me the matrix epsilon zero times T zero plus some other matrix, let's say omega zero t prime times f. So that's a typical bilinear system and of course Of course, the controls are missing. The absolute part is controlled by U1 of T, and that part is controlled by U2 of T. And then the system evolves from the Plot Sphere, or if you evolve on a sphere, yeah. You should start on this Dloch sphere, maybe you start on the North Pole and you want to bring your system testing. Bring your system that's a good integrator. For example, you compute a nominal control which does the job maybe time octa. Then you suddenly realize that there are some dispersion effects. There's something which is called a normal dispersion and a transverse dispersion. It's not important exactly what they are or how they are interpreted or where they come from. How are they interpreted or where they come from? For us, it's only important that these are uncertain, so that we get uncertain model parameters. So now we know that this omega, this ranges in that interval and the epsilon in another one. And now you can ask what happens if we now apply the control which was computed to omega zero or for omega zero and n. 0 of 4 omega 0 and epsilon 0 to these to the different values in this range. And that's the first picture. So let's say that all of the systems have the same initial value, the North Pole. If you apply the nominal control, you will end up here on the equator. If you have this LAMO uh dispersion, you will get something like that. You will get something like that, which is not that bad. But if you have additionally, or if you have only the transverse dispersion, you get something like that, which doesn't look very nice. And if you put both together, you will end up with this mess. So that's not a good idea to compute this nominal control and then apply this to all the systems. Yeah, okay. So there's another way to picture So there's another way to picture what happens. So this, in the other part, you can see again the system. So the work was done by Navin Kanisha and Stephanie Garse. So they discretized here this rectangle, which is formed by this W and epsilon range. And all of these systems, all of these arrows represent one. These arrows represent one parameter value in this range. Okay, and now and here in the upper part you can again see what happens if you apply the nominal control to all of these. And here in the lower part you will see what happens if you apply this very complicated control. And since they are uh huge different they are different time scales. They are different time scales. So, in the time scale of this control here, that's after one shot, you get the result, but not the result actually you want to have. And from there on, nothing happens here. But here's something going on. And finally, almost all of these numbers are pointing to the right direction. So they could solve this problem in this particular case. Problem in this particular piece. And now, so far, the problem or what we observed here simply by these numerics, that this bilinear sample seems to be approximately controllable. But the question is why? What is the reason why maybe this bilinear sample does have this nice probability and others not? So let's go back to. So, let's go back to the linear case. In the second part, I will talk about the bilinear case. First, let us try to understand the linear case, and I always start with finite also, so with only finitely many parameter values, which is in the linear case well understood. I simply quickly recall the results because later on, when we are dealing with infinite ensembles, we want to, so to say, to imitate some of these. To imitate some of these conditions in the infinite case. So, in the final case, you have a finite parameter set, you have a finite bunch of linear systems. They even don't have to have the same dimension. You have an input space which addresses all these systems simultaneously, and maybe there is some output, but since I will not But since I will not talk about ensemble observability, the output doesn't, I don't care about the output. And then the entire state space is simply the Cartesian product of the individual ones. You build up these block matrices and you get a huge higher dimensional linear system. And the problem is under one And the problem is, under what conditions is this higher dimensional system here, ABC, controllable? What do we have to assume for the individual ones? And that's, of course, well known. And the connection is called a parallel connection of these systems. And the result, and I'm not sure, but I found individual, someone to address this result to Robert. To address this result to Roger. I'm not sure about. Maybe I'm too young to know that. Maybe the older people here in the room know that this is really a result by Roger or not. But it says the following. So you have these three assertions. The onsort is controllable. The subsystems are controllable. And this intersection property of the spectra is satisfied. So the spectra are disjoint. So the sample controllability, of course, implies that all. Controllability, of course, implies that all the individual systems have to be controllable. If all the individual systems are controllable and this intersection property here is satisfied, then you get horsemen controllability. And in the case of single input systems, you even have equivalence. And it's really a simple exercise. Choose your favorite controllability test, maybe how to or something like that, and you will immediately see that this is a To be seen that this is X. Then Paul Fuhrmann worked on the case of M being greater than 1, and he came up together with Uber finally with a result of this form, which says there's a paper by Powell around the 70s. It was only for two systems, and then finally. Systems, and then finally, in the book of Powell and Moe, they generalized it to finitely many. And what you have to do, okay, everybody who knows Powell knows that he loves these factorizations. And so you know that AB is controlled if and only if these two are left co-prime. So what you have to do, you have to find a right co-prime factor. A right co-prime factorization of the transfer function. It's not the full transfer function, but it's still this rational matrix. And once you have found the right co-crystation, you can immediately check whether you have uncontrollability. All the subsystems have to be controllable. And these denominators here, they have to be mutually co-prime. In the single input case, you can choose as Eij simply the determinant of your or the characteristic polynomial and you see immediately that it boils down to the condition you had before. Okay, that's for the rest of the talk not that important because uh so far we could not put out any any ideas of this result. Result. I did not really completely or fully understand what the idea behind. Anyway, for us, this one here is more. Okay, now let's move to the countable or continuum page. So in the countable page, you have an index running over your face. An index running over your favorite countable set, so let's say the natural numbers, and you have infinity of these systems. And in the uncountable case, your parameter is varying in some set P and we usually assume that it's a compact set in R. And of course, you can go, you can do more fancy things, think of manifolds, or even an infinite-dimensional parameter space. But the systems you are interested in, they take this form. And you want to know whether these, well, I would like to know whether these systems are controllable or not. Do you have any constraint on the U or? So far no. Of course, it's a good question. What can be done if there are some constraints on you? But so far we analyze things only under the assumption that there are no constraints on you. And that's for the And that's for the entire talk. Okay, now the problem comes in. You have to choose the right state space. So far, there is no state space. In the finite-dimensional case, the state space was cleared. It was simply the partition product of these finite dimensions state spaces. But here you have the problem, or you can also interpret it as a kind of freedom. How to choose the state space. And in the countable case, I would suggest. In the countable case, I would suggest take your favorite sequence space, either one of these LQ spaces, assume that the sequence of matrices is in L infinity because the look ecosystem has the form of this A matrix can be pictured as this. Huge infinite block diagonal matrix, and then your vector is sitting in here, so you have some, let's say, x1, x2, and so on. And once you start multiplying, you have to guarantee that the outcome is again in your sequence space. And so, if you choose one of these, the best choice you can do is. The best choice you can do is you can choose A to B infinity. So which simply means that the sequence is bounded. Then it's guaranteed that the multiplication, you end up again in one of these spaces. Of course, you could also think, why not choosing the set of all sequences? Why not choose the state space X to be R to the power of m? Uh I have to be careful. Okay, here I think we I did assume that all the individual stage spaces are identical. You don't have to do that. But I think I did that. Yeah, you can see the dimension doesn't change. So then you have something like two to the top effect. So that and then this would be your sequence. Then this would be your sequence space. You can do this. I'm not aware of any results in that direction. The problem is that it's no longer a Banner space. And then everything gets a little bit shaky. It's a Frochet space and so on. So I think one should first understand these cases where you have some nice sequence spaces, which are Banner spaces, and then you can try. And then you can try to analyze the controllability of your countable or sorghum. And there are not many results available in that area. Not many people analyzed the countable page so far. You have an ensemble. How realistic is it? I mean, these are particle systems agents. How realistic? How realistic is it to think of them as having different A matrices? Like, usually you have copies of the same thing. Usually, you have copies of the same thing. If you have any of them or are they different? The A matrices are different, they're not going to be much different. I totally agree. And if in the continuous case, we will always assume that all of them are identical. But here it's just purely from the mathematical point. Purely from the mathematical point of view, you don't have to really assume that they are all identical. So in the finite and the comfortable case, you don't have to. Most things go through, but I agree that in the applications I presented, all of them usually would be identical. And then the controllability, if they're identical, of course, does not apply, which means that you cannot individually. Which means that you cannot individually control them. But typically, in such cases, you want to collectively control them. Yeah, but you sometimes you can even collectively control. Okay. Okay, just one remark. I always assume that my matrices are complex and are also a problem. Complex, and I also have always assumed that the individual state spaces are complex, and then I allow complex controls. And everybody knows if you have a real system with real controls, then it is controllable if and only if the corresponding complex version is controllable. But this makes things sometimes easier because once we come to spectra and things like that, we don't have to switch between real and complex variables. Complex variables. So now comes the continuous case, same game. Choose your favorite function space. These function spaces are again one of spaces and again a good idea or a good choice for A would be to choose A to be a continuous function in the parameter theta, which then, if this is a continuous function, it's guaranteed that under multiplication, you stay in that space and You stay in that space, and under matrix three, you also stay in this space. Of course, you can also, for these spaces, you could enlarge this to L infinity, but L infinity is a beast, so I did not touch that. And the same here. In principle, of course, you can also choose L infinity here. There's no reason why you should not do that. Oh, actually, there is a reason why you should not do this. Because Because finally, we want to have approximate controllability, and the reachable set in this setting here at least is always a separable subset. So then we take the closure, and the space we get is a separable subspace in our huge state space. And this state space for p equal to infinity is a non- It's clearly the paradigm for a non-separable space, and so there you have never you have no chance to get approximate controllability simply because that infinity is much too big. Sorry, I missed one of the assumptions on P. But what are the assumptions on P? The assumption on P is that it it was that uh uh a c a compact second order. And if of course in here you you want to have that interior constant. And I hear, of course, standard libx forces. You could do this also for some other measures, regular measures. Okay, that was the wrong direction. Okay, so that's for the rest of the talk I use this unified notation so I don't have to switch between the counsel and the uncounterple. The countable and the uncountable case, and the solvent problem then finally boils down that given two initial states here in your function or in your sequence space, you want to find a control, or you want to find for every epsilon greater than zero, you want to find a control such that this is less than epsilon. And that's of course the same as saying that if you think of If you think of or if you define by your matrix A theta a multiplication operator on your function space in that way, of course you have to guarantee that it's invariant under this multiplication operator and you define also here a kind of multiplication operator for this P operator. Then you can write down your ensemble as an infinite. Essemble as an infinite-dimensional linear system of this form. And our assumption will be always that x is a boundary space and that A is bounded, B is always bounded because it starts from a finite dimensional space, so to case. And then you can write down some first more or less standard. More or less standard, I don't want to say trivial, but some standard observations which say: okay, all of these statements are equivalent. You have this ensemble control. So the system is ensemble controllable, which is nothing else than saying that the system AB is approximately controllable. It is only really a reform of which, nothing here. Then, of course, controllability. Then, of course, controllability is in this case similar to that the range of the input to state map is the closure of this map is the entire space. And you can do this for any T. If you work in operators getting unbounded, that's not clear that you have this property for all T. It can happen that for some T's at the very beginning, it's not satisfied and then suddenly you have everything. And then suddenly we have everything. Here, it's the same proof as in the finite dimension. And okay, and then we also have this generalized Kalman condition. You can reformulate this generalized Kalman condition in this kind of approximation conditions. So here you apply complex polynomials to your operator A and then you evaluate these operators at the At the columns of B, of your B matrix, you take the span of all them, and this should be, again, the entire space X. So this is, but this is, if you take a close look, you see there's no difference. And this is simply interesting because sometimes results are Because sometimes results are, so to say, hidden in the literature under cyclicity conditions in the operative theoretical communities. And this is simply to remind you that this is nothing else than to say that this operator A, or that the vectors B1 up to Bm, these are the columns of B, that they form a cyclic set of the operator A in the sense that The sense that applying A and all the powers of A to these vectors and taking the standard A. So in the single input case, it simply says that B1 is a cyclic vector for the operator. Okay, these are first observations. So problems coming in due to the fact on the one hand the operators are in this setting are usually bounded, that it's nice, but Bounded, that it's nice, but since they are multiplication operators, usually the spectrum is no longer discrete. So that's the problem. That makes things, on the other hand, sometimes complicated if you want to refer to some standard results in books like Curtin Swark, or there's a book by Paul Fuhrmann, and so on. So they often assume that they have a kind of eigenbasis of accountable. A countable eigenvector a basis of eigenvectors, a countable basis of eigenvectors. And of course it's well known that in most of these infinite dimensional settings the you cannot expect exact controllability, you have to work with approximate controllability, but this we already built in But this, we already built in the setting form itself. And the fact that it's non, in general, not exactly controllable simply results from the fact that this B operator is a compact operator. So if you plug in bounded controls, you end up with a pre-compact set. And then simply you consider all the countable. Consider all the countable union of all these P-compact sets which you have, if you get, by simply choosing balls of radius n in your control space. And then finally, you see that the entire regional set is a countable union of pre-compact sets, and that's always this won't give you the entire space. You the entire space because it's a Banach space. This is this Bayer, this result by Bayer. Bayer, there's category theorem. Okay, now let's come to some, yeah, not, okay, no, first of all, no, that's still a general result which we derive. It it's uh it's an infinite-dimensional version of this uh parallel connection you have seen before. But only for written for two systems, okay, you can of course generalize this to more than two systems. And here, let me actually know that this one. So we know that if the individual ensemble If the individual ensembles in the finite dimensional case, if they are, no, yeah, if the individual subsystems are controllable, and if this condition, this intersection condition holds, then we get ensemble controllability. And now this is this results as resulting. This, so to say, it regeneralizes this idea to Here to the infinite-dimensional case. So think now of A and B and A1 and B1 and A2 and B1 of infinite-dimensional operators. You know that these two pairs are approximately controllable, no longer controllable, but at least approximately controllable. You have this intersection property and yeah. And yeah, this is a technical condition which I think can be, if one does the proof, or if one slightly tries to improve the proof, I think we can get rid of this condition. But this is an interesting condition I think you cannot get rid of. It says that it says they should be non-separating. So the picture you should have in mind is Have in mind is this is the spectrum of A1 and that's the spectrum of A2. That's allowed. What is not allowed is that That this is the spectrum of A1, and here this is the spectrum of A2. That's not all good. And that's due to these non-separating conditions. Non-separating means that if you remove the spectrum of A, you have still a connected set in the Set in the context. Sufficient, right? This is only sufficient, but that's not allowed, right? That's not allowed. This is not allowed due to this. They are in the sufficient condition, which is sufficient. It probably can be replaced with some index condition, right? Because the screen replaces this index of this directory. I'm not sure, maybe I mean. Is that a standard definition of non-separating? And that one may be found in the literature. No, I hope in the complex literature. I want to think about there being some sort of homology or something. Simply connectedness. Yeah. Of course I think you can reformulate it in in these terms, but I think standard textbooks of complex analysis. Books of complex analysis, they usually refer to this one. I think I just missed, you said that non-separating means that when you remove one of the spectrums from the space, then the remains has some property. What was that? If you remove either the spectrum of A or the spectrum of A2, then the resulting set The resulting set should have only one connected component. Well, let's say we were going to do like a cohomologist present. Speech components have a compressible. It should be some index conditions, but it's like it's index theory for yeah, and I I guess there's some but uh what is it if if this would be the spectrum of A, you will see in the mono payment in a moment why I need that. Agreement in a moment why I need that. If this would be the spectrum of A, and this would be the spectrum of A1 and A2, this actually wouldn't bother me. I have to make this more precise. If this part or if this here, a spectrum of A2, has a part inside here, then you have a problem. And why is this the case? Because now let's think of this case here. Let's think of this case here: that the spectra are separated like that. How do you prove this? What is the idea of proving this? You choose a holomorphic function on the union of these two. So choose a holomorphic function, let's call this G, and slightly enlarge these sets. This and this and this. And this is then the union of these I call omega. So choose a holomorphic function on omega to see if the property g of c is one if you're in here, let's say oh this is omega one and zero if g is in omega two. That's your function you're interested in. From this one From this one, of course, you can immediately build one minus, and then this is a function which has, of course, the convergence property. So, and define h to be one minus c. C. The next step is now choose complex polynomials. Call them P or a sequence of complex polynomials which approximates A in the uniform topological. uniform topology on on omega of course then one minus let's call them q of n which is one minus p of n is then a sequence which converges to uh which converges to to H sorry. To H. And can you immediately see if you have this condition here and you want to have omega to be omega would be something like this, omega 1, not omega, but omega 1. Omega 1 would be something like this. And if you know that your polynomial approximates Approximates the function one on this ring. Then simply do the maximum principle. It also approaches the function one in here. And then you cannot do this construction. And that's the reason why you want to have to separate these sets. We want to separate these sets, so we want to avoid this condition. If this condition here shows up, then you could simply fill this out. This wouldn't bother you. But no parts of this spectrum have to be resident. So then you have this. This is just from the separating point conditions for only. Conditions for polynomial estimation, right, from weight stress. So if the controls can be used to separate spectrum in each individual system, and then the system is at simultaneously more. I think that's the underlying idea, Trent. Yes, so far it's only a result that says that I that guarantees that some that some parallel connection That some parallel connection is again controllable. But we will apply this in the direction you mentioned. Yep, in a moment. Right, so now you are having sufficient conditions, and in many cases, unnecessary conditions can also be contracting. I think we will get to that later. Okay, thank you. And now it seems no no truth. Since now choose some some function, some some some some y in uh x1 and some y2 in x2. And you know that there are polynomials p a1 let's assume that you have a Let's assume that you have a single input system here. So you have there is a polynomial P1A1 times B1, which is epsilon close to Y1. Let's call this, P is not a good idea. There's a P, there's a P. Maybe I call this R, and here's some S, some other polynomial which Which does the job for y2. And now you can put things together. Now you can define a new polynomial. Call this new polynomial maybe W. So we can define a new polynomial which is the same. find a new polynomial which is of the form. So let's see W n Pn R plus Q N S. And this one now, if you plug in in this W n, now the new operator A1 A2 and you apply this to When you apply this to B1, B2, you will see that this will approach W1, W2. So you will have the approximate controllability of this tree. And all these topological conditions are simply to guarantee that one has these polynomials. Okay. So now Now let's analyze the case where here where we choose as our state space, this was a general result so far, but this now is the case where we choose as our state space the complex, the continuous functions from P to Cn. And this is a trivial observation that if you That if you know that your ensemble is uniform, you call it uniformly controllable if this is your state space and you are working with the maximum. And the standard terminology is now uniform ensemble controllability. And you know that if your system is uniformly ensemble controllable, on I think that's a type, but I think here should be simply P and here should. P and you should be simply a compact subset of P. It's not important that you have a compact interval. But then you immediately see that the uniform or soft controllability on the bigger set implies the uniform controllability on the smaller set. And if you want to prove it, maybe you should use something like Keats' extension theorem. So you have a Extension theorem. So you have a continuous function on this smaller set, and since the smaller set is compact, you know that you can extend it to a continuous function. On this bigger set, then you use the uniform approximation, that the uniform approval control on the bigger set and margins on it. Okay, it was trivial, but we have from that we immediately have some nice, necessary assumptions, necessary conditions for ensemble controllability. We know. We know first that for every fixed theta we have to have that these systems are controllable. We are in the single input case. Yeah, here. This immediately, this is of course equivalent that all the eigenvalues is simply standard linear theory, that the eigenvalues have to have multiplicity two magic multiplicity one. And then again, the spectra, if you choose theta one and theta two different in your set P, the spectra again have to have a trivial intersection, or an empty intersection, could be the same. Yeah, and now the interesting thing is that P has no interior points. And that's not a result from Th that's not a result from these these three, they are in Darwinic results from from the lemma before. This is not uh this is not straightforward to see from the lemma. Here it says what one should analyze maybe either is this the case. It's it's it's of it's slightly re of course it's it's also related to to the lemma, but it's more complicated and I It's more complicated, and I try to convince you only for a simple case. One can use this. Assume that your set P, assume that your set P is something in the complex plane, and assume that your set P has interior points. And assume that, let's say, zero is an interior point, and after some scaling, you know that. Scaling, you'd know that the uniserver is in here. That's the uniser. Okay, if your system is if your system is uniformly ensemble controllable on that set, it has to be uniformly ensemble controllable on the unison by this. on the units by this uh reduction result every one. Now choose the following function. Now let's consider only this is now our new our new set P and let's see whether we have a uniformly a uniform or something. Uniform ensemble controllability on that set. So choose as your function you want to approximate, choose the function 1 over z. That's a continuous function, restricted to the unit circle. That's of course a continuous function on the unit circle with values in C. And by By the assumption that the system is uniformly ensemble controllable, we can approximate this here so we can find a polynomial. Sorry, I missed something. I try to convince you that this is true for the case that our operator the case that our operator that a of theta is nothing else than multiplying by theta. So you have to show that you can reduce the problem to this situation, that this is here the multiplication operator by theta and the set you are working with is simply the unit circle. So maybe we should write here the Should write here instead of C D such and D and the B vector, the B vector can be then easily normalized to 1. So then the uniform orthogonal controllability of this system on that set means that you can find a polynomial in set such that this Such that this minus this will be arbitrarily small and then somewhere this is less than some absent. But this But this was this function, so you have P of C minus 1 times C is we multiplied this equation by C, and so we have C times epsilon 0. So we can, since C is on the unit circle, we have still here less than epsilon. Okay, now if you choose epsilon small enough, this is the situation on the boundary, on dd. If you choose epsilon small enough, let's say smaller than one, let's say one half, then you have, you see that this function here is on the boundary, the value is less than epsilon equal to one half, but in in zero, But in in zero, this one here, this is a complex. This holomorphic function. And if you evaluate this at zero, so this here evaluated at zero gives you one. That doesn't make sense. That contradicts the maximum principle. So let's and the whole the whole work in that proof is that you really you Is that you really have to show that you can finally transfer the original problem to this simple example, where you can see that you cannot have uniform controllability. It just does not fit with the maximum principle of complex analysis. Yeah. If I work with the state space of continuous functions, I always assume that A is has to be continuous to guarantee that I do not drop out of this thing. Okay, what is this now? This is a first result where we have an if and only condition. If and only only condition, if and only if condition. But this is only for, as you can see, for a scalar pair, single input and scalar. Single input and scalar you can completely characterize the uniform ensemble controllability. It's equivalent to the fact that A is one to one and P is not, B does not vanish on T. And maybe I can convince you during the last five minutes and then we can continue with higher dimensional systems after the break. So what is the idea behind the implication from A to B is given by the former colony. Of course, this is necessary for the end. But the interesting thing is. So the interesting thing is why is this sufficient? So for simplicity, really for simplicity, let's assume that A is defined as our compact set, a compact set in C. So let's assume that this compact set is simply an integral. Then, and assume that after some scaling, since P does never vanish. Since B does never vanish, we can scale B such that it is equal to 1 on this compact set. Then consider the approximation condition we have. The approximation condition in a couple of slides ago was that applying complex polynomials to our operator and then evaluating this at the columns of the B matrix, this set This set, here's a one due to this assumption. So what we have to show is that this set here is equal to the set of continuous functions over this interval theta one, theta two. Now, the map A is one-to-one written here. It's on a compact setup. It has a It has a continuous inverse. And we can use this continuous inverse, so simply call this guy here W, and then you can easily see that you can transform this approximation problem to this approximation problem. And now it's in standard form to apply Weierstrass. This is again a compact subset, and now you're. Set, yeah, and now you apply simply Meierstrasse, which says, okay, I can approximate. Actually, I do not here, if this is again real, I do not even do not need complex polynomials, I can restrict to real polynomials and I get this condition which was necessary and sufficient for uniform ensemble controllability. Now the more the delicate More the the the delicate part is what happens if uh this does not map to R. So if the image of this one may look like this So the image may be something like this in the complex plane and now you have to show that any continuous function here on this On this riddle, here can be approximate can be approximated with complex polynomials. And there's this nice theory by Mermilian, which says actually you can do this if the set where you want to do this approximation. If it's, let me see. If it's let me see, if it's again, yeah. The the statement is if the set we want to do this approximation, let's call this S, if C without S is again connected, so that S is not separating, then Meghan says every holomorphic function, Function f from S to C can be uniformly approximated by polynomials. And we use this result because in our case, S has no interior points. So this assumption that F has to be, did I say that? F has to be holomorph. So for the interior points, For the interior points, you have to have a holomorphic function, and the boundary has to be continuous. If f is continuous on S and holomorphic in the interior of S, and you have this non-separating condition, then Maguire says you can approximate this with complex polynomials. And this is exactly here the case because we have no nuclear loop. The case because we have no nuclear parts. So this holomorphic condition simply drops out. Okay, and I think that's a good point to stop and to reconvene after, I think, have an hour or something that's any question? I had a question about one of the applications on your first or second slide. So I'm sorry. But I don't, maybe this is totally