So are you seeing the wrong slides now? Yes, we do. We could see your slides. Thank you. Not now. Okay. Give me a sec. This should be good now, is it? Yeah, it is good. Okay, thanks. So yeah, hello, everyone. So yeah, I'm Nicolas Deutschmann. I'm a postdoc in Maria Gesey's group at IBM E. Directly Greg Gesis group at IBM Research. And I'm going to talk to you about a project which is still a work in progress in which we try to quantitatively assess interpretations based on attention. And so this is work I've been doing together with my master's student, John Attenhab, and which we're going to get into. So I mean, I think I don't need to introduce attention to you. It's a tool for machine learning and for deep learning that's been really popular in the recent. That's been really popular in the recent years, and that has seen a lot of successes, especially in particular on the type of data that you can cut into smaller pieces that you can then see as a collection of instances. So it's been successfully applied in NLP, in graph neural networks, where you can see a graph as a collection of nodes, and also for set machine learning, where the idea of a bag of instances is really clear, that you have a collection of elements. Elements. So attention is really intriguing for the context of interpretability because you have this weight distribution over instances that is supposed to reflect the importance of different instances inside of your data point composed of multiple instances. But of course, there is a bit of anthropomorphism that is going there, which is pretty common in machine learning when we use this type of nomenclature to give a kind of human. To give a kind of human psychological notion on something which is a mathematical object. And so the question is: do these attention distributions actually reflect some kind of distribution of how much signal is contained in the instances, or is it us putting the interpretation in something that's not so clear? So it's definitely not a new debate, and there's been quite a bit of discussion on this topic. This, uh, in this topic, and in particular, in the field of NLP, where people have been focusing on transformers, which have been very successful. And so, you probably have heard of this paper, Attention is Not an Explanation, which I've then had a reply, which is attention is not explanation. And what these papers have been focusing on is mostly formal arguments or systematic approaches, which focus on properties of attention at the mathematical level. And we think that one of the ways we should move on from this discussion to try to have a definitive answer is to try to evaluate the explanations themselves exactly as they are on the data instead of trying to understand the properties of attention as a mathematical model. So this is a really hard task in the context of natural language processing because even in a sentence attributing instances, attention for our importance to individual words for a classification task, For a classification task, for example, is not a well-defined task. So, if we want to evaluate how well attention attributes, how much signal is carried by instances in a collection, we should probably first focus on something that is a lot simpler. And so what we propose is to focus on multiple instance learning, which is a setup in which we can really quantitatively evaluate how well. Evaluates how well attention does at highlighting which instances in a collection are the ones carrying the signal. So, just for those who are not familiar with the notion of multiple instance learning, it's a set machine learning setup in which you're trying to classify bags of instances, so sets of instances with a finite number of elements in them. And there's two categories of bags: there's positive bags and negative bags. There's positive bags and negative bags, and there's two types of instances that are contained in the bags: positive and negative instances. And the label of the bags is given by the fact that the bag is positive if and only if it contains a positive instance. So here in this case, in the example, you can see negative red bags that contain only negative red instances, but then the blue positive bags will contain a mix of red and blue positive and negative instances. And negative sensors. And so, this is a weekly supervised learning setting in which only the bug labels are available at training time. But of course, what we're interested is understanding what happens at the instance level. So just to give you an example of how this could be structured in real life, like a bag could be a computer network, instances could be individual computers in the network, and then a positive instance would be a computer infected by the virus. And now you're trying to predict whether. To predict whether a computer network is compromised by some infection. So, the type of architecture that people have been focusing on to try to perform this classification task at the bag level using deep learning is using a permutation invariant neural network. So, the idea is pretty simple, which is that you have these bags which are unordered, and so you want to have as an Ordered. And so you want to have as an inductive bias that your model doesn't care about the order in which you feel it the instance. So it's something that was pioneered by Zahir and co-authors in which they introduced peak sets, which are something which is pretty natural by now, where you process each instance individually with some kind of featurizer, which would be a neural network, and then you aggregate them in a permutation invariant way. So the original proposal was using a maximum function or a mean function, which gives Function or a mean function, which gives you a fixed size representation that you can then process to get the finite prediction. Then the use of attention was proposed in this context was proposed by Itzen Welling, in which they suggested using an attention weighted average, which is again permutation invariant, but now allows you to have some kind of dynamic weighting, which should be reflective of what weights the. Of what weights the UBU instances carry for the final prediction. So here, this is not about self-attention. The attention is with respect to some reference vector, which are learned as the task goes on. And so, of course, this is a specific subset of all possible attention problems, but it's something that actually has a lot of applications, in particular in the biomedical science. In particular, in the biomedical sciences. So, there's been actually a growing interest for these multiple instance learning type approaches, in particular for instopathology. So, it's the medical field in which people look at microscopy images and try to predict the clinical status of patients, and in particular, of cancer patients. So, the reason why standard machine Machine image analysis techniques don't work out of the box for these types of images, is that they're humongous and that they're not regular images. So you get these very non-rectangular patches of tissue that have holes in them because of artifacts and because of the acquisition method. And so the way people go about it is that they cut these huge images into small patches that they then process one by one, essentially. And so what you get then for a given patient is a very For a given patient, is a variable number of patches which are not always at the same positions. And so you need a way to handle all of these different patches which are not perfectly well organized. And multiple instance learning is really a good setup to do this. Another very promising application for this type of problem is molecular biology with a lot of single-cell data that is available at the moment. So for a given patient, you get measurements on thousands of cells at this time. Measurements on thousands of cells at the same time. And you have interesting labels that you want to predict for the whole patient. But the only information that you have is the properties of the cells which form a dyslexic colour. So the need for interpretability in these two kinds of applications is, first of all, the classical safety-related type of concern that people have about using machine learning in this safety critical. In this safety-critical context, where you want to make sure that your model is not doing complete nonsense, but also there's a lot of interest to apply these for hypothesis generation, where basically you have complex data that you don't know how to analyze, and you hope that your model is capturing some pattern, and you would like to then understand what are the patterns that the model understood, so that you can then try to understand the biological system that it so while we want to give more. Why we want to give more a deeper look into these attention mechanisms is because there's really no formal guarantees that attention will follow the pattern that we intuitively expect, that the instances that carry the signals are going to be the one that have the highest attention. And if this is not the case, in the best case, if your model fails, then you'll have a lot of noise in what you interpret as an important instance is. You interpret as an important instance is, and this might lead to wasted time or to wasted efforts. And in the worst case, your model is feeding completely random or wrong detention maps in the sense of interpretation for signal pairing. And then if you are trying to generate hypotheses, this might lead you on a wild boost chase where you're trying to, this might lead to the design of experiments that people will do in wet labs, and this will cost a lot of money and efforts. And of course, in the clinical And of course, in a clinical setting, this is a major issue because you might be missing hairs that your model is looking. So, okay, so of course, so can we trust these attention distributions? Are they making mistakes? And if you look at the instance level, it's pretty clear that the models will do mistakes, right? And so, if you look at individual data points in your problems, you will pretty easily find. Problems, you will pretty easily find individual ones on which your model is doing nonsense. So, here, what you see on the right is a simple synthetic problem in which we take n list numbers and we put them into bags of 10 numbers, and we classify as a positive instance a bag that contains at least one number nine. And both of the bags that you see here have been classified as positive, but only the first one gives a high attention. First, one gives a high attention to a nine, whereas the other one gives a high attention to a one, and the other and the nine that is in the DAG actually gets a very low attention. So, of course, I mean, we're doing machine learning on data that is not perfect, so there will always be some error rates that you have, and you might want to quantify it. But the problem is actually deeper than this, because you can get model-level therapies. So, basically, what can happen is that the attention What can happen is that the attention distribution that you will get on your instances in some of the models that you train will be completely reversed compared to your naive expectations. So, what you have on the right again is two models that we train for the two lines. And what you see is the attention distribution for the positive and negative instances in positive and negative bags. So the negative bags themselves are not super interesting because they contain only one type of instance, but the positive bags are the one that we want to. The positive bags are the one that we want to get. So, the top model that you see here has exactly the pattern that we expect. The negative instances get the low attention and they're well separated in terms of attention compared to the positive instances that get a high attention. But on the other hand, the model at the bottom does exactly the opposite. So, if we try to use it in order to find what are the interesting instances that carry the signal, we'll get completely false. And this is not something. And this is not something which is related to the fact that the model is performing poorly in the task it's doing. Actually, both of these models have super high accuracy because this is a because we're doing this on a pretty simple context. So in light of these possible failures of these models, of course, maybe the safest way would be to just completely drop attention, but a lot of anecdotal evidence that people have gathered over the years seems to say that. Gathered over the years seems to say that most of the cases models are doing a good job. And we need to basically try to understand how often are they doing wrong and how can we avoid this pathological behavior that will lead to wrong interpretations. And actually, multiple instance learning is a context in which we can really quantitatively evaluate how well Quantitatively evaluate how well this attribution of important instances is doing. Because we can really formulate the problem of evaluating the interpretation as a binary classification task. What we need to do is to classify instances as important or not important based on the threshold of the attention. And if we generate synthetic multiple instance random data sets, we have ground truth positive and negative instances. And since it's the presence or And since it's the presence or absence of a positive instance that is bearing the signal, it's really the positive instance that are the important ones and the negative instances that are the not important ones. So we can use attention with the threshold to classify positive and negative, and we can use usual classification metrics to evaluate how well our attention maps are doing at separating the positive from the negative or the important from the non-important. So, as I said, this is still work in progress. So, for now, we're working on a very simplified synthetic setup in which we draw. So, in the natural multiple instances learning setup, we have two distributions, one for the positive instances, one for the negative instances. And then we will choose a back size, so the number of total instances and a fraction of these instances that is positive in the positive backs. And so, in our case, the two populations that And so, in our case, the two computations that we chose are just n-dimensional Gaussians, which are somewhat separated but a little overlapping. And basically, this is how we constitute our simple multiple learning setup. We can extend the problem by a little bit by making it slightly more complex, and basically by having multiple important populations. So instead of having, so we'll always have a negative unimportant population, which will always be there. Population, which will always be there as a background. And now we have multiple populations, so we can play logic games for our prediction tasks. So, for example, we could say that an instance that the bag is predicated contains elements of population one and population two, or population one exclusive or population two. And this gives a more complicated pattern that the model has to. And this is not a purely academic example because this is actually something that, in the case of Something that, in the case of predicting tumors or predicting the invasive neck of the tumor, is really interesting because what is suspected to be one of the factors in the evolution of complications in cancer is the case where you have heterogeneous tumors in which you have multiple populations that can be either collaborating, which would be and supporting each other, which would map to this end logic problem, or competing against each other. And then the tumor would only grow if you have only one of the two and which would map. Have only one of the two, and which would map to the exclusive or problem. So, before we go really into the analysis of the attention maps, we just want to make sure that the model is not doing complete nonsense. And so the kind of a sanity check that we can do is to check that our models are actually using the positive instances to make the prediction. So, the way we assessed this was to look at our negative tags and our positive tags. At our negative bags and our positive bags. And so we train the model with a fixed setup of the data sets. But then at test time, we feed the model the bags in which we progressively remove some of the instances. So for negative bags, of course, there's only negative instances. So we can progressively remove the number of, basically reduce the size of the data of the bags. And you see that basically this has absolutely no impact on the performance of the model, which kind of makes sense, right? You have just a number of negative. You have just a number of negative bags, and the model is looking for the positive one and it's never finding one. But for positive bags, now what's interesting is that we can progressively remove the amount of positive instances that are contained. And you see that as we do this, the performance of the model is slowly decreasing, meaning that there's less and less signal that the model can pick up and therefore make it recognition model. And again, this is the case for all of our models that achieve a good performance. Achieve a good performance. And so this is really independent on the attention distribution that could get it. But on the other hand, we do get models that perform well and that are clearly using, you know, for the bag level prediction, are using the positive instances, but yield misleading attention maps. And so what you're seeing here on the right are the Uh, the area under the rough curve for ensembles of models where we ensemble the attention to predict whether instances are interesting or important or not. And what you see is that in the case where you have a single model, which is the first bin for both of the plots, you have quite a few instances that screw really poorly at the prediction and that really score worse than a random model. Than a random model. It's happening for about 10% of the models that we trained. And as I said already, the fact that the attention is not performing well is not correlated with the fact that the model is performing well at the bag level task. So we don't really have a good way to select away at least 10% of poorly behaving models. But what we can do, because we have a majority. But what we can do, because we have a majority of well-behaving models, is that we can perform an ensemble basically. And so this will smooth out the poorly behaving examples as long as they're a minority of the training samples. And hopefully, I mean, basically, as long as this is the case, we can protect ourselves from these random, poorly behaving models and we can reach a satisfying level of trustworthiness. Level of trustworthiness of our explanations. This is not always the case. So, what I'm going to present here is really definitely preliminary, and I think we need to do some more checks of this before I can entirely confirm this. But this is really interesting. So, I thought it was worth mentioning. So, there's one case, which is the logic problem, where we have an end. So, we have three populations, just to remind you, where there's a background population. Remind you where there's a background population and two interesting populations, and the bag is positive only when the two populations are present in the bag. And here, the attention maps are actually doing mostly nonsense in the term of attributing what is important. And it's more than half of the models that are doing poorly, and so ensembling really doesn't help at all. And again, so this is something we really don't understand. So, this is something we really don't understand. And as far as we can tell, it's not related to the difficulty of the task because the performance of the model at the bag level is doing just as well as for the exclusive OR case, which is doing much better in terms of interpretability. And intuitively, I kind of have the feeling that it's easier to capture the pattern that you have the two instances present than the exclusive OR case. So to me, the fact that this is happening is. To me, the fact that this is happening is kind of surprising, but also it's, as I said, it's quite preliminary, so this warrants further research. But if this turns out to really be the case, I think this is really worrying for the use of attention for interpreting what is important and what is not. Because in real life, you don't know what the actual logic problem that you're going to have is, right? This is something that you might want to discover on your data. Discover on your data, and that is not given to you, right? If you're looking at cancer images, you don't know if you're going to have these two populations of cells that are predictive of survival, or if you're just going to want to look at whether the tumor is invasive or not, based on some local patterns. So, this is also something we're looking at, but these very much in the early stages, which is trying to understand what the logic rules of the problem we're looking at are based on what our model is. Are based on what our model is doing. And so intuitively, you can kind of understand that the performance of what you want to, of your model should depend on the number of instances you're providing. And especially if you focus on very few instances, you have like a threshold in the number of instances you need in order to solve the problem at all, even at the fundamental level. Is it possible to do or not? So the multi-person learning case. So, the multiple instance learning case should work with one instance, right? Then you can have like one positive and one negative or one negative, and the one negative should be classified negative, unimportant, and the one positive negative and the one positive should be classified positive. The end case, on the other hand, should need at least two instances because you can never classify a bag positively unless you have the two bags. And we're seeing some. We're seeing some patterns that seem to say that we can look at the accuracy or other classification metrics when we feed the model very few instances, because we see this gap in performance when we go from one instance to two, basically in the case where you should expect two. So this is something that we have to make more rigorous. And so basically trying to understand how to quantify how much a gap there is and how to. A gap there is, and how to significantly separate it from the case where you have no gap, which is basic multiple learning. But hopefully, this is something which might be very useful to kind of eliminate what is the underlying problem that you have when you're trying to do these tag-level justification in biology. So, this is essentially all I have to say. So, just to summarize what I told you a bit, set machine learning and multiple Set machine learning and multiple consensus learning is a great context to have a super controlled environment in which you would like to evaluate attention-based explanations of models and of data sets. So most of the time for many problems and for many models, attention maps behave in the way you expect, but you can randomly have some models that have good performance, but poor explanations. And a way to And a way to find this for many cases is to use ensembling. But up to further explanation, we seem that some tasks, in some tasks, actually, these attention patterns are really unreliable. And then this might be a problem for real life where we don't know what the actual underlying task is. So as I said, this is work in progress. And so we still have many things we want to explore. First of all, we want to go to more complex data. And one of the things I find really interesting would be to go. the things I find really interesting would be to go into graph neural networks where we would go from trying to understand the node level importance to because of the way graph convolutions work we could try to understand what local structure what neighborhoods of the node are important and this is again super interesting for biological image applications where you're not necessarily interested at the cell level but maybe at the tissue level and you want to aggregate local structure. For the work we've already done we want to For the work we've already done, we want to also focus on the actual details of the attention mechanism, for example, the number of reference vectors, and also where we're doing this averaging, this weighted averaging, either in the latent space, but we also do it in the final prediction layer. And I suspect this might include things in terms of independent ability. And one thing we really want to do further is to try to be able to extract logic rules based on looking at the model. Based on looking at the model behaving on a few instances. So, this is essentially all I have to present. So, I'll thank you for your attention. This is the only pun I put in this presentation and open to any questions. Thank you very much, Mika. Okay, so do we have some questions for Nicola? You can't go here, sorry. You want the last minute, you can also come here. Yeah, so I just have a very short question. So you have to have what was the type of your synthetic data? Was it also image data or was it very simple synthetic data set? Okay, so for now, so we are basically Now, so we are basically moving on to image data at this point. So, what we've done quantitative analysis on for now is really just drawing random vectors in some vector space. And now that we have basically all the machinery to perform the analysis, what we want to focus on now is both using image data and using molecular data. So, basically, using single-cell type of transcriptomic or proteomic measurements, we can create Measurements, we can create synthetic data sets in the same way and perform also this kind of analysis. Can you hear me? Can you hear us? Yeah, I can. Okay, I will try. So thanks, very interesting talk. I was wondering about your end case. So you show basically that ensembles do not work. That ensembles do not work because if all the models have the wrong intentions, then ensembling doesn't work. But do all the different wrong models basically focus their attention on the same things? Or could you use this to say, okay, if they don't agree, then I shouldn't trust attention here? Yeah, exactly. So this is something I haven't talked about and that we're looking into, but indeed, I suspect very much that this is the case. So we're basically on the process of quantifying this. The process of quantifying this, but I think this would be a great way to diagnose a possible problem. Thank you. Do we have any other questions? Well, I had a related question actually, Rita, but you already answered. So, like, the combinatoriality of this that is that you find in those texts, like if you check, if you analyze this, because I guess that's some, I don't know, like you mentioned, some variable. I don't know, like you mentioned, some variables can be reused, or the model can pay attention on something always the same in different scenarios, but with other variables from time to time. So the combinatorial behind the instances that you find in those attention patterns. I'm not sure I understood the question, sorry. Can you come closer to the mic? Can you come closer to the mic? Because there's a bit of echo that makes it really hard to understand. Or, Maria, if you can repeat that also. Okay, the question is about the combinatorialities of the instances that you find in those attention bags. And I think that this is relevant, especially for those logic problems. So imagine that you have some instances that you find to be important in a specific scenario, but they are maybe equally important in others. Are maybe like equally important in other when they are together with other instances. So, you know, like how those bags are actually populated. Maybe like the one that are more explainable are more homogeneous and then the one that are more difficult are like more heterogeneous, or maybe they go together with different things. So I don't know if you arise actually the content of those packs in different tasks, for instance. So this is why we want to move on to more complex data, right? So at the moment, Data, right? So, so at the moment, I would say there's really two steps: there's like the classic instance learning, which is supposed to be easy, and then there's the two complex logic cases, which are not realizable with the linear function, which are and and exclusive or. But these should have really the same kind of symmetry type of property. But so at least this wouldn't explain why it's working for one and not the other. But I think indeed if we move on. But I think indeed if we move on to more complex data sets where the different populations can have distributions that are very different from one another, I think indeed we'll find that for some type of dissymmetry between the different populations, it will work well and for some other it will. Okay, okay, okay, thank you. Okay, so do we have any last question or from people online? Okay, in that case, let's thank Nicola again. And with that, we move to the last speaker of the