Yeah, so Ophilia did a great job of giving us an overall picture of the idea of manipulating microbiomes. We're not doing it well. We're contributing to that in a small way. So here's a review. Ophelia was on this with Chris Larson. Went from top down, looking at actual microbiomes to bottom up, which is where we are in the math department looking for the simplest possible communities made of two cousins of E. coli that are intermingling. And we're looking at a few different aspects of A few different aspects of delivering engineered mobile genetic elements. And so, Long Ching did a great job of introducing us to conjugation and motivating that. So, this is where we are. So, we started this work a while ago with just a very simple description of the simplest description we could come up with for a conjugation process, describing the dynamics of the conjugation process. So, two populations, donor population, specific population, and then a conjugation event. And then contribution events. And this is one of the first experiments we did in the lab. And so we wanted to keep things simple. And so we just did filter mating. So we just plate the two populations, let them sit, grow, and mingle, and then shake them up into a fluorescenter at time points with fluorescent signals here. Let fluorescent signatures indicate which population is which, just depending on the population. So very simple. We're just getting triplicate observations of populations for time. And so that's Time. And so that's a simple description of averaged population behavior and aligns very well with a very simple differentiation model. So this bruce Levin also was mentioned in Lam Chu's talk. And this model is 45 years old, very simple model of quantification. You've got the three populations growing exponentially and just a math action from the conjugation. So, and this is So and this this is this is from an epidemicological perspective looks like an SI model right so this is very familiar yeah what's like what's the black represent oh this is just the this is the this is just the chromosome so why is it black scratch because they're different they're different so they're just chromosome one chromosome two just to show the different strengths different the trichogne is scratch so the conjugate oh so the so the the conjugative plasma The cognitive plasmid is the grain, which is transferred into a recipient making a transpony. Okay, so this earlier work we did is just look at this Levin model and extend it a little bit to try to get a decent agreement with these filter-based results. We did some model comparison there and some uncertainty analysis. I won't go into details, but the one thing I want to sort of highlight is the model fitting was very straightforward. Fitting was very straightforward because we had data that exactly matched with a simple deterministic ODE description going on here. So, just a regular weighted sum of squared errors is all you need. It's a textbook goodness or fit function. And what we're doing now is moving closer to reality and so looking at spatiotemporal behavior. And this is where this question of calibration gets a lot less obvious. So, we're using here these. So, we're using here these microfluidic chips, which Matt Bennett introduced yesterday, so we're not excited about this. So, this is how we have things set up, the seller trapped in monolayer. So, they're in focal plane the entire experiment. And so, this was set up with College of Campaigns Hearing at Waterloo. And so we can collect a lot of data from this experiment. So, this is all from a single experiment, right? So, the multiple tracks and the microphone is not very stage, but this is small. This is over a day and a half. So, they basically run forever from what we can. So, they basically run forever, from what we can tell, and we can image them regularly. So, we do phase images every 90 seconds, and the reference every six minutes before the photo bleaching. So, we get a really good description of what's going on here in terms of population behavior. We don't, once they fall out of the trap, where they're gone, which is your real mother machine. It's like a mother machine, but you're charactering a population to a typical mother essential. That's that's essentially the same, exactly. And then Caleb mentioned today that red plus green makes yellow. Red plus green makes yellow. So that's the other thing here. Is we're starting that starts with red population, green population. Transfer of that color is giving us that yellow population, which you can see start to kind of emerge through time. And then in some cases, basically takes. Okay, so this is a lot of data, and it's messy data, but it's single-celled data expected to be messy. And so this is sort of, we're good, this is a work of progress, but we're getting to the point where we're going to sort of embrace this. We're going to sort of embrace this messiness of conjugation at the single cell level. So, not just arriving at a single number that is conjugation efficiency, but thinking about how conjugation relates to the contact or proximity, how it relates to factors of health and the ignition rate and that sort of thing, transient effects of conjugation from individual cells. So, that's where we're headed with all of this. And one of the first steps then is to think about, first of all, getting First of all, getting from videos to data. So, Marcela just mentioned this image project in depth that are required. And this is lots of tools developed from the community to do this. This didn't show up at all, but that's a raw image and segmented and then tracking in terms of cell motion and vision. And then we go from these videos to data sets, which are relatively low-dimensional descriptions of each cell in terms of position, orientation, length. Of position, orientation, length, fluorescent signature, that sort of thing. And then what we want to do is really use that to calibrate some descriptions in terms of models. And so again, the obvious choices here are agent-based models, which are morphologically accurate. So we're using here a tool for Tim Rogers proof. Or we're also looking at coarse brain PV models that look at density. But I want to talk about the H models today. And so some of the parameters. And so, some of the parameters of those H-based models, we can just observe directly and just plug into the model. So, how long are the cells? How fast are the elongating? When was the length of division? We're observing those. So, we can just collect those distributions and then assign them and have a related model. But then there are other parameters that have to be inferred. So, see, these are some of the biophysical parameters that come into what happens when cells grow and start twisting one another around. So, there's a various ways that it's. So, there are various ways this could be done, but you need a handful of such parameters to describe those effects. And then, once you're going to do some interesting biology, so for population, for instance, that we have these additional parameters that are going to describe that process. And these are going to have to be inferred. And so, in the context of the OD model, that inference was very straightforward. We just do a theta sum of squared error, minimized sum of squared errors, such as maximum microbiod, and done. And here it's much more complicated, right? Because now we're It's much more complicated, right? Because now we have something that looks like this, and we want to compare it to something that looks like this and say, somehow, this one's more like that, that one looks, right? So, and there's a lot of spoilicity. And so it's kind of a, there's a number of ways one can proceed. And one that we've taken some inspiration from comes from ecologists, so people who do macrobial ecology, elephant, plants, they've also been doing age-based modeling with spatial temple data for a long time. And a paradigm. And a paradigm that's emerged within that community is what's called pattern-oriented function. Which sounds kind of hand-wavy, it is kind of hand-wavy, but basically the idea is you identify patterns in the phenomenon, and then you want to match those patterns. And the thing that's unsettling about that is that you and people have to, you have to, people have to be clever to identify the patterns. So that's not entirely satisfactory, but that's one way to proceed. That's one way to proceed. So, the first task we're giving ourselves here is just to look at the biophysics of growth. That's sort of the simplest piece of the mechanism that we're looking at. So here are a bunch of individual microcolonies growing from individual founder cells. You can see there's a lot of variability there in terms of colony shape and distribution of cells. And then we have, right, the ability to generate lots of simulations. And we want to get in the idea is we want a collection of views to look like this collection in some. Look like this collection in some way. So these patterns can be lots of different things. We put together a little review that kind of describes how some of these tools directly from ecology have been applied to microbial setting and other groups have done a little bit of this as well. And this kind of looks like a collection of image features you might see in forestry list, right? Again, it's kind of like might be this, maybe this, that. And so this is things that are sensible to be looking at, but it's Are sensible to be looking at, but it's again a little bit uncomfortable to have humans make that call. We can look at things like sensitivity analysis to gauge which of these might be more or less informative, but ultimately you're relying on the cleverest humans, which is, as a human, I don't have a lot of common cleverness. Okay, so what we've just started looking at is a complement to that, which is at least systematic, it's not entirely satisfactory, which is to look at Satisfactory, which is to look at representation learning to identify features. It's basically just PCA. Thinking of representation learning as fancy PCA. And we were part of this paper that came out recently from Stacey Finlay's group. So they were looking at a similar task, calibrating H-based models. They're looking at images of tumor progression, where individual pixels are scored based on the population density within that pixel or subpopulation. And their agent-based model is an on-lattice model that generates. Model that generates data the exact same thing. So they've kind of got a simpler task in the sense that the model and their data are directly commensurate, and there are all these fantastic tools in terms of looking at representations of images, right, in learning and learning. And so that's sort of what they were doing. So we want to follow the same path, but we have something a little bit trickier to work with. In particular, in terms of comparing images, we don't really expect to be able to look at the image features here. Be able to look at the image features here and the direct connection to image features here. So, what we've done again is extracted the extracted data in terms of where the cells are, where they're oriented, what in this case their elongation rate, that sort of thing. And that's the exact same data that you get out of it. That's what the simulation produces, those pieces of information. Okay, so then what we've done, this is, we've just started this, we're playing with different things, but a representational learning approach, essentially, we Learning approach. Essentially, we choose somewhat arbitrarily an embedding dimension that we're interested in using to represent our original input. There's a challenge here, which I'd be happy if anybody has a website on, which is our inputs, our time series, but the amount of data from time point to time point grows expensive between the experiments. And so the standard ways of doing this thing is just zero padding. So the first input has almost all zeros and a couple of cells at the top. By the end, you have two. The top by the end, you have 200 cells. And that doesn't seem to be the smartest way to do it, but I've talked to lots of smart people. They haven't been pointed yet to any solutions. Anyway, if anybody has any suggestions there, I'd love to hear them. Maybe you could generate a latent embedding space with like an autoencoder that has the longest time series. Yes. And then consider projections on that space, random projections to capture shorter truncation. Yeah. Okay. Yeah, okay. So that sense what we're doing is we were looking at the where we're capping the longest size and then we're just zero padding. And I hadn't, yeah, so we thought about a couple different ways of doing that sort of representation. And I don't know what's the best way in terms of capturing the time series features accurately. So maybe yeah, so that's it sounds like a good idea. And then we're using kind of standard time series. Standard time series layers here, and the triplet loss, which is essentially penalizing the model for putting parameterizations that should be separated nearby in the latent space. And this seems to be working pretty well. So this is just four different clusters from four different parameterizations, so simulations, four different parameters. And this is just PCA recognization of four-dimensional embedding showing that there's sort of Showing that they're sort of clustering separately. It seems to be a promising approach, anyway. And a nice compliment to this idea of human-inspired feature selection. And then just standard Bayesian approach, Bayesian inference to arrive at these parameters, but also just direct regression in terms of this deep learning approach. Okay, and then beyond that, we have these conjugation effects, and there are additional complications there. So what we're seeing. So, what we're seeing is the signature, the fluesal signature that comes from the frontation event, but that's delayed, of course, because of depression and maturation. And so the video here is showing individual signaling events the first time you see the color change. And the conjugation event appeared 40, 50, 60 minutes before that. And so this is a contact network that corresponds to its cell type. And then so the task here to go back in time. And then so the task here to go back in time to figure out when the complication event occurred is exactly analogous to a contact tracing in heaven acknowledgments. If you got sick on Friday, they want to know who you were hanging out with on Monday. So there's some nice tools from that literature that we're able to take advantage of. We have the advantage of full data, right? We know where everybody was basically all the time. But we also have a slightly more complicated scenario where, for instance, our patients are providing two at the same time scale as the Providing two at the same time scale as the infection process. Okay, I think I'm just about out of time. Do I have five minutes? Okay, I'll go briefly. Okay, okay. So the second part, this is all published stuff, so I'll just go briefly through this. Okay, all right, seven, I'll do seven. Okay, so we've done a little bit of work recently on optimal parameter design. Again, so the theme here is sort of thinking about how data fits into calibration. Fits into calibration. So, Brian mentioned this morning this idea of fisher information-based alpha-panel design. Basic idea, right, is that we're going to end up with some inference of parameters that characterize behavior. And we'd like this 95% confidence region to be small. And if we choose a well-designed experiment, we'll have more frequency to do that. And the tool that we're using is this fish information approach, which is a local approach that is complementary to what Jared described in terms of different. To what Jaren described in terms of preservation approaches. And this is a set of tools that are particularly useful when you've got dynamic experiments. So when you've got questions about when to measure things and when to perturb things. So this is a review we put out a couple years ago sort of highlighting that a lot of those types of experiments are becoming more accessible in this area. And so it's a good time to be thinking about applying these kinds of tools. So, three really quick stories here. So, this one we're looking at characterizing effects of physiology on gene expression. So, that's come up a couple times of this week with Terry's talk last night, which also discussed this as well. And we're using Terry's framework here to think of a growth rate as a sufficient statistic for a lot of those features. And so we put together a model that kind of explodes that model. That kind of explodes that base model of gene expression, again, like what Sarah did last night, where we're separating out gene-specific parameters, which should be portable from context to context, from physiological parameters, which are specific to the context. And these are all based on the wealth of data that's available for E. coli and balance growth. And so the generalization is a second task. And then, in terms of the experiment, so again, Brian had a list like Experiment. So, again, Brian had a list like this about an hour ago of what sort of things might you want to consider. So, here we're considering features, constant features like a growth rate, which is fixed by just by the media selection, and then an opposite activation. And so, we have a time variant input that we can select, and then when we sample, what we sample, and how often we sample at time points. I'll go through, I'll skip the details of the optimization task here, is challenging, so we recast this as a You can say we recast this as an alternatival problem and use the multiple shooting approach, which is on that slide. But the details are published there. So I won't go through them, but let me just skip the pipeline. So this is an optimal experiment that we sort of set for ourselves, which is sampling growth rates at the two bounds that we had considered, looking at these are screen size input profiles, a kind of thing that would be very difficult to kind of come up with. And then likewise, And then, likewise, sampling schedules and densities that are not triggered. And this was all a simulation study, but we compared to null designs in terms of the original optimality score, the optimality, parameter variance, and a prediction accuracy in feminal design. Often femininal design seems to be going well. The student who did this work was then going to move into the lab and validate, and that was beginning 2020. So, that did not happen. So, he shifted to. Did not happen. So he shifted peers and put together a Python package that describes the same very nice contribution. So this is a really general use Python package for auto general design and related methods. So meant to be kind of worked off shop. And a couple standout features addressing Nagausa institutions of data and the sensitivities which are needed for the visual qualification of HVAC 3. Needed for the configuration of H makers are all generated by audio differentiation. So that's an efficient and accurate way to get that. Yeah, and also a non-trivial thing to code. So this is a toolbox that allows for that to be a bit more accessible than it would be otherwise. I'm going to skip through this very briefly. One little project looking at applying these vocal approaches to multimodal systems, which is for not where we fit. And so we looked at a single A single dimensional auto activating module and put together a populate log likelihood as a Gaussian mixture, use some tools from sympathetic analysis to arrive at descriptions of those behaviors at the branches and between the two branch points, and arrive at optimal designs, which have the sort of standard flavor of sampling at the extremes, and in this case, sampling at the middle. And in this case, sampling in the middle, which is where the visibility of the interesting stuff is happening. And I'll close with this. So, this joint work with Chris Barnes at UTL looking at a deep reinforced learning approach to optimal terminal design. So, you can frame optimal terminal design as a feedback problem where the idea is at each time point wants to make a selection in terms of the most informative next step in an experiment. And so, reinforcement learning is one way to do that, right? So, the idea is the agent. One way to do that, right? So the idea is the agent is aware of the system, it chooses the next experimental action, and then it receives as a reward the fisher information that it's achieved as a content for that action. So we looked at this simulation study in the context of a fermentation process. And the advantage here compared to sort of standard approaches is that we should be able to have something a bit more robust to To model parameterization and that can mix optimizing an objective along with learning the model, as Jones approaches doing as well. And the limitation, of course, here is that it's data hungry. And so this is a simulation study, so we kind of got to skip that. And Joan and I had a discussion about parallel biractors to do this sort of thing, but that's a challenge. Our baseline here was sort of standard approaches. Here was sort of standard approaches to optimization, and the out-of-the-box reinforcement learning algorithm kind of did okay, but was not performing as well as quality predictive control, which is a standard here. And so we made some modifications to allow the agent to have more access to information and to have a simpler avenue for exploring the action space. And that led to something that has And that led to something that has roughly equal agreement with equal requirements with model predictive control. And then what we did is train that agent over a distribution of parameters so that it was able to essentially learn not only what it should do, but also where it was in the parameter space. And that sort of robustly trained agent performed as well as a model picture controller who knew what the parameters were. So, in a sense, the MPC had embedded over the It over the agent, and they performed roughly equally well. So, we saw that as a decent first step in this structure. Okay, I will end there. So, just thanks to the students who, of course, did all the work, especially Aaron, who produced a lot of the signal cell data and that, a lot of the analysis. Nate, who drove a lot of the optimal design stuff, and Nathan Trellier did the deep learning and then supports, and that's it. So, thank you very much. So it's not. Yeah, so it can be, but it's not. Right now, it's just getting a future creation report. Reward which I don't know. Yeah, so it's getting it's getting iterative pressure information right from step to stuff and so it's getting that one that's the that's the only reward that we have there right now, but it would be straightforward to extend it to what you're doing in terms of having it having an objective awesome to see if that you know kind of makes at least that particular optimization task once you incorporate that less data it's already kind of maybe data hungry in the sense that it's trying to find the best. In the sense that it's trying to find the best and most informative things, but right, right, that would be nice to see. Yeah, no, no, for sure, for sure. That's uh, it would be a great next step. Actually, I just kind of want to talk, like maybe brainstorm more about the problem you're talking about with the padding and the oh, yeah, yeah, yeah, yeah, yeah. Okay. Can you just talk a little bit more about like you can just. Bit more about even just like what is that vector current looks like? Like, what is the so? Yeah, okay, I'd be happy to. So, um, so, like I said, this we just started this this summer, actually, so and I don't have a background in machine learning. Um, but so the the characterization of every cell at every time point is just a vector of maybe about nine numbers, which is its position, its length, its orientation, its state in terms of. Its state in terms of genetic states, probably a few other, but that's basically it. So, you think about the agent-based model, what is the agent-based model? What if it's internal memory to state? That's all it is. Just where are the cells and any internal state gaps without assignment? So, we've taken the image data and extracted just that information. And so, those are exactly compatible. And so, it's just a spreadsheet, right? Where you start with one cell, and so you've got one line vector. So you've got one nine vector. By the end, you've got 200 nine vectors. And we want to take advantage of the fact that it is a time series, right? So we don't want to just lump everything together. We want the model to be able to say, okay, this was time one, this was time two, this was time three. And so zero padding allows us to do that. That way the first one is just that first cell and 199 zeros. And at least it knows that that's the first time. Okay, so that's where we are. And yeah. So here the feature is talking about time theories feature, not that snapshot point, the spatial pattern feature, right? So no, this is just the straight data. So not the features, these features are not, these aren't included. These aren't included. This is just the data, where the cells are and what they're. I think he's got features, but he doesn't have features. So that's right there. So these features we extracted from our brains, right? Extracted from our brains, right? Which is which is why it's slightly unfeeling, right? These are just things that make sense as patterns to identify. And these features are some deep learned representation, you know, whatever they happen to be. The thing is, we don't really care if it suffices to match those features to get the behavior we're looking at, then we're happy with that. So we don't really need to be able to interpret them. That can be nice to be able to. Yeah, if you want a compact representation for any time point, but you don't care about the relationship between time points, then you would just run the automational auto-variational encoder on individual time points. But if you wanted to develop a machine learning model that where two time points add and ratchet, each are coupled, and you wanted to predict the trajectory through the compact representation, you would use what is called masking. Would use what is called masking. So you would show the, so you would encode all of the time points as your vector, right? And you would train the model, and there's no network or a transformer or whatever. You would train the model, masking one of the time points, and then asking it to predict what that kind of constant should look like. Consultation look like. And that would be part of that training. And of course, it would randomly. And the size of the mask can vary. When I say mask, I mean like mask could be might be one of the time parts. It could be any of the time. And you would actually set up a training, training test or whatever. Okay. Where you would actually mask all possible time points and combinations so you would have a more generalized model. Right. Okay. Okay. And so you're not using. Model. Right, okay, okay. And so you're not using a time series-based approach. No, no, no, you are. Well, but it's not a surface. You are each input array is the entire connection. But when you want to train the model to develop the relationship between the time points, how do you do that? You are totally masking the literature or taking away one of the time points. And you're asking the machine learning model to get something out of between. Yeah. Okay. Learn something between. Yeah. Okay. Oh, that's super cool. Okay. Okay. Thank you. I'll definitely check it out. Great. Thank you so much. Brian, appreciate it. All right, folks, we have one last talk and appreciate your patience. Last talk from Lucia Maruchi and TOB presenting from University of Bristol. Lucia, can you hear us? Yeah, I do. Can you hear me well? Do can you hear me well? Uh, a little bit louder, maybe. Um, but if not, we'll make it work. So, um, is it better now?