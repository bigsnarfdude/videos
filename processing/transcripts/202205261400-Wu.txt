Okay, hello everyone. We have two sessions this afternoon, four speakers. And the first speaker will be Professor Chang Bao Wu. He will talk about dealing with undercoverage problems for non-probability survey samples. Chang Bao, it's all yours. Thank you. First, I want to thank the organizer for this workshop. It's a lot of work to get this kind of a hybrid format. Get this kind of a hybrid format working. And I really hope I was able to travel to UBC or Galagan. But I actually got COVID three weeks ago. So you may see a little bit of my lingering cough during the talk. This talk is on dealing with ender coverage problems with non-probability survey samples. The topic undercoverage is not specific. Coverage is not specific to non-profit samples. It's indeed a topic for any service. So, what I'm going to do here is first I will go through some of the estimation procedures which are developed and the ideal situations with assumptions. Then, I will discuss what will happen if some of these assumptions are broken down. Broken down. And so then there's an issue with undercoverage. Some of the processes deal with undercoverage better than others. And so we will see some of the discussions there. The problem with non-probability samples is part of this bigger research topic these days on data integration. There's two kinds of scenarios with data integration using data. With data integration using data from multiple sources. The first scenario is: if you just use data from one source, you still have valid statistical inference. But because of the limited sample size, so it's not very efficient. So combining data from multiple sources is more of to increase the efficiency for the inference. Calibration estimation using non-population totals averages. Averages is a very old example of that kind. The other scenario with data integration is if you just use the one particular data source you have, you won't be able to have valid estimation result. So typically, the estimator will be biased if you don't have anything near us. So combining data from other sources is more about correct bias to have valid statistical inference. Statistical inference. All this estimation method with non-probability samples is actually a part of the second scenario. So you do need additional information in order to make a valid statistical inference. There's two major issues with non-probability samples. One is the sample is biased. You don't know what's the sample inclusion participation mechanism behind. And the second is you actually even And the second is you actually even don't know which population is represented by the lump probability sample. And oftentimes, it's not the target information you have in your mind. And one technique difficulty here is the biased nature of non-probability samples cannot be corrected just by using the sample itself. So this will become very clear in the next few slides. You need information at the population. Information at the population level to do valid inference. And while the settings we were using here is we assume certain orbital information is available from existing probability survey. So here's a general setting. You have a target population, U, with N units. And for each unit, there are some baseline measurements, typically the demographic variables. The demographic variables, we call that x. Then there are specific variables, we call that y, the study variables. And the goal here is to estimate the population mean of the y variable. And you can always go beyond, but this is always the starting point when you develop methodology for estimation. So you have a non-probability sample called SA with sample size a little NA here. And you have measurement on both the covariance and the study variable Y. Covariance and the study variable y. Now, you could have multiple y's, but the discussion here focuses on one particular y. Because the sample of non-probability sample is biased, so if you just take a sample mean of the observed Y in SA, it's not a valid estimate for the population mean. And as we mentioned earlier, the key issue here is I don't know what's the inclusion or participation mechanism. Inclusion or participation mechanism for the non-probability sample. So you can introduce the indicator variable R, one if you need I is in the sample and zero otherwise. Now there's even an issue, is this R can be modeled? Is that a stochastic mechanism behind this R? And sometimes we can argue it's not, right? But let's just say, suppose we can model this indicator R. This indicator R. And then you can define this pi IA, which is the probability I is being included in the non-probability sample given the features of the units. And I use this term propensity scores, which is borrowed from mission data and causal inference. And there are other authors, they call this participation probabilities. So either way, it's the probability of inclusion. Is the probability of inclusion, and then if you can estimate the preparedness scores, then you can develop some kind of weighting scheme to have valid estimation. So to get started, those are the three assumptions used by quite a few authors. So the first assumption, A1, basically says conditional on the features of the covariates. On the features of the covariates, the inclusion of the unit i in the sample is independent of the variables yi you are going to measure. So apparently, that is something very similar to missing at random assumption when we deal with missing data problems. The second assumption, A2, is a so-called positivity assumption. So every unit in the population has a non-zero Has a non-zero probability of being included in a sample. Now, this will be one of the major issues with undercoverage problem because typically in practice, this is not true. There's a third assumption, say the unit of I and J being included in a sample are independent of each other given the covariance Xi, XJ. From a statistical inference point of view, S3 is not very crucial. A3 is not very crucial. Even A3 is not valid. A lot of this estimation procedures developed are still valid. It's just a matter of efficiency issues. So everyone, this missing at random assumption, if it's true, then the preparedness score is a function of the covariate Xi. So there's a way trying to have an estimation procedure to Estimation procedure to estimate this profanity scores pi IA. The second assumption, A2, basically says two things. One is you have a complete sampling frame. Okay, if you do have something called sampling frame for the non-proprietary sample, that frame is complete. And also, there's a hidden assembly here is there's no hardcore non-respondent. Non-respondent. So everybody selected, they will participate. And apparently, that is also problematic. Now, in the next two or three slides, you will see if even under assumption A1 and A2, to estimate the perfectness scores, you need information at the population level in terms of COVID rates. In an ideal situation, it will be you have this complete observation information available. Have this complete optional information available for every unit in the entire population, and but typically that's not something you can have in practice. So, this is the assumption if for a lot of people use this phase is to assume there is a probability sample sitting there available to you with information on the covariance X. Covariates X, but not on a specific study variable Y. So you have this reference probability sample. We call that S B, where you have covariance XI and also the survey weight. And this will give you a way to trying to estimate certain population quantities related to the X variables. In terms of approaches for inference, Approaches for inference with non-probability samples. There are three popular ones. The one based on inverse probability weight, and if you can estimate the probability scores, and this would require a model. We call that Q on the preparedness score. Once you estimate the proprietary scores, then you can do inverse probability weighting using the so-called sodo weight. It's a reciprocal of the preparedness score. The preparedness for the second method is based on a prediction approach. And in this case, you need a model of y given x. And using the term from missing data or causal inference, this is called an opt-camp regression model, so cosi. And if you can model y given x, and that fixed model can be used for prediction. Can be used for prediction. And while the common used model would be a regression model, where this mi is xi beta. So the focus would be the estimation of the regression coefficients, beta. And then there's a combination of inference probability weighting and model-based prediction, and this so-called double-robust inference. So the final estimate is valid if while the two models. If one of the two models is correctly specified, so inverse probability weighting based on estimated preventative scores, under the first assumption, you can assume a parametric model for the preventive scores. So logistic regression would be a popular one. Other possible model is also used in practice. Also used in practice. So the focus is: you have a parametrical form, functional form pi, and the focus is on estimating the model parameters alpha. So if you use a likelihood-based approach, so the full likelihood needs to be defined at the population level because this pi IA is defined for every single unit in the entire. For every single unit in the entire population. So the full log likelihood would be given by this form. And that is the way this observation comes into play. Because now you see, this is something not computable if you only have the non-probability sample, because it requires pi iA to be defined for every i in the entire population. If you rewrite If you rewrite that log likelihood by putting the second term as part of the first term, and then the first term is only depending on the probabilities, the non-probability sample SA. And the second term depend on entire population, but it's in the form of a population total. So this is the way the so-called solo log likelihood function defined in the paper. Likelihood function defined in the paper by Yiling Chen. So you replace the second term here by an estimate using the reference probability sample because here you only need the X variable. And if you also have the survey weights available for the reference probability sample, and this entire same error star become computable for any given alpha. So if you maximize this error star, then you get this so-called so. Then you get this so-called so-called maximum likelihood estimator R head. And then you get the estimate for the profit scores. And once you have the estimated perpendicular scores, you can do the reweighting in the typical way, the so-called IPW estimator. And this IPW estimator is consistent. If the two assumptions A1 and A2, they were both valid. And the argument is very simple. The argument is very simple. First, you need this performance scale model to be correctly specified. And then, assumption A1 enable you to split the R and X when you need the expectation of R. And then if you rewrite this inverse weighted version of Yi in the non-proprietary sample, and you take the expectation and the preventive score model, and you can show this kind of thing. You can show this kind of equation hold. But a key message here is this argument do require the pi i is positive for every unit in the entire population. Otherwise, this argument is not valid. The model-based prediction approach for non-probability samples has another name. It's called a mass imputation. So the idea here is if you So, the idea here is: if you look at your reference probability sample, so you have the covariance available and you have the survey weights, but you don't have anything on study variable y. So, if you look at your reference probability sample, if you just say, if I treat yi as missing for every single i in the sample, then I have 100% of missing values on y. If you can impute If you can impute each of the YI, then you have a data set where you have the survey weights. And if you do know the YI, this would be the typical Holson-Thompson estimator. You can use the design-based inference. But because you don't, so if you impute every single Y, that's called a mass imputation. So you get an estimator here using the reference probability. Using the reference probability sample sp and the survey weights. So I use this notation MI represents mass imputation. So there's a confusion here. Sometimes you may think that's called a multiple imputation, which is not a context I use here. So the key is, can you impute yi for every single i in the reference probability sample? Well, under the first assumption, a Under the first assumption, A1, the Y and the indicator R, they are independent of each other given the covariant Xi. In other words, this outcome regression model for units in the non-probability sample is the same as this the model globally. So, in other words, you have this non-probability sample, SA, with probability sample SA with the pairs y x observed and if you use those pairs to build a model and that model is actually valid globally and so you can use that model and the information X i in the reference probability sample to do prediction and that would be a valid argument so this is the so-called model-based prediction and or mass imputation estimator which is a true Which is a true under the assumption A1. And so, a key message here is: I didn't really use the assumption A2 here because it doesn't involve any propensity scores. So, those are the estimation procedures developed and the ideal situation of assumptions A1, A2, and A3, and A4. So, the very natural question is: in practice, when you're going to In practice, what are you going to do with those assumptions? Right regarding the first assumption, A1, and that is the same kind of concept with two missing at random in missing data analysis. And there's no way you can test if that assumption is true or not, because this is the well-known fact in missing data literature. The MAR assumption cannot be tested by using the sample data itself. Tested by using the sample data itself, unless you have additional information. So, what people usually do with assumption A1 is to check the covariance XI you have and to see if this covariance describes the participation behavior for the non-probability samples. So, if you feel confident, so the X variables you have, they are the key factors or features. Key factors or features for people to be included in non-proprietary sample, then you have a confidence, say maybe the assumption AY is okay. But we do have another difficulty here. That is, the estimation procedures we developed there requires the X variable that is also available in the reference probability sample. So this can sometimes limit your ability to make that kind of adjustment. You may have 10x. You may have 10x variables in a non-probability sample. You may have 15x variables in your reference probability sample, but the common set of x variables has only maybe three or four. And all your argument is based on that common set of x variables. So that is one of the difficulties in practice. The second assumption, A2, the so-called positivity assumption, is typically not true because Not true because there are always scenarios where, whether it's a non-respondent issue or whether it's a convenient sample issue, you know for sure, okay, certain people are not possible to be part of your sample. So what's really happening in practice is you may have a subpopulation is called U0, where everybody got a non-trivial chance to be included in a non-probability sample. Probability sample, and then there's a other part of the population of your target. We call that UN, where units will not be included in your non-profit sample anyway. So that kind of a typical issue of undercoverage. Now, the severity of undercoverage typically depends on two things. One is how big is this subpopulation not covered, not represented by your sample. Not represented by your sample. And the other issue is: what's the difference between these two subpopulations? One represented by your sample, the other is not. And I have other formula in the next few slides where this will become even clear. So in terms of violations of the second assumption, the positivity assumption A2, there are two general scenarios. The first scenario I call The first scenario I call this a stochastic end of coverage. And this is the scenario where your non-probability sample comes from a subpopulation U0. But then if you look at the subpopulation U0, it looks like a big sample from your target population. Okay, so here are two simple examples. So the first example is your participant from non-profit sample may be recruited from a Be recruited from a list of units, and that unit is indeed the list of units in the existing probability survey. And you have this consent from everybody in that probability sample to be contacted for other surveys. So this would be a typical scenario. Your non-probability sample comes from this unit in that probability sample, but that probability sample also comes from the target population. From the target population. The second example here is not obvious. See, suppose your targeted population are some kind of consumers in a particular region. So the way you're trying to recruit people for your non-profit sample is you should set up tables in the shopping centers in a region and approach people randomly, okay? And over a particular time period, say over three days. So apparently, your non-probability. Your non-probability sample comes from those people who visited the shopping centers over those three days, right? But if you think about this, this type of consumer in the targeted population has a long trivial chance to visit one of the shopping centers over that three days period, then you can imagine that U0, where your non-profit sample comes from. Okay, where your non-profit sample comes from can be viewed as a bigger sample from the target population. So, this would be what I call a stochastic coverage, which I will try to make that clear in the next slide. Now, there's always issue with non-respondent, right? So, if you have this so-called hardcore non-respondent, people will never participate in any service, then you are in trouble because that's going to be the Then you are in trouble because that's going to be the part of the population you will never have a coverage. So, in that situation, this will become a deterministic undercoverage, which I will discuss later. So, with the stochastic undercoverage, remember we have these indicators Ri, so when if I is be included in the non-probability sample zero, otherwise, so now if I introduce this di, okay. Is di okay for if for any units in the entire population di is one if the unit i belongs to the subpopulation u zero then what we're trying to say here is this okay so the ri equals zero if the i is a part of the u zero then i have a non-trivial probability for i being included in the sample. For I be included in the sample. If the I is not part of this U0, then I have zero probability to have this unit in my non-profit sample. So, but then if you think about this U0, everybody in the targeted population has a non-zero chance to be part of that subpopulation. In the examples I described in the previous page. So, if you combine. So if you combine this A and B, and you realize the unit I being included in the sample has a probability that's the product of these two, which is non-negative. So in other words, for stochastic undercoverage, it's really a non-issue of undercoverage because you do have this positivity assumption, which is true for everybody in the entire population. So with estimation procedures, we developed. Estimation procedures we developed under the ideal setting can be borrowed in this kind of scenario. So, under coverage is typically not a serious issue in this scenario. Now, the so-called deterministic undercoverage is when you know for sure, okay, units with certain features will have no chance to be included in an unpropagated sample. So, here is a very simple example. A very simple example. Suppose the participation in a non-probability sample requires internet access and a valid email address. And suppose there's a group of people in the population don't have access to the internet and then don't have an email address. So in this case, you know for sure that group of people will have no chance to be included in the number of business. In the number of beta sample. So, in this kind of scenario, you have a clearly defined two subpopulations. You know for sure one part of the subpopulation will never be covered by the non-probability sample. Now, if anybody are familiar with this issue with this undercoverage, there's a cash book formulation on the bias due to undercoverage. So, the data. So, the data you have represents a subpopulation, presumably, you can do m-bias estimation for the subpopulation U0. If you pretend there's no undercoverage issue, and if you use that estimator to estimate the population mean for the targeted population, and the bias typically involves two components here: so, one is the size of the subpopulation not represented by Population not represented by the sample. The other is the difference between the two population means, okay, the subpopulation means. So, this is what I mentioned earlier. So, how big the bias? Well, it depends on first is how big is the subpopulation not covered by the population. And the second is how different are those two populations, right? So, sometimes you may be fine, even with undercoverage, because the Coverage because the two sub properties just look similar. But in any case, if the undercoverage is small, and typically the bias is not an issue. So certain estimation strategies we developed in ideal settings can handle undercoverage for deterministic undercoverage. Now, we are not talking about remove the bias completely. We're talking about some kind of mitigation. So where the bias. Kind of mitigation, so where the bias is not severe. One of these approaches is called the calibrate IPW approach. The so-called calibrated IPW approach is this. So if you use a maximum likelihood estimator or the SODO maximum likelihood estimator and the parameter alpha for your prepared scores, the estimate alpha hat solve the score equations. Have solve the score equations. Okay, this u alpha equals zero. And this would be ideal if you can. But then if you stay away from this strict maximum likelihood-based approach, and there's a development on the so-called n-biased estimated functions. So you can get an estimate of alpha by solving an estimated equation system like this, okay, where this h is a Okay, where this h is a user-specified functions with the same dimensionality of alpha, and then you can argue this estimated equation system is unbiased in the sense that if you take expectation, it has a zero mean. And one particular choice of this function h is the x variable divided by the parenthesis score, so which leads to the equation two here. And this is a so-called calibration-based approach. Because what you see here is if you set this equal to zero, the first part here is the weighted average of the X variables in the non-probability sample with the inverse probability weighting of the preventive score. And the other part is indeed the population controls for the X variable using the reference probability sample. Probability sample. So, if you set this equal to zero, this is the typical calibration constraint we use in survey sampling. Now, of course, one of the issues here is if you claim this is an unbiased estimated equation system, you do need that assumption A2, because otherwise, if some of the pi i is equal to zero, and then this unbiasedness argument is not valid. Now, why people interested in this? Okay, you solve this equation. Okay, you solve this equation system to get alpha hat, and you put alpha hat in, you get estimated estimated performance scores, and then you use the estimated performance scores to do inverse probability weighting. Now, why? This is something of interest. Well, let's call this probability score is a pi ic. The c means calibrated. And then you can do inverse probability weighting by using the same format as before. Format as before. Now, this estimator has another interpretation. That is, if you have an out-cam regression model, and that model is linear, so the expected value of y given x is a linear combination of x, then you can actually argue if your prevalence score is estimated through this calibrated approach, then the model expectation. Then, the model expectation of this inverse probability weighted estimator. There's a P here. The P refers to the probability sampling for the reference probability sample. So, in this entire estimation expectation argument, I'm not talking about the prepended scores. That's not part of it. So, if you take a model expectation, model-based expectation, you can actually show this is approximately the same as the model-based expectation. The model of expectation of the parameter of interest mu i. So, in other words, this is approximately model and biased, okay, regardless of the preventive scores. So, somehow there's a similar argument of a double robust here with one difference. So, I'm not really fitting an outcome regression model to estimate the model parameter beta. So, this argument is valid without the estimation of beta. Without the estimation of beta, so that is the sending point here. And in assimilation studies, we show sometimes this estimator has smaller bias, even if the assumption A2 is violated. Now, there's some kind of interest in model-based prediction in dealing with end-coverage problem. The model-based prediction requires The model-based prediction requires assumption A1. Now, as I mentioned, if the covariance includes those demographic variables which characterize the participation behavior, then you can justify the assumption A1 such that the model you fit, the opt-camp regression model you fit, using the observed values in the non-probability sample. In the non-probability sample, it is valid for the general situation. So, you can use that fitted model to do mass imputation, as I mentioned previously. So, I do have this reference probability sample. I do have this survey weight available. So, I'm going to mass impute this Y using this fitted model, opt-camp regression model. And then this would be valid even if. Valid, even if the second assumption A2 is violated. Now, the real danger here with the prediction-based approach is this. So the model you fit to have this beta hat is using data from the non-probability sample, SA here, because that's where you have these pairs. And then that fitted model is used to units in a non-probability. Two units in a non-probability sample to do the prediction of the YI. And this is sometimes called this model transportability assumption. So the model is transported from one sample to the other sample. And the potential issue here is you may have this extrapolation problem as you often see in regression analysis. So you have observed data. So, you have an observed data set once you fit the data model, and you're trying to use that model to do prediction. But anytime, so if your new data points with the X variable outside the range of that original data set, and you have an issue, okay, this issue is the model you fit may not a good one for your prediction. So, one way people can check if you have the confidence that you don't have this issue of You have this issue of extrapolation, is just check the range of the x variables in the non-probability sample, and then you compare this range to the x variables you observe in the reference probability sample. So if they looks very similar, and typically the concern with extrapolation would be not that serious. Now, this undercoverage problem is Problem is where you have this idea of the targeted population is split into two parts. That's one part that has been represented by the non-probability sample, and there's another part which is not covered by the data we have. So it's a very natural idea to think about, is there any way I can identify a split of my target population where I have a good idea of what's being covered and what's not being covered. And it was not being covered. Now, in the current setting, this is a particular opinion because you do have this reference probability sample, SB, which is supposed to represent the entire target population U, not only the U0, but also U1. So if there is a way, you can split SB into two parts. So one part is coming from the U0, the other part comes. The other part comes from U1, which is not represented by the non-propagate sample. So that would be a very important, very significant first step. And then you ask the other question, say, once I have a split, is a better way I can develop my estimation procedures, right? So can you split? That's the first question. Second, yes. Once you have a split population, can you develop a better estimation strategy where the Where the bias can be handled better than without splitting the population. Now, let's make this very clear. Even you have a good split of the population, that doesn't mean you will be able to have estimation strategies with no bites because technically you don't have sufficient information for the uncovered part you want. So, one of the things I'm gonna describe is Described is if you want a perfect situation, you do need additional information on U1. And while this additional information is going through a sub-sample of the SB1, where you know SB1 come from U1, and if you can take a small sample out of that SB1 and to measure the study variable Y, then you can develop very some defendable explanation. Very song defendable estimation strategy. But if you can do a sub-sampling, then that's out of the question. Okay, so the first part of the question. Can you split the population into two parts? And in a recent paper, we explored this concept of accessibility function phix, because basically this is a coming from that motivated example where you have this population. Uh, a population where certain units you know for sure will not be um including the non-profit sample because of the feature of the X variables. For example, you don't have access to the internet or you don't have email. So, certain X variables dictate the accessibility to the non-propagated sample. So, this idea of phi, the key message here is: I assume this function is convex. Function is convex. Okay. And then there's some kind of a threshold C. And above C, you have this chance to be included in a sample. Below C, you don't have any chance to be included in an unpropited sample. Now, the only thing I need in the next few slides is I assume this function is convex. I don't need to know this form of this phi, and I don't need to know this actual threshold of C because this has all been taken. Because this has all been taken care of explicitly. And then the reason why I need this function to be convex is the following. So look at all these X variables in this U0 where your non-proprietary sample comes from. And there's a convex hole defined by all the X variables for units in that SU. And so technically, this And so technically, this convex whole H0 is unknown, but I do have a non-probability sample SA, which is coming from that sub-sample U0. And so I can define explicitly the convex hole generated by the XI for units in the non-propagated sample. Now, if the non-propagate sample is large enough, and typically this convex hole HA. HA is very close to that convex hole for that subpopulation, U0. And so that is where my starting point. So I have a convex hole here I can define with data I have. So then I'm going to try to split the reference probability sample SB by looking at each of the units there to see if they belong to this convex hole HA or not. And that is something doable. So for So, so for every unit in that SP, I want to see if the unit is part of this my convex whole HA or not. Now, this is something computationally implementable because what you need to know is for every unit in that, let me see what I did here. Okay, so what you need to know is you look at a convex combination of XI's. Convex combination of Xi's in non-probability sample. So for any SJ, okay, J belongs to this reference probability sample. If there is a possible solution, then that unit J is part of this SP0. Otherwise, it belongs to the other part. And so this idea of using the convex whole formulation to split this population S B into two parts seems to be computationally simple. Be computationally simple because it's very easy to implement. There's algorithms out there to do this kind of complex identification. Jae Pong Kin mentioned a similar idea in his 2018 SLC talk by using the so-called modified Niels label method. So the idea is this for every unit in SB, he looks at a possible match in SA. So if there is a unit here, So, if there is a unit here in the SA, which is very close to that unit, he called there's a match. So, then he would put that as part of this SB0. How about you have two minutes? Okay, so there's an issue there. The issue is you never have a perfect match. And so what you really need to do is you have to define some kind of threshold where there's a closeness measurement of that match. That match. And so then in practice, you have a problem. That problem is how to define the closeness, right? So that is a problem. So this complex formulation seems to be easier to implement than this modified Nielsen approach. So once you have a split of the population, so you basically treat the population as a stratified situation, and then you Situation and you estimate the population mean using the splitified estimator. Now, the message here is this: okay, once I have the split, I can develop a better estimate for the y, mu y0, which is the first part of the population. And then, second part, I just do whatever I can. For example, you can do double robust situation with the first part and then do prediction for the second part. So, somehow, Part so somehow you do a better job with the first part, and then the hope is overall the bias will be smaller. As I mentioned earlier, a perfect solution would be you have some kind of additional information for the second part of the population. And so one way to have that additional information is use a sub-sample of this sp1, where you take a smaller sample out of it. You already have X information available. All you need is information available, all you need is just trying to get this Y variables for that smaller subsample. So then you can do a good estimation for the first part. You can also do a reliable estimation for the second part. So then the combination of these two will give you a valid estimate for the overall population mean. I think I'm going to skip the simulation study here and I will make the Study here, and I will make the slides available. So, the information here is this: all the approaches I mentioned here don't eliminate the bias completely. But under certain scenarios, the bias can be smaller. And then there are cases where one way is better than the other way. For example, the prediction approach sometimes is better than the calibrated IPW, and sometimes the other one is better. Other one is better, but you never completely remove the bias unless you have additional information, which is my last one here: the YSS. So the bias is all small in any of the scenarios. Right. So just to finish very quickly, this research on non-probability sample is an active area. There are some active researches on all kinds of different aspects or Of different aspects of non-profit samples. For example, there's a non-parametric estimation of preference scores, and then there's a method for handling non-ignorable sample participation. So while this talk this morning by Rebecca is actually belong to that one. And then this split population approach, there are further work to be done to really make this as a tool that can. As a tool that can be used in practice, I listed a few references here. You see, they're all mine, but the actual literature in this area is growing. So that's indeed a very, very long list. And I list my own book here with Mary Thompson. And so there's a chapter there, which is the last chapter on this. So the reason I put my own paper here, I think this could be a good. Be a good opportunity to do some shameless self-promotion indeed. If you haven't seen this book and I think it's a very good one, you may want to consider to get one. I'm going to stop here.