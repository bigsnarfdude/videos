And quite a new and different conference for both me and Lara. So we were enticed by the idea of combining geometry, education, art, and research. I think both of us are pretty far away from art, although we love visual explanations. And we are educators, but more practicions than trained as mathematics educators. Trained as mathematics educators, but we really enjoy teaching and want to continue to improve our teaching. We're much more on the geometry and research side, and in particular, we do some data analysis. And we thought it'd be neat to try to challenge ourselves to give purely visual explanations to data science. So, how would we explain data science to an artist who's not a mathematician and doesn't even A mathematician and doesn't even maybe have the mathematical training to read a data science textbook. But I do think it's important to, you know, data science is changing our world in a lot of ways and to have an appreciation for, roughly speaking, how some of these algorithms might work or how do you build your intuition when a data science algorithm might be able to accomplish a task versus not. So, yeah. So, yeah, we're going to have some fun conversations together, and hopefully, our pictures will inspire some questions and responses. And maybe it's a low probability, but if any artworks comes out of these conversations, we'd be thrilled. Laura, would you want to introduce yourself and say a few words and get us started? Yeah, certainly. So I'm Laura and I'm a PhD student. And I'm a PhD student. Most of my work focuses on explainable mathematical methods for machine learning. And yeah, I just love machine learning a lot. And particularly the not the black box aspect of it, but the parts where you can actually explain and things you can understand and explain to others, right? So understand what your algorithm is sort of doing. So this will be bruffly. So, this will be roughly the topic of our sessions today and tomorrow. And we'll be talking about geometric data analysis, which is kind of really is machine learning, a special type of machine learning, which is the first topic we're going to explore a little bit together, just to like demystify or like to dispel the magic or whatever of that topic. And particularly, you're going to start with even a more general branch. A more general branch called artificial intelligence. So, we're going to start with like the mother branch and then go into the details. So, I really like that Mew a lot. I've seen it quite a bit in a couple of places. And, you know, usually when we try to, you know, look up blogs or anything to try and learn about artificial intelligence or machine learning, you're either faced with two things. You have the media aspect of it, right? What you see in newspapers. You see in newspapers or on TV or media, where artificial intelligence is thought of as this really kind of thing that's going to take over the world, right? It's that something that people don't really understand, but it maybe sounds scary, right? And other blogs or other things you try to learn on the internet are mostly super mathematical or super high level. So they require a lot of understanding or a lot of background. Understanding or a lot of background. For example, you would need quite a bit of knowledge in math or other areas to be able to understand it, right? And what this meme is really trying to say is that, you know, AI is not really that, you know, thing that's going to distort its world. Like, oftentimes, it cannot really distinguish the picture of a cat from the picture of a dog. And what we're going to do today is we're not going to fit into either world, right? Into either world, right? We're going to be somewhere in the middle where we try to describe a little bit artificial intelligence or machine learning in a more approachable way where you don't really need to know all of this knowledge like mathematics or computer science to try and learn about these things. So we're going to talk about a specific area called classic machine learning, which is really super mathematical in its nature. And the methods we're going to focus on are geometric data analysis methods. Data analysis methods. So these are methods or tools that you are able to understand and explain using simple geometry. So we won't be using any notation. And also, we're not in a, we've prepared a couple of methods and we're happy to spend more time on the things that people find more interesting. Maybe I'll give my impressions on this. Um, uh, comic. So, you know, somebody walks up and says, When a user takes a photo, the app should check whether they're in a national park. And the developer says, Sure, easy geographic system lookup. Give me a few hours. And then the next person, the person says, and check whether the photo is of a bird or not. And then the scientist says, Yeah, no problem. Just give me a research team and five years of hard work. Years of hard work. So, the point of this comic is for an outsider, it can be hard to tell sometimes whether a task is going to be easy for a data scientist or machine learner or statistician or mathematician. And it requires a fair bit of domain expertise to predict ahead of time, I think, we're going to be able to accomplish this in 10 minutes or 10 hours or 10 weeks or 10 years. But I think this is important, something important that we try to teach even to non-natural. Teach even to non-math majors, non-STEM majors, so that they feel more comfortable using machine learning algorithms, data science algorithms in their everyday lives. So I think this is one of the benefits of if we can improve our education to make it more approachable, not only will the scientist have that intuition, but also maybe the manager will have that same intuition. All right. Any thoughts before we start? I don't know if people have any thoughts or anything to share about AI in general that we could maybe address or talk more about. I'll try to keep checking the chat. Not that great at it, but let's see. Barb, I wonder if there's a slide so that you can slide how much you see people's faces versus God's screen. I don't know, Bart. I don't know, Barbara, if you see a lot of faces, maybe you can move the faces over to the side to see the screen more. All right, let us know. All right, so I guess I'm going to get started a little bit to talk about the area or branch of classic machine learning. Well, before that, I'll be using quite a bit or talk quite a bit about. Quite a bit, or talk quite a bit about high-dimensional data. So let's try and describe what we mean by that, or why does it appear a lot in everyday real data. So I'm going to start with one-dimensional data. So what does it mean when my data or data point is one-dimensional? So certain things that we can describe using one-dimension, other words, using one number. For example, we can think One number, for example, we can think of mass, right? We can say something is of mass per kilograms, right? We can think of temperature, say negative 10 degrees Celsius. We can think of pressure, density, and so on. These are all physical quantities that could be described by a single number. And one way to visualize or to plot that, we could simply use one axis. Use one axis. And for example, if I'm going to plot mass or temperature close to negative degrees Celsius, for example, T equals, let's do that over here. So T is negative 10 degrees Celsius. I could plot it on a single axis. And usually we denote that by a point, right, to represent that this is the point we're referring to. That would be my For referring to. That would be my temperature axis. And you can imagine if I have a temperature of, let's say, 15 degrees Celsius, that would be visualized by a point on the axis. That's temperature of 15 degrees Celsius. So what happens when we move on to two dimensions? So now we're starting to think of what are physical quantities that we need more. Quantities that we need more than two numbers to describe? Or what are objects that we really would like to describe them using more than one number? Any thoughts on that? So what are physical ones that we either need two numbers to describe them, or generally any object you would like to describe using two numbers. A social media user. Great. User. Great. Awesome. Brian also said in the chat location on the globe. That's perfect, right? Like coordinates, altitude, right? So, for example, if I'm standing over here, how would I tell a person? So typically, let's call this the origin or O. For example, it's kind of impossible to tell someone my location on that red point using only one number, right? So there are typical, there are a couple ways to do So there are typical, there are a couple ways to do that, right? We can either specify, we can call these x and y directions, we can specify how many steps do I need from the origin in the x direction, and how many steps do I need in the y direction to be able to describe this point over here. I'm just curious, any thoughts on how I can use two different numbers to describe this point? Different numbers to describe this point? Different way of representing that same point. Direction and distance, that's perfect, right? So instead of specifying, for example, in this case, needing three steps in the X direction and four steps in the Y direction, I could alternatively say an angle, right? A direction. Right, so that would be measured by some angle. We won't go into a lot of the details. And then, if I tell you, okay, I'm going in this direction, you need to tell me when to stop, right? Else, I'll go indefinitely. So, these are two different numbers to represent the same point. So, we call these, if you want, coordinates or features, right? These are my dimensions or coordinates. All right, I'm gonna step this one more. One more. So, for example, here three dimensions. So, you can think of what are objects or what are physical quantities that I need more than two numbers, in this case, three numbers to describe, right? Or you can think of what are objects that I could characterize or describe using more than two numbers. So, for example, if we want to create RGB colors, that's perfect. So, for example, if I want to describe a color, Want to describe a color, right? It's typically combinations of R, which is red, G is green, and B stands for blue. Great. So in this case, to describe a color or a pixel, I need three values. And we typically represent them, let's say, if we're doing, let's say, 1 to 255. So let's say this is a 55, so let's say this is a 10, 100, and maybe a 50. This is some color, maybe closer to green, I don't know. But, and these are three dimensions, right? So these are three coordinates. So how do we represent that? Well, let's say we call this the R-axis. So this is the red axis, the green axis, and the blue axis. And on each of these, I'm going to specify these values. I'm going to be a little bit rough here. These values. I'm going to be a little bit rough here. So let's say this is a 10. Let's say this is 100. Not super to scale. Let's say this is a 50. And this is the fun part of trying to plot in 3D. There you go. So we kind of really need these dotted lines. Else, if I were just to put that point here, we can't tell. Here, we can't tell what it is, right? So, from as you're going to see in a lot of our slides, we're going to be using or denoting representing points by just dots. And what these dots mean, as you're going to see them in a bit, in this case, these are dots in two dimensions, except we omit these axes. Maybe it is either irrelevant or it doesn't give us really anything. Doesn't give us really anything useful or additional. So, oftentimes, you're going to see going to be plotting or describing points or data as simply points. And these are usually have two coordinates. You can, of course, generalize that to 3D. And these would be representing some sort of physical quantities represented by two. Okay, and one last thing. And one last thing before I pass it on to Henry. When we think of high-dimensional data, we're really starting to generalize a little bit more, go above 3D, right? A lot of data, I know someone once mentioned social media, right? Often data really is more than 3D, it's really high-dimensional. And you might have heard of the word big data, right? So you can think of So you can think of, let's take an example of a think of patients or I'll leave the patients example, Henry. For example, you can think of users, right? You go on Amazon and purchase things and give them reviews, or you go on Netflix and watch movies and give them reviews, right? Or if you use Facebook or Instagram, you have certain friends or followers or following other people, right? So all of these sort of are data. Sort of our data points in really high dimension. So for example, I can think of, well, let's say this is Henry, who's really a data point in Netflix. And Henry rated movie one. Let's say Henry gave it five star review, movie two. Maybe Henry did not really like it, gave it only one star, and so on. I know Netflix changed that. You can only give likes and dislikes. You can only give likes and dislikes, but let's say this is movie 100. Right? So, for example, this is an example of high-dimensional data that could really go on and on. And you can imagine not only Henry, but like thousands of people that use these services. So, these servers have a lot of data, and these are often high-dimensional. So, we'll be dealing with these sorts of data and trying to understand them as well. Can I try to give a high-dimensional data example? Yeah. If you go back a slide, the RGB example inspired me just because we have artists. So, you know, usually when you ask people what is four-dimensional space, they think, well, it's three dimensions and maybe time, right? But four-dimensional space can just be, you know, you're encoding lists of four numbers. So what if your color is not described by red, green, and blue? Described by red, green, and blue, but red, green, and blue, and say transparency or saturation or brightness, right? So I don't know what CMYK is. Does somebody want to explain what CMYK is? Cyan, magenta, yellow, and black. Oh, there we go. Print colors. Okay, I'm making a note. Fantastic. Yeah, so CMYK space is perfect. When you think of four-dimensional space, think of Of four-dimensional space. Think of it as you're just encoding a CMY, a color using CMYK coordinates. So I think it's a mental shift. You don't need to think about four-dimensional space as this physical impossibility. You know, it's just lists of four numbers. And so I think that's a challenge for mathematicians to try to make four-dimensional space seem less intimidating because we work with it as mathematicians all the time. And it's pretty important in data analysis, as Laura described. Data analysis, as Laura described. So, here's another example that I like to give. This is a very old data set, so late 1970s, but these patients who were either had diabetes or were suspected of having diabetes had physical characteristics measured about each one of them. So, each patient had his age, his or her age. His age, his or her age recorded, and weight, and then insulin response, glucose response, and SSPG is a measure of the interaction between insulin and glucose. So to each person, you just recorded five numbers, age, weight, insulin-glucose interaction between insulin and glucose. And then this is a small data set by today's standards, only 145 patients. 145 patients. All right. So back in the 80s, we didn't have great data science algorithms yet, or even visualization tools, or computers were not as strong as they are now. So this data set in this paper, they study what's the best way to visualize this five-dimensional data set. Okay, so they found this 3D visualization that works pretty well, and they had an artist go in by hand to Artist go in by hand to try to draw this central core and these two lobes sticking off of it, trying to visualize the data. And the point is that the geometry of the data, the shape of the data can really reflect important patterns within. So the egg in the middle, the central core, is the pre-diabetic patients who are still healthy. Whereas the two flares on one side versus the other are type one versus type two diabetes, you know, early on. Diabetes, you know, early onset, late onset. So you should really think of these as two related but different diseases that you might want to treat differently. So in any case, it's a nice interaction between research and geometry and data and mathematics, you know, but also art a little bit. Back in, you know, when you're in 2D or 3D, you want to visualize things. How do we teach computers to help us visualize things in higher development? Help us visualize things in higher dimensions as well. Really crucial for solving a lot of data problems we're confronted with. Great. So I'm going to give a little overview of the topics we're going to talk about, and then we can get started. So this is kind of the visual introduction to classic machine learning. And as you're going to see, most of the topics we're going to focus on are geometric methods. Methods. So, typically, when we think of classic learning or classic machine learning, and maybe just to define this term a little bit, so when we talk about machine learning, it's when you are learning simply from data. So, you have a lot of patients, right, or a lot of users, or a lot of data points, and you simply try to learn something, given only the sheer amount of people you have data for, right? So, it's the For right, so it's the it's learning really from data, and typically in machine learning, there are two things or two categories, and these are unsupervised and supervised learning. So, in unsupervised learning, our data is not labeled in any way in a sense that, for example, I have a collection of these patients, but I have no idea what disease they have, right? What disease they have, right? So I just have the collections of people, let's say, height, age, heart rate, maybe insulin levels, and so on. And I want to be able to sort of group them, be like, oh, I have maybe like a collection of these people, right? That kind of have look a little bit similar. So in this case, the data is not labeled in any way. I don't have like a specific target or ground truth. Ground truth. Compared, for example, to label data, this is where I have a label in mind. So, for example, the first picture we looked at, right, I knew that the picture was a picture of a cat, right? And maybe I want to teach my machine to be able to distinguish between a picture of a cat and a picture of a dog, right? So, in this case, I have a target and I know ground truth. I know this is a picture of a cat, or this is a picture of a dog. Of a cat, or this is a picture of a dog. And you can also label data numerically, right? For example, you have enough information about a person and you want to be able to predict or know, let's say, what grade they're going to have in a certain class, or you want to figure out the salary or something like that. So, in this case, that would be a value. And usually, the way I like to distinguish Usually, the way I like to distinguish between categorical and numerical values is: you know, whenever you're using an elevator, you can pick certain floors, right? You can pick floor one, floor two, floor three. You cannot pick somewhere in the middle. So this is how we think of categories, right? It needs to fit in one of these boxes or these bins. And when you think of numerical, it's like more of a stream, right? It's a value that doesn't necessarily have to fit in a bin. Let's see. So, these are common tasks that we're going to explore together in machine learning. In the unsupervised world, there are two things we often, two common tasks we often deal with. The first one is called dimensionality reduction. So, this is the case maybe of the case Henry gave, right, where you had the dimensions and five-dimensional data, and maybe you wanted to visualize it in three dimensions. And maybe you wanted to visualize it in 3D, right? So we know how to visualize in 3D. So you want to reduce the dimension of that data. Or, for example, if you have a 3D scene, right, you want to sort of paint it on a 2D canvas, right? You want to sort of capture this 3D object on a 2D sheet, right? So you're reducing, in this case, one dimension. And there are a lot of methods to There are a lot of methods to do that, and we're going to describe them in a bit. But often, whenever you're reducing the dimension, you have something in mind, right? You want to preserve it properly. You have something in mind that you want to preserve, right? You're losing a lot of information, but how do you want to summarize this or how do you want to summarize the data you have and visualize it in only three dimensions, especially if we're talking about really high-dimensional data? We're also going to talk a little bit about clustering. That will be the first method we're going to explore together today. And clustering is, for example, suppose you have a collection of, let's say, all of us here watch different types of movies. I want to be able to see, for example, that, let's say, Henry and someone else, let's say Brian, have similar tastes, right, in movies. So we'll be able to, what we call. Able to what we call cluster them in groups, right? So I don't have a label, I don't have a target to assign to Henry, right, or to Gwen or anyone else. All I know is the movies they've watched and the ratings they have given for these movies. So I'm able to group them and be like, oh, these people tend to watch the same things, right? This is an example of clustering. And in the supervised world, this is where we have a target. Target, so classification. For example, I'm going to be able to say, given enough data points, I'm going to be able to say, for example, maybe someone has diabetes type 1 or type 2, right? Or maybe no diabetes. Someone has COVID or no COVID. This is a picture of a cat or a dog. So in this case, I do have some sort of ground truth and I'm trying to predict this target or this label. And lastly, And lastly, regression. This is tend to be explained a little early on for folks studying math or other similar fields. This is the case, for example, if you know, let's say you want to predict stock prices or maybe you have enough information about people like their education level, maybe their years of experience, or maybe where they want to look. Where they want to be looking for a job, right, to sort of predict what salary they might want to negotiate or ask for, right? So these are the four methods or tasks that are pretty common in machine learning. And we're going to explore dimensionality reduction together. There are a lot of methods for that. Again, each sort of prioritize or have a certain property in mind that they want to preserve. In mind that they want to preserve. In particular, we're going to talk about the most popular one, which is PCA. We're also going to explore clustering together, and we're going to talk about k-means. So this is one of the simplest and earliest methods for clustering, and it's super geometric. Then classification, of course, there are a lot of methods you're going to explore two together called decision trees, three, decision trees, k-nearest neighbors and a K-nearest neighbors and SVM. And I missed something, Henry, didn't I? So, in clustering, I'm going to talk about two methods, and then topic modeling. So I missed that part. Talk about agglomerative or hierarchical clustering. Yeah. Clustering, we won't really touch upon that, but it's something we can definitely talk about. You mean regression? Yeah. Did I say something else? Did I say something else? I don't know. We can check the video later. Laura, let me give you a break. I'll share my screen for a while. Does that sound good? Yeah. Laura's doing the heavy lifting today. All right. So I want to just make this a little bit more visual. This is a little bit more visual supervised versus unsupervised learning. So, in unsupervised learning, imagine you're just given data points. Okay, so this might be people watching Netflix and they've rated movies, and each person is a data point because you have their list of ratings on the number of movies they've watched. And maybe there's only 10 movies that you have everybody's ratings for. So, this is a So, this is a set of data points in 10-dimensional space. Each data point is a person, and we've plotted them based on their ratings for 10 different movies. So what are you going to do with this data set? I mean, maybe you're not even given a task. Maybe you're not supposed to predict anything specific. Netflix just wants to understand what movie watchers look like. How do we make How do we make sense of the people in the world? So, clustering is one of the unsupervised tasks that Laura mentioned. And in clustering, you might try to, as Laura said, group movie watchers who are similar to each other. So these eight people had similar ratings on most of the movies that they watched. So maybe this is my wife and I, and we love romantic comedies. Romantic comedies, right? And then over here, you know, you might think of this as a group of people who potentially all like action movies, you know, and maybe you have some users who are really, really unique. Now, there's no right answer, right? I might look at this data set and say, I think it groups into these five gray clusters, right? Whereas Lara might look at this data set and say, no, this is really. Set and say, no, this is really one, two, three clusters. So there's no right answer in these unsupervised learning tasks, but the point is that you can still glean a lot of information from the geometry of the data. Supervised learning, the data points come equipped with a label or a color. Okay, so we have red data points and blue data points. And blue data points. And if I give you a new data point, your task is to predict: do you think it's red or blue? So, you know, blue might be images of cats, and red is images of dogs. And you're trying to distinguish a new image. Is this an image of a cat or an image of a dog? You might have three different labels or colors, right? Cat, dog, or elephant. And you might even have a continuum of things that you're trying to predict, like maybe the weight of the animal. Like maybe the weight of the animal in the picture. So, one way to try to do supervised learning is to try to define a boundary between the two regions. Okay, so it doesn't need to be a linear boundary, but actually many fancy algorithms that get used in the real world today are linear boundaries in some sense. So, if I had to draw one straight line and say everything on the right side. And say everything on the right side was blue, namely a cat, and everything on the other side was red, namely an image of a dog, I'd probably want to draw this line roughly around here. Okay. And I'd get some labels wrong, right? You know, I've labeled this point red, even though I would guess that this point is blue, even though it's labeled red. And I would guess that this point is red, even though it's labeled blue. But if I look Labeled blue. But if I use this line to predict new data points that I see, you know, for this particular relatively easy data set, I'll do a good job at predicting whether the data point is red or blue on average. Any questions about that distinction between supervised and unsupervised learning? I guess I kind of have one. This distinction about whether or not there's ground truth makes sense for supervised versus unsupervised, but the unsupervised one, you've still picked what to measure, what is in the data. That still feels very supervised to me. And sometimes it gets talked about like it's totally neutral. Is that a thing I should actually be worried about in data science? Completely agree, Brian. You're sort of saying, you know, how did I choose to represent my data points? Did I choose to use the movie ratings or did I choose to use how they responded on, you know, to messages on Facebook? Yeah, completely. I think you raise a very valid point. We inject a lot of biases into data analysis, even if it has a name like unsupervised, which makes it sound like the ground truth it's not. Sound like the ground truth, it's not. And for example, ethics and many of these issues need to be thought about carefully. Laura or Brian, I was wondering if you had any follow-up comments. Yeah, I just want to say typically for supervised, the machine is learning from the output. Like you gave it a collection of images the first time, you gave it a catch, but it labeled it as a It labeled it as a dog. It learns next time to improve on that. But yeah, and like unsupervised, this is where it gets a little bit ambiguous, where we have a lot of tasks, a lot of goals in mind, right? If we have really high-dimensional data, we don't really know its low-dimensional representation. It's something we desire, maybe. So, yeah. So, yeah, it is definitely a bit. Let me say something else. I mean, when you get into it and read all data science, you'll often mix the two. So your boss might tell you, I want you to predict whether these images are of dogs or cats. So you've been handed a supervised task. But you might say, as your first step, well, as my first step, I just want to understand my data. And here are these different And here are these different groups that seem to behave similarly, right? And once I've grouped the data, then I might try to sample data points from each group. So maybe I get some color information in each group. And then within each group, I can try to build a supervised A supervised classifier, right? So then within each group, I could try to find, you know, a decision boundary. So it's good to think of supervised and unsupervised as different things to start, but you'll end up merging them oftentimes. You might try to understand your data in an unsupervised way and then try to do a more supervised task on subgroups. All right, Laura, I think I'll pass it off to you. Cool. All right, so I guess we're going to get started with unsupervised learning curves. Are you all able to see what's we? Yeah, Andrew has a nice comment. So a lot of categorization methods are iterative. Andrew, do you want to say more? Algorithm starts with a guess and refines it based on error minimization. Ah, I see. Yeah. Hi, Henry. Yeah, so just in terms of k-means and stuff like that, right? I mean, k-means starts with a guess and then you pick different points depending on how good they are on a centrality measure, right? So I mean, you know, it is pretty unsupervised in that way. So the error often guides us in terms of choosing a better In terms of choosing a best, let's say, categorical fit for a given data point. Anyway, sorry, I just thought I'd throw that out there as an addition to saying, like, the machines are not, they're not going to take over, I promise. They're just minimizing error usually. Right. And optimization, though, is a good thing to keep in mind because if you do have an most machine learning algorithms do have something that they're trying to optimize, and you have to watch out because, you know, if you're trying to. Because if you're trying to, that's a way that bias can be included into algorithms where you would rather not have bias present. If all your labeled data, for example, has bias, it's labeled with a bias, then that bias will be transferred over into the algorithm that the machine learns in order to optimize their task of learning your labeled data as well as possible. Cool. All right. So, our first task is going to be clustering. So, I'll go over that in a bit as well. Let's see. So, just a reminder, clustering is you have a collection of data points. You sort of want to naturally put them into groups, right? And here we're looking at two dimensions, right? But oftentimes, data is really in high dimensions. So you cannot really look at it and be like, well, yeah. Really look at it and be like, well, yeah, I sort of have maybe two groups, right? So oftentimes we can't see that. So we don't really know what is hidden in our data, right? So something you explore about your data, right? Something you try to understand. And one comment based on the discussion we've been having. So for example, I can look at that and say I have two groups. Another person might think, well, I have maybe three. Another person might think, well, I have maybe three groups, right? And you can even decompose it even more. Just as in, I'll try to vary my examples, but one thing that just came to my mind is just a common thing that people often talk about. For example, companies who produce clothing, right? When you often buy stuff, you have a couple of sizes, right? You can start from really extra small, maybe to really extra large, right? And so the data points might be, you know, different body types. Know different body types, right? Maybe different heights, masses, you know, shoulder width, and so on, right? So maybe this is a decision that a company does, right? And maybe like, okay, I want to place or categorize all these maybe into size small, maybe this is large, and maybe this is extra large. This is a lot of decisions company would do maybe to ignore groups of maybe minority or different body types. Minority or different body types, right, which is kind of an issue. But this is an example where you could think of I'm grouping people into categories, right? So this is a decision you can do whether you want to think of maybe I have, I'll put all of these in a large size and maybe that is a small size. Okay, so usually in clustering, Henry's going to talk about a different method that I'm going to talk about. The method I'm going to talk about is called k-means. We're going to break these into parts. Going to break these into parts. So, this k here stands for the number of groups you would want to try and find in your data. So, in this case, for the data I have, I want to try and look at two groups. So, in this case, I'm going to set k equals 2. And the way k means is going to work is the following. So, starting with your data, you typically choose two random points in your space. So, in this case, I chose this blue point and then this orange point. Blue point and then this orange point. So pick any two points you want in your space. The first step of this algorithm or this method is for each black point you see, you're going to measure its distance to the orange cross and to the blue cross. So for each point we see, you're going to measure its distance to the blue cross and to the orange cross. The one that it's closest to That it's closest to is going to take its color, right? It's going to inherit its color. So, in this case, the black point that I chose is closest to the blue cross. So, what I'm going to do is I'm going to assign it the color blue. And I do that for every single black point. So, for example, we take a look at this black point over here. We notice it is closer to the orange cross than it is to the blue cross. So, it's going to be assigned the color orange. So, these dotted. So, these dotted lines that you see, these are the assignments. So, all of the points that are connected with this orange dotted line are going to inherit or take the orange color. And all of the points that are connected to the blue cross are going to take or inherit the blue color. And this is the assignment we do. And we end up with these color assignments. So, you can see on the left, a couple of these have the color blue. Color blue, and two points have the color, whoops, the color orange. And maybe, if you're someone who's looking at this, you might think, well, this is not really a good binning or it's not a good way of grouping things, right? These colors maybe should be blue, right? So let's see, as someone mentioned earlier, this is this kind of an iterative process, so we start. Process right. So we start somewhere and then try to learn or go from there. So this is the first step of the algorithm. The second step is: well, for each group or coloring you found, find its mean. So this is the second word over here. So what is mean means? It means finding the center of mass or the average, right? So where is the average of these blue points? So maybe it is somewhere over here. So find the center of mass or the average for the blue points, and find the center of mass or the average for the orange points. These are these two points here. So if you remember, first we started with two random points in our space. Now we did a little bit more of an educated guess or a better starting point. And now once we find these averages, we start right from the beginning. We start right from the beginning, right? So I forget all of my assignments and I start right from the beginning, except now I have two better guesses for these points. So the idea here is again for each black point, measure its distance to the blue cross and its distance to the orange cross. The one that it's closest to, it's going to inherit its color. So in this case, this is going to be assigned the color. Going to be assigned the color blue. And we do that for each of the plot points. So these are the distances measured. And these are the colors we assign them to. And now we obtain something a little bit more to what we think is more intuitive, grouping these into a single group, and this is into a single group. And this is what we refer to as a cluster. So this is a group or a. Cluster. So, this is a group or a category. And so, how do we know when we kind of want to stop? Well, let's do this method one more time, but remember these two assignments. So, in this case, for each color or for each group, we're going to compute its center of mass or its average. And these are denoted by these crosses. Crosses. And after we find these center of masses, we go back from the beginning, ignore all of the colors that we have. Instead, now we have two crosses. I was saying crosses, these are the means or the averages. And then we do the same process again for each black point, compute its distance to the orange and to the blue point, that was way off, and find the one that it's closest to. In this case, it is also close to. It is also closed. And we do this reassignment again. And notice from the previous picture we looked at, the assignment did not change, right? The algorithm or the method here sort of what we call converge, or it sort of reached a conclusion that we have these two clusters: the one on the left that we gave the color blue, and the one on the right that we gave the color orange. And this is at how. And this is that what the algorithm would say: well, these are the two groups or the two clusters that I have. It's saying the one on the right are similar points, the one on the left is similar points. Any thoughts on this? Your questions on this method? Yeah, let me mention that I think k-means minimizes the squared error as opposed to the error. Can you say this again? I think in the end, k-means minimizes the squared error, the variance, rather than the actual distance r. Distance are. I'm not sure the consequences of that, but it's interesting mathematically that it's you may want, you may think that you're minimizing the distance, but you're minimizing the distance squared. Yeah, so exactly, because here we don't have like a ground truth to measure versus this ground truth. So typically, in practice, to determine this K when we sort of don't know it is, as you mentioned, Is as you mentioned, minimizing the distance from the mean, right? So, what are maybe what value of k gives me the smallest error in terms of how close are these assignments to their means? Exactly. Yeah, you'd want, you know, taking K larger and larger will allow you to reduce the error. And there's ways you can look at that rate of reduction. Can look at that rate of reduction, but yeah, I see what you're saying, Joseph. You're saying that this optimization, it doesn't minimize the sum of these purple lengths and these yellow lengths. It minimizes the sum of their squared lengths. Exactly. So I guess if we wanted to visualize that, we could, on each edge, we could draw a square, right? And then you're minimizing the sum of all the areas of all of those squares. That's a nice way to look at it. Yes. That's a nice way to look at it. Thank you. Any thoughts? So one thing I want to mention about this method is we discussed maybe one disadvantage of it is how to pick this K, right? Especially if it's really high-dimensional and I have no idea what K I want to pick. Another maybe disadvantage. Another maybe disadvantage is the choice of the two points that we start with, right? So, what if these two points maybe look something like this, where what if they are close to two, two of them are close to one group of points, what we think is a cluster. So, what happens in this case? So, we're going to try out this together. So, we're going to go over the process again, maybe a little, you know, a little faster. A little faster and see what happens in this case. So, as usual, so for each black point, we pick, we compute its distance, so the blue cross and the yellow cross, or the orange cross, to find the points that it's closest to. And in this case, we notice that these points are closest to the orange cross, and these points are closest to the blue cross. And this gives me the following assignment of points. And as you can see, it's And as you can see, it's kind of shifted on the right cluster because this is where the two points we started with. So, the way this algorithm learns is that now instead of starting from two random points, the points that it's going to start with are the center of mass or the means for each of these colors. And once we compute these means, we start again and compute the distance of each black point to the blue and orange cross. And you can see slowly. And you can see slowly the algorithm is maybe doing a better job at assigning this as a single cluster and maybe this as a single cluster, except that we have this one blue dot that we're going to see if it's going to change its mind on or not. So if we do one more step or iteration, we notice now that all of these points on the left are assigned the color blue, and all the points on the right are assigned the color blue. All the points on the right are assigned the correct orange. And in this case, if we do the algorithm one more step, you're going to see none of it's not yet changed its mind on any of the colors. And this is when we say the algorithm sort of converged or it agreed on a certain grouping. And this would be our last step of the algorithm: computing the means, finding the distances, assigning colors, and And we obtain these two clusters that we, the same clusters we obtained with our first try, with our first initializations. So that's the idea of k-means clustering. This is one way of performing this task. I'm gonna pass it over to Henry, but let me see if people have questions first. Have questions first. Yeah, Brian has a great question. He asked: Is there a data set where this process can cycle infinitely? So you don't end up at a fixed point, but you end up in a periodic orbit? And I was actually using Lara's slides when I taught this algorithm in an applied topology class a couple of weeks ago. And a student asked this question. Can you have an algorithm where you don't converge to fixed clusters, but the clusters keep cycling around? Keep cycling around. And my graduate student who was listening in found a paper implying that that's impossible. And correct me if I'm wrong, but my understanding is that someone has proven that you, for any initial choice of centers, you always end up at a fixed point. Now, different choices of centers might lead you to different fixed points, but it's never the case that you can find a choice of centers on the data set that you get stuck in this periodic orbit. Andrew, if you want to send out the link, or else I can post on Slack afterwards. So, Laura, I think I'll share my screen. All right, so I'm going to talk about agglomerative or hierarchical clustering, and I'll And I'll talk about the simplest such method, but the advantage of these methods is that you don't have to decide ahead of time how many clusters you want, as you did with k-means clustering, where Lara decided ahead of time she wanted two clusters, blue and yellow. So what we'll start doing is we'll, well, let's see. Yeah. Okay, let me show you the process. We're going to add in edges between nearby data. In edges between nearby data points, and first we're going to add in the shortest edges, and then we're going to add in longer and longer edges. So, think of first throwing in all edges that are at most one inch. Okay, and at this scale, you know, I have one, two, three, four, five, six, seven, eight, nine, 10, 11, 12, 13 clusters. But now, when I add in all edges of length at most two, I have fewer clusters. Things have grouped. Fewer clusters, things have grouped up, and then add in all edges of length at most three inches and four inches. Right now, I have five pretty nice clusters. Okay, and then keep increasing the length of the edges that you allow. We lose a cluster. Right now, we have a pretty nice grouping into three clusters. And keep going as you allow the edges to get larger and larger. And larger, eventually everything will connect up. Okay, so that's the idea of single linkage clustering. You pick a scale and you have a notion of what's clustered at what scale and you allow that scale to vary from small to large. So visually, I want to explain for you the dendrogram that gets produced. Gets produced. So, whenever you're doing hierarchical clustering, agglomerative clustering, in this case, we're doing a particular type of that, you can produce a dendrogram. Okay, and I'll show you how to read this dendrogram. And the benefit of the dendrogram is that here our data is in 2D. So you can just eyeball it and you can say, I see five clusters. Okay. But if your data was in 10 to But if your data was in 10-dimensional space, you can no longer eyeball it, but you can still get your computer to produce a dendrogram. So once you get practice reading dendrograms, it's almost like you can see a little bit in 10-dimensional space. You can see some of these clusters. So the dendrogram contains the scale on the horizontal axis. So this is the scale. I was talking about. I was talking about, you know, one inch, which might be this scale right here. Maybe this is at all edges are included up to five inches. You know, maybe this is all edges are included up to seven inches. All right, so the scale in the dendrogram is increasing from left to right. At this choice of scale, you get five clusters, the green, yellow, blue, red, and brown clusters. Blue, red, and brown cluster. Okay, but you have to practice imagining that you can't see this. Okay, and the way you do that is you say, ah, at this choice of scale, I have five clusters. Here we have a later scale, and at this scale, you have three clusters. Okay, and you can see how that matches up with our actual data set. But let's go through the progression. So let's start where we add in all edges of length at most, you know, 0.25 inches. Well, no data points have connected up. And then when we include all edges of length at most one inch, some things have connected up. So, you know, these two data points have merged. Okay, so all of our on the left, and then you can see these merge events or these two data points. merge events where these two data points have merged. And then I think this right here is these two data points have merged. We increase the scale, more has merged. And then we increase the scale more and we get down to these five clusters that you've already seen before in the colors, right? So you can see the history here of how the green data points have. Of how the green data points have merged up to form this green cluster, and the yellow data points have merged up to form this cluster, etc., for blue, red, and brown. All right, so we merge to four clusters, finally to three clusters, which you have seen before. All right, and then eventually we get down to two clusters, and at the end, every And at the end, everything links up as one cluster. So, what's nice about this is you can, after the fact, look at the dendrogram and say, hey, look, I think that there was a pretty robust regime of scales where I had three clusters. And that might be an argument for saying that I really think this data set is best modeled by three clusters, as we saw here. Or your friend might say, Or your friend might say, Oh, I really like this range of scales. It seems pretty robust. And that's my argument for maybe modeling this data set as five clusters. And there's no right answer whether this is five clusters or three, but hierarchical clustering allows you to give some sort of a measure of how likely do you think this is to be, I don't know. It allows you to argue for different models. Of different models based on how long are those clusterings robust to changes in scale. What questions do you have? I want to quickly share a Mathematica demo. We've only been talking about connected components. Connected components, but my area of research, applied topology, looks not only at connected components, but also the shape of the data as you do this process. Could I actually ask a quick question? I mean, and maybe you're getting to it. Oh, please do. Which is, it seems like in the previous thing, what you were drawing was kind of a model of your data in kind of negative curvature. You're trying to make a tree out of it. And so I'm wondering, you know, a lot of this stuff is data sitting. And so I'm wondering, you know, a lot of this stuff is data sitting inside of Euclidean space, but maybe the data, you know, should actually, like, is actually sitting in some kind of negative curvature space sitting inside of Euclidean space. And I'm wondering if, I mean, I saw something on archive recently about people thinking about, oh, whether this data is more naturally modeled on negative curvature rather than, let's say, Euclidean space. And I'm wondering if I'm guessing this is actually somewhere where it looks like you might be heading towards some of that as well. I don't know if we'll have. I don't know if we'll have time to get into it. We don't plan to get into any of that, but I love the question, Jayadev. I think the, so correct me if I'm wrong, I'm just sort of spoofing here, but I believe the internet is negatively curved. So I mean, a tree is negatively curved, right? Like a tree is like the model. So yeah, I mean, you don't expect, you know, maybe you have some regions of really dense linkages and you have some, some, some zero curvature somewhere, but a lot of the time you have a lot of branching and you have kind of negative curvature happening at a large scale. Yeah, I think, I think, yeah, broadly speaking. Scale. Yeah, I think, I think, yeah, broadly speaking, your statement sounds quite correct that the internet ought to be negatively curved. That's right. And it doesn't need to be. I mean, because the internet is a graph, right? So if each website is a vertex, and then whenever one website links to the other, you have an edge between. Well, you know, you do have loops, right? Yes. Yes. But on a large scale, the internet is negatively curved. So you should model it by, I don't know, like crocheted, cool. Crochet quote that has. Yeah, yeah. Well, speaking of crochet, Dinah has a complex. Please go ahead. Yeah, I think in the chat she said internet's negatively curved because it makes it most efficient. Yeah. Very quick, the reason the internet is negatively curved is that. The internet is negatively curved is that nobody cares about your web page. Can you say more, Henry? So, um, well, it's tree-like, right? The only people who care about this thing are the people in your sub-sub-sub-topic. Nobody else out there in the world cares about it, right? You have to be much more sort of more like a complete graph. If it were more like a complete graph, everybody would be interested in what everybody else is doing. Be interested in what everybody else is doing, and that's just not the way the world is. I see, I see. So, maybe the point of this conference is to make the internet a little bit less. Yeah, so that's a great comment. The point of conferences is to increase the curvature of the internet. That's exactly what we're doing. There we go. There we go. All right, nice. Put that into the next proposal. You know, I write that's a good idea. You know, I write, that's a good idea. Yeah, you know, so we'll talk about dimensionality reduction next, Jayadev, and we'll be doing dimensionality reduction into Euclidean space. But I think it's a fantastic area of open research to think about dimensionality reduction into negatively or positively curved spaces. Awesome. Thanks. Yeah, so to just finish my thought before I pass over to Lara again, you know, you can think of this higher. You can think of this hierarchical process where we're throwing in more edges, right? But we care not only about connected components, you might care about holes as well. And maybe a hole measures something periodic in your data, right? So you can also think of hierarchical techniques where you don't care just about clusters, but what's the shape, right? So maybe my data set was really drawn, noisily sampled from a circle. Okay, like such. And so, in low dimensions, humans are really good at blurring your vision to say, I think this data set was maybe noisily sampled from a circle. But how do you teach a computer in higher dimensions to blur its computer vision and tell you that, yeah, this data set in 10-dimensional space was noisily sampled from a circle. So lots of fun things you can do hierarchically. Laura, Laura, I have no idea where I'm leaving you, but I'm going to pass it over to you again if that's okay. And we'll be wrapping up in about 20 minutes, I guess. That's perfect. So I think, so today we're going to continue talking about unsupervised learning and move on a little bit to dimensionality reduction thinking. But any general questions on clustering? We looked at one of the simplest The simplest, oldest methods, which is K-means, and you'll get something a little bit more sophisticated that doesn't require the number of clusters. Any questions on that? Yeah. So is it, I think it's the case that with the hierarchical method, that if you have like a double helix. Helix, imagine a double helix in 3D, that the hierarchical method would connect them in spirals, whereas let's say k-means would just break it into two pieces. Because things are close together in a tendril, it's not a question, it's more like an observation. Yeah, there are completely different types of clusters. Different types of clusters you would get. I was going to say double helix for folks who maybe are not familiar with it. That's a great point. Henry, would you like to add anything? No, thanks. Okay, cool. All right, other questions or thoughts? Alrighty. Okay, so now I'm going to move to the topic of dimensionality reduction. As you saw in the little map I had, I listed a whole bunch of methods to do that. Each of them have a different property, right? So when we go from, let's say, two-dimensional to one-dimensional, let's say I'm recording the maybe age of a person and maybe their height. Right, and I'm going from this two-dimensional two-piece of information to maybe like a one-piece of information, right? That might not necessarily be interpretable. It's like a summary of these two, right? So often when we're reducing this dimension, we're losing some information. And different methods try to preserve a certain property or properties about the data. The first method we're going to look at is called PCA, one of the Called PCA, one of the, I guess, most popular or most fundamental methods, and stands for principal component analysis. And for folks who are maybe familiar with a little bit other techniques, it's also can think of it as SVD or singular value decomposition. So oftentimes these words are used interchangeably. So the idea of PCA is the following. I have a collection of points. As you said, we're just denoting them by We're just denoting them by these black dots. And the idea of PCA is to try and capture the variability of the data. So to make this a little bit more concrete, suppose this is the center of mass or the average of these points, right? And what I'm going to look for is I'm going to have a straight line pass through this red cross, right? Right, and there are infinitely many of these. We can think of many straight lines passing through this mass, this mean. And we're looking at which one of these straight lines is the line that sort of captured the most variability, right? Or that points in the direction where the data kind of varies the most, right? So maybe if you take a little bit closer look, you can think of this blue line as the line that sort of is pointing right. Pointing where it's aligned the most with our data points, right? And the idea of PCA is to try and find which one of these infinitely many straight lines is the one that kind of best aligns with my data. Another way to think of it is for each of these lines, I'm going to go ahead and compute or find the distance from the black point to the straight line. Black point to the straight line. And these short, the shortest distance from this black point to the straight line is given by this perpendicular or 90 degree angle. So for each of these black points, I'm computing the shortest distance. What PCA does is that the straight line that captures the most variability in the data is Variability in the data is exactly the same straight line that minimizes these green distances. So, if I want to compare it with the other lines that I propose, let's take a look at that. So, let's go back to this. If we look at this red line, you can think of it as it does kind of the worst job at pointing in the direction that aligns with the data the most. And so, if I want to compute these green distances, you can see there are quite a lot of them. There are quite a lot of them, right? These long, these green lines are pretty long, right? If you want to look at the orange straight lines, this is one of the lines we looked at. Maybe it does a slightly better job, right? These green lines are a little shorter. Purple does a little bit better. And you can see that the blue kind of does the best job. So, what PCA does, you can think of it two ways. Either finding the line that kind of minimizes Finding the line that kind of minimizes these green distances or the straight line that sort of points or aligns the most with our data. So, how do we reduce the dimension, right? So, the idea is the following. You have these data points. Think of it as you have spotlights over here, maybe two spotlights over here. And really, what you're doing is projecting these black points onto the straight line, right? Points onto the straight line, right? So you're looking pretty much at the shadow of these black points on the straight line. And in that way, I would take these points that were living in two-dimensional space and bring them down or boil them down to a single straight line, where they are now represented using a single number. So, for example, I can take this straight line and really you want to rotate it into this axis. Into this axis, if you remember, we looked at at the beginning and plotted here, right? So I went from this two-dimensional space into simply one axis, which is one-dimensional space. And that's the idea of PCA. And you can think of it in these two different ways. So let us go back to how these two ways are connected. So this is the red line, which we Red line, which we are at least I thought was like the worst line, right, out of the lines I suggested. And if I want to do this technique of projecting, right, put a spotlight and I project it. Notice how these points are now going to be projected. They're all going to be mushed together. They're all going to be squeezed together into this little tiny space, right? In comparison to projecting it along the blue line. The blue line, and you can see how spread out they are. And this is this idea of the straight line that sort of minimizes these green distances, actually the one that preserves or show you the variability the most. So here you can still distinguish between these points. You can still see the variation between them versus here they were all mushed together. This is how these two ideas are sort of related. Any thoughts before we look? Any thoughts before we look at some images, some examples of that? All right. So one example of high-dimensional space that we sort of emitted, or maybe, oh, we did mention a little bit, we talked about RGB, but another way to think of images is, you know, when you capture a picture in your phone, for example, Picture in your phone, for example, could be like an eight-megapixel image or whatever. You can think of, in this case, I'm looking at grayscale images. So each pixel value is, you can't see this color. I'm going to exaggerate this a little bit. So each pixel value, right, is a number between 0 and 1. And you can think of having a lot of these. And to be more specific, in these images, we have its 112. Have it's 112 by 92. So I have about 10,000 pixels, and these are 10,000 dimensions, right? And the idea is, for the example I'm going to show here, we're going to use PCA to go from 10,000 dimensions to about 100 dimensions. And the way that... Which is funny because you were before you're drawing pictures from 2D to 1D, right? And now you're going from 10,000 dimensions to 100 dimensions. From 10,000 dimensions to 100 dimensions, but it's really the same math, which is cool. Exactly. So, if you want to look at the analog, so here, for example, is an actual image in 10,000, let's say, dimensional space. And this is its reconstruction using only 100 dimensions. So, if I want to look back to the image we are looking at, the actual image was, let's see, the actual images were these black points. So, these are actual images. So these are actual images, and their projections are the green points, or these are the reconstructed images pretty much, except we're amplifying the space a lot. So, how does this work? So, if you remember, so these are this 10,000-dimensional data point. Dimensional data point. In PCA, there are things that are called eigenfaces. And if you want to link it back to our image before, these are our straight lines. So our straight line is a one-dimensional object. Here I'm thinking of my straight lines or what we call principal components as these images. So if you look at them, these are like ghost-like images. And the idea of PCA is And the idea of PCA is for each image you have, you're trying to represent it as a combination of these goals, right? So each phase is a combination of these goals, right? So it might be, let's say, 40% of this phase was typically it's not like a huge number, it's like 10% of something, 5% of another phase, 30% of another phase. So it's like a different percentages of these Of these ghost-like faces. And in our case, I chose only a hundred ghost-like faces. And using these 100 ghost-like faces, I was able to reduce the dimension of these images to 100 dimension and then be able to reconstruct them again. And as you can see, you can at least recognize these images. So if we look at this one and its reconstruction, you can see it's This one and its reconstruction, you can see it's a little bit lower resolution, right? It's a little smoother, but you can at least be able to distinguish these faces going from 10,000 dimensions to 100 dimensions. Any thoughts or questions? Maybe one comment before we go to a bunch of questions, which is, you know, this is really important when we're working at home, this dimensionality reduction of images, because Reduction of images because when your Wi-Fi is slow, right, and you try to load a web page, it's not that you can't see an image at all, it's that you see a blurry image. And the reason is because you're seeing a lower dimensional representation of the high-resolution image. And it's very valuable to be able to see that blurry image when your Wi-Fi is slow, because otherwise you'd be even more frustrated than you actually are. Is this also what's working when you're compressing audio? So, like when you're streaming Spotify, is that what's happening as well? Is that what's happening as well? I would say so. I mean, Laura? I don't know about this particular application, but it's probably, yeah, it's probable. I mean, PCA and SVD are quite used a lot in data compression. So you have an image file, you want to compress it into a slower or smaller JPEG or whatever resolution. And I guess this is what goes on. And I guess this is what goes under the hood. It's a slightly different process, but I don't know if that answers your question. So Brian says the eigenfaces are determined by the image, so they have to be recomputed for each image separately. Yes. Brian, what's going on here, and correct me if I'm wrong, Glara, is you're computing the eigenfaces from this entire data set of people, right? So from the 100 people in the 10 different pictures. In the 10 different pictures of each person, you computing the eigenfaces that work for all of them. And so then that's where you get the low-dimensionality, right? If you only need 40 ghosts to represent me or Laura or Brian or anyone else on the call, then you can sort of store this low-dimensional, this low-resolution picture of me as just 40 numbers. How much of these 40 ghosts do you need? How much of each of the 40 ghosts do you want to add together to get? 40 ghosts you want to add together to get Henry. So that's why I avoided to bring up data compression because data compression you can do it on a single image. So you can compress a single image on its own. But on this case, we don't really think of much of, you can think of, yeah, typically you don't think of much of eigenfaces and things like that. Rodrigo has a question, I see. I think I had a question, but I think Brian's question may have somewhat sort of answered it. But my question is: how do you come up with these eigenfaces? Because when you put the word eigen in front of something, I just have to think about linear algebra and a choice of vector that behaves in a certain way. And so I was hoping that you can sort of give some, make some comments about that because I have this intuition and I just want to know how this intuition should interact with what you're saying. What you're saying? Certainly. So, here each image that we have is kind of so great. So, there are two ways of representing this image. I can either represent it by saying it's a 112 by 92 image, or I can say it's an image of 10,000 pixels, right? And so, for each image that we take, which we call a matrix, is I can vectorize. matrix is I can vectorize it. So instead of having it in this 2D grid, right, I can take each pixel and store them as a one-dimensional or store each image as a one-dimensional vector. Yeah. And I would do that for all of the faces I have in my data set. So for each of these images, I would store them as vectors or one-dimensional. For one-dimensional objects. And you can imagine doing that for all of these. And now we obtain a new matrix that sort of is a collection of all of these images. And now, if you're familiar with the method SVD, we do SVD on this or PCA on this on this matrix. On this matrix. So the data points that I was referring to here, whichever one, these are the actual images written in this vector form. It's a list of numbers. So these are pixel values, let's say between 0 and 1. So this is the list of all of these. And we apply PCR-SVD on it. And the eigen. On it, and the eigen part of this word comes from the eigenvectors of these images. And one thing I wanted to say is that the dimension of these, again, I'm not sure how much to this was very helpful. Maybe I missed it here and you already said it, but this was very helpful. I now understand what you mean. And it's very cool to begin with. And anyway, thank you. Yeah. Anyway, thank you. Yeah. Yeah, I just wanted one last thing to say is that the dimension of these eigenfases is the same dimension as these images, but now you're using fewer of these to represent or reconstruct the images. Thanks. We only have a couple minutes left. Let me just make a comment. So my impression is that Rodrigo has learned about linear algebra. I don't know Rodrigo yet. Looking forward to meeting him more, but learned about linear algebra. More, but learned about linear algebra, but is not as familiar with PCA. And I think we as mathematicians are going to be rare breeds along those lines in the future in the sense that I feel like data science is going to be such a great introduction for so many people to get into linear algebra and then into mathematics. I mean, even for me, it was an undergrad data project that led me to become a research mathematician. So, you know, a big topic of conversation. You know, a big topic of conversation now is: hey, I'm so interested in data science, but I don't know linear algebra. You know, what should I do? Should I try to learn things intuitively first? Or when should I try to learn linear algebra? And it's a great opportunity for mathematicians to take, there's such a thirst for learning linear algebra, which is the language behind a lot of these techniques Laura and I are discussing, which is a great opportunity for us. You know, just like maybe video games led to more. Video games led to more computer science majors. You know, maybe data science can help us attract more math majors. Awesome. Henry, what are your thoughts? Would you like to wrap up today or give a brief? No, let's wrap up. If there's any final questions or comments, we'd love to hear them. Otherwise, we look forward to seeing you all tomorrow. So, yeah. All tomorrow. So, yeah, any other comments or questions? Cool, thanks. This was a lot of fun, and we look forward to the poster session coming up next. Thank you so much, Arki. Thank you so much. You can continue learning tomorrow from your second book. Okay. So, yes, we have a few minutes to start the poster session. And I want to remind you about the Jamboard activity. Please feel free to use the Jamboard for poster session and add any artwork that you have, and you use it for either your research. You use it for either your research or teaching, or add any sentences that you want to contribute about your research or your art piece. So the Jamboard is available. The link is on our registration page. Okay, so the breakout room. I'm going to create two breakout rooms, and each one of our posters will happen in one of. This will happen in one of these very courtrooms. And also, maybe I can send this Jamboard link in the chat room. So it shouldn't just say there is that. Yes. This is some people started to insert some fun images. We've got animations as well. This is great. Yes. Yes, that's great. That's great. So this is the link for the Jamboard activity for all of us.  So we can move from any room to, but there are also three rooms. One of them is for tetram, one of them is for max, and one of the rooms are for collective work. Rules are for our collective work for the hosters the year 2021 hosting.