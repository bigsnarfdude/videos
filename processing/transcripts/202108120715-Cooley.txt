Yeah, this is based on a project which is joint work with June Lee, who is one of the organizers, and Jean-Benouli Ravelomana, both from Frankfurt. And yeah, as I mentioned, unfortunately, we didn't quite manage to get the paper onto the archive. But before I start, let me just talk a little bit about what we'll be doing. I'll be talking about warning propagation and in particular. About warning propagation, and in particular, a proof method which has appeared in a number of different forms, but ultimately is doing more or less the same thing every time. So, what we decided to do was to attempt to sort of generalize it, put everything into a unified framework. Firstly, so that we unify all the previous proofs, but secondly, in the hope that this will prove to be a useful tool for people attempting to do similar things in future. People attempting to do similar things in future. And before I start, I would like to give a quick shout out to two more organizers, namely Amin Koyaoglan and Bichan Kang, because this project sort of grew out of a different project, which we were working on together with them. And we needed the results of this project for that one. So they were involved in the early stages of this project, but ultimately we kind of went separate ways. Kind of went separate ways. Okay, so to jump into the talk a little bit, I am going to start off by giving an example which everybody knows about, namely cause. And I will explain how a sort of approach that is used to the core problem is actually something similar is used in various other problems as well. We'll then sort of attempt to give a sort of unified description. Give a sort of unified description of how you can approach these problems, which we'll do in the language of warning propagation, which I will introduce. And finally, I'll get on to some results. And again, there I'll start off with one very special case and ultimately say how we can generalize it. So I guess that everybody knows what the K-core is and we all know about the peeling process. So if, for example, in this graph, I wanted to find the three-core. I wanted to find the three core, then I would kind of strip away the vertices which have degree at most two first, and then some other vertices maybe have low degree, and I strip those away, and I carry on until I can't really do anything else. And what I have left is, in fact, the three core. Okay, so we all know about this. So what are we going to do with it? Well, here's a question. Suppose we were to look at the Erd≈ës-Reni random graph, and this is most Random graph. And this is most interesting in the range when the average degree d is some constant. And suppose that we'd like to know about the order and size of the k-core in this random graph typically. Well, here's a proof idea. And now, incidentally, we see why I chose this problem as an example, because I get to name check another one of the organizers. So Malloy had a very nice proof in 2005. Proof in 2005, which, if I sort of am ridiculously vague about things, kind of goes as follows. Let's suppose that we were to analyze the peeling process for some number of steps, T0, where this T0 will be large but bounded. And suppose we can do that, then it turns out that after that, very little is going to change. So after T0 steps, you're pretty close to where you're going to be at the end of the day. At the end of the day. Okay, well, that was all very, very vague. So let me try and be a little bit more precise about this, at least in heuristic terms. So this step one, analyzing the peeling process. If we're going to analyze T0 steps and figure out what happens at our favorite vertex, then of course it's enough to look at the neighborhood of this vertex up to depth T0. And this neighborhood is sort of has the structure of a Is sort of has the structure of a Poisson deep branching process. Okay, so we kind of pretend that it is a Poisson deep branching process and we track the peeling process on a tree that looks like this. And then we can kind of say, well, the survival probability of the root at the end of that is some alpha of T0, right? So this will, of course, change as T0 changes, but okay, you can calculate what this is. And it turns out that this. And it turns out that this tends to some limit, which we'll call alpha star. Okay, so we sort of imagine that a vertex survives with probability alpha star. And then heuristically, we can sort of imagine what the surviving local structure looks like of all the vertices that have survived the peeling process so far. And if I lie a little bit and make some approximations, then I can describe it in terms of another branching process. The branching process. Okay. And then I sort of say, okay, what happens here if I were to delete the root? Okay, so this may of course have knock-on effects on its children whose degree goes down, but they must only get deleted if they themselves had exactly k minus one children. Otherwise, this deletion will not propagate any further. And so if you do some calculations, you can kind of figure out. Calculations, you can kind of figure out how this deletion propagates or not. And it turns out that the propagation of deletions is actually a sub-critical process, meaning that it would tend to die out quickly. And if you've sort of formalized this, that is exactly the same as saying that after T0 steps, not very many more things will change. So, of course, I've skipped over all sorts of details here, but that's more or less this. That's more or less this proof of Molloy from 2005. And at this point, I should mention that this sort of strategy was already suggested as a heuristic about 10 years beforehand in a very famous paper by Pittel, Spencer, and Warmold. But yeah, this paper of Molloy made it formal. Okay, but that's just one example. It turns out that we can sort of do something similar in a number of different settings. A number of different settings. So let's suppose that I want to analyze some combinatorial structure. It might be the K-core or it might be something else. Well, we're going to define a recursive algorithm to produce it. In this case, it was the peeling process, but it might be something else. And we're going to analyze this recursive algorithm for some number of steps, T0. And hopefully we can do that. And then we're going to show that subsequently very little will change. So after T0, you're Little will change. So after T0, you're sort of close to where you're going to be at the end of the day. Okay, well, I mean, this is again all very vague, but this sort of approach has been adopted in various other problems. So I'll just give a few examples. So we already talked a little bit about cores, not just in graphs, but also in hypergraphs. In fact, the paper of Molloy that I've been mentioning really talks about hypergraphs. But you can also do other cores than. But you can also do other cores than the obvious ones, maybe multipartite graphs, for example. You can do all sorts of things there. Also been applied in constraint satisfaction problems. And I mean, in some sense, not quite a constraint satisfaction problem, but closely related is the sparse parity matrix. I mentioned this because, yeah, I'd like to give a trailer for a talk that's going to happen tomorrow. So one of my co-authors, Jean, will be Of my co-authors, Jean, will be talking about the sparse parity matrix, and that's the problem that this all arose from. We wanted to solve something in that. And I'll also mention planted partitions to bring this all back to the title of the conference. So suppose you were to have, say, the stochastic block model and you want to reconstruct it, then you can use this sort of approach there as well. Okay, and there are more examples. Okay, and there are more examples, but I don't want to go into too much detail. So we'd like to somehow unify this, provide a framework that we can talk about all of these problems together. And what we're going to use is warning propagation. So this is a message passing scheme that is kind of related to belief propagation, which we've also seen in some of the talks in this conference. Introduced by Mezard and Montanari in By Mezard and Montanari in 2009. And I'm just going to give a specific instance. So, again, I'll be talking about cores, in this case, the three-core. So, just as an example. So, here we're going to say that each vertex is going to send its neighbor's messages, and these messages will be 0 or 1. So, along every directed edge, you have a message passing from one vertex to its neighbor. And this 0 or 1 is supposed to have some. This zero or one is supposed to have some meaning. So, in particular, in an informal way, a zero is supposed to signify that a vertex is saying to its neighbor, look, I'm not in the core, you can't expect any help from me. Whereas a one is supposed to indicate that the vertex is telling its neighbor, well, if you're in the core, then I'll be in too. So, I can help you out if you need it. Okay, well, you know, this is all very well and good, but Is all very well and good, but how do you generate these messages? Because we don't know about the core a priori. So we're going to initialize in a very optimistic way. Everyone is going to imagine that they're in the core. So all of the messages will be one initially. And we're going to update this as more information comes along. So we're going to have an update rule which will say the following. We'll stare at the message from a vertex U to a neighbor V, and you will look at the message. And you will look at the messages that it receives from its other neighbors apart from V. And if two of its neighbors are sending one, then it's going to say to V, well, yeah, okay. I mean, if you're in, then I'm in, right? So I have two guys helping me out. So if you're happy, then I'm happy as well. Whereas on the other hand, if it only receives at most one message of one from its other neighbors, then it sort of doesn't matter what V does. It's never going to be happy. It's only going to have... Going to be happy, it's only going to have degree at most two. So, this will be the update of messages, and this is maybe sort of quite abstract if you haven't seen it before, but we'll see an example in a moment. I'd also first like to mention that we also keep track of some vertex types. So, we're going to mark the vertices according with marks sort of C or N, and a C will indicate that it really. A C will indicate that it receives at least three messages of one, and an N that it receives at most two. So, this is sort of supposed to indicate whether a vertex is in the core or not. Of course, initially, this may all be wrong, but hopefully at the end of the algorithm, everything will be correct. So, here's an example. Suppose I'm looking at this graph and I want to find the three core here. Okay, well, you can probably see the three core instantly, but we want to understand. But we want to understand the algorithm. So let's start off with our initialization. We're going to send messages of one along every directed edge, which kind of looks a bit scary, but okay, everything's one so far. And we're also going to give each vertex marks, so according to the number of ones it receives. So, for example, this vertex here receives five ones. So, okay, that's more than three, it gets a C. We have a bunch of vertices that receive. Have a bunch of vertices that receive fewer ones, so they get an N. And then we're going to stare at our update rule, which I've repeated here. And you can check this out, or you can just believe me that these messages change from one to zero in the next step. So some things change from one to zero. And then, of course, we need to update the vertex types as well. So if we stare at this vertex, it used to receive five ones, but unfortunately, three of those ones now change to zero. So it only receives. To zero, so it only receives two, so it had better change to an n. Okay, and yeah, then you can go through the next update step. And I promise that these messages will change from one to zero now. And as it turns out, this is sort of the last thing to change. And if we now stare at this graph, then yeah, okay, the vertices labeled C are exactly the three core. Oh, good news. Okay. Well, that, of course, is not a coincidence. You can prove that this is genuinely correct. That this is genuinely correct. You do come up with the three core at the end of the day. But more than that, if we kind of look at what happened to the vertex types throughout the process, then it turns out that they exactly track the peeling process. So the vertices that had this sort of label N initially, those are exactly the vertices that get peeled off in the first round of the process. Whereas this vertex, which is the one that changed in the second step, That changed in the second step, that's the one that gets peeled in the second step of the peeling process. Okay, so among other things, this algorithm gives us information about the peeling process. We can recover the peeling process from it. I mean, it tells you a lot more, but in particular, it tells you this. Okay, so what are we going to do? Well, we can ask the following question. Suppose I were to pick a random directed edge in this graph and say, what's the message? In this graph, and say what's the message that goes along it. Yeah, I mean, of course, this is a random object, so I want to say what's the probability that it's equal to one. And I'm going to give you two different answers to this question. So the first answer is to say, well, you know, we're in g n d over n, so we have something like dn directed edges in total. And let me suppose that alpha star is the proportion of these. Proportion of these messages that are one, right? Well, then, if I pick a random directed edge, the probability that I pick up a one is exactly the proportion, right? It's alpha star. Okay, well, I mean, that's all true, but I mean, it's not very exciting. What I said there was more or less a tautology. I just gave it a name. So, let me give a second answer to this question, which is maybe a little bit more interesting. The message from U to V. The message from U to V is dependent on the message that U receives from its other neighbors, which I'm now going to call children. Okay. So let me see if I can figure out what messages it receives from its children. Well, how many children do I have? It's Poisson D, right, in this random graph and this regime. And I'd like to say that the messages from the children are each one with probability alpha star. One with probability alpha star, as I did before, but is this independent? Well, if, for example, I had some common child of both C1 and C2, right? So they were both adjacent to a further neighbor, then the message coming up from that neighbor might be more likely to be the same to C1 and C2. And that kind of would mean that the messages from C1 and C2 to you are dependent on each other. That would be a problem. Would be a problem, but basically, it turns out that we know that the graph is going to contain very few short cycles, right? So we can sort of ignore those. And that was the example that I gave here. And yeah, if you rule out short cycles, then the dependencies you have are sort of long-range dependencies rather than short-range dependencies. And heuristically, at least, you can sort of imagine that these will not have very much effect. And so we can sort of treat them. And so we can sort of treat the messages coming from the children as being independent and identically distributed. And so what we can say is that the number of one messages that use receives from its children is this Poisson alpha star d. And, well, it sends a one to its parent if that random variable gives you at least two, right? That was the definition of the update rule. Okay, so I now gave two answers to the same question. The same question, and if I've done something sensible, then those two answers should actually be the same, right? So, on the one hand, I got alpha star, on the other hand, I got this probability. So, this should really be the case, right? And in other words, we have this implicit equation for alpha star, so we know that alpha star is a fixed point of some function phi d, which is this probability of a Poisson. Okay, uh, so. Okay, so this means that we can formulate the standard strategy in a sort of similar way to before, right? So not just for the core problem, but perhaps more generally, we're going to say that we have some, like, say, a proportion of ones, alpha, which is dependent on the time t, right? So how many steps of warning propagation we've done. And, you know, we're going to say that it's a, it's, you know, That it's a, it's, you can apply this update function to your initialization alpha zero. You apply it t times, and it turns out that it converges pretty quickly to its fixed point. Okay, good. And then we say, all right, if this fixed point is somehow stable, whatever that means, then you're kind of going to stay there. You're not going to overshoot or fly off somewhere else, right? And stability in this case kind of means that this update function has derivative. That this update function has derivative less than one at this fixed point. In some other problems, it might be more general because you might have a multivariate distribution or something. In general, stability is really intrinsically connected to the subcriticality of the subsequent change process. So, we talked, for example, in the K-core problem about deleting a vertex and the subsequent effect that this will have, and it turns out that that's a subcritical. And it turns out that that's a subcritical process in some cases. The reason it's subcritical is exactly because the fixed point is stable. There are some reasons behind that, but I mean, this is intrinsically entirely, intuitively entirely natural. So I don't want to go into too much detail. So let me, in the last five minutes or so, talk about the results that we've obtained. Well, firstly, I could kind of say that, yeah, in this standard strategy, Say that, yeah, and this standard strategy that we outlined in very broad terms works for the k-core in this random graph that we were looking at. So, in that example, we had everything works nicely in the sense that, okay, you can look at t0 steps and the local structure will be a Poisson D tree, but you can also say what the messages from the children of a vertex are up to it, and they're independently identically distributed as. Identically distributed as some Bernoulli alpha star, right, where alpha star is the appropriate fixed point. And once you've done that for T0 steps, after that, very little will change. Okay, well, you know, this is all very nice, but not new at all. I mean, this is more or less exactly the proof of Molloy from 2005. I mean, I stated it in slightly different and much more complicated language, but it's essentially the same thing. It's essentially the same thing. But what's really new here is that we're able to generalize this to a much more diverse range of situations. So we can generalize it in the following way. Firstly, we allow your warning propagation rule to be arbitrary on some finite alphabet. Of course, if you want to look at something other than the K-core problem, you shouldn't be looking at this warning propagation rule, which was designed. Warning propagation rule, which was designed for that problem. So, you are going to pick your warning propagation update rule to be the one that will actually work for your instance of the problem. And we don't just have to look at the Erd≈ës-Renny random graph. So we can generalize that in various ways as well. In particular, we can allow your vertices to have different types. And these types may behave differently. Behave differently. We are allowed to prescribe the degree distributions. So you're allowed to say that I would like my vertices of type 1 to have this degree distribution, vertices of type 2, this one, and even specify which types of vertices they're supposed to be adjacent to and so on. And finally, you don't just have to start with some obvious initialization, right? So for example, in the core problem, we started with everything being one. Started with everything being one. You could start with everything being zero if you wanted, or you could start with something in between, right? So you could let every message initially be chosen according to some random distribution. So maybe it's one with probability a half, for example. Okay, so I'd like to just briefly comment on some things that this would cover, for example. So the model of random graphs. The model of random graphs that we would be allowed to look at would include, for example, the stochastic block model. So you have types V1 up to VK, maybe, and you have different edge probabilities between types or within classes. You could do that. It would also cover, for example, random regular graphs. So when I say we can prescribe degree distributions, if you want to, you can prescribe it to be deterministically d. It to be deterministically d every time, and that would exactly give you random regular graphs. And also, if you're interested in hypergraphs, I mean, this is all formulated in terms of graphs, but if you turn the random hypergraph into its factor graph, then it turns out that you can cover that case as well. Okay, but I mean, of course, I can't say that it's always going to work, right? I mean, any warning propagation rule, any graph in this. Any graph in this model, there will be some cases where the results are just not true. So we have some conditions that you will need in order for the results to be true. But our hope is that people studying similar things will basically just have to come along and check that these conditions are satisfied and then say, okay, good. This theorem will do all of the heavy lifting for you and you don't need to do it yourself. So I'm going to list these conditions that we require in an appropriate We require in an approximate order from what I believe to be the most fundamental ones, as in you really need this, otherwise, there's no hope that the results would even be true, to at the bottom, maybe the more technical ones, which we require for our proofs, but maybe you can hope that these could be weakened or got rid of in some way. So, first yes, I can. I can. We are going to choose our messages according to. Messages in according to some initial distribution, you need this distribution to have a limit, right? If you hit it with the update rule many times, right? That's kind of obvious. You need this limit to somehow be stable in some appropriate sense, which I won't get into. That's also obviously necessary. And we need bounded average degree because short cycles are going to mess you up and cause dependencies. So these are all pretty fundamental. Are all pretty fundamental. Slightly more technical, we kind of need that if you have a fixed degree sequence, then any two graphs with that degree sequence are equally likely. We can weaken that condition a little bit, but I won't go into it. And finally, a couple more things. We need the vertex classes not to be too unbalanced. So within a multiplicative constant factor is okay, but yeah, not outrageously different. And finally, we can't have too many. And finally, we can't have too many large degree vertices. So you need your degree distribution to be falling fast enough. But up to these conditions, which are generally fairly easy to check in most examples that we think that you would want to apply this to, we can kind of say that this warning propagation method does actually work. So at that point, I'd like to thank you very much for your attention. Thank you.