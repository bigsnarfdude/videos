Everybody, I would like to thank Anna and Matt and Stefan and Warren for the kind invitation. And yes, this work that I will speak about today is a joint work with Andrea and Stefano. And it's about a new interior point method for derivative-free optimization. So this is the outline of the talk. It's basically five parts. Five parts. In the first part, I will briefly introduce the setting of the problem and the black box optimization. Then I will switch to a very basic algorithm that will be used in the third main part of the presentation. And then I will discuss some convergence analysis for the main algorithm and then report some, let's say, preliminary. Let's say preliminary numerical results that we collected recently. So, this is the setting in which we move. We have a constrained optimization problem. Both the function, the objective function, and the M constraints, non-linear inequality constraints, are the results of a black box. Black box, and we assume that the calls to the simulator are expensive, and we also assume that the objective function is not defined outside of the feasible region. And of course, the derivatives are not available to us because they could be untrustworthy, or they could be difficult to obtain, or too expensive. Or whatever. So, these are motivations, but I would skip this because I mean, all of us are really into the real optimization. And these are non-exhaustive list of other approaches and papers in constraint related free optimization. I will not go into all of I will not go into all of them just to see, just to give you the idea that people are working on constraint documentation and derivative-free methods for constraint optimization in recent years also. And so what we assume here is that we have to do that. What we assume here is the, of course, as I already said, the functions defining the problem are of the black box type. But we assume that even though we cannot use their derivatives, they are continuously differentiable functions. And we assume that the constraints, the inequality constraints, are not relaxable. And And we also assume that the feasible set defined by the inequality constraints is compact. And of course, we assume that the interior of the feasible set is not empty and that we know a strictly feasible point. So, okay, here's the definition of stationarity for this. Stationarity for this constraint problem. So, what we want to do is to solve the constraint problem by resorting to the use of log barrier penalty function of that type over there. And so, we define that merit function. The merit function on the feasible set, of course, given this barrier parameter. And so given a barrier parameter, we can consider the basically unconstrained problem where we minimize the log barrier function on the interior of the feasible set. So, of course, when μ is fixed, this is essentially an unconstrained problem. And as we will see in a few slides, for mu fixed, this problem can be solved by adapting. I mean, every derivative-free methods, I will consider a method. Consider a method of the class of line search methods. But of course, to solve the original problem, the barrier parameter must be driven to zero, must go to zero, in order to get convergence to stationary points for the original constraint problem. So, this is the basic line search algorithm with just a few modifications to take into account the Modification to take into account that we have to stick to the interior of the feasible set. That's the calligraphic S set. And so this method basically uses the coordinate directions as search directions. We have this alpha tilde over there. They are initial tent. Initial tentative step sizes, and in every iteration of this method, the coordinate directions are checked to see if degrees along them can be obtained. And in that case, a line search is performed to exploit these degrees. So, we, of course, have to control also the feasibility. Control also the feasibility of the tentative points. So, if we have feasibility and if the merit function can be sufficiently decreased along the given direction, then we exploit the given direction, performing a line search along it, and we compute this actual step size alpha. If this is not the case, then the opposite direction could be a good descent direction, so we check the opposite. Good descent direction. So we check the opposite direction and we basically do the same thing. And if none of the these directions, if along none of these directions, we can achieve sufficient reduction of the merit function or feasibility, then we deem the current direction as a failure. And so we set to zero the actual step size. Then we, of course, update. Then we, of course, update a point, and there is room for a search step to search for further improvement of the merit function after checking all the coordinate directions. So, this is the basic line search algorithm for derivative-free algorithm. The only difference is that we maintain Is that we maintain feasibility of the hyderates. The expansion step, which we use when sufficient degrees is deemed along the direction, and is used to exploit the good direction, with the send direction. So the expansion step is activated when the three conditions on top are satisfied. So the current point, the tentative point are feasible. The tentative point are feasible, and we have sufficient degrees. In that case, we try to expand the step size as much as possible by looking for a new point, for a new step size that has that formula over there, which gives provides again sufficient decrease and the feasibility of the point. Point and by expanding even more the step sites, we get a failure in the sufficient decrease condition or a failure in the feasibility. So, if the expansion step is activated, what we get is a step that gives sufficient decrease. Gives sufficient decrease, sufficient reduction, sufficient decrease, and a step size, which is bigger than the actual step size, which gives a failure, meaning a failure in the sufficient degrees or a failure in feasibility. So very basic properties of the, I won't go into details of these basic properties, they are very basic and so the line just. So, the line, the expansion step is well defined. That is, it always produces a step size if we activate the expansion step size. And all the step sizes defined by the algorithm, that is all the actual step sizes and the tentative step sizes, all of them go to zero. And of course, exploiting this. Exploiting this property, since that quantity, the maximum of the step sizes, can be used to bound the stationarity of the hyderates, then we can also prove that the limit point, every limit point of the sequence defined by the algorithm is stationary for the merit function. Of course, here μ is held fixed. Up to this point, μ is still fixed. Mu is still fixed. So, what we have to do now is try and modify this basic algorithm that we just saw in introducing some mechanism to update the barrier parameter so that we can approximate the original problem more and more and more. And in this way, hopefully. This way, hopefully, obtaining convergence towards stationary points of the original problem. So, the main ingredients that we have that we can use are, of course, the algorithm that we just see. And we also know that the barrier parameter cannot stay fixed. It must go to zero. Go to zero, and so we have to define a rule to go to drive to zero the barrier parameter. And all the building blocks to do such a thing are attend to us because we have the following quantities computed by Following quantities computed by the algorithm or defined by the algorithm. So we have the maximum of the step sizes, which we can use as a rough measure of stationarity for the basically unconstrained problem. And we know that the barrier parameter gives us a rough measure of the A rough measure of the quality of the approximation that the merit function gives of the original constraint problem. And we also have and we also use another quantity which measure the measure how close the iterates produced, how close the points produced by the algorithm are to the boundary of the feasible. R to the boundary of the feasible set. That is this Jimmy K quantity over here. Then to prove convergence, we see that the measure of stationarity must go to zero faster than the two quantities mu k and g mean k. This means that the This means that the first order information that the method recovers by sampling the merit function must be recovered faster than how rapidly the precision of the approximation gets better, and also faster than how rapidly the sample points approach the boundary of the feasible set. So we get to put together all these requirements and we propose this updating rule. This updating rule, meaning that when this inequality is satisfied, we check this inequality. When this is satisfied, we update, we decrease the bearer parameter. So the algorithm, it's exactly the same. It's exactly the same, unless that we have mu k now here, so mu is no longer fixed. And at the last after the minimization on the coordinate direction of the merit function, we check for the barrier parameter update, and of course. And of course, we can, there is room for also for a new point that gives that improves the final point defined by the inner cycle right here. So, the algorithm is exactly the same. The only difference is this if here, the barrier parameter update and And it is not too difficult to prove that both of this one and two, both of the quantities, the maximum, the step sizes and the barrier parameter, both of them goes to zero. Just a rapid sketch of the proof. So we first prove one. So we assume by contradiction that the By contradiction, that the barrier parameter doesn't go to zero, meaning that for case sufficiently large, it stays fixed. And if this is the case, then for case sufficiently large, we have this condition, and then the merit, which means that the merit function converges to a limit p bar. On the other hand, by the updating. By the updating rule, which we recall is the updating rule means that which is not satisfied because the barrier parameter stays fixed for k sufficiently large. This means that the maximum of the step sizes is greater than the minimum between mu bar and gmin k. And so that gmin k Gmin k converges to zero. This means that xk converges to the boundary, to a point on the boundary on the physical region, for which the merit function is plus infinity. And then proving two is at this point is very simple because of one and of the updating rule. Updating rule. So we have the basic ingredients to prove convergence. And in fact, of course, to prove a convergence, we need a farther assumption. I mean, meaning that the feasible set must be well behaved, sort of well-behaved. And so we require the Mangasarium from all its constraint qualification condition. And which is needed to prove the main convergence proposition of the algorithm. And the MFCQ is basically needed to prove point I over here. That's these that the sequences of the Lagrange multipliers are bounded and Are bounded. And so we can manage to prove that every limit point of this sub-sequence of the iterates, the sub-sequence defined by the case when the barrier parameter is updated is a KKET point. So again, a very rapid sketch of the proof. The very difficult part of this proof is point I, which in fact I won't speak about. Which, in fact, I won't speak about point here. But okay. So we have to remember that XK belongs to the interior of the feasible set for all k's, and the feasible set is compact by assumption. So we can so we have this, and then to prove two. We use the sufficient decrease better than that, we use failure in the sufficient decrease produced by the algorithm and the mean value theorem to write this inequality here. And then we are almost there. What we need to do is to drive, to take the limit for k to infinity ending. limit for k to infinity ending k ending capital k and use of course this one because because we have to to prove that the multipliers are are bounded because otherwise we cannot prove such an area and that's it's exactly where the mangas areium from all its conditions come into play come into action uh And basically, okay, for the numerical results, I need to say something about first I need to say something about extensions of the what I presented because I just talked about this type of constraints, but of course more general problems. More general problems can be considered, and the same convergence properties can be proved in this case, where we have also equality constraints, which of course are not treated by the log barrier merit function, but by a sequential penalty approach. And And we also allow for the presence of bound constraints on the variables by, and we handle the bound constraints explicitly in the algorithm. I didn't consider this setting in the presentation because the presentation would have been too cumbersome and hard to follow. And also, we can also allow for inequality constraints that are violated by the initial point, which can be the case. And when this happens, those constraints are treated like equality constraints by a sequential penalty approach. Uh, penalty approach like a progressive barrier, you know, just like the equality constraints are handled. So, for the test set of problems, we use the q-test collection and we selected problems, of course, with both inequalities and equalities and problems with Problems with a starting point that satisfies strictly at least one inequality constraint, of course, because we wanted to test our method and our method needs at least one inequality constraint to be strictly feasible. This gives us this 99 problems, and with these dimensions, so number of variables. Number of variables varying from two to 41 and number of constraints from one to 44. And this is to have an idea of the distribution of the number of variables in the test set and also to have an idea of the distribution of the proportion of the strictly feasible inequality constraints with respect. Inequality constraints with respect to the number of total number of inequality constraints so so for comparison we used NOMAD the version 3.9 of course and this this this is um This is what we have done. We use the default setting of NOMAD, except that I don't know if this is to do a fair comparison with our algorithm and also pretending to be considering a problem where some constraints cannot. Problem where some constraints cannot be violated. We specify the extreme better approach for door constraints that are feasible at initial points and the mixed approach progressive and extreme for all the other constraints. And comparison is done using the performance and data profiles. It's quite It's quite classical, just we define this f at of x0 because the initial point could be and is in fact invisible in almost all the problems, I think. A fart thing that we do before Before comparing our method log BFL, is to try to improve it a little bit by using some quite known behavior of interior point methods. The first thing is that between two consecutive points where we update the penalty, the barrier parameter. The barrier parameter, a descendant direction can be defined, and this descent direction is a good descent direction, frequently a good descent direction. It points, in fact, toward the central path. And furthermore, we try to reproduce the behavior. Reproduce the behavior on NOMAD using the mixed approach. So, also in our code, when a constraint that is initially validated becomes strictly feasible, we switch the type of penalization for death constraints from a sequential exterior penalty to an interior penalization. Interior penalization. So, these two improvements, in fact, improved the method. And here in blue is the improved method with heuristics. And so we compared with the NOMAD the improved method. Not the basic one, but the improved method. And so this is the comparison with NOMAD. Comparison with NOMAD, with the default settings, and on the entire test set. And we also run a comparison on a subset of the test set of problems where both methods find a feasible solution. Again, normally with its default setting, meaning that With its default setting, meaning that, for example, models and also the initial search using the NALDRAMID, I think heuristic are activated. And in fact, NOMAD is really performing, it's really good. But then just to see how our globalization strategy compares. Globalization strategy compares with the globalization strategy of NOMAD. We tried to disable models in NOMAD just to see how the two methods, the two strategy compare to each other. And that is the situation. Again, these are the results on the entire test set of problems which in which includes also problems where none where one or the other of the algorithms compared cannot find the feasibility and then we we compare on the subset of problems where both methods find a feasible solution but but again i But again, I have to say that this is not a default version of NOMAD. And again, I have to say that we run NOMAD with the default settings, but I know that maybe that is not the best thing that we can do, but we don't have Sebastian or Charles near us to guide us choosing the best set of parameters. So this is what we did. So, this is what we did. And so, these are the conclusions. And so, we define this log barrier penalty. We proved convergence to stationary points, of course, without using dense set of directions, because at every single step, the function that we are minimizing is moved under the given assumptions. And we have pretty good numerical. Pretty good numerical preliminary numerical results. And the code is already available on the DeliverD3 library. And we are also planning to extend, but this is very, very future, very, very possibilistic extension. And that's all, I think. Yes, thank you for your attention.