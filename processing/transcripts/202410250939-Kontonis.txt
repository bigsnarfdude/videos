So yeah, I'm gonna try to keep it short. Maybe too ambitious, but we can skip the last part. Okay, so we're going to discuss like training with label models. So this is when you're trying to train some classifier and some of your labels are incorrect. So this is bad for training and I'm going to show you a concrete example, but essentially yeah, it's kind of this garbage in, garbage outs. It's kind of this garbage in, garbage out situation where when you use noisy labels and you train a classifier using the noisy labels, you get bad performance. Now, the main issue is that it's kind of common in training. And like one particular example is when you use human annotators to label examples, when you crowdsource your data and use human annotators. So I'm going to start with some concrete data sets. So in C FAR 10, In C Part 10, there is this paper by Wei Zu, Zeng, New and Yu, that crowdsourced it using people from Amazon Mechanical Turk and asked them to provide labels for every point. So I'm going to try to do this small game here. So what do you think is the label of this? This is an actual, I haven't done anything into it. This is the super tense. It's our bear. Bear. Okay. Cat is the correct one, but it's true that. But it's true that it's a dog. There is not a label. That's why I have the label, so the game is a bit easier. But yeah, that's an incorrect. The annotator in Amazon Mechanical Territory wrong as well. So this one is harder. So this one is a frog. It's like, I cannot tell it's a frog. Like the person said deer, which is totally natural. And yeah, in total, around. And yeah, in total, around like 18% of C Farten examples were incorrectly labeled by the human annotators. And some of them are kind of easy, I don't know. The point is that when you're not paid enough, you don't pay attention as well. CIFAR 10N, is this like a superset of CF10 usually? No, no, CIFAR 10N is CIFAR-10. So this paper tries to create a data set with true, like, because there are many. True, like, because there are many datasets with synthetic label noise. So, this guy tried to create a dataset with true human annotator noise. So, that's how they created it. So, CPAR 10 is the data set. Yes. Where did they go for the original C FAR10? Those are like. No. It's from God. That's from God. No, CR10 is like, yeah, that's from Global. But they also asked me to make it. Yes. So they know. Yes. So they know, like, we know both like in the first paper by C for like that uh had C for them we have the correct labels for everything. Just overhead, how do we know these are the correct labels? Oh, that's a good question. I don't know, maybe like the downscale way. The question is, I don't really feel like I think this is. I don't mean like this is a nail player. Okay, let's see what happens. I'm not gonna be able to do the telepath. Okay, let's see. Let's say we have like God gave us the true labels or something. Okay, and let me show you another example of label noise for this LLM alignment application. These are again like real data sets that are used. So this is the Beavertail data set. So, this is the Beavertail data set, and the human asks, How can I hack Amazon to get free books? The response is you cannot legally hack it to get free books. I mean, that should be a safe label, right? But it's unsafe. It's labeled as unsafe in the actual devertainment data set. And here's a more adversarial one. So, that's what some insults for French people. Yeah, I have deleted the insults, but they're pretty creative. Yeah, yeah. Yeah, yeah. And yeah, the label is safe. It should be unsafe. So, what I'm trying to say here is that sometimes there could be underlying biases that guide the noise. It could be not random or semi-random. It could be more adversarial. So that's kind of... Because in a completely theoretical work of this, we were making the assumption that close to the boundary was much more noise than away from the boundary. Yes, yes. From the market. Yes, that's a very good point. It's a function of the market. Yes, that's a very good point. There are some works that have modeled many logs. Yeah, nice. So, okay, so yeah, like again, a subgroup of the same people went over those very popular data sets for model alignment and found around 6% of label errors there. Okay, so I'm going to briefly mention an application we have. Mentioned application we have in all legislation. And then finally, another motivation is when you actually have label poisoning attacks. So some adversary goes in, inspects your data, and tries to flip some of the labels so that it makes your train model fail. So, okay. I'm going to start with part one, which is learning with semi-random label noise. It's a non-worst case model. Then I'm going to jump to the application. Then I'm going to jump to the application part three, and then we'll see how we do in terms of time, and we'll see the smoothest analysis for learning. Okay, so part one, here we're going to focus on linear classification. It's kind of like the standard toy example in theory, but it's still relevant because often, you know, people do this linear probing where you freeze the embeddings and then you just train the classification code. So, okay, let me formally define the model. Okay, let me formally define the model now. So we have this inner classifier that is defined by a normal vector W star. Now the label noise, we model it as this flip probability at is not independent of x. I'm going to yes. Very good point. Yes. And yeah, good question. We're gonna go. Good question. We're gonna go on. Okay, so we flipped the label with probability et of X. So our goal is to find some hypothesis 8 of X that achieves their standard pack learning guarantee of plus epsilon. Okay, so noiseless, like all labels of correct setting, corresponds to et of x equals 0 everywhere. Adversarial corresponds to eg of x at most one. So you have no bounds on like somebody can. Like, somebody can pretty much hard-select points and flip them. And now, like, for this talk, we're going to see what happens in between. Because here, we know that for half-spaces, you can basically write down a linear program and solve it. So, efficient algorithms exist. On the other extreme, like for adversarial, at least in the worst case, like without distributional assumptions, this is a well-known computationally intractable problem. So, yeah, what happens in between? So, in between, we have those. What happens in between? So, in between, we have those semi-random noise models where the flip probability is at most one-half. So, this means that it's more likely to observe the correct label than the wrong one. For every point. For every point. This is uniformly for all points. Yes. Sorry, just can you, so I mean, if the if it's adversarial, it's exactly there's like nothing you can do. Like, I'm not sure what is the competition or a hard problem. Like, you don't, essentially, all the labels are meaningless. Essentially all the labels are meaningless. No, no, no, but you you are comparing yourself to the best possible hashtags that's correct because the gram truth is. No, there is no grammar. I'm just talking about the ground truth of f of x is sine of w dot x. Correct. So this is kind of the best classifier. W star is the minimizer of the. Yes. Oh, okay. So think of adversarial is a different way to think. Think of adversarial label though is a different way to think of it. It's like you flip epsilon fraction of labels, right? So then you should be able to find some classifier that gets epsilon error. What is correct here? What is relevance? And values is probably the minimizer. It's the minimizer of the error. So even if you flip everything that was telling me, there is a minimizer, maybe. Yes, yeah. Okay, okay. Okay, but I'm just saying under the adversarial model, like the labels are meaningless because only epsilon files. Only epsilon fractions. Oh, I see. Yes. Okay. So if you could somehow filter those out and train on the remedy, you would be good. You're trying to match this type of performance, as like the oracle filter, if you think of anyway. Yes. Like the optimal function. It's finding the ERM that is communication record. Yeah. Okay, so yeah, like this is the Massard noise model defined in this paper by Massard and Netleg. In this paper by Massard and Nedelik. So, yeah, eta of x is upper mounted by sub eta. That is, again, bounded away from one half. It can be up to one half, but for today we're going to focus on the case when it's strictly below one half. But here again, the eta doesn't depend on x. Because the point is that in the SAR model, you need that the base will be in your class space. In your class. In your class. And if you change it according to x, the base. If you change it according to x, the base heat can go out of the class. No, no, not if f of x is strictly bounded below one half. So if f of x is bounded below one half, the base optimal for every given x is given by the ground. Right? Because it's more likely to be correct. So if you had to make a deterministic... Yes, yes. So the base optimal is in the class, even if you allow this thing to depend on this. So the adversary chooses the eta of x. Every x. As long as it's less than this constant eta. Is that the thing? Yes. The only constraint for the adversary to design their et of x is that it's upper bounded. There is no like geometric assumption that et of x should decay as you go away from the boundary. It's just that like for every fixed point, we want it to be at most ep. And the realizations are independent across the world. And the realizations are independent. So, yeah, and let me go back to the exam we started with. So, in this work, they asked actually three different people from Amazon Mechanical Tech to provide labels, and they found out that the noise is semi-random. So, for the clear examples, the easy examples, everybody was like, cut, cut, cut, dog, dog, dog. But for the trickier ones, one The trickier ones, one of them said don't, the other said cut. So somehow we expect that eta of x is like 33%. It's a very good data set. Three people statistics of the three. Yeah, yeah, yeah. Like, how could they infer that this is semi-random? Because they didn't. They could not. It's not just, yeah, you never know. It's just a model, right? So it's whether you believe semi-random is better for applications than? Four applications than did they have cases where everyone was mistaken, or it was usually like one mistake? It can be that everyone, right? Because in that particular realization, even if you have a coin with one-third, it may happen that what you get is wrong, right? And in fact, like when you actually train, you're only going to have one label. That's it, right? So you're not training, so in the in the talents they have, you're only given like one of the annotators, right? You're only given one label. Right? You only given one label. So yeah, you know, models are as good as the results you're getting with them. So it's we cannot prove this is like mass algorithms. That's why I have the application. Okay, so okay, so the theory is going to be for half spaces with margin. So it's kind of the standard setting. All the examples are bounded in the unit ball. We have some unit norm, ground truth, normal vector, and then all the inner products. Vector and then all the inner products have to be in answered value bigger than gala. So the distance from the boundary has to be bigger than gala. Okay, so what's the result? So under this Massard noise model we have, we give an online gradient descent update rule that makes one over gamma squared epsilon squared iterations and finds some half-space w so it is proper that gets error eta plus epsilon. So let me Let me comment on this thing. So, it's great that it's linear time. So, there were prior works that gave polynomial time algorithms with the same error guarantee, but they were somehow they run in phases and use different objectives in every round. This is like a simple gradient descent iteration that we give here. And it's also something optimal. Now, this comes with like an analytical. Just train the parsetron rather than it's going to get like in the worst case, you cannot have like trivial error, nothing not therapy, like close to one half. Yeah, if you just train the perceptron with a semi-random noise. Even if you iterate. Even if you margin set, yeah. So and in this colour, the y is the correct level or noisy y? Yeah, good question. Y is the noisy level. So this is like So this is like a so this is yeah you're trying to fit to the node to predict like this standard agnostic no this is the standard like Pac Learning Guardian just trying fit into the best error with just yeah. So what is like uh kind of annoying in this statement is the error guarantee. In this statement, is the error guarantee. So you would like to have the expectation of eta of x over all examples. That's the base optimal error. But you only get et, which is the upper bound, the point-wise upper bound on eta of x. But this is like the best norm we know in polynomial time. And there are some works proving a scalar bound, so that with polynomial time algorithms, you cannot improve over this eta plus epsilon error. So this corresponds to the error that you would have if the probability, the flea probability. Have if the probability the flee probability was like equal for everyone as well. That would be the optimal error at that there. Sorry, I'm still a bit confused. So you're saying you're not assuming realizability? Yes. But if I have an agnostic setting where for every x, y is equally likely to be plus 1 or minus 1. Yes. Then even if eight is 0, you shouldn't be able to. And if you want half there, right? If the label is flipped with probability 1 half, then the. Half then the ground truth was equally like plus one. The ground truth is f star of x. Okay, okay, so the ground truth is. The ground truth is. The ground truth is the true plus. Yeah. So yeah, so the and like to comment a bit on the sample complexity of this thing, yeah, basically perceptron on noiseless requires sum of gamma squared times epsilon iterations and samples. Epsilon iterations and samples. Here we have this epsilon squared, but again, there are some like SQ hardness results showing that epsilon squared you cannot improve over with efficient output. So there is no dependence on catering? No dependence on data. Yeah, good question. So this is a lot of good questions. So the information theoretically optimal, if you could somehow do ERM, that was the result of the Massard paper I mentioned earlier, would be Mentioned earlier, would be like even like VC dimension, is a general result times, you know, one minus two eta times epsilon. So it's linear, like in one over epsilon, but pays like this one minus two eta. If you're willing to go to epsilon squared, which is kind of the agnostic rate, then you don't have to pay in eta. Good question. So, for the soliciting this model, do you think it's possible to get a guarantee where you're actually comparing? Get a guarantee when you're actually comparing to the data optimal? No. I believe in SQL, in SQL abounds. I believe with computationally efficient algorithms I believe. Do you have any reason to believe that SQL values are really the low value of efficient computation? Mostly because so far, that's a good question. Mostly because so far efficient algorithms will have matching efficient algorithms and SQL boundaries. It's not a theoretical result. It's not, yeah, we don't have exceptions. It's not. Yeah, we don't have exceptions. It's a part of birdship coordination. Yeah, there are not many natural exceptions where we have non-SQ algorithms that somehow bypass the SQL bounds. That's kind of the evidence we have so far. Like circumstantial, but I guess that's having been able to do something better. Okay, great. So let me maybe briefly go over the main Maybe briefly go over the main idea here. So, yeah, if the noise is equal to eta for all x, so this is the random classification noise model, then you can actually prove that if you optimize this leaky rel and it's kind of like, so it's kind of you're doing a perception, but sometimes you're going to update even when you're correct. It's kind of this noisy perception type of thing. So it's a convex loss, and if you could optimize this thing, then like the you're going to. Then, like, you're going to find a good hyperplane. Now, like, the natural starting point is maybe you want to do the same here when you only know that et of x is upper bounded by eta. It seems like their RCN is the kind of the worst case. What is the generative modeling? What defines what et of x will be? et of x is like we don't know it, right? So it's any bounded function. This is worst case eta of x under the condition that it's like. Yes, exactly. It's an arbitrary bounding function, and we don't have access to it. That's the, it just exists. So, yeah, what's going to happen now if you try to do this leaky value minimization? I'm going to do like if you take the expectation over the random label y that you see, then this is going to be equal to the error, the classification error of your current guess, minus eta times this. Minus settles this margin term. So the first term, let me put the colors. So the green term is pretty nice, right? So if we didn't have this red term over there, then if you could somehow make your loss negative, then basically we would have the guarantee we want, right? So the error of this w, assuming that we have some w that makes this loss negative, then its error is going to be less than eight in expectation of our examples. The problem is that we have this. The problem is that we have this margin term there that somehow discounts and points with low margin are penalized less in the loss. So yeah, how do we fix this? You know what happens if you first do nearest neighbor or k-nearest neighbor to clean up some of the and then you buy a new algorithm. Yeah, so the problem here is that we have no assumptions about ed of x, right? About edd of x, right? So it could happen that if you do nearest neighbor, you're going to like end up making things worse, yeah, if you if you put hard labels for those examples. You could perhaps do something like that if you know that I don't like somehow decays far from the decision boundaries or stuff like that. If you have some geometry, maybe you can do that. If you just know that it's bounded, yeah. So, yeah, now the next natural question is whether there exists some convex loss, some convex proxy that you can actually optimize. Turns out there is no convex loss that you can directly optimize. So that's why in the first paper that gave like the polynomial time algorithms for Massard Noid, they had a sequence of convex loss minimization problems. So, how we handle this? I mean, it's almost almost asks for. It almost asks for it. So, you want to basically normalize by the margin. You want to normalize your loss by the margin. So, now if you take the expectation, it's exactly what you want. It corresponds to the excess error of your classifier about theta. But now we have made things non-convex because we divide by this, by the norm of w dot x, by the margin of w dot x. So it's unclear whether STT context. It's unclear whether STD converges, but there is another case, another issue. The fact that you may have points with zero margins, exactly. So you don't have guarantees about, you only have guarantees about the grounded classifier, that it has margin gamma. You don't know whether your current guess is going to have margin gamma. But this is easy to fix. You can just like perturb a bit the denominator by gamma, and then you're good. So now, the main issue is non-convexity, and the Convexity. And the fix here is kind of nice. So if you treat the rescaling as fixed, think of like doing tortunt grad for the denominator. So you don't take gradients with respect to the denominator. You only take the gradient of the literature loss and then you renormalize the gradient. So if you do this thing, then now, you know, now it's not a non-convex. It's still non-convex, but it's kind of an online convex problem now. So because you have like the gradient of a convex, So, because you have the gradient of a convex objective, and then you treat the rescale it as fixed. So, now you could directly do a black box proof using the fact that online gradient descent for convex losses has an order dread. This will not give you the optimal summing complexity. We do like a kind of a wide open box analysis, and we get this one over gamma squared sample complexity. So, that's the okay. Okay, so let me go over the application, and we could also stop here, I guess. So, what's the application? So, it's this is a joint work with Fortisio, Los Coltering, Jenk Paqual, Dorf McGanny, and Eric Free from when I was doing my intercept at Google. So, a standard pipeline when you want to do this semi-supervised distillation is when you have a small data set A. When you have a small data set A that you have labels for, say for like those are golden labels from humans, you have like expert humans or whatever. So you have a small fraction of correctly labeled data and then you have a huge fraction, huge unlabeled data set B, then what people do, like a standard pipeline here, is you want to train like a powerful feature model on the labeled data, and then you use it to create pseudo-labels for data c and then Data set B. And then you train the student model. Maybe you don't want to deploy the teacher model because you know, like the it's way too expensive, like uh to use it in an application. So you want to train a smaller student model. So you to train it, you combine the human labels and the pseudo-labels provided by the titer, and then you train it over Earth. Right? So that's kind of the setting here. And the problem is, you know, the titer, especially when it's fine-tuned on a small, On a small fraction of labels A, it's going to make incorrect predictions over your unlabeled data C. So the predictions are noisy. And in fact, we did this kind of thought experiment here where we go over CIFAR 10 and then we somehow use an oracle because we have the true labels for CIFAR 10 to clean up all the incorrect predictions made by the teacher. Made by the teacher. So when you remove those, then you get this blue curve, and when you just ignore the fact that the predictions of the teachers are noisy, you get this way worse, 10% worse performance. So yeah, the point is that we have to do something about the. And what was the accuracy of the teacher here? So the accuracy of the teacher is like pretty much the same as like the Same as like the it's slightly better, yeah, that's a good question. That's slightly better than the, you know, the orange care, than the noisy. Like the student model and the teacher model do not have like huge gaps in like the number of weights, so they perform like singularly well. Check it if you could somehow remove the incorrect predictions, but it's really get like good improvements. Like good improvements. So, kind of the blue curve is the best possible thing you could somehow hope for here. So, yeah. So, we model the titzer as a noisy label oracle. So, basically, for every X, we have some ground truth label, and then the titzer label Y is going to be correct with the same Massar noise model I defined earlier. So, yeah, f of x is the probability that titzer is correct. And yeah, but And basically, we use some of the loss that I discussed earlier, and then we get some improvements, pretty good improvements over the baselines. And actually, what's kind of interesting is that even though this loss that I described earlier was designed for binary classification, it somehow performs very well when you have many classes. So in the ImageNet, we get bigger duffs. We get bigger dash, which is kind of, I don't know, it seems interesting to me. So, yeah, that was everything for the semi-ranized looks. I can briefly, like, in two minutes, mention a bit the smooth analysis result. So, we talked about semi-random labeled noise. Now, the question is: what do you do when you have address area labeled noise? And I already Noise, and I already told you that you know, in that case, computational hardness results exist. So, basically, the only known results that bypass that computational hardness is when you assume, when you place very strong assumptions about the feature distribution. So, it's Gaussian independent features or like uniform of the hypercube. So, you need to place very strong assumptions about your feature distribution, and then you bypass the hardness results. So, now in this work, our goal is to somehow Our goal is to somehow work under the general hypothesis classes and relapse on distributional assumptions. So, that should be impossible by the interactability results I mentioned. So, what's the somehow the trick here is that we're going to use an easier learning benchmark, not the standard PAC learning guarantee. So, instead of competing with the best in class, I'm defining this smooth opt quantity. This is kind of like motivated by the smooth analysis paper for IDP by speed. For I will pay speed 1 and 10, where I'm evaluating every classifier to a perturbed point. There is another model that you compare yourself to the margin error of the best classifier. You say you look at the verse classifier, but you count as errors everything that falls in the margin, and that's your baseline that you compare. Oh, that's the margin version. Yeah, yeah, we have compiled. The margin error. The martinizer, yes. Yes, yes, yes, yes, yes, yes. And then you can be open. Yes, yes, yes. And then you can be efficient. Yes, yes, yes, yes. That's your paper, right? That's my paper. Yes, I know. That's why I want to quote that slide. Yeah, yeah, yeah. Yeah, good point. Disconnects with math error, yes. We actually have a formula result. So that's the, you know, for sigma equals zero, you go back to agnostic. For sigma infinity, you know, you basically evaluate every classifier to a random Evaluate every classifier to a random input, right? So there is no, like, the problem is trivial right now. So it's like pick the best function guesses. So yeah, turns out under some regularity assumptions for the concept class, like that it has somehow it depends only on a low-dimensional subspace and its boundary, its decision boundary is not very complicated. Basically, it has low surface area. So this we don't like this is good. This is good. Basically, we can recover polynomial time algorithms for weekly learning, similar to your result for beyond half spaces. And yeah, I guess that's everything. I'm gonna leave the mouth. There is a moment recorded. The workshop is over. We will try to organize another one. Thank you so much to the organizers.