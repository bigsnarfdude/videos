Ankit, who is a student who is currently at Wisconsin. Okay, so this is quite new work. So I usually like after I give a talk, I say, oh, this is coming to archive very soon or something, but I won't be making any promises. This is coming to archive hopefully in 2021, but maybe 2022. So, okay. So, let me start with just what is the classical hypothesis testing framework. So, we're going to assume the simple Uh, so we're going to assume the simple hypothesis testing. So, two distributions p and q that are on k alphabet set, so delta k minus one. And we are going to assume that we get x1, x2, xn conditionally independent, either according to distribution p or q, depending on the hypothesis. And for hypothesis testing, we need to find some function phi so that it will take in the samples and return the correct hypothesis with probability. With probability greater than 90. So, with probability of being correct being, let's say, 9:10. And this means that you need the total variation distance for the product distribution to be larger than, let's say, 0.9. And it's known that the sample complexity, so the number of samples n that you need to ensure the probability of error is less than 1 over 10 is of the order of 1 over the Hellinger distance squared between. The Hellinger distance squared between P and Q, where the Hellinger distance is given here. So it's a square root Pi minus square root Qi squared. So the way this is obtained is so the probability of error is related to 1 minus total variation. And when you take a product distribution, you look at the total variation between P to the N and Q to the N, and then use bounds, use the bound for Hellinger in terms of total variation and show that. Terms of total variation and show that Hellinger tensorizes and so on. So this is fairly well-known stuff that the sample complexity happens to be captured by the Hellinger distance. Right. So what this talk is going to focus on is a communication constraint hypothesis testing framework. So this basically the idea is that you have this hypothesis H, and based on H is equal to 0 or 1, you receive Or one you receive n sample, so x1, x2, dot dot dot, xn and x1, x2, xn are now forced to go in some channels. So let me call those channels w1, w2, wn. And out comes y1, y2, yn. So I haven't yet told you what the channels are. And all these y's together go into something called the fusion center. And from the fusion center, you get the output phi of y1, y2n. So, the w's, so there is some processing going on. So, you're not allowed to have access to the XS directly, but you have to look at the XS through some channel. And the channel is, so Wi are channels with output alphabet at most T. So, output size. Size less equals some d. So it could be like a binary output or a tertiary output or so on. So it's up to us what channels we need to pick. We want to pick the best possible channels that preserve information, sort of that don't destroy a lot of information, but we are not allowed, but we have to make sure that the output size is less equal T. So the setting here. So the setting here, the way to think of this is imagine there are these n different sensors and each sensor is sensing a reading Xi and each sensor is trying to communicate to some central server, this fusion center. But the sensor is only allowed to send binary messages, for instance. And the job of the fusion center is to look at these messages that you're getting from the end sensors and then decide whether it's hypothesis zero or not. So now looking at So now, looking at this setting, there are some questions that there are several other model choices that one could make. So, first is whether we are in an adaptive or non-adaptive setting. So, adaptive or non-adaptive means that is the choice of WIs, for instance, allowed to depend on each other. So, maybe you first pick W1, look at the output Y1, then you pick W2 and so on. Or is the communication going on or can the YI sort of? Or can the YI sort of talk to each other before they send everything to the future center? So, for instance, a W1 might send Y1 and so on, WN might send YN, but they might reveal their wise to everyone. And then there might be a second round where everyone updates their voice to Y1 prime, Y2 prime, and so on. So that would be the adaptive setting. So both choices are kind of modeled choices. Do you allow for adaptivity or non-adaptivity? If you do allow for, if you have a non-adaptive setting, If you have a non-adaptive setting where so the W's, you're not the sensors are not talking to each other, then you could also have a stricter setting that the channels are identical or non-identical. Are you forced to use all the WIs the same or can you pick different WIs? So in this talk, I'm going to look at kind of the most restrictive setting, which is non-adaptive. And mostly, most of the talk will focus on non-adaptive and identical. And identical. And since we are looking at upper bounds, looking at the most restrictive setting kind of makes sense because if you can show that the upper bound for the most restrictive setting happens to match a lower bound for the least restrictive setting, then everything in the middle is tight. So that's what we'll focus on. So that's the problem. So today we are looking at identical channels, simple hypothesis testing, and what's the sample complexity in that case. In that case. So here's a simple setting of one possible test that one could use, which is Cheffet's test. So Cheffet test is a test for hypothesis testing where each sensor gives a zero. So it's just a binary output. So D is the number of messages. So D here is a binary output. And the set is, so you decide a set A where P of X over Q. A where p of x over k of x is greater than equal one. So this is precisely the set where oh no, there's a question. Yes. So it seems that T and Q can be known. So what's the meaning of the adaptive setting then? So the adaptive setting means that the choice of the WIs can, so the WIs can, so P and Q are not known, but each sense. But each sensor will, so each sensor only receives their own observation XI, and it is their job to pick what WI they need to pick. So, if they are not communicating, if it's non-adaptive and every sensor kind of just picks WI based on it, I'm confused about this. Oh, P and Q or some fixed unknown distributor, some fixed to unknown. All right, okay, okay. So, this is a warm-up that Chaffee's test is a binary test. So, each sensor basically just checks whether the X they have received lies in the set A or not. If it lies in the set A, then you send one, otherwise you send zero. Okay. So, the question is: what is the sample complexity of this algorithm? And all right, so what happens here? All right, so what happens here is that let's say P and Q after this transformation, so let's say X was distributed as P and Q, but after the transformation, Y is distributed as, let's say, F of P or F of Q, depending on whether the hypothesis is 0 or 1. Then the sample complexity is going to depend on the Hellinger distance between f of p and f of q. So sample complexity is Is like one over d Hellinger square of the transform distributions. And the original sample complexity was without the communication constraint, the sample complexity is the Hellener distance between the usual P and Q. So, how different can the original sample complexity be from the sample complexity of this transform distribution? It turns out that they can be quite. It turns out that they can be quite different. So the sample complexity here can be as bad as 1 over d Hellinger to the fourth power of P and Q. Okay, so you go from a quadratic term to a fourth term in the denominator. And there are examples where this actually happens. So you can construct examples where the original Hellinger distance is something like epsilon, but the new Hellinger distance turns out to be epsilon square. Inger distance turns out to be epsilon square. So, sample complexity for Chaffee's test can lower quite a bit compared to the original sample complexity. So, this doesn't depend on K, the alphabet size, the original alphabet size? No, not the sample complexity. It just depends on the Hellinger. So, no, no, it does not depend on the origin on the on K. It depends on, I mean, D is fixed here because it's the binary output, so it depends on. It's the binary output, so it depends on that. Yeah, yeah, yeah. Thanks. Okay. Okay, so the question is, of course, can we do better? So what we really want to do here is something like a reverse data processing inequality. So the picture that I have in mind is you're looking to find a channel W so that if P and Q go in and something like W P Uh, wp, or maybe I've been calling it f, but let's say wpwq come out. Then you want to make sure that uh the hellinger distance between the output, okay, maybe I'll keep calling it fp and fq. So the hellinger distance between the output Fp and FQ is large. So compared to is large. So W can depend on P and Q or W cannot depend, but W cannot depend but w is some um p and q are fixed so you can this is w can depend on the knowledge of that of what these p and q are so you can cleverly choose your w so that the output fp and fq are as far apart in herringer as possible okay and you would like to pick that channel and that channel will give you the best possible sample sample complexity so the the quantity to look at is this uh this factor r so this factor Uh, this factor R. So, this factor R is trying to say, well, how large can the output Hellenger be if the input Hellenger is D Hellinger of PQ. And if you and you want to find the best possible, the infimum of this overall W. So, again, the restriction on W is just that the output of this W is limit, is alphabet size T. And if you are able to calculate R, then the new sample complexity is going to be the original sample complexity. Sample complexity times this blow-up factor R. And clearly, by the data processing inequality, this blow-up factor R is greater than equal to 1. The question is, how big is it? And in the warm-up example in Cheffey's test, it turned out that R can be as bad as 1 over the Hellinger square. So the sample complexity was 1 over Hellinger to the fourth. So this R is 1 over Hellinger square. And but perhaps the double. And but perhaps the W that is that happens to show up in Chef S is not the best possible W. That if we were to choose the channel cleverly, we could make R very small. And in particular, it would be really great if you can make it a constant, because then you're saying that there is no loss in sample complexity that even if it is communication restricted, you can solve the problem with the same number of samples. Okay, so that's that's kind of the idea that we. So that's that's kind of the idea that we need to come up with some kind of a reverse data processing inequality. And what so here's a question: here's a slide about what we already know about this problem. So the paper to look at first would be this paper by Sitziklis called Decentralized Detection. And in it, Sitziklis showed that the optimal channel is in fact a deterministic function. And the deterministic function is a threat. deterministic function is a it's a threshold rule so it's it's very similar to chaffet test so in chaffee test this gamma i was just one but uh the optimal the optimal channel is a different threshold and even more um kind of making it slightly more complicated is that each Xi can have a different gamma i so optimal in the sense of um we are looking at the probability of error so for the prob the probability of error is minimized by if everyone uses By if everyone uses a threshold rule, but everyone's threshold can be different. So, in some sense, this corresponds to the non-identical channels. But Sitzakius also showed that if you only care about asymptotic error, then there is no difference between identical and non-identical. That the rate that you get for the best possible rate that you get for identical channels is the same that you get for non-identical channels. So, at least that gives us an idea that if what we are seeking is an idea that if what we are seeking is an upper bound, then let's try to let's try to figure out what is the sample complexity for an algorithm that uses the same gamma everywhere. And then we try to optimize over the gamma and try to find the best possible sample complexity. Okay, so I'll just state the main results here and then give a brief proof sketch. So the upper bound or the sample complexity result that we have is that We have is that you can find a binary function which is a threshold function such that the sample complexity blows up but it doesn't blow up by a lot. So this is the original sample complexity one over Hellinger square and it blows up by a log factor. So log of one over Hellinger square. Okay, so essentially you can carefully choose this threshold gamma so that the sample complexity is we don't get the completely optimistic scenario where it's exactly the same, but Scenario where it's exactly the same, but it doesn't grow too much. And for the lower bound, we can see that this is pretty much the best you can do. So you can always find distributions whose original Hellener distance is rho, which means the sample complexity without any constraint is one over rho. But you need one over rho log one over rho samples if you have a binary communication constraint. So it's a tight characterization. So it's a tight characterization that 1 over Hellinger square log 1 over Helinger square for the binary setting is the quantity that is determining your sample complexity under communication constraints. Okay, and I'll give a proof sketch, a brief proof sketch. So the idea is that you so let's say you divide the support of P, which is this K area alphabet, into four sets. alphabet into four sets where each set is based on this ratio p over q so in particular you look at p over q lying between 0 to 1 half one half to 1 1 to 2 and 2 to infinity and overall Herringer squared is you can just break it up as a sum over all these four sets but it's it's useful to split it into these two terms one is the blue term and the the mageta term and the blue term term and the blue term intuitively corresponds to those symbols where p and q are very different so the ratio p by q is large or the ratio q by p is large and the magenta term is where the terms p and q are close to each other so at least intuitively one would see one would think that uh if the hellinger distance is dominated by the blue term then it's an easier problem because the p's p and q's are quite different from each other and and if it's dominated by the magenta term then it's going to Dominated by the magenta term, then it's going to be a harder problem. And formally, you can show that. So if you say that suppose the blue term is actually greater than one half the Heleneer distance, then without loss of generality, one of the two blue terms is greater than one quarter the Helinger distance. So under this assumption, you can show that the optimal map should simply be to take all the x's that show up in this set A2 to infinity, map them to 1 and everything else to 0. them to one and everything else to zero. And so essentially this map is saying that this map is saying that that particular set is already good enough to distinguish P and Q. So just give that one and everything else zero. And what you can show is that the Herringer square is the same order as the Herringer square after this transformation. So in particular, the blow-up factor is a constant here. So you don't really lose. Is a constant here, so you don't really lose, um, you don't lose anything. Okay, so this was indeed, uh, this was indeed the easy part. The hard part is, uh, if the major term is the one that is dominating, so this means that, yeah, all the p's and q's are kind of close to each other. So, here also, you can do the same assumption. If the magenta term had two terms, if the sum of the two terms is greater than Herringer over two, then one of them has to be greater than Helinger over four. Over 4. And okay, so you can rewrite that term a little bit and interpret that as the expectation of a certain random variable y. So I haven't done anything particularly smart here. So I've just pulled out a qi from this square and called delta i to be p i over qi or 1 plus delta i to be p i over qi. And then use some bound on square root of 1 plus delta minus 1 to get to get this quantity. To get this quantity. So it will become clear why I'm doing this, but just the thing to keep in mind is that the original Hellen square happens to be the expectation of a random variable. Now, if you were to now pick a threshold gamma, so it makes sense to pick a threshold between one to two. So that's the set that we are assuming is the nominating set or has the most contribution to Hellinger. So you can show by So, you can show by a simple calculation that for a fixed gamma, for a fixed threshold gamma, the right-hand side or the Helinger between the transform distribution is this quantity. So it's delta times probability y greater than delta. Okay, so roughly what we are asked to do is to compare the expected value of y for a random variable y with delta times probability y greater than delta, but the choice of delta is with us. So we can take a supreme. Delta is with us, so we can take a supremum over all delta. So, of course, Markov's inequality tells us that the supremum over delta is always less than equal to expected value of y. But what we need is a reverse Markov inequality. So, we need to say that there is a nice enough choice of delta so that delta times probability y greater than delta is big enough compared to expected value of y. Okay, so this is the kind of the one of the One of the heart of the whole proof that you can show a reverse Markov inequality of this form: that if y is a bounded random variable between 0 to 1, then there is a choice of delta so that this kind of a reverse Markov inequality holds. And the idea for proving this is quite simple. So, suppose delta times probability y greater equal delta is less equal c for all delta. Then that means that probability y then that means that probability y greater equal delta is less equal c y delta, but it's also less equal one. So it's less than the mean of these two quantities. And expected value of y happens to be the integral of this quantity, probability y greater equal delta. So this is going to be less equal integral c by delta comma one. So minimum of this d delta. And this it turns out to be, I think, c log. turns out to be I think C log 1 by C C log 1 by C okay so basically it kind of says that if C is very very small then expected value of Y will also also be kind of solved because C log C is going to zero so this gives us a lower bound on on so this tells us that C has to be greater than or equal to something if you need to get expected value of y there and in particular you get that c should be Be uh uh sorry, so in particular, you get the bound that delta times probability y greater than or equal to delta should be this e of y over log one over e of y. So, and this is true for the the we did use the fact that it's bounded uh between zero to one here. Um, okay, so so yeah, so the proof is basically just a three-nine proof, just assuming that there exists this reverse model. Just assuming that there exists this reverse Markov inequality, it would have been nice if this log one by y were in fact a constant. If it were a constant, then you would have gotten a very tight kind of Markov and reverse Markov would be tight. But log 1 by y is something possible, is going to be something much larger than 1. So you're not going to get exactly a constant. But it's still good enough for us. And this reverse Markov can directly then be plugged in into the two Hellinger distances that. The two Hellinger distances that we were looking at. So the left-hand side was precisely the Hellinger distance of the transformed distributions. E of y was the Hellinger distance of the pre-transformed distributions. And plugging the correct values gives us the desired result. Okay, and so that's the achievability for the lower bound. So Syncyclis tells us that it's good to consider all gammas to be different, but you can show Different, but you can show that the sample complexity for all gammas being the same is the same as the sample complexity for all gammas being different. So the optimal sample complexity. I have a proof sketch here, but it's not really that deep. So I just skip it. So at least, so what this says is that it's enough, you reduce the problem to everyone having the same threshold. And once you have the same threshold, the idea is that the reverse Markov inequality that we grouped. Markov inequality that we grouped is tight. You can construct a specific distribution where this is like C by delta, and there is like a delta distribution here, so on. So the reverse Markov inequality happens to be tight. So you can construct a distribution for which reverse Markov is tight. And using that, you can go back and explicitly construct P and Q so that the sample complexity, the lower bound force. Okay, so everything kind of rests. Okay, so everything kind of rests on the preverse markup and it being tight. So I have about seven minutes, so I'll just say what happens when you go from two messages to D messages. The theorem is that that's the factor of D pops up here. So if you have D messages, the sample complexity decreases kind of linearly, so divided by D. And this characterization is also tight, so you can always show that there is some distribution so that you Show that there is some distribution so that you need those many samples to distinguish them. Okay, and just the main idea for the D messages is that Since ICNSI's paper also shows that even when you have D messages, the optimal test is a threshold test with D minus one threshold. So you send message zero if E by Q is less than gamma one, you send message one if it's between gamma one and gamma two and so on. Between gamma one and gamma two, and so on. And now, what we require is kind of a reverse Markov inequality, but this quantity is a bit more generalized. So it's kind of like you're approximating the linear function by a stepwise function and saying the expected value of y is greater equal delta times probability y belonging to delta to delta plus one. So it's it's slightly more generalized than the previous. More generalized than the previous reverse Markov inequality, but a somewhat similar proof sketch also works. And then you get this factor of D in front of frontier, which then goes to which becomes 1 over D in all the sample complexity bombs. And the lower bound construction is also almost identical, that this reverse Markov is also a tight inequality. So you can use it to explicitly construct P and Q so that you need those many samples. Need those many samples. Okay. There's a question. Yeah. Just in this reverse mark of the I like you, like this was for every I on the left-hand side, there is an index I. Oh, I see. So this is a sum. I go from maybe zero to d minus one. So yeah, so I'm trying to take the expectation. Yeah, I forgot the summations. The expectation, yeah, forgot the submissions. Okay, yeah, it makes more sense. Okay, thank you. Okay, um, all right, so that kind of completes the story that I wanted to tell for simple hypothesis testing. This is a tight characterization for the sample complexity for simple hypothesis testing in terms of the Heavenly distance. At this point, I'm going to completely pivot and go from 2 hypothesis to M hypothesis. And yeah, I don't have a ton of time. So I'll just say that what we need here is again some kind of a reverse data processing inequality. So if you are given M hypothesis so that all of them are sort of far apart, we want to make sure that they still remain far apart even after we pass them through a channel. In particular, the minimum total variation distance between the outputs should be. Outputs should be maximized overall channels W. And intuitively, this suggests that we should be doing something like Johnson-Lindenstrouse-like embedding. So we are starting with KRE distributions, going to DRE distributions, and we want to make sure that something in the L1 distance is preserved. And yeah, so I would say much more just that that idea essentially works out that. Essentially, it works out that you can show an order k m to the fourth sample complexity bound. But this, of course, doesn't really work well if k is infinite, for instance. But you can also show that k doesn't have to be infinite. You can always reduce k to something like order m square by having like an additional pre-processing step. So eventually you end up getting an achievable rate of some polynomial, like order m to be 10 in this case. And there are some things that I did not talk about. So, I did not talk about non-adaptive and non-identical upper bounds. I did not talk about adaptive upper bounds either. And I didn't talk about lower bounds. So, the first three bullet points are all for M-ary hypothesis testing. For adaptive, there is this lower bound of order M using this direct sum argument by Braberman et al. This argument has also been used in. Argument has also been used in the privacy literature. I think one of Gautam Kamat's paper uses that similar argument. And yeah, and I didn't talk about reverse data processing inequality. So we proved a reverse data processing inequality for Hellinger, but the same idea goes through for more general F divergences than Hellinger distance. And open problem, there are three of them, but probably the most Probably the most interesting one would be the privacy one. That what are the tight bounds if you have like an epsilon differential privacy? So if the channels are not communication constraint, but they're privacy constraint, then how can you characterize the sample complexity of simple hypothesis testing in terms of that epsilon? And I guess what's missing is, I'm not aware of any analog of citlist-like results there. Analog of sick list like results there. Like what is the best possible algorithm to use in that setting? One can always come up with upper bounds, like come up with an algorithm that gives you upper bounds, but to get lower bounds, you would need something that says that the best thing to do is to some specific method. And yeah, and of course, our bounds, our lower and upper bounds are, although they are polynomial, they are Polynomial than M, the exponent might differ. So maybe there's a better way to get tighter upper bounds for M-ary hypothesis testing. So that's the end. Yeah, so the paper will hopefully come to archive in about a month. We don't yet have a title, but yeah, please check archive at some point. Thank you. So I guess there's nothing about the There's nothing about the P and Q being discrete distributions, right? This only depends on the likelihood ratios, right? Yeah, yeah, yeah. So it's just that if P and Q are discrete, then the bounds can improve a little bit because the reverse Markov inequality, so the X, for instance, we have E of Y divided by 1 over E of Y or something. But if E of Y is very small, then this can become bad. Then this can become bad, but you can also have E of Y over K in that case. So something like this is the actual. So if the alphabet size is small, then you can use that to your advantage. But yeah, the results hold even for infinite or continuous spaces. But for the opponent size is infinite, then you have this upper bound that decreases as D grows, so there's no upper limit on D. Right? I mean, I was confused by. Right. I mean, I was confused by that, that it seemed like you could make D very large and have your sample size go to zero, but of course it's upper bounded by k. Right, right, yeah. So if d is, if you have a finite k, then d should be less equal k because the moment d becomes greater than k, then you can just send the correct sample. Right, so something changes if k is infinite. Yeah, it changes if k is infinite. That's right. So if k is infinite. So, if k is infinite, so the upper bound, okay. So, if k is also infinite and d is also infinite, then it still probably suggests that there must be some coding that you can, there must be some one-to-one mapping that you can do from this infinity to that infinity as well. Just based on the boundaries, I think that like when d becomes log, the thing in the numerator, that one that can't. The thing in the numerator that when that cancels out, that tells you that's kind of how much you need. Is that right? Should it be useless, right? Right, right, great. Yeah, maybe the way to think is like think of K as being very, very large and B as being something less than K. So at least in that setting, there are no issues. Do you know? This sounds like a really cool pay for it for seeing. Like a really cool paper. I look forward to seeing it online. I'm curious whether, do you know if your results extend to like, I don't know, the agnostic case, some call it like fancy estimation? I think the name hypothesis selection is also suggestive. You know what I mean? I don't, so actually, like, I was surprised to see that the simple hypothesis testing wasn't already out there. But it seems like the, I guess the simple hypothesis testing because you have only Because you have only, we are interested in a different problem, right? Like, we are interested in tightness with respect to the Helene distance between P and Q. And typically, like, when you have hypothesis selection or Mary hypothesis testing, the parameter of interest is that M. Like, how does sample complexity change with M? So, I guess that's why we didn't find simple hypothesis testing anywhere. But yeah, I haven't looked at hypothesis selection for communication constraints. So, I believe there's some work out there for hypothesis selection. Some work out there for hypothesis selection for privacy, but I haven't seen anything but communication constraint. Okay, thank you again.