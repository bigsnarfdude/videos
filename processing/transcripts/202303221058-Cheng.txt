Good morning, everyone. My name is Haiyan Cheng. I'm a professor in computer science at the Valley Mete University. It's a small liberal arts school. So I'd like to start with my talk with my understanding about model. So this is research I did during my PhD. So at that time, I was given this chemical transport model, which has several inputs. Has several inputs, and then we feed everything into the physics and the chemistry. There's reaction, abjection, diffusion, and then at the same time we have observation. So I was interested in why do we even make forecast? Because we want to make policy based on the forecast, right? But also at the same time, we want to have information about our company. Have information about our confidence level. So we need to quantify the uncertainty in our forecast. So with the quantified uncertainty, we can provide better air quality and then better informed decision making and the scientists can build the models. Usually the scientists don't understand the policymaker's perspective and the policymaker didn't know where did the forecast quantity come from. Quantity comes from. So we need communication, and usually, when we provide the arrow bar, only we have the mean and the variance, which is not enough. And we know actually there are two kinds of uncertainties involved in the forecast. And I think Laurie mentioned that, electoric and epistemic. And we're trying to quantify the uncertainty that due to lack of knowledge through data. So that's the beginning of my investigation. Beginning of my investigation, and that's a model actually reflected that picture. We have this chemical transport model capture infection, diffusion, transportation, and emission. And then we have uncertain initial condition, uncertain boundary conditions. So this model actually models 93 chemical species, 213 chemical reactions, and use kinetic preprocessor as a chemical time integration. So it's very complicated. So it's very complicated, physics, chemical give us space, state, and time evolution. So I was thinking how do we quantify this initial condition? So the general method we use Monte Carlo. We introduce perturbation in those uncertainty quantities and hope to capture the arrows through the spread. However, Monte Carlo is not efficient. And then we know the convergence rate is very slow, so it's almost impossible. Low, so it's almost impossible. So that's why I started to investigate the polynomial chaos method, which is a representation of a second-order process as serial expansion. So basically, we use a Kapuna-Bui expansion to expand the bigger state into a series of linear coefficient combinations. So here's the formula. And then the problem with polynomial chaos is you can only You can only capture a small number of uncertainties with large uncertain magnitude. So, actually, one of the professors published a paper called Generalized Polynomial Chaos. And then in this case, actually, we treat the model state as a random variable. And then we have to pick the distribution for a random variable. And if you pick the random variable, you have to follow through use-specific orthogonal. Use specific orthogonal polynomials to guarantee the convergence. So, in our chemical species, of course, we choose beta distribution because it has finite support and we know the maximum value. So, if we chose beta distribution as our state distribution, then we use Jacobian orthogonal polynomial. And at that time, I was told from the domain expert, environment scientist, if we want to focus Want to focus on ozone concentration, there are high uncertainties coming from those initial conditions, which are the ground emissions of A voc, B voc. So literally they tell me there are seven independent uncertainty quantities that should be used as input. And then we choose a Jacobi polynomial, other tool. But with this uncertainty, But with this uncertainty quantification with polynomial chaos, not only can we represent the final quantified uncertainty and then deliver this information to the policymaker, but also we can generate a smooth probability density function. So earlier one speaker talked about we have mean, we have different mode, but what if we have the whole PDF? Then we can generalize all kinds of high-order statistics. So the benefit of this So the benefit of this approach is in the end, you can literally just sample the random variable and produce many states. And then the histogram is literally very slow. So with that, we were able to quantify the uncertainty in the predicted states. And at the same time, I also published a paper on uncertainty apportionment. So with this linear expansion, This linear expansion, you can actually figure out where are the major uncertainty come from. So you can apportion from the uncertainty source. So here is an example of total uncertainty that's represented from seven different initial uncertainties and then also higher order terms. So this way we have a better information about quantified uncertainty. And then when you look at individual bar charts, You look at individual bar chart, you can look at which initial information contributed most. And then, in terms of uncertainty reduction, we also want to use observation. So, everyone understand, you know, there are two different kinds of data estimation schemes, variational and sequential, and each has their advantage and disadvantages. I remember at that time it was a computation. Should we use 4D UR or even KF? Or ENKF, and then we figure out why not just connect them together. Because with 4D WAR, we can take consideration of all the observations within one assimilation window at the same time. And then with sequential data assimilation, at that time, the assimilation frequency is controlled by data availability. And then later on, I remarked, well, oh, now we have satellite data, actually, we can control, we can choose. Control, we can choose which data we should use for assimilation. At that point, I even considered a study information content to look at which data contribute most in terms of prediction. So how about variational plus sequential? And then we proposed two ideas of hybridized approach. The first one is update and correct error covariance matrix at the end of each assimilation window. Assimilation window. So earlier, another speaker talked about: okay, we general assumption is we consider them constant. So this error covariance doesn't evolve. If we can introduce some kind of mechanism, use the look ahead feature, and then update this error covariance, they will better capture the model error over time. So we invested running a 40-bar short window and look at the error reduction direction and use that direction to. And use that direction to generate a space. And then that's how we update the error covariance. Another way is to look at ensemble, how we generate ensemble in the EUKF method. We can also run a shorter window 40 bar and then use guided direction to help you to generalize initial ensemble instead of just generate a random Gaussian distribution. So later on, I also look at So later on, I also look at the hybrid particle filter method. So earlier, because when I did my PhD, I was under the guidance of Professor Adrian Sandu. And then at that time, we considered, okay, the state, actually, we know everything inside. However, when we look at the time and space model, at every time we can think about state as just a quantity. Is there any way we can describe this? Can describe this state by their PDF. If we want to get PDF, then how about we just generate a sample? So that's where the particle filter are introduced as a representation of all the variety of arrows. So in that case, with particle filter, you don't have to consider this they be normally distributed. The problem with particle filter evolving over time is notorious. Over time, is the notorious particle degeneration problem. There are all kinds of approaches trying to introduce, for example, regenerate particles from time to time to prevent them collapse into one or two particles. And I tried to use a particle swarm optimization to alleviate this particle degeneration. It helped a little bit, but PSO does not have a rigorous theoretical proof. Theoretical proof. So earlier I saw Professor Sandu propose a new approach called particle flow filter. So that's another interesting thing we can continue to experiment. So that was my understanding of model. We actually know everything goes into model. We know the physics, we know the chemistry, we evolve that over time. And I know there are lots of atmospheric scientists who have deep understanding. Scientists have a deep understanding of the model itself. Until I went to a statistic conference, I felt like when not talking the same language, when I talk to statisticians, the model they talk about actually is from data only. So they start with their data and the model they talk about, so what kind of model? Oh, and then later on I understood, okay, actually your model is derived from the data by your assumption. Your assumption of the data follows certain patterns. For example, the basic one is linear regression. If you see linear regression, okay, you're just fitting the linear function. And your goal actually is to use data to find the best parameter to fit into that function so you can minimize fitness. And then later on, you can introduce the complexity of the model, like multiple linear regression, or you can introduce the degree of polynomial, higher polynomial. Degree of polynomial, higher polynomial, or you can even introduce other functions. So, this complexity of the function pool becomes a challenge for data scientists. So, now we see, okay, computational scientists and data scientists actually are doing different things. So, this graph shows the computer scientist computational science takes computer science as a tool and use mathematics as a foundation to guide the theory and it has to have applied discipline. It has to have an applied discipline. So, in our case, atmospheric science. However, the data scientists start with data and then they are trying to torture data and then so let data to confess, to infer the model, to learn from the data, and then use the model they build from the data to predict the future or predict something they haven't seen. So, that's the initial beginning of data science. Of data science, and then these days we all like know this term about machine learning. So, what is machine learning? I'm trying to give you a lecture that everyone understands. So, basically, machine learning is a subset of artificial intelligence, and these days it's the core of data science. So, basically, that's all the algorithm for the data science field. And the algorithm in the machine learning field is use data to learn and then use what learned. And then use what learned to make predictions. So that's the kind of the model in data science. And this chart shows a timeline where the machine learning come from. And starting in 2019, we see actually deep learning become very popular these days. People just treat deep learning as a toolbox or black box and throw everything into it and then generate output and then try to understand what And then try to understand what's going on there. How can we use this model we built to infer what we don't know? So, in general, a machine learning algorithm can be divided into supervised learning, unsupervised learning, semi-supervised, and reinforced. So, it depends on the kind of data you are given. First of all, you need to figure out what kind of task you want to implement. So, if your data come from supervised, that means you are labeled, you already know the predictor. You already know the predictor, you already know the truth, and then you can choose certain algorithms to try to replicate that truth. So that way you can know if your model has predict correctly or not. And then if your data does not come with labeled, then you have to go through the unsupervised learning and semi-supervised meaning partially labeled. So that brings the question of these days there's a Of these days, there's a machine learning called human-in-the-loop machine learning. So, actually, label the data is a major task, and it involves human effort to make sure your data has those labels. And reinforced learning introduce human activity to interact with the environment and then use the reward feedback to learn best action. So, for Python programmers and this Python cheat sheet actually. And this Python cheat sheet actually provides all the algorithms that you can do. I think this is a brief guideline for domain experts. If you want to introduce machine learning in your research, you don't have to start with learning most sophisticated algorithm. You can start with some basic traditional machine learning. For example, we have lots of historic data, like what happened, and for any quantities. And just look at those patterns. Just look at those patterns can provide any information. If you detect there are certain patterns, there are certain correlations, then that can definitely be used as a prior condition in your future model development. So here, based on the data size, based on the kind of machine learning algorithm, that's the first thing you can do. You can do classification, you can build regression, or you can even cluster it. Can even cluster it because the data we're dealing with coming from different sources, they have different varieties, and sometimes you don't even have direct observed quantities. How do you generalize, how do you infer, and how do you aggregate them based on time, based on different features? So there are lots of decisions need to be made just on the data part. And for that part, you definitely need to consult the main experts. Experts. And then now talking about deep learning. So, from machine learning to deep learning, what is deep learning? So, deep learning is an advanced or a subclass of machine learning. So, in traditional machine learning, oftentimes you have to manually choose feature. You want to have your outcome or predictor selected. I'm going to use this one quantity or use this two quantity or multiple quantity. This two quantity or multiple quantity combined to predict one quantity. But in deep learning, those features are automatically extracted. So that's why deep learning is so effective in visual recognition. You just give you a picture of your face, you don't need to specify what to look at. It will capture your mouth angle, the shape of your eyes, your brow, and every component, and automatically extract feature. And then train it. Then train it and then give you a better model. However, deep learning is not very computational efficient. It usually requires lots of computing power and requires hardware and long time to train your model. So the quality of your deep learning model often relies on the amount of training data available and also the representativeness of your training data. So this day we'll see there's a big ethical issues in Ethical issues in deep learning about certain portions of data is missing, certain part of data is missing that causing your model being biased. And then in deep learning, there's simple neural network and then deep learning neural network. So basically, for deep learning, you just need to provide input and output, and then you can set up the parameters. So earlier, another speaker. Parameters. Earlier, another speaker talked about to use deep learning, you have to build the architecture to decide the parameter to be fit in, and then to decide the number of layers. And then you just let the packages, like fortune package, you know, treat them as a black box. So here's the idea of machine learning. You literally extract a feature based on the recommendations. Based on the recommendation from the domain expert, based on your observation of the trend you observed in the explore analysis, and then select the formal function to plug in to build your traditional machine learning. And then in deep learning, actually the feature extraction and the model building is embedded together. So we don't know the physics involved in there. Involved in there. So that refers to the earlier question from the audience. So literally, we just treat that as a state-space model. We evolve it from one time to another. So computational scientists build mathematical models, like the mathematical model involving chemistry and physics, to represent underlying physics of chemistry so we know actually what happened. And then with this challenge, And then, with this challenge from data sparse to data abundance, we can definitely use more data to infer a better forecast. However, statistician or data scientists, they are trying to build a statistical model. So the machine learning model, statistical model, other model, actually we rely on the assumption of the model format and heavily rely on the existing data. And then we hope. And then we hope that we can use this model to make inference, make decisions, make prediction. So now we see why do we have to separate them? How about we add data model into physical model? So the hybridized method always take advantage from both worlds. And the question is how? So that's the part I want to investigate the role of machine learning in. Of machine learning in atmospheric science, which is very, very general. So, first of all, for any kind of atmospheric model, we already have a large amount of historic data. We can definitely introduce machine learning to study historic data. For weather forecast, climate model, air quality model, and even a remote sensing model, not only we have all the observation data, We have all the observation data, but we also have large amounts of simulated data. So, if from historic data we can figure out certain correlation, we can definitely use that to make short-term forecasts to produce like emergency response reactions. So, through that, we want to learn patterns and correlations, and that can help us to improve short-term forecast. So, traditional machine learning. So, traditional machine learning process, including those steps, first you need to have your data available, and if you don't have, then you have to collect. But data preparation also takes a long time. You have to clean up data, normalize data, and pre-processing and handle missing data, and transforming and smoothing. Like Laurie mentioned, what if we don't have a direct observation? Which part of data should we use? How do we make sure the data is? Make sure the data is ready to be injected and to be analyzed. So, after your data preparation, you start with this long process of exploratory data analysis. So, that's when you need to ask questions, generate questions, consult a domain expert, what kind of things you might want to do. So, when I see this traditional machine learning process, it's very different than software lifecycle, because that's another. Software lifecycle because that's another class I'm teaching software development. Because usually we have a full understanding of the requirement and then we have a specified goal. But the machine learning process, oftentimes you don't know the predetermined outcome. And those questions need to be raised or generated during the explorative data analysis. So don't expect to have the full specification developed in the very beginning. And your research questions totally. And your research questions totally depends with the understanding of the real model, understanding of the data, and the new discovery along this process. And then after that, you can extract features because features are the input in your model. In that process, you can use dimension reduction techniques like principal component analysis or other kinds of reduced order surrogate model to capture the To capture the essence of the state, you don't have to use a full-fledged feature. And you can even engineer new features based on current features. The next step is to select the model. So there are lots of things you can do depends on your outcome. Do you want a binary outcome? Do you want a continuous forecast result? Or do you want to do other things? So you can select in general the kind of machine learning model. The kind of machine learning model you want to use. And then after that is basically plugging your data, and the training part is parameter optimization. So everything in machine learning, the backbone of machine learning algorithm actually is constraint optimization. There are all kinds of tools that can help you to tune the parameter to achieve a better optimization. And with the so Uralay people. So usually people divide, if you have all the data available, you provide a percentage to divide training and testing to prevent from overfitting. Because you want to use the training and testing accuracy to assess the quality of the model. So after training, you have to evaluate the quality. And again, evaluation, you can choose different metrics. So overall, in machine learning, I found the metrics. Learning, I found the metric is the most important thing. Depends on what metric you use, and then you can describe your model in many different ways. And then you can also try several models, try an ensemble of models. So in production, people don't just use one machine learning model, they'll try several models and then combine them together. So model tuning, you need to try different ways and combine them together and then Weigh and combine them together and then tune different parameters and finally document and launch. Hopefully, with this trained model, you can make forecast or apply it to the data that you haven't seen. So that's a general process for machine learning. And recently, there's a big community called scientific machine learning. Actually, because earlier we just think, oh, we treat machine learning as a toolbox, as a black box, and we just throw everything and then And we just throw everything and then let it work for us. But how about we have a better understanding and then have fine control of the functions we choose? So scientific machine learning community, I think actually June, they have another workshop here to discuss advanced algorithm. So how about use the scientific machine learning to carry out parameter estimation and speed up scientific application and help even optimize. And help even optimal design and hypothesis generation and accelerate scientific computation. So, in the following, I will just try to give you an overview or survey and in the attempt in this field. So, earlier, another speaker talked about the physical informed machine learning. So, basically, we're trying to explain what happened in this black box. What if we introduce our understanding? What if we introduce our understanding of physics into machine learning? So people try data-driven solutions of non-linear PDEs and the physical-informed neural network. So I think the first speaker used the physical-informed neural network. And then some people even introduced a surrogate model. So we don't have to use our full-fledged chemical model and we can just use a surrogate model. So the paper I saw. So, the paper I saw, one of them used encoder operator decoder neural network. This is one of the surrogate model approach. And they are used in atmospheric chemical systems. So, I don't have time to go into detail, but I provide a reference just that you see different kinds of approach. So, another research carried out in the University of Washington is by Professor Nason Kutz. So, he's been working on this deep probability. Working on this deep probabilistic Koopman operator and then use that as an evolvement of probability density function. And then they have several papers that use the Kuppman operator to combine with the neural network, try to capture the essence. And yet another work is physical-inspired data-driven weather forecasting, integrating data simulation with deep. With a deep spatial transform-based unit. So I saw in different papers they all test in a very specific scenario. So that tells us, even including all the applied talks previously, I think there are still a lot need to be done to generalize this approach. And yet another one, here we use resolved physics and unknown physics. And unknown physics just from the observation, how do we integrate them together? And then another paper talks about the universal differential equation for scientific machine learning. So they introduced this kind of model structure that you just use this one thing fit off. So I think the future of combined machine learning with atmospheric science is definitely multidisciplinary, interdisciplinary and transdisciplinary. Transdisciplinary. And from my role as a computer scientist and educator, I think it's my privilege to come here to learn. And then I hope to continue collaboration with domain experts and then make better forecasts. Thank you. Okay, we have time for questions before lunch. Thank you very much. What can we do? What do you think we can do? What you can do. I think that the future is to work together. You need to have a team that have everyone. That's why we're not here, but in reality. That's why we're here, but in in reality, oftentimes we're just rushing to dive into one algorithm and go from there. I think the actually a better approach would be to understand overall structure and blend it out. Because I feel like there are lots of lack of communication. So in the beginning of this workshop, I just learned, oh, yeah, actually, satellite data is so challenging to deal with. And we have lots of experts from NOAA, from NASA. Actually, they have deep domain knowledge of the Have deep domain knowledge of the kind of data we have. We have column data, we have profile data, when to use what, and how do we, you know, data cleaning, feature selection. So every decision for computer scientists to implement, for statisticians to choose, needs to consult with domain expert. So I just call for close collaboration and better brainstorm for ideas. So when you say So, when you say surrogate model, is that the same thing as a model emulator? Yeah, pretty much. So, basically, you don't, you just consider this model state as like evolved over time just from this time to that time. You don't even plug in this state into all the chemistry, physics, etc. Just a time series. So, that's why you can use the particles to simulate that. Any other questions? I will conclude by just a short comment. I liked very much