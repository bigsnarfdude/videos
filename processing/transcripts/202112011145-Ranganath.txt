All right, so I used to work on a lot of Bayesian inference, mostly like computation for Bayesian inference. I worked on methods that made it easier to compute posteriors with variational inference. I also worked on different kinds of models, simple multilayer models like deep exponential families or Bayesian non-parametric models like correlated random. Models like correlated random measures, built better approximations. So, the problem with variational inference is underestimate variances. And we wanted to use some of the ideas from building hierarchical models to build better posterior approximations. And also studied different kinds of divergences for inference using Stein's method, for example. But recently, I've like encountered a bunch of problems where either like taking a Bayesian approach to correct what's Taking a Bayesian approach to correct what's happening is very, very difficult. That's like this little guy on the right here who's climbing the stairs, or it's just like not particularly clear. I'm like, do I go this way? Do I go the other way? And I'm going to talk about two of them. The first one is in survival analysis. Survival analysis, you're trying to model the time to an event. The events can be right-censored. Imagine that, like the doctor is trying to see if you have some kind of complication, but you decide to move. But you decide to move from New York to Florida, so we don't get to see what happens because we're not on the health record. We'll also assume that censoring is at random given the features. And recently in machine learning, a lot of people have been working on deep survival methods. All that really means is like you imagine like your standard survival analysis models, like the Cox model, and then you make it deep. You might take like the function that is linear, use a neural network instead. There are a huge number of papers. There are a huge number of papers in deep survival analysis now. I think the first papers were probably five years ago or so, now hundreds, maybe more. And part of the motivation is this figure that's at the top here, where like you can imagine for a lot of variables that you collect, especially in healthcare, the risk of bad stuff happening, which is on the y-axis here, is not a linear function of like the inputs. For example, if your blood pressure is too high, probably you'll have bad stuff. Also, if it's too low, probably have bad stuff. Also, if it's too low, probably have bad stuff. And the other motivation for these methods is that they discriminate really quite well. So, if I look at an evaluation that says how well do I order patients in terms of like their event times with respect to the predicted event times, and it does quite a good job. But if you were to like plot a different evaluation metric, in this case, we're looking at calibration, these models aren't particularly good. So, this is a deep survival. So, this is a deep survival model trained on negative log likelihoods. And what we're plotting on the x-axis here are like the model predicted cumulative risks. You can think about them as like the CDFs. Once you have CDFs, you get binary events from them, and then you can look at like what the observed fraction of events with the stated probability are. So, for example, if I take the value 0.4, I can find all the time cutoff points for every single observation. Points for every single observation in the held out set. And I can ask myself, like, who had the event before those times that the model set corresponded to a risk of 0.4? And you can just compute the observed fraction. And when this is good, these points would lie along the x equals y. You can see that the observed fraction is higher than the actual stating risk. But these models still discriminate well. But, like, you might say, like, maybe it's not important. Like, you know, if you were a machine learner, I'm a machine learner. You're a machine learner, I'm a machine learner. Calibration is not something that directly comes to you. Like, I can predict the relative orderings of times, and that's useful in some cases. Like if you're trying to do, say, some kind of competitive resource like organ transplantation, then knowing like the relative benefit for one person or the other is what you care about. But for a lot of places where these models are used, you care about calibration because it's an individualized prediction. It's an individualized prediction. What I mean by that is: if you were to open the American Heart Association or World Health Organization's guidelines on cardiovascular disease, they'll tell you to like treat somebody based on their 10-year risk of an event. And you can imagine for cancer, similarly, like you don't necessarily care about your survival time relative to somebody else's. You want to understand what the distribution of your own survival is so you can plan or do what you need to do for it. Need to do for it. One way you might accomplish this would be to just like try to go for it directly. And one way like people actually do this evaluation and practice is not only do they report calibration, they report briar score. Why do they report briar score? Well, it measures some combination of calibration and discrimination. Any proper scoring rule does in some way, but the briar score does it in a way that people find very That people find very interpretable. There's this decomposition that you get like an error in the probability estimate, plus some kind of refinement term for how well you can discriminate. And the briar score is used for binary classification. Here I've shown for like, if you fix a time A and you ask yourself, like the binary event is, did the event happen before that time A? Then you have the corresponding probability from your model and you compare the squared error. If you want to make this appropriate for survival, Want to make this appropriate for survival if you have discrete times, you can sum across a you can also integrate if you have continuous time. But, like, if I wanted to use this briar score directly, the survival times are often right-censored. I gave an example earlier of the person moving from New York to Florida. And what that means is what you get to observe is not necessarily the actual event time. You get to observe the minimum of an event time t and some sensoring time. Time T and some sensoring time C. Like if the event occurred before they moved, you get to see it. If they moved, you saw that they moved and you don't get to see anything else. And you have an indicator of like of the time you get, you have an indicator that tells you, is it T, is it an event time or a failure time or is it a sensoring time? And you know, I denoted these like notational pieces here. We don't need to go into it too much detail that describe the sensoring distribution. Distribution. And why do I define these things? Well, the Briar score that I just described requires a complete data distribution, like data from the complete data distribution, but we only get data from the sensor distribution. And you can use the sensoring distribution, like if you had access to it, in this case, the conditional sensoring distribution, you could use it to adjust and make it so that given data that is censored, Given data that is censored, you can compute what the briar score would be on the full data distribution. And effectively, what's happening here is not particularly complicated. People that get censored have a probability of censoring time that is going to be relatively, you know, like if people get censored, they will be censored. Censored based on the probability of censoring. If you time that you observe an event is later, the chance that they get censored is going to be higher. And so you want to upweight those people. And what's nice about this is that you can compute it if you have the sensoring distributions. You can compute it just given data that you observe, the minimum of the failure time, the sensoring time, a sensoring indicator. But you should ask yourself this question is like, who provides the sensoring distribution to you? The sensoring distribution T. And you might say, well, okay, I just have data, I don't know what it is, so I really can't use this. Well, we can try to estimate it from data. And if we try to estimate it from data, what do you want? Well, you want good calibrated probability estimates because these are used in re-weighting. You're basically re-weighting by the inverse probability of censoring. And we earlier said that, like, okay, if I. We earlier said that, like, okay, if I want good calibrated probability estimates, the Briar score, the integrated Briar score, might be a way to get it. And you might be like, well, how do I compute that? Again, you observe the minimum of the event time and the sensoring time. You have these indicators that tell you like, is the time that you get an event time or sensoring time? But it also, like, if you flip this problem in reverse, you can see that the sensoring times are censored by event times. Times are censored by event types, meaning that, like, if you're running a study and you have this person that's in New York and you see the event, you're not following them for longer and being like, hey, did they move to Florida or not? And so you're left with the same problem. Like to be able to estimate that bioscore and use it as a loss to fit a model, I need to compute this sensoring distribution. To compute this sensoring distribution, I have sensoring again by the event times. Censoring again by the event times, and then I'm like, okay, what do I do? You could try to just, you know, create a loss function and add these two things together. So this is the inverse probability of sensor waiting briars score for the failure times, and then the inverse probability of sensor waiting briar scores for the sensoring times. And they depend on both parameters. The parameter is for the failure time, and the parameter theta C is for the failure. Time and the parameter theta c is for the sensoring time. And if you just add these two things together and you're like, let me just try to minimize it, you'll find that, like, even for this one time step process where there are two scalar parameters, this tells you, like, does the event happen at time one? And there's only one other time step two, which is just one minus that probability. You'll find that the optimal solution, which is x here, is not the true distribution, which is the star here. So in this, I'll mark it. Star here. So in this, I'll mark it as like doesn't work. Instead, we could try playing games. And what I mean by playing games is that, like, rather than having like the same loss function for both parameters, you can have different loss functions for each of the parameters. So specifically, there's a failure player who has some parameter theta t, and they minimize a loss function or index just L F. And then the sensor player is minimizing. And then the sensor player is minimizing a loss function Lg over its parameter theta c. What are the loss functions? Well, you can define them exactly the way we had them before, but notice that they're different. One, the parameter theta t takes the inverse probability of sensor weighting briar score for the failure distribution, and the one for theta c is symmetric, it's for the sensoring distribution. And if we were to just make this step, something nice happens. First, you can see on the left is just the objective that I plotted before. On the right is their gradient field for the game. So you take the gradient with respect to one parameter for its loss and the other parameter for its corresponding loss. And you can see that the stationary point or a stationary point in this game is the true solution. And this is what's And this is what leads us to inverse-weighted games for survival analysis. It's relatively simple. You set up a loss function for the failure player, a loss function for the sensoring player. They have separate parameters. You optimize them. It's not an objective anymore because they have different loss functions, even though the parameter like theta c appears in LF and the parameter theta t appears in LG. You can do some theory that's comforting. You can say that if the scoring rules That if the scoring rules are proper, then the population distributions are a stationary point. This follows somewhat directly, where, like, if you imagine that I have like this inverse probability of sensor-weighted Breyer score, if I plug in the true sensoring distribution, then I know that this is just Breyer score. And then if that Breyer score is proper, then I know the truth will be a minimizer. A minimizer. The stationary point we can also show is unique for discrete finite problems, but in general, if you want to recover the complete distribution, you're going to have to truncate to manage positivity issues. Simply put, if everybody gets censored at a particular point or everybody has a failure time, say, like if you're a human and you're like, time to death will be less than 120 years, then saying what happens after for the corresponding effort distribution is not possible. Distribution is not possible. And the algorithm that you get from this is also quite simple. Like, if you can set up these loss functions, you'll compute their gradients, and then you have two different losses for each of the players. And then each player follows the negative gradient according to some step size gamma. Does it help the original problem that I set out to do? From this plot, yes. Originally, we have the model that was. Originally, we have the model that was trained with negative log likelihood. Now we have in orange here the Briar score gain. It's much closer to the x equals y line. And for this simulation, the briar score improves from 0.13 to 0.078, which is close to the truth of 0.064. You can try to look at some other metrics as well, too, and the results are kind of interesting. You know, at the start, I said, well, Well, these models are very good at discrimination, and discrimination would be measured by concordance. Here, if I look at what the standard approach would be, which is in blue, training with negative log likelihood, we can see that the games, and I've also included another game, which instead of BIRS score, looks at the Bernoulli log likelihood, which also requires sensor weighting, actually get better discrimination with less data. I'm not sure exactly why. Exactly why, but in the metric that we did care about, there are better. And you can see it on a different high-dimensional data set. In machine learning, there's always some version of MNIST, which is a image collection of digits. And here, the data set is constructed so that the survival times relate to like which digit it is. And you can see similar kinds of results here as well, too. And if we go to like benchmarks, and these benchmarks are maybe not the best motivation for machine learning in that they're relatively old, relatively low-dimensional, you see something similar as well. You could ask me, like, how does this evaluation look if I look at, say, the negative log likelihood as would be the training objective for the standard deep survival models. And there, you'd see like. Models. And there you'd see, like, you know, mixed results. Like, sometimes, if you train for a negative log likelihood, it's better, especially in the higher dimensional settings. Other times, there's not really much of a difference. To do this evaluation, though, you have to assume that sensoring is completely random. And these are like Kaplan-Meyer-adjusted versions. Because when you have real data, you don't know. You don't know what the sensoring distribution is. And typically in the literature, people say that censoring is completely irrelevant. Say that censoring is completely random. So it gives you an idea to see if you just estimate that separately with something that's relatively easy and then computed the scores, like how well are you doing. But to put it all together relatively quickly, we started with like, hey, deep survival models are useful. Then we went to, they're not really well calibrated. They have poor evaluations under Breyer score. Let's try to train for it directly because that would be a way to fix that. If you train for it directly, If you train for it directly, you can't do it because the problems are coupled. Like you need one distribution to estimate the other. If you try to set it up as an objective, the truth isn't the minimizer. If you set it up as a game, it is, and it performs well. So I'm going to jump into a second problem, which is another place where we've sort of went with like a direct objective to fix something that we encountered in data. And that's predicting in the presence of spur. And that's predicting in the presence of spurious correlations. So, a typical issue that occurs when you're building like deep predictive models is that they can classify using information maybe that a human wouldn't use to classify. So, imagine that I'm trying to classify cows versus penguins. Here we have this nice cow on this grass here, and penguins are generally going to be on some kind of snow. Then, what people have found is that. Then, what people have found is that if you pass this image, this image of penguins on grass, a lot of deep predictive models, especially if you train them just straight out, will say that this is a cow. And the reason they say this is a cow is because like here the background is very highly predictive of what's going on. And the models that people trained are very good at like picking up texture. And the texture here has a lot of grass in it. You're like, okay, this is a cow. Be like, okay, this is a cow. You might say, well, this is maybe not so interesting of a problem. It's a little bit contrived. But, like, it occurs in healthcare quite a bit. And this is an example of somebody trying to classify what hospital a chest x-ray came from. And you can see that, like, you know, it picks up on things in like the outer boundary, like alignment token, like which way the x-ray is. The x-ray is. And if you can have a model that can predict which hospital it came from, if the hospitals have like differential rates of a condition, then that could be predictive. But it would be predictive in a way that wouldn't be stable. And you can see this even within a single hospital, like this study took a bunch of scanners that were hip X-rays were collected on and then asked the question, like, if you have a randomly initialized convolution. A randomly initialized convolutional neural net. This model is not trained. So, like, you set up a neural network, the weights are random. You project down to two dimensions, and you see, like, okay, can the model separate scanner? Turns out yes, pretty well. But you can also see that, like, fracture is more commonly measured under one scanner than a different scanner. So, this green scanner here, which is the X5000, also has more fractures down here. And even if you take a model that's been pre-tried. If you take a model that's been pre-trained, so like rather than initializing it randomly, you train on some different kind of data, you can see similar kinds of separation. And this problem like really came to a head last year when very early on in the COVID-19 pandemic, testing was a little bit scarce. And so people were like, maybe we can use test x-rays to help distinguish between COVID positive and COVID-negative cases. And people took data. And people took data sets, existing data sets for chest x-rays that were old and didn't have COVID-19. And at the corresponding time on GitHub, people were assembling a data set of positive cases. And they're like, okay, now I have positive cases. Now that I have negative cases, let me build a classifier that classifies between the positive and negative ones. And then that could be helpful at detecting whether somebody has COVID-19. The problem with this, though, is like The problem with this, though, is like you can imagine from just what I said, is like there are a lot of things that are different. Certainly, different hospitals might have like different ways they collect the scans. And you can see that in the visualizations that this paper by the Grave et al. picked up, where they show that like the shoulder positioning is important. I mean, something simple, like in one hospital, you might be a little bit lower down in the scan. Another hospital might put your shoulders all the way up to the top. And like, if you can detect that and all your positive cases are from. Detect that, and all your positive cases are from one hospital, and all of them are from another, your ability for, you know, like your hope that this model would generalize to any sort of new setting is going to be quite low. And it's a common issue. Like, it's not one that's so easily solved by saying, like, let's go and collect better data. It's kind of what happens, at least especially in healthcare, is that like how sick you are really alters like the data that you can collect and also is related to your outcomes. Related to your outcomes. For example, if you're very, very, very sick and somebody wants to collect a chest x-ray on you, first, inside the chest x-ray, you'll be able to see all the devices that are helping you out. Second, if you're very sick, you might not be able to be put into the correct position for a chest x-ray because they're like, we can't move you at all. You're on your side to maintain your blood pressure. And the image will look really awkward. You could say, well, if we had an You could say, well, if we had a nice data set, we could do something about it. But for the first thing, you really can't take out those assistive devices. For the second thing, it's very hard to move them. It's not practical. And this is a common issue that occurs in sort of any kind of like high-dimensional signal collection in healthcare. But it appears in other places too. People in natural language processing are talking about this when they want to study the entailment of one sentence from another. They find that these models are very, very good. Models are very, very good at using the number of common words that overlap between the two, but you can see that it will fail. For an example, like the doctor visited the lawyer is not entailed by the lawyer visiting the doctors. And other examples have this as well, too, like age is probably the canonical example for physiological signals, where you can detect somebody's age, but how that age relates to some condition might Age relates to some condition might change in different populations. But I will say that, like, without more knowledge, the problem is unsolvable because you're asking a particular question. You're like, here are some covariates X. Here's a label Y. Okay, I could tell you, like, you know, like, forget about estimation for a second. I can give you the population distribution. Then you're saying, okay, now that this population distribution is bad, the way that you say it's bad is actually by looking at data from somewhere else that somebody didn't give you. Somewhere else that somebody didn't give you, and being like, hey, this doesn't perform well, or maybe some kind of extra knowledge that a human might have. Being like, it uses information that's outside of the chest to classify. And if that's the case, I know it can't be physiological because everything physiological is in the chest. But like, you can kind of try to formalize this problem by thinking about the idea of a nuisance variable, where in many of these settings, the performance In many of these settings, the performance that you get relies on a stable relationship or a fixed relationship between the nuisance and the label or the nuisance of the response. For example, if the age-label relationship is fixed or like the machine that you collect on and your survival time is also fixed, then prediction would still be good. But, like, in all these cases, what we're talking about is not fixed. Like, people are shifting their chest x-rays, ages or ages, and uh. Or ages and the rate at which conditions appear are also changing. And you can really like just formalize this directly. Like in this example, for classifying cows versus penguins, the background is a nuisance. Like if you look at the animals, you can classify them, but the background does provide some information about what the label is. Another example is diabetes. This is a real example that people in my lab are thinking about, where for diabetes, Where for diabetes, BMI is predictive in America and Western Europe of your type 2 diabetes status. But if you were to go to East Asia or South Asia, it'd be a lot less predictive. There are a lot of people there with like very, very skinny diabetes. So to formalize this, we can try to do this in something we call the nuisance varying family. And it's exactly what the setup was before. What the setup was before. Here we have two distributions, D prime and D. They're over some features, covariates X, a label Y, and a nuisance Z. And the distributions are the same. The only thing that changes between them is the relationship between the nuisance of the label. So the probability of Z given Y or the density of Z given Y. And your goal here is to find a low error predictor for. To find a low error predictor for all of the nuisance variant family, the first idea might be like, well, okay, the nuisance label relationship is the problem. Let's break it. So you could try to construct a nuisance randomized distribution where the nuisance and the label are independent. And then we'll say like a good predictor would just be the conditional distribution in this independent nuisance randomized distribution. Nuisance randomized distribution. But it really doesn't, it doesn't work by itself. You can construct like a simple textbook example where you'll have like y and z are independent. They have a common child x, but when you condition on x, y and z become dependent. And the predictor could become bad because you'll keep track of z. Here's a concrete example of that. Y is normal. Z, if things are normal. If things are randomized, it would also just be normal redistributed noise. X1 is some function of Y and Z with some noise, and X2 is the different function with different noise. Effectively, like a good predictor here, just forgetting about like generalizing for different members of the Newsen Sphering family, would be one that weighed X2 a little bit more than X1 because X2 is less weight. Because X2 is less noisy than X1 is. But if you weigh them differently, then this Z is gonna stick around in some way. And it's not gonna be the same if I move to a different distribution. So like if you use Z in a particular way and Z starts like including the label in different ways in different families, you can expect that the prediction quality will break down. And you can show in this example that you can set up a value of A where that this To a value of a, where that this distribution just predicts worse than chance, which is just guessing from normals here a lot. But you do know here that x1 plus x2 is just going to be like some statistic of y and noise and should be a good predictor of the response for all of these PAs, for the entire nuisance variant family. And so that's where idea two comes in. We're going to set up uncorrelating representations. Up uncorrelating representations. What we want to do is we want to just limit to a set of representations, which is just like a function of the features that do not correlate y and z under this randomized distribution. Because in the previous slide, we saw that this correlation was the problem, like when you saw it, when you had explaining away. So let's just like limit those away. And then you can do something nice if you give me a representation in this set of uncorrelating representations. Uncorrelating representations, you can write down the performance for any test distribution, and quickly you can get something that the performance is equal to the performance plus some positive term because it's an expectation of a KL divergence. Here, performance is negative KL divergence, so higher is better. It's easier for you to think about. You can think about it as just held out log likelihood. So you get this like weak statement, but an okay one that like before. But an okay one that like before performance wasn't better than chance, but now, like, if I limit myself in this particular way, performance will be better than chance for any sort of nuisance-label relationship. But what's the best one? I guess, like, you know, if I tell you it's like better than chance, that's okay. It means that it's not arbitrarily degrading. You kind of want to do a good job as well, too. And so you can ask yourself: when is a representation R2 better than R1? If you assume. If you assume that R1 is independent of the label given R2 and the nuisance, so meaning that it doesn't provide any information about the label given R2 and the nuisance. In words, we'll call that R2 blocks R1. Then you can do a little bit of algebra and write down what the predictor Y given R1, R2 is. And you can see that the way R1 appears in this middle term. R1 appears in this middle term, there's no y. So it means that the predictor doesn't really change when the value of r1 changes. So, in words, once I know r2, if I tell you r1 also, your prediction is not going to change. It might look like that follows directly from this conditional independent statement, but here on the left-hand side, there's no z, so it's not exactly that. And you can push this intuition into something quite formal. If you define an argument. If you define an R-star that's maximally blocking, so it blocks every other uncorrelating representation, you can show first that its performance is best. But its performance being best is nice, but like we're in the business of building algorithms, so we want to turn it to something that we can actually estimate with. So then we say that not only is it best, but it has the highest information with the label over any other representation. But if this is greater than or equal to, there could be other. But if this is greater than or equal to, there could be other representations that are equal. So in the last step, we say if they're equal, then the performance is the same, which is the best performance. But, like, put in words, the best performing uncorrelating representations have highest information at the label. And that lets us turn it into an algorithm. The first step of this algorithm is nuisance randomization, where we try to construct this independent distribution where the label and nuisance are independent. Are independent. One way you can do that is by re-weighting. Here, if you were to extend this out, the p d of y given z would cancel, and we'd multiply by p of y. You can also fit a generative model to estimate this, because we know that p of x given y comma z is assumed to be the same. And if I could estimate that directly, that would also let me construct the Newsen's randomized distribution. The second step is trying to find this representation. Trying to find this representation, which called distillation, where here you're building a model that maximizes log probability, which is maximizing the information between the representation and the label, subject to a constraint that you live in the uncorrelating set. To estimate this information, we use likelihood ratios. We use a similar idea for variational inference where you train a classifier, and that classifier gives you a likelihood ratio, which then gives you an estimate of gives you an estimate of KL divergence or the information. And we also penalize the joint information rather than the conditional one for the unclering set, which narrows the set of representations you find because there are local optima you can run into where the representation is only a function of the nuisance. Now, if we go back to our examples, this is a classification version of the regression example I showed earlier. Example I showed earlier. And we can see that, like, if we were to just train a model the straight way, the performance on a test distribution would not be as good. Its performance on the training is better because it's using a correlation that only exists in the training distribution. If we build a generative model or try re-weighting, we can see that the performance is more stable. It's close to what the best linear predictor would be. We can go to a real example. So, people try to set up cameras to monitor. Set up cameras to monitor animals in the wild. And in this monitoring, like often because backgrounds are static and different animals are in different places, getting them to generalize to new locations is hard. And people in machine learning have set up an example of this, which they call water birds versus landbirds, where waterbirds are generally over water, landbirds are generally over land. And if you build a classifier, these classifiers tend to just use land or water to. Just use land or water to predict. And so here we have a training set. This training set consists of 90% of images: water birds are over water and land birds are land. The test set is the flipped where most water birds are over land. And we can see if we train a model just in the straight way, the performance degrades quite a bit. But if we use the re-rating version of our algorithm, it's more stable and it's close to what you could do if you had the extra information at test time. The extra information at test time of what the background was. And what's cool here is that we didn't actually even use the knowledge of, like, hey, is this image taken over water? Is this taken over land? We just said the nuisance is this high-dimensional object where we cropped out the center patch where the animals typically are. We said that the rest of it is the nuisance. And even though the center patch has ocean in it, the outside patch is enough to control for and remove that dependence. We can move to another example. This is one of the motivating examples for pneumonia classification. Pneumonia classifiers feel like they should be relatively simple. And given what we've done in image classification in the last 10 years, you should be able to build a chest x-ray classifier that could detect pneumonia inside of it. It's a relatively common condition. But there's not one that's been good enough that it's been approved or deployed. And these two, one of these. And the issue, one of the issues is really what we've been talking about here about spurious relationships. So we tried the same idea. We can see that, like, on the training data set, these models do quite well. But when you move to a test data set where the relationship between which hospital has more cases of pneumonia changes, the performance degrades. For generative nerd and reweighting nerd, there's still a performance drop, but for reweighting nerd, you But for rewaiting nerd, you can do better than chance. It's stable, or more stable, I'd say, but not enough for use in the clinic. And it's also difficult to get like a ground truth evaluation here, because in this case, like having an idea of all the nuisance variables or how they might occur is not complete. Like orientation, and here you can see there's a nuisance in the top corner here, which is text that's been written on these chest x-rays. But going back to a high-level. But going back to a high level, we had two problems. We had bad calibration, poor evaluation on metrics that people cared about in deep survival models. And we had poor generalization when the nuisance label relationship changes. In both cases, we designed an estimator directed at the goal. For survival analysis, we set up these inverse-weighted games where we estimated briar score weighting by the inverse probability of censoring. Probability of sensoring, and we needed the sensoring distribution. So we weighted by the inverse probability of the event, and we showed that it worked well. For generalization under the nuisance, the label relationship, or generalization under the nuisance label relationship changing, we use representation learning. But when I think about like Bayesian inference and I ask myself, is it helpful? Is it easy? For the first problem, I'd say probably yes. For the first problem, I'd say probably yes. You know, like what's happening intuitively, like, why are you getting bad calibrations? These models are so flexible that they start interpolating. And then when you interpolate, you're like fitting your data in a very, very particular way. When I'm interpolating for classification, what it would do is like make my predictions very, very confident. And you can see this in examples outside of survival analysis. If I could do Bayesian inference well here, I would know that there are like many, many, many ways that I'm interpolating in between. That I'm interpolating in between, though, at this fixed point, I still might say, like, I can predict this exactly, but uh, it's a little bit hard. You know, I think that posterior inference and like a neural network that's sufficiently sized, progress is happening, but it's still hard. For this one, I honestly have no direct idea. Like the problem formulation even feels unnatural for me because I've moved from having like a single data distribution to having multiple data distributions. Maybe it could be some kind of Distributions. Maybe it could be some kind of hierarchical model, but I'm not sure about that. And that's it for what I have. The references for the first two high-level things on inverse-weighted survival games, prediction are the first two bullet points. The last three are the older stuff on posterior inference. Oh, you couldn't hear me. Any questions from the floor, but there doesn't seem to be any urgent questions here. Anyone online? Any online questions? Any online questions? Just unmute yourself and comment. Phil David posted a comment in the chat about scoring rules that work with censoring. No one else. They seem all happy. So it looks like we don't have any good answers to your final questions. So we'll just leave it as questions. And then maybe in the interest of And then, maybe in the interest of we still have like some discussion ahead. So, please stay around. We still have like a brief five-minute discussion by Elena coming up. Then, thank you again for the presentation and the talk. And thanks, please hang around and maybe still participate in the rest of the program. Thank you. So, we have like So we have like just five minutes. Elena Portolato.