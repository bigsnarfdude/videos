Talking about non-uniform network design models. This is joint work with my advisor, Chandra Chikuri, and based on a paper that I kept this year. Okay, so I'll start with just talking about what non-uniform network design is overall, and then I'll talk about two specific problems. I'll spend most of my time on the first one. Hopefully, we'll have time to talk a little bit about the second as well. So, the flavor of network design problems that I'm looking at in this talk is you're given a graph, and you have some undirected graph, and you have some edge costs, non-negative edge. Costs, non-negative edge costs, and you have some connectivity property you want to satisfy. And your goal is to find a cheap subgraph of the graph you're given that satisfies this connectivity property. And as you guys know, we say vertices u and v are k connected if we have k edge disjoint pads, or if you remove k minus 1 edges, they should still stay connected. So some examples, k edge connected spanning subgraph, this is saying we want everything to be k connected. A single pair, we won't want specific pair of vertices to be k connected. Want specific pair of vertices to be k-connected and plenty of others. And we can generalize this fully. So we can say that, you know, instead of having a spanning subgraph problem or a single pair problem, we can generalize this to something known as the survivable network design problem. And in this, every pair has its own connectivity requirement. So I can say for some pair UV, they should be RUV connected. And I want a cheap subgraph that satisfies this. And equivalent. And equivalently, you can think of the cut-based formulation. So, as I said earlier, if things are k-connected, that means that any cut separating them also has to have k-edges. So, the cut-based formulation of this question is for every cut, the number of edges crossing that cut should be the maximum of the connectivity requirement of terminals that are separating. Okay, so why do we care about this problem? And the two motivations I want to talk about kind of come directly from the two definitions of K connectivity. Definitions of k connectivity. So the first is fault tolerance. It's a very natural motivation. You want stuff to stay connected even if your edges might fail. And the second is you might just want multiple paths per flow. So even if you're not scared of failure, you might say that, you know what, I want to route things along multiple paths or have multiple flow points. And there exist two approximation algorithms for this problem via iterated rounding from 2001. Okay, so if you think about these two motivations, they're very different. Two motivations, they're very different in terms of actual application, but they end up being the same thing in SNDP because of this kind of duality between flow and cut. So what non-uniformity is, is saying that why do we care about treating all of the edges uniformly when it comes to fault tolerance? So the question we're asking is, maybe all edges are not equally likely to fail. Maybe you have some settings where some edges are more bound to failure, some are less bound to failure. And with this question in mind, you can define new. And with this question in mind, you can define new models that capture this. So, I want you to think about non-uniformity as any model where the failure of the edges just isn't uniform along the edges. So, some examples, you can say, maybe the simplest is just, I'm going to mark some edges as these can't fail. These are safe edges, no matter what, these edges are not going to fail. And this is what flexible graph connectivity corresponds to. But you can also generalize this further. You can say that, you know what, some Oracle is going to give you all of the specific subsets. To give you all of the specific subsets of k minus 1 edges that can fail. Maybe these are correlated in some way. And you need to mark against, you need to be fault tolerant against those specific failures. And this is the bulk robust model. So motivation, one, on a just practical setting, as I said earlier, you can think of a lot of examples in practice of network design where the edges may not behave uniformly. You might have some roads that are more susceptible to failure, some wires that are cheaper and might break easily, things like that. But from a theoretical standpoint, it's also interesting. But from a theoretical standpoint, it's also interesting because these problems don't have the nice clean structure that SNDP and all these other problems do have. So all the techniques that we used for iterated rounding or for primal dual, these nice techniques kind of go out the window. So this gives us an opportunity to develop some new nice techniques and maybe that these will be useful in other fundamental problems if never present. Okay, so let's talk specifically about flexible graph connectivity. So as I said earlier, our edge set is partitioned into safe and unsafe edges. Is partitioned into safe and unsafe edges, safe edges can't fail. And the simple one-connectivity version of this is just saying we want a cheap subgraph that stays connected even if a single unsafe edge fails. So if you just had a tree of green edges, the green is safe, by the way. If you had a tree of safe edges, that's good. Unsafe edges can't fail because we don't have any, so this is satisfying our property. Instead, if these two were unsafe, now this does not satisfy our property. If either of these two unsafe edges fail, Property: if either of these two ones if I just fail, the graph is disconnected, but we can object. And one thing that I want you to notice in this example is this problem kind of gives us a way to interpolate between one and two connectivity. If everything is safe, this is just simple, you know, spanning tree. If everything is unsafe, it's two-edge connected spanning subgraph. So the section of the graph that has a lot of unsafe edges looks like a two-edge connected subgraph, and the rest looks like a tree. So this kind of motivation helps us define the higher connectivity version of this. Connectivity version of this. And so, what we're going to say is we still want the same motivations as before. We want fault tolerance in multiple paths. But our fault tolerance now is only with respect to the unsafe edges. So these two definitions are no longer equivalent. Now we have two parameters, P and Q, and Q corresponds to the fault tolerance, P corresponds to our multiple paths for flow. So we say that two vertices are PQ connected if they have P P edge disjoint path between them, even if any q unsafe edges fail. So you'll notice this kind of gives us a way of interpolating, in this case, between p edge connectivity and q plus q edge connectivity, instead of just one it. And equivalently, every cut should have either p safe edges, because if a cut has p safe edges, then that cut is going to have p edges even if you remove any q unsafe edges, or p plus q total, because that way, even if you remove q on safe edges, you should still have If you remove Q on safe edges, you can stop it. Any questions about the definitions? Okay, and then you can define the natural SNDP problem. Every pair of vertices has some specific requirement, and now we want to find a cheap subgraph satisfying this. For simplicity, I'm going to assume for the rest of this talk for all UV, PUV, and QUV is either some fixed P or Q or 0. Everything generalizes pretty easily from that setting, so it'll make this notation. That setting. So it'll make quotation easier. Yeah, so another example here of just higher connectivity. You can see that if you remove any two unsafe edges, you still have two edges straight paths between S and T. Okay, so this problem, a little bit of background. It was introduced in 2013 by Jay Shfili in the single pair settings. This is only where you have one pair S and T with some requirement. Every other vertex in the graph, we don't really care what happens to it. And got some results there. Nothing happened for a long time. Nothing happened for a long time, and in 2020, people started working on this problem again, got some more approximation results. They're here, I'm not going to go through all of them in detail. And the other version that's been studied in the past is the spanning case. So the spanning case was also introduced in 2020 in the same paper as before, and then extended upon in 2022 very recently. Sorry, what was the difference between the two slides? Yeah, yeah, so this is the same. Yeah, yeah, so this is single pair. Here you go, only a single pair has some PQ connectivity. Okay. And this one is a global connectivity requirement. So every pair has the same requirement. So you can think of these as two special cases of the full lesson PQ generality. So this was studied in here. 1,1 case was studied, 1,2,2p,1, and 2pq. And these results are summarized here. And what's interesting about this is one, SNDP in full general. Is one, SNDP in full generality wasn't really studied in the past at all. We'll get to that towards the end of this talk. But secondly, in the P,1 case, this is a case where we only care about one unsafe edge failing, or in the 1, Q case, that's where we only want one connectivity once a bunch of unsafe edges fail, we were able to get constant factors when P and Q were constant. So you'll see here that all the approximation factors just depend on P and Q. But somehow, as soon as we got to the general PQ setting, for both the General PQ setting, for both the spanning and the single pair case, there's a log n approximation in there. So this is kind of strange. Even in like 2,2 setting, which is like 2 or 4 connectivity, we're not able to get anything better than log n. And so this inherent difference is something that we wanted to understand better. So our big motivating question was, can we get something that's f of p and q for any fixed p and q? And the answer is yes for some very small special cases for now. For now. So we got an O of P approximation for p, 2, p, 3, p, 4 when p is even, O of q approximation for 2, q. And in the single pair setting, something depending on p and q, it's an SC function, so I didn't put it in there, for some special cases of p and q. And essentially what's happening with this function is it's when one of the two can be large, but they can't both be large. And independently, Kishan Jose, Kishan, Joseph, and their co-authors gave a constant factor approximation for P,2. Yeah, also very interesting paper. Okay, so at a high level, the biggest problem here, as I said earlier, is that these problems don't have the same cut structure and the same nice structure that SNDP has because of the safe and unsafe edges. And a simple exercise to think about how complicated this problem really is is to think about how would you even verify a solution. In SNDP, I give you a solution and you can see I give you a solution, and you can say, okay, let me just run min cut between every pair of terminal pairs and check what the min cut is. In this case, it's not clear how to even verify a solution beyond just removing every subset of Q edges and then checking min cut. This is one of the reasons we care about small Q and fixed Q, because then at least we know how to verify solutions. The idea is to use the augmentation framework, hopefully that's been introduced by Vera, so I can skip through this a little quickly, but the idea here is let's just focus on augmenting from is let's just focus on augmenting from some connectivity to the next higher connectivity. And so we'll start with a solution to p, zero. P, zero is just p connectivity in general. Let's ignore safe and unsafe edges. And then we'll keep iteratively increasing our q parameter. So we'll go from p, 0, p, 1, 1 to 2, et cetera, until we get to our requirement. So why is this nice? If you're at p, k minus 1, that means that every cloud has to have either p saf. that every plot has to have either p safe edges or p plus k minus 1 total edges. And if you want p, k, same thing except for this number goes up by 1. So if you'll notice, what are the cuts that we need to hit? If they already had p safe edges, we're fine. We don't need to do anything. And if they had p plus p total edges, we're fine. So the cuts we need to hit are exactly the ones with p plus k minus 1 total edges. And those cuts, all we need to do is add an edge. We don't really care if the edge we're adding is safe, unsave, any of that. Adding a safe, unsafe, any of that. So, this allows us to frame this problem as a hitting set problem, and we can ignore the distinction between safe and unsafe, I just from now on. Okay, so a family of cuts is uncrossable if they uncross. So if you have two cuts that cross each other, essentially you can think of this as a nice structural property that lets you find two cuts that cover the same kind of boundary of edges, but they're not crossing each other anymore. And informally speaking, And informally speaking, if the set of cuts that you're trying to hit and trying to cover is uncrossable, we have a 2 approximation and we're done. So it would be nice if in our problem as well the set of cuts we needed to hit is uncrossable. And in the 2, Q setting it is. So we just keep hitting these and this gives us an O of Q approximation. Unfortunately, that's not quite the case as we go to higher connectivity. This simple example shows even 3,1 to 3,2. This is a 3,1 connected subgraph. Connected subgraph, and these two cuts A and B are violated, but they don't uncross. Okay, so our approach, as I said, set of violated cuts is not uncrossable. And in a lot of network design problems, people are a little stuck once we get to something is not uncrossable. What do we do next? There isn't too much techniques or tools developed for that case. So our approach is: okay, let's work with what we know. We know how to deal with uncrossable families. How can we make this thing? Families, how can we make this thing look uncrossable even if it's not? And the idea is: well, even if our set of violated cuts is not uncrossable as is, maybe we can divide it into smaller cuts, smaller families of cuts. And if each of those are uncrossable, then we're good. Cool. So I'll talk about the P, Q spanning result. I think this is the simpler of the two. But in this case, what we show is that, as we said earlier, the set of cuts we're trying to hit are the ones with less than P safe edges. less than p safe edges and exactly p plus p minus 1 on safe edges. And we can divide these based on the number of safe edges. So less than p safe edges is anywhere between 0 and p safe edges. And what we show is that if you fix the number of safe edges, so if you say, suppose let's look at all the cuts with 0 safe edges or one safe edge or two safe edges, and you cover these iteratively, each of those subfamilies become unconscious. So now what we can do is we can get an O of P approximation by covering everything. It's zero. Covering everything with zero safe edges, then one, then two, and at each time we pay two times off, and overall we get a d approximation for it. In the single-pair setting, unfortunately, this exact approach doesn't work, so you have to do something a little bit more clever to break it into uncrossable families. But the general high-level approach is similar, that we look for structures existing in the cut and show that even if this family is not uncrossable, it breaks down into some small number of uncrossable families. Well, in the rest of my time, I'll talk a little bit about this bulk robust. So, in this setting, I guess everything we've talked about in the past, the violated cuts still had some structure. We were like, okay, they're not as nice as the unprosable stuff, but there's some structure that we can exploit and we can use the machinery developed for unprosable subgam at least. What if that wasn't true? What if there's actually just no structure to the set of violated cuts? Is there anything we can do then, or are we just completely stuck? Can do then, or are we just completely stuck? So, our model here is: you're given as input a graph, and you're given a bunch of scenarios. And these scenarios are sets of edges that can fail, and they each correspond to some terminal pairs. So what you want is a cheap subgraph such that if any of these scenarios fail, so in g minus f sub i, all the terminals k sub i are still connected. So it is really kind of the most general form of SNDP, flexible graph connectivity, all this non-uniform design. This non-uniform design. So, this problem was introduced by Jay Schfilly and co-authors in 2015, and they got some approximation algorithms for the spanning and single-pair case. Again, the general SNDP case wasn't studied in this setting. And there are lower bounds for unbounded width, but again, with the fixed width, there's no lower bounds. There's nothing saying that we shouldn't be able to do something good for width 2 or width 3 or something like that. And the width here, sorry, I didn't say this earlier, is the math. Here, sorry, I didn't say this earlier, is the maximum number of edges that can be deleted. So you can think of width as kind of the connectivity equivalent. Okay, so what we show is that in the full generality SNDP version, we're able to get something that, an approximation ratio that relies on the connectivity, the width, and some polylog approximation on n. And what I want to highlight here is actually just the framework more than the results. More than the results. So, this framework was developed in 2022 in Fox by Chen and co-authors. And this was a framework developed for group connectivity SNDP. If you're not familiar with the problem, don't worry. It's not really relevant to this talk. But it's also a hard network design problem with very little structure. So kind of an example of how techniques can cross across multiple different network design problems. And what we show is that we can modify this technique and get an interesting result for the bulk robust setting. The bulk robust setting. And this new tool allows us to solve very, very general problems. So the high-level idea is, again, we're going to use the augmentation approach because things are too messy to solve directly. And what we want to do is we want to buy paths between terminals, right? We want to say that if a scenario goes down, there should still be some path that we can buy between them. So we can solve that actual LP and try to use these LP fractional solutions to guide which paths to buy. But on a general graph with very little path structure, it's not. With very little path structure, it's not clear how to do this. So they go via tree embeddings. They say, let's embed the graph into a tree. Trees make it really easy to solve routing problems, so let's solve the problem with the tree, and then let's map that solution back into the graph. And specifically, they use congestion-based tree embeddings. So what this is, is it's saying that you have this weighted graph, you're going to get a distribution on weighted trees, and every tree edge is going to map to a path in the graph. And essentially, we want every And essentially, we want every tree - the tree edges, we don't want too many tree edges mapping to the same edge in the graph. So, we want low congestion back in the original graph. And this kind of makes sense from a connectivity standpoint. If you're solving some problem, you want edge-district paths. Those should kind of correspond to edge-destroying paths in the real graph as well. And informally stated, Ruckier's result of 2008 is that there is a distribution of trees where you can get congestion log n. Okay, great. Okay, great. So take one, let's solve the LP, let's use those LP values in our tree distribution, sample a tree, solve the problem of the tree, map it back to the original graph. This seems promising at first, but I'm adding a couple details here. The first is, what is solving the problem on the tree actually looks like? And the second is, what do I mean by variables xe for each edge? In particular, what capacity should we give edges we already chose? Remember that we're in the augmentation framework. We're not solving single connectivity. Augmentation framework. We're not solving single connectivity. So we've already bought some edges. And if we give those edges we've already bought capacity zero, well, now you might not have a good amount of flow between your terminals and the tree. So that's no good, because then solving the problem when the tree becomes difficult. But if you give them capacity xe, then you might just end up buying pads that you already have, and that's also no good. So what can we do? The solution they came up with here is to do this multiplicative weight style type of update. Do this multiplicative weight style type of update. Saying, okay, let's do something in between. Clearly, these two extremes are not working. So let's give them some weight rounded down to some factor of log n and connectivity and stuff. So that we choose them, but we don't send too much flow on those edges. And they do some clever analysis to show that if you do just oblivious tree rounding on these trees, with high probability stuff gets connected and you don't use your existing edges too much. So I'm not going into any of the details of this. Very, very interesting results, definitely would recommend. Interesting results. Definitely would recommend taking a look at it if you have time. But what I wanted to highlight here is that this is completely oblivious to these scenarios. So, although they use this result for group connectivity with little modification, there's nothing here that says anything specific about the structure of the cuts. So, even if the cuts are completely unstructured, we're able to get some decent polylogram-like approximations. Okay, cool. So, just to summarize, this non-uniform model gives us a new way to model connectivity. Model gives us a new way to model connectivity problems and has both practical and theoretical interest. We gave some approximation algorithms for special cases of the spanning and single-payer case for flexible graph connectivity that only relied on P and Q. And we also give the first poly logarithmic approximation for this general bulk-robust S and DP setting, which also extends to the flexible graph connectivity situation. So, open problems, this is a very, very new area of study, as you guys saw in the history space. So, there's lots The history space. So there's lots of room for improvement both on the lower bound side and the upper bounds. Most of these algorithms and approximation ratios I don't think are tight. I will mention that the polylogarithmic approximation ratio, we're working on improvement right now, so there's probably going to be an improvement to that coming out soon to better polylogs. But still the question of can we get something that doesn't rely on n at all for some fixed small width is wide open. And I think that's the And I think that's the interesting setting both for practical and theoretical perspectives, just to look at small fixed widths. Can we get things that don't rely on that? All right, thank you. Thank you very much. Other questions? So is there any uh hardness known at all? In particular the certificate question for Q which is not a constant. Is that it be hard? NP hard? Uh, good question. So there's NP hardness shown for some special cases in like the single-pair setting. Um, spanning setting for F spanning setting and the SNDP setting for flexible graph connectivity generalize the SNP setting. So NP hardness follows from there. But the certificate problem, I'm not sure. As for single pair, there's some integrality gaps and things that suggest we're going to have to have ratios that rely on P and Q, but actual hardness. But actual hardness is not really much fun. There's also some connections to capacitated network design and some hardness that follows from there, especially in the 1, Q and Q, 1 settings. The capacitated network design is a hard. In certain settings. But yeah, there are some connections there. They're not direct, so the hardness for capacitated doesn't translate directly. Because these problems look like capacitated network design with only two. Capacitated network design with only two capacities, and a lot of the hardness results of capacitated network design rely on multiple capacities. But that's where a lot of the integrality gaps and things like that come from. The width, so if it's not constant, I mean you could have exponentially many scenarios. How do you sort of access these? Does that matter for you? Yeah, so we're mostly focusing on the fixed width setting. Focusing on the fixed width setting. In the ValkRobus setting, the way the problem was introduced was saying that these are actually explicitly given to you. So you do get extra runtime in that case because these scenarios are all explicitly listed. So now you're able to do things in runtime that depends on the number of scenarios rather than just the size of the graph. But yeah, I'm not sure if people have studied just Oracle models of this or anything else. Oh, the version of the paper that I've read is explicitly given to you, and it gives you the extra runtime. Okay, any other questions? Let's thank Crea again. And before we close, let me remind you of the skills that now there should be lunch in the Viscouse dining room, then the guided tour through the bunch centre should start at one where we have lunch. One where we have lunch, and then don't forget to be here again at 2:40 for the group for them.