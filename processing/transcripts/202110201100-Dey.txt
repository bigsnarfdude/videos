You can start. Okay. Welcome, everyone. Thanks for the introduction, and special thanks to the organizers for giving me this opportunity to share my work in this platform. So the main thrust of today's talk can be divided into two parts. In the first part, I will define the problem of digital. The problem of degeneracy loci of matrices. And next, I'll talk about its configuration and connection with symmetroids for four linear operators in R3. And then I will generalize this idea for m plus one linear operators in Rm. And in the second half, I will talk about hyperbolicity cones and derivative relaxation of the PST cone. And if time permits, then I will like to report. Permits, then I will like to report some of my recent results based on work in progress. So, let me first define the problem of degeneracy loci of matrices. So, we consider here a collection, calligraphic A, collection of R linear operators in Rm. And we look at the degeneracy locus, which we denote as. Which we denote as X is the set of points such that the rank of this matrix is less or equal to m minus one. And our aim is to study the geometry of this Diginesy locus, its configuration, dimension, and etc. So this problem was initially motivated by one of the prominent results in phase retrieval. So let me So, let me give you a brief tour to phase retrieval and its connection with the low-rank matrix compression problem. So, in phase retrieval, one is interested to find X in Cn up to sine such that it satisfies a bunch of quadratic equation. And this can be translated equivalently to the second problem. So, this is problem A, this is problem A. Is problem A, this is problem B, and this is C, suppose, where one can look for finding X such that it satisfies a system of linear equation over the condition constant that X should be rank 1 PST matrix. So, after this translation A to B, this phase retrieval problem can be viewed as a equivalently viewed as a Viewed as a rank minimization problem. Can I stop you just a minute? There is a question by Maurice. You can see the question or the discussion. Actually, not because I'm sharing the screen to my. You can matter. Yeah, I can see. Okay, it's better because it's not. A complex vector, not just its phases, why it is called phase retrieval. It seems to indicate only, yeah, that's true, but in general. Yeah, that's true, but in generalized phasory trivial, also people are interested to look for scalar real cases. And I will tell you in next slide why that is important, at least from mathematical point of view. Thank you. Thank you for your question. And that's an important point to be noted that the result I'm going to explain is for real paramet operators. Okay, so. Okay, so yep, as I was saying, that it can be related to a low-rank matrix completion problem, where one can seek for minimizing rank function over a affine size of a VST cone. So as I said, that we are interested in our context, we need to define generalized phase retrieval. And there, one is interested to recover a vector x in R. vector x in Rm up to sine from non-linear measurements or quadratic measurements for a given collection of linear operators A1 to AR in Rn. And here it is all known that if this collection has phase retrieval property, then this map MA from Rm modulo plus minus 1 to RR is injective. R is injective. And furthermore, if we are considering these linear operators in Rm, then there is a result by Wang and Shu. It says that this map is injective at some point x, if and only the vectors a1x to a r x span the entire entire Euclidean space Rm for a collection of symmetric matrices Ai. The collection of symmetric matrices, this result actually can be restated, equivalently restated as follows. The collection of symmetric matrices does not admit phase retrieval if and only if this degeneracy locus contains a real point. And furthermore, it is also proved that if the collection is greater than or equal to 2m minus 1, then a To 2m minus 1, then a generic collection has the phase retrieval property. And if m equal to 2 to the per k plus 1, then r equal to 2 minus 2 collection always says that the degeneracy locus always contains two real ones. That means for 2m minus 2 collection of linear operators, phase retrieval is not possible. And that motivated us to study this case. So we are interested to determine. So, we are interested to determine conditions on collection of 2m minus 2 real matrices. Another question? Yes, yes. From Marissa, of course. It's a set of points where rank of this matrix is let me cut this one. Thank you. Yep. Thank you. So here. You so here I am. Yeah, so our aim is to determine conditions on a collection of two minus two real matrices which ensure that the degeneracy locus is entirely real. Okay. And we abbreviate this degeneracy locus as DL, and for real degeneracy locus, we call R D L. And in this talk, I'll talk about its application or About its application or interaction with singular symmetroids and hyperbolicogramming. And maybe other two cases I can speak about it in another talk. So here in our setup, the first interesting case arises when we have m equal to 3 and r equal to 4. So four linear operators in R3. Then the degeneracy locus. Then the degeneracy locus, recall that the set is the point for which this rank of this 3 by 4 matrix is less or equal to 2. And then we have shown that for a generic choice of linear operators in R3, A12, A4 in R3, X, this regeneration locus is a zero-dimensional substrate of degree 6 in P2. I'm sorry. I'm sorry. So, here is a brief sketch of the group. We look at the locus of rank 2 3 by 4 matrices, which is a nine-dimensional subscheme in P11. And then we use the degree formula for determinantal variety to compute this degree 6. And then we observe that for a generic choice of E1. That for a generic choice of A1 to A4, actually this will give us a two-dimensional space subspace. And then when we take the intersection of this determinantal variety for a generic choice of A1 to A4, we get a zero-dimensional subscheme in P2. So now our aim is to determine conditions on these matrices AI such that these six That these six points are rational points. Furthermore, they can be expressed as rational functions of the entries of the matrices A12A4. So here is our setup. We consider the variety X1 of four-tuple 3x3 matrices sitting inside P35, such that these matrices are in the linear span of four fixed Rankon matrices. Four fixed Rankon matrices. So one can choose this EI, which are Rankon matrices. First, choose EI and then construct A1 or AI. If we have this tuple, then we have shown that for generic matrices A1 to A4, this degeneracy locus consists of six rational points, and they can be actually obtained by intersecting, taking pairwise intersection of four projects. Pairwise intersection of four projective lines in P2. So, idea behind this group is as follows: As EI is rank one, so kernel of EI is a line. So, for each pair of indices, when we take the intersection of kernel of EI with kernel of EJ, we are looking, we generically may get a point xij. And if we are in that point of intersection, then the point of intersection, then the vectors a1x, a2x and a3x and a4x, they actually lie in the two-dimensional space. And specifically that two-dimensional space is spanned by the images of EK and EL, which K and L is in the complementary of I and J complementary set. Then it turns out that X is that the genetic locus is nothing but the union of these points, which correspond to pairwise intersection of four projective lines. Of four projective lines. And each point actually can be, each point can be expressed as a rational function of the entries of the matrices. And here I'll give you a demonstration through one example. So suppose our EI, that rank-on matrices, is spanned by EI, a standard basis vector in R3, and E4 is all one vector. Then the data. Vector. Then the degeneracy locus is nothing but the pairwise intersection of two of these four lines. So observe that this are the normal vector of these four line four lines. So furthermore, notice that these points, they are colleagues. They are collinear. They are not in general position. They have special criteria. So we observe that there are four triples which are collinear. And this is a very known configuration in phase projective geometry, in particular incidence geometry, known as quadrilateral configuration. So six points with four triples collinear. And in dual case, we have quadrangle configuration. Case we have quadrangle configuration. So, here is a visual demonstration of quadrilateral configuration. So, on the left-hand side, we have 6243, that means 6 points, 4 lines, and 2 points per line, per point. And per line, we have three points. In the dual picture, In the dual picture on the right-hand side, we have four points, six lines, and three points per line, and two points, two points per line, and three lines per point. So then we asked the following question. Oh, here I would also like to mention one more important factor. To mention one more important fact that in our proof so far we have not assumed that AI to be symmetric matrices. So, in general, AI could be anything, not even symmetric, PSG, nothing. It's generalized passwords context. And EI, if someone impose that AI to be symmetric, means one can choose EI, that rank one matrices, to be symmetric matrices. And we are interested to know: is this the only? Is this the only way to get six rational points for symmetric case as well as non-symmetric case? So it turns out that if we export some facts about quadrilateral configuration, then we can answer to this question. And it's true in this case, converse statement is also true. So here some facts that we have proved that two quadrilateral contributions. Proved that two quadrilateral configurations are projectively equivalent and also the fact that degeneracy locus of A1 to A4 is the same as degeneracy locus of E1 to E4 when AI are in the linear span of rank on matrices EI. It does not depend on the choice of parameters. And also, we have provided a recipe to compute four tuple of matrices from a given set of six points, which are in quadrilateral configuration. Which are in quadrilateral configuration. Okay, now we are ready to see the sketch of the proof to see the converse statement is true. So converse statement is as follows. If the degeneracy locus consists of six rational points with quadrilateral configuration, then these matrices are in the linear span of four fixed Rankon matrices. Matrices. And here, briefly, we can say that as we have proved that for a generic choice of four tuple of three by three matrices, we have a quadrilateral configuration, six points in the degeneracy locus with quadrilateral configuration. So we have an associated rational map from A36 to P26. This is a Hilbert scheme of zero-dimensional sub-scheme. Then using this identity, we have shown. Using this identity, we have shown this identity, and this reveals that the MAP phi commutes with the group action of GL3, and which ensures the fact that it is sufficient to prove the statement for a fixed degeneracy locus. And that's what we have done. Here, we have used the dimension counting and shown that we have shown that for a fixed X, this dimension of X1 is the same as the dimension of the fiber of this map. Dimension of the fiber of this map, and it is turned out to be 24. Now, next part, I will establish the connection of this degeneracy loci with a symmetric, with a symmetroid. So, if we blow up P2 at these six points, we'll get a singular cubic surface. This is well known. This is a well-known fact. But on the other side, if we'll use Hilbert Boots' theorem, we see that degeneracy locus can be thought of as the set of points defined by vanishing the maximal minus of the matrix L. And then we observe that there is a identity for such an L one can find on matrix U such that this identity is. U such that this identity is true. And observe there on the right-hand side, this is a linear matrix pencil. And if you take the determinant of this, this determines a Kelly cubic symmetroid, a special type of cubic surface. And here is one example of these cases for blowing up. We'll get this determinant of polynomial. And if we'll consider this identity, we get this determinantal polynomial. So they are projectively equivalent. Now, briefly, what is scary symmetroid? So a surface is called symmetroid if it has a determinantal representation of the specific form, which was the case in our example here. Here. This would be of our interest for the next part of the talk, also. So, I need to briefly mention that if we are restricting these matrices to be symmetric, then it's called very real symmetroid. And cubic symmetroid in P3, it's known as Kelly cubic symmetroid. And one can see this paper for an excellent introduction about this Kelly symmetroid. Now we are ready to generalize this idea for m plus one linear operators in Rm. First, we'll mention a few results about five linear operators in R4. So in this case, we observe that degeneracy locus will have a special configuration, deserved configuration, and it's a quinmaccoli substrim of V3, of codimension 2. V3 of codimension 2 of degree 10. Again, we can use Hilbert Boots' theorem and get the connection with symmetroid. And but we cannot prove the reverse converse statement here. So we can only super prove that if we have five developed matrices and which are coming from which are in the linear span of five rank one matrices, then the digital locus will satisfy this condition. But still, This condition. But still, we don't know how to prove the reverse, converse part of this statement. And in general case, it will be m minus three-dimensional subscript of degree m plus one choose two. And degeneracy locus will satisfy this generalized desired configuration. And it has correspondence with MT symmetroid in PM and their nodes. And their nodes. Okay, now for the next of the next half of this talk, I will talk about hyperbolicity cones. So let me briefly recall, because in this conference, many times we have encountered these hyperbolic polynomials. It's a homogeneous polynomial F of degree D in n variable. N variables is called hyperbolic with respect to some degree. With respect to some direction E in Rn, if it satisfies these two conditions, E is not a root of F. When we plug in E, F does not vanish. Second, oh, that's sorry, it should be zero anyway. So, and all roots of this inevitable restriction are real. So I start. So start just let's look at this example, one example and non-example also. So if you start at any point in Rn and look at along the direction E, draw a straight line. So we'll see that this straight line will intersect this hypersurface exactly at two distinct points. So it should be always like the same number of the degree of the polynomial. Of the polynomial. It's a known property, a rigidly convex property, and by which we can analyze that the first polynomial here, it's a hyperbolic polynomial with respect to 0, 0, 1. But on the right-hand side, we have a non-hyperbolic polynomial. Hyperbolicity cones, in my opinion, In my opinion, in recent years, people are interested about hyperbolic polynomials. That could be the reason that it has a geometric feature associated with it, this cone, hyperbolicity cone. So if f is a hyperbolic polynomial with respect to direction E, then hyperbolicity cone defined as the set of points x for which f of x plus Which f of x plus t is not equal to zero, that means t get equal to zero. In other words, we can say that the roots of all this infinite polynomial is negative. And if we consider its closure, that will denote as lambda plus. And a foundational work due to Gerding in 1959, which proves that if F is hyperbolic with respect to E, then this hyperbolic. To E, then this hyperbolicity cone is a convex cone. And here we can see this convex cone, Lorentz cone, for this example, when F is hyperbolic with respect to this direction. Any question? Okay, now another key example comes from the fact. A key example comes from the fact that if the polynomial has a definite determinantal representation, then these polynomials are always hyperbolic polynomials. So what is definite determinantal representation? The polynomial can be written as determinant of some linear matrix pencil, and this pencil should be strictly positive definite along at least one direction. And hyperbolicity cone associated with this definite determination. Associated with this definite determinant is actually a spectrahedron, a feasible feasible set of semi-definite programming. It's the intersection of the cone of PST positive semi-definite matrices with linear subspace. And in special cases, we can see that if we'll consider the polynomial as the product of linear forms, then this spectrahedral cone is this polyhedral cone actually, or hyperpolicity cone is a polyhedral cone. Or hyperbolicity cone is a polyhedral cone. And if you consider the polynomial as the determinant of some symmetric matrix, then this hyperbolicity cone is a positive semi-definite matrix, positive semi-definite cone. Then using this hyperbolicity cone, one can talk about hyperbolic programming. So hyperbolic programming is an optimization problem where Minimization problem where one needs to minimize this linear function subject to this feasible set is an affine slice of hyperbolicity cone, closed hyperbolicity cone. And as a special case of hyperbolic programming, one can see linear programming, second-order cone programming, and semi-definite programming. But it is still not very clear how extent this semi-definite Extent this semi-definite programming could be treated as a special case of hyperbolic programming because we don't know still we are interested to know is hyperbolic programming more general than semi-definite programming. And in this direction, to answer this question positively, another fundamental result by Guller in 1997. In 1997, it shows that if you have a hyperbolic polynomial f with respect to one direction e, then minus of log of a is a self-concordant variable. So it opens the possibility to use interior point interior point method, which is classically used to solve conic programming problem. And this leads to a A major open problem in this area. And already Mario has also mentioned in his talk about generalized Lax conjecture. So here, let me first tell you what is Lax conjecture. It says that every hyperbolic polynomial in three variables has a definite determinantal representation. And in 2002, Helton and Winniker. 2002, Helton and Vinnikov have proved this statement. And later on, one paper which explicitly mentioned using this theorem, they have shown that Lax conjecture is true. So in geometric version, generalized Lax conjecture states that every hyperbolicity cone is a spectrahedron. And in the algebraic setup, In the algebraic setup, it says that if you have a hyperbolic polynomial with respect to E, then there exists a polynomial G such that F times G has a definite determinantal representation. So here I just didn't mention one fact that there are hyperbolic polynomials which may not have determinantal definite determinant representation. And elementary symmetric polynomial could Elementary symmetric polynomial could be one class of such polynomials, which are hyperbolic, not all. Some elementary symmetric polynomials, they are hyperbolic polynomials, but they don't have definite determinant representation. And Brandon has shown one example using Mamos metric. That was the first example. So, here, what we are saying is that, okay, geometric object should be the same. Algebraic object should be the same, but algebraic description could be different. So that is captured in the second condition. So when we are studying the algebraic version or looking at the algebraic version of generalized Lax conjecture, we impose two conditions. If F is hyperbolic with respect to E, there should exist one G such that product has a definite determinant representation. And also the second condition, which is more The second condition, which is more subtle, to preserve the geometric object is the same. So we impose this condition that the hyperbolicity cone of G contains the hyperbolicity cone of F. So when we take the intersection, we get the hyperbolicity cone of F itself. So we can see that definite determinantal representation is more strict class than cone being spectrum. Sorry, cone being. spectra sorry cone being spectrahedral now i'll briefly talk about the notion of derivative relaxation or raining derivative to present my work so we know that if f is a degree d hyperbolic polynomial with respect to e then we also know that the kth redirectional derivative is also a hyperbolic polynomial with A hyperbolic polynomial with respect to E. And one can geometrically actually one can find this nested sequence of hyperbolicity cones. And this can be visualized by the Rolle's theorem or intermediate value theorem that if you have a hyperbolic polynomial, its derivative roots of the derivative polynomial actually interlace the roots of the original polynomial. Polynomial. So, and so on. This containment works. Here is one example of a symmetric elementary polynomial in n variable of degree n. And we fix the direction E towards all one vector. Then the derivative, first derivative of this elementary symmetric polynomial with respect to all one vector is nothing but the elementary symmetric polynomial. symmetric polynomial of n minus in n variable of degree n minus one. And if we'll keep on differentiating, we'll get all the elementary symmetric polynomial. But definitely we need to differentiate in the same direction. Otherwise we'll get something like permanent. Now if we have The polynomial, which is a determinant of a symmetry matrix, then and direction is the identity matrix. Then, the derivative of this polynomial, first derivative with respect to identity direction, is the elementary symmetric polynomial in the entries of the matrix X. And this is nothing but elementary symmetric polynomial in the eigenvalues of X. eigenvalues of x so and it turns out that it is also the sum of n minus in n minus one principal minus of x and when we take the kth derivative k could be 0 to n then we'll get all other elementary symmetric polynomial in the eigenvalues of x so here one may be interested to the fact that To the fact that whether this computation of derivatives needs more, is it expensive or something? So it turns out that because of Gering paper, Guller paper, that one can actually, no, I'm sorry, this is mentioned in Reniger's paper. So via interpolation, I should have mentioned the reference. One can compute the derivatives, and it turns out that. Derivatives. And it turns out that another way to think these derivatives as the coefficient of t to the power k in this characteristic polynomial of matrix X. So here is a brief list of this polynomial result to like provided positive evidence for generalized lacks conjecture and And its first one says that for Sunnal in 2013, it shows that the hyperbolicity cone of n minus one anti-element elementary symmetric polynomial has size n minus one. And then a more generalized fact proved by Brandon in 2014. He shows that this is another thing I have done mistake. This is for any Don't mistake. This is for any kth elementary symmetric polynomial. He has proved that hyperbolicity cone is spectrahedral. And here the size is larger. It's n to the power k minus 1 size. And then in 2016, Amini shown that hyperbolicity cones associated with multivariate matching polynomials, they are spectrahedral. And then Kumar also showed that hyperbolicity cone of Hyperbolicity cone of specialized Hamos polynomial and Sundar Shan has proved that first derivative of symmetric polynomial, first derivative of the PST cone is of size n plus 1 choose to minus 1. And there is another story about projective spectrahedra and spectrahedral shadows. That part I didn't mention, but here is. I didn't mention, but here is a list of literatures in the literature, list of the results, which gives positive evidence towards generalized lacks conjecture. And here, rest of the minutes, I will just briefly mention about the results I was working on. So this is mainly based on Channel spectrahedral representation for n minus one. For n minus one eighth elementary symmetric polynomial in the direction of all one vector. So he first observed that polymetroid, if this is the direction, then polymetroid associated with kth elementary symmetric polynomial is exactly the uniform metroid, UKN. And realization of this uniform metroid can be found by this matrix. And one such matrix, one can choose of this form. So here I of this form so here i scripted i n minus one is the identity matrix in r r to the power n minus one and all one vector one is all one vector and then we can show that spectrahedral representation a hyperbolicity cone of n minus one elementary symmetric polynomial of the determinant of the polynomial which is determinant of the matrix x. Of the matrix X has spectrahedral representation of size n minus one by using this simple observation that elementary symmetric polynomial of x is nothing but nth n minus one th elementary symmetric polynomial of the eigenvalues of x which can be written as determinant of this i matrix with diagonal of lambda x here diagonal of Lambda x here diagonal of x means the matrix with denotes that only the diagonal entries of that matrix and itan transpose. So it turns out that the hyperbolicity cone of determinantal polynomial determinant of a positive symmetry matrix matrix is nothing but this and this size is n minus one not n plus one strictly less than this size which was proved by Sundarshan here and then furthermore we can also provide the spectrohedral representation of The spectra had a representation of n minus 2th derivative or second elementary symmetric polynomial with respect to the identity direction. It has spectrahedral representation of size n plus 1 and here is the explicit representation for this. Actually, E2x does not admit definite determinant representation, but when we find this another polymer. find this another polynomial e1x obviously hyperbolicity cone of e1x contains the hyperbolicity cone of e2x so second criteria is fulfilled and now we can see that if you will take this product this can be written as determinant of this matrix and this has size n plus one and another fact is that we can talk about Is that we can talk about hyperbolicity cones of symmetroids? Like if we consider the matrices Ai are in the linear span of rank one matrices and then construct the determinantal polynomial. For this class of polynomial, we have seen that the hyperbolicity cone is the fast derivative relaxation of some specified polyhedron. That's another interesting fact I wanted to mention in this talk. In this talk. And yeah, so we can conclude that the hyperpolicity cone of real polynomials, which determine some symmetries, are the first derivative cone of some specified polyhedral cone. In the proof, it's a constructive proof and it gives the explicit explanation what would be your polyhedral cone, whose first derivative cone would be the hyper. Derivative cone would be the hyperbolicity cone of the real polynomials, which determines metroids. Okay, so I thank you. Thank you for your attention. And I would also like to thank all this computer algebra system, as we have seen throughout our workshop. They are really very helpful, which was not there a long time back, as Cynthia was mentioning. Okay, thank you. Okay, thank you, thank you so much. Are there any questions? Thank you, thank you. Are there any questions? That's the question. Paprick, could you go back to the this elementary, this representation? Yeah, here, for example, or the one before. I think I'm a little confused. So E2 of X is the elementary, like this, the. Elementary is like this: the derivative with respect to the identity matrix of the determinant of a symbolic symmetric matrix, right? Or enough derivatives of that. X is symmetry matrix and n minus 2th derivative relaxation. In minus 2th, I may just be confused, but the matrices you listed, so you've written. You listed, so you've written them in terms of the eigenvalues. Yes. That's not sort of a determinantal representation of the polynomial, right? The eigenvalues can't be expressed sort of as linear entry, like as linear entries in the entry, like as linear function of the entries of the variable. So what's the I'm sorry, I'm sorry. This part you were saying that we cannot express That we cannot express the matrix in terms of the eye values of the matrix? So, you're giving us, you want to say you're giving a spectrahedral description of this polynomial e sub n minus one, right? So, this should you give a linear matrix in the variables of the polynomial, which are. Variables of the polynomial, which are the entries of the matrix X, not the eigenvalues. Yeah, can you please explain me why we cannot use eigenvalues? Because X is anyway, it's symmetric. So there exist one orthogonal matrix, which will make it diagonal. And so we also have shown that this is equal to. Is equal to this. That's the first of the equation. I agree with the equation. I agree with the equation that you've written down. I don't necessarily agree that that's a spectrahedral, gives a spectrahedral description of the comb of that size. Yeah, that's the thing. Maybe in offline, I can talk to you and you can tell me why it can. Can tell me why it cannot be. Mario, for example, feel free to jump in if I'm missing something. Yeah, yeah. I or poverty, feel free to. Are there other questions? May I ask a question? So I would have a question on the first part of the talk. I would have a question on the first part of the talk. So, you had this general setup where you had a certain number of linear operators and then you had ground space. They were called, I think, R and M. And then your main results, the reality results, they were referring to the case where the number of operators is one larger than the dimension of the space. So, you had, say, four operators in three-dimensional space, say. And do you also have some reality results where the number of operators is? Results where the number of operators is not one larger, or maybe I missed it. That's a question. And the other question is: is this the most relevant case, or is this the case where you can say something? I mean, is this the most relevant case for the case of face retrieval, which you mentioned? Yeah, yeah. That was the motivation, because if it is m plus one, then it was fitting into that result, which was known in the literature of phase retrieval. So 2m minus 2. 2. If we look at this result to m minus 2, so this was the reason which motivated us. So if we have r equal to 4 and m equal to 3, so we have this scenario. So when it's just one extra, and there is no consequence of this result for if it is not m plus one. not m plus one. Okay. And yeah. And later on we found that we could we cannot do this result for m equal to suppose five or four. There was no nice geometric configuration that we studied. We couldn't find anything which could have been useful in the context of haze retrieval. On the other side, if we considered this implant Right, if we consider this m plus one linear operator in Rm, that gives us some connection with symmetroid and all. So that was another motivation for us to look forward in that direction. Okay, so thank you.