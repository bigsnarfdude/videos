Thank you very much for the opportunity to speak. Noah gave me a title to think about, and it's included at the top of the page here. In 25 minutes or so, I'm not sure I can say very much about any of these things. So I've decided to pick a couple of topics which involve my favourite things, which are approximate Bayesian computation and things to do with population genetics. To do with population genetics. That's very helpful. There we go. So I thought I'd start with a quick reminder of Yule just to set the scene. So there is our hero of the day. He is indeed responsible for Yule trees and some of the distributional properties of Yule trees. So I'll say just one minute's worth of that in a second. And I'm going to look at two illustrative examples, which I hope will get the spirit of what my chapter is about. My chapter is about. So, the first of these concerns counts of counts data. So, species in genera, specimens in species, individuals in families, samples in alleles, and so on. Counts of counts data are sort of standard fare for statisticians. And the second bit is too, and what I'm going to try and do is give you a very, very touchy feel for what a new version of approximate Bayesian computation looks like. Bayesian computation looks like. So, this is something I've been working on for over 25 years. And finally, we think we've made a little bit of progress with a different way to think about it. And I'm going to illustrate it with a very toy model, which is a birth-death process with mutation in it. So first, the Yule process. Yule's process is indeed usually described by a tree, and I've drawn one here. Time goes from left to right, and I've labeled the species. And I've labeled the species as they appear. So one lives, well, I've labeled offspring below their parent species. So what you see is this familiar binary splitting. And what you'll notice from this simulation is that the branches get shorter and shorter as you get near the present time. We can say a little more about that, and I'll do that. So I just imagine that Z of T is the number of species at time. Z of t is the number of species at time t. Starting from a single species, the lifetimes or split times are independent with parameter lambda. And so if you write down the chance of observing split times at u1, u2, and so on up to u n minus 1, and n things at time t, you get this pretty little formula, which is a simple thing to derive. But it allows us to write down a couple of properties. Allows us to write down a couple of properties that I'm going to use later on. So, the first of these is that if you integrate over that simplex, you end up with the distribution of zt. This is a standard stochastic process, textbook thing, done usually with differential equations of some sort. The other thing that it tells us from the same representation is that if you care about the tree having a particular number of species by time t, then of course you can simulate. Then, of course, you can simulate that by simulating lots of trees and throwing the ones away that don't have the right number at the end. But the little result above shows that you can actually do this more sensibly. And in fact, the times at which the splits occur, if you know there are n species at time t, are the order statistics of n minus one independent uniforms. And those uniforms can be generated with this very simple formula at the bottom. So this is useful for showing really how the short. Useful for showing really how the short branches near the current time pile up, and I was surprised when I simulated it just how small the branches get at the ends. So that's a very quick introduction to Yule. And now what I want to do is use it to do two things. So the first one is a sampling problem that arose originally in ecology, but has appeared in population genetics and many, many other contexts as well. Contexts as well. And I think the easiest place to start is with this famous paper of R. A. Fisher and others, Corbett and Williams, in 1943. The whole problem relates to the issue about how the number of species and the number of individuals in a random sample are related to each other. And this paper is typical Fischer. It has some remarkable punts. Remarkable punts in it and some remarkable conclusions. And in fact, the theory of this paper, which I'm going to explain a little bit about, has exercised, I don't know how many dozens of authors in the intervening, what, 70-odd years, 80 years, trying to interpret what Fisher says. This, of course, is not the only paper where people have tried to interpret what Fisher says, but I'll try and set the scene a little bit. So Fisher argued in the context of Fisher argued in the context of these butterflies he was looking at that the expected number of species found j times in a sample should have the form like this log series thing. So the expected value of cj is some constant theta times eta to the j over j. And as a consequence of that, if you add up the c's, you get the number of You get the number of species you've found, and that therefore has mean minus theta log one minus theta. And if you weight the C's by adding up C1 plus 2, C2, and so on, you've got the number of things in the sample. And under his little argument, the expected value of that N, the number of observations, is theta eta of 1 minus eta. So the first part of Fisher's paper was about solving these equations s equals S equals the thing on the right and n equals the thing on its right, and therefore getting estimates of theta and eta. So it was typical clever Fisher statistics in some sort of moment estimator. So one of the things though that he introduced in this paper for the case p equals two, two sample, is what is the variance in replica populations of these s's? Populations of these S's, the number of species you see if each of your samples has size little n. So, the general setting, if we have P samples, the sample variance of the S's is this little formula, the usual statistical version of a variance. And the peculiar thing about this, and the reason that everybody went ballistic about Fisher's argument, is that if the samples were independent of each other, then the expected value of Vp should grow like Value of Vp should grow like theta log n. If, on the other hand, they are from continued sampling from one population, the samples are replicates but no longer independent, he claimed that the expected value of this sample variance is theta log two. So these two things have very different implications for the ways in which inference works. So I'm going to try and explain a little bit about where this came from and This came from, and you will come into the story. So, many people have tried to explain this, and the discrete version of this we wrote a little bit about last couple of years ago with by Polly de Silva and us. And that has a summary of the issues that are going on, a list of about 15 papers where this argument has been discussed. I'm going to pose it as a problem about a generative. Pose it as a problem about a generative model for the counts that you see. And the model I use is one which actually appeared in Carlin and McGregor in 1967, but was anticipated by several people like Bart and so on before. So the idea is that you watch a process of arrivals of families. They could be Poisson arrivals or renewal process or whatever. And once those families arrive, the growth of the families follow a given process. Families follow a given process, like a Yule or a birth death or a branching process or a Markov chain, and so on. So, I'm going to look at the case of Poisson arrivals and Yule process growth of the families and just show you what happens if you do this. So, of course, I apologize for using the word families and immigrants and things like that, but I'm too old to change at this point. But yeah, I think I mentioned a few other versions you could use to Few other versions you could use to describe the same phenomenon. So let's see how this works. The whole thing hinges, as you might guess, on marking the points of a Poisson process. And this little diagram shows why Carlin and McGregor's result works. Along the x-axis are the time points where the Poisson process events start and drawn up the y-axis are the The y-axis are the points that arrive in each family that's initiated at the point at the bottom. So, by marking the points in each of these independent Poisson points, we get a marked Poisson process for which we can compute things explicitly. In particular, as Sam and Jim showed, the expected value of Cj of T is theta times this integral. And that's why I put this, sorry, that's why I put the little integral in the Euler. Put the little integral in the Yule formula before because we're going to use it here to work out what this gives us. So if we plug into this result, we end up with an expected value for the number of individuals at time t in families of, sorry, the number of families of size J at time t has this standard, this form that we saw before in Fisher's thing. It's a log series form. En passant. En passant, I'll note one thing: that if you condition on the size of the population at time t, then the joint distribution of the counts is elementary to write down, and it is the very celebrated Ewan sampling formula, which appeared in 1972. Oddly enough, all of the people who worked on this formula for roughly 35 years did not do any conditioning, otherwise, they'd have had this formula immediately. And Warren. And Warren always says that he's very glad they didn't do what they should have done. So, anyway, so what we're going to look at is this Poisson process of CJ of Ts. So, what is Fisher's problem about? It's really all about how species arrive, and now I'm calling them species, as you sample through time. So, on the left-hand picture here are the counts of Are the counts of the species as you see them? So, first species turns up, and then the next species you find in a sample is either something you've seen before, or it's a different one. If it's one you've seen before, you just add one to that type, and if it isn't, you add one to a new type. And you keep going here for 50 samples, and you get a little distribution, which is, in a sense, a realization of the Ewan sampling formula. Where the time comes in. Where the time comes into it is that if you do this for the next 50 points, you're going to see two things, of course. One is lots of points, lots of species which you've already seen. These are the pink ones which turn up in the left-hand box. And then you'll find some species that you've never seen before, the new ones. And what Fisher's problem is really about is trying to understand the interplay between these two things. And actually, most of the work is done by just two of these little pictures. You don't need to do it more. These little pictures, you don't need to do it more generally by symmetry. So, this is an ideal case for doing some sort of marking, and the marking we use is this one. So, S of A and B is the number of families that arrive in 0 to A. So, is 0 to A, this interval here. And we want to see which families arrive in here and have offspring in the brown band between A. In the brown band between A and B. So these are going to be the ones who've appeared in the left hand you've seen, but you then get some extra ones that you've seen before in the next bin in this A to B region. The marking theorem allows you to write down with just a little bit of calculation that SAB is Poisson, as you might have expected, and you can compute its mean. And you can compute its mean quite easily. It's this little formula with theta log of e to the b minus e to the a plus one. So all the work is being done by the marking here, and that's what makes the things much easier than the usual version of this problem. So what do we do when we have two intervals? So let's look at S of A and B and S of C and D and ask what can be said about the covariance between these two. Covariance between these two. So that's trying to say something about the overlaps in my little picture I started with. And in this case, A to B is this interval along the bottom here, and C to D is this interval. And we're going to compute the covariance of the two. And again, a little work. This one's a little bit more messing about, but it's elementary to do. And you get this very nice closed formula for what the covariance looks like. The covariance looks like. That's all you need to be able to evaluate Fisher's problem. So, what we're going to do is look at a series of time points, this time, not just two intervals, but B of them. Start from zero, and the first interval goes from zero to T1, and then from T1 to T2, and so on. And we let Vp be the sample variance of these S's defined in the same way as before, but for each of these abutting intervals. And Vp is our sample variance. And VP is our sample variance that we want. Now, of course, there's a bit of a trick going on here because we don't really know how to match up the discrete version of Fischer with the continuous version of this. So the easiest way to think of that is, it turns out, to look at the case where the time intervals are such that the expected number of individuals you get in each bin are these N1, N2, and so on, which are the counts in the Which are the counts in the story Fisher was telling. And it's elementary to work out what those t's are. ti is the log of the sum of the n's minus log of theta. And if you do that, then you get a very simple, explicit formula for what the covariance looks like, and therefore what the expected value of the variance of the sample variance looks like. And in particular, if all the ends are equal, then this simplifies quite a lot. The term on the right disappears, and you just get. Is, and you just get E of Vp is asymptotic to theta log 2, which is exactly what Fisher meant. Except, I think we can now understand, we hope, a bit better what the mechanism by which he got there might have been. This formula in the middle appears in his paper with, and it comes out of thin air. So there's my first example of things to do with Yule and some immigration. The second one is a thing about. A thing about likelihoods. So, Fisher's paper on likelihoods appeared just before Ewell's paper, actually, in the other one, Philtranse, in 1922. At a talk many years ago, I was giving this talk with David Cox in the audience, and I had this little slide up, and it said, who needs likelihoods? And David stood up and shouted from the back, you do, young man. So, here we go. What we do is to look at so-called ABC methods. So-called ABC methods for intractable likelihoods. So you've got some data and some summary statistics of those data. And the mechanism runs, as Jonathan Pritchard and others showed, you start with theta from a prior. You generate a simulation from the model with that parameter. You compute the statistics of the prior from your simulated data. And then you accept theta as a posterior observation if the distance between the simulated data and Distance between the simulated data and S is small enough, and then repeat, and you get more observations. So, this mechanism has been looked at many, many times, and it resulted in 2019 in a very large book, Handbook of Modern Statistical Methods on ABC. And that was where it sat for a little while until a rather wonderful paper appeared in Bioinformatics by Raynow called ABC Random Forests. And I'm going to go. And I'm going to go very briefly through a new version of that. So I'll begin with their method. First of all, it's extremely useful for dealing with summary statistics. One of the standard problems with ABC is deciding which statistics you want. The beautiful thing about random forests is that they get rid of that decision for you. You put in anything you like, and you will get back something plausibly sensible. It also has. It also has very useful diagnostics, but it unfortunately only does things one marginal parameter at a time, but it does it very nicely. So here's my toy example. We are indeed a cancer lab. My example is far from a cancer problem. It's a very, very simple one. It has no copy number variation or selection or anything in it. If you want to see more of the details of that sort of side of things, Of that sort of side of things, then there's a paper on the archive which describes this model in much more detail. But I'm just going to look here at a birth and death process because that's our topic with split times which are exponential with rate one. So that's not a parameter we're trying to estimate. At the end of the split time, these random, a random cell is chosen that either dies or divides. The in this, sorry, in this In this example, the split probability is uniform on 0.5 to 0.8. Mutations occur at rate theta. Here, they're uniform on 10 to 20. Again, it's just a toy example. The birth process grows until it has a thousand cells, from which we then sample 250 at random, mimicking what we do with the biopsy. The SFS, the site frequency spectrum of the data, is pulled though into bins from zero to point. Bins from 0 to 0.1, 0.1 to 0.2, and so on, up to 0.9 to 1, including 1, actually. So, those are our summary statistics, the number of mutations, and this pooled SFS. And then, what we do to run ABCRF is develop a reference set, which starts from 10,000 things where you simulate from the prior and simulate a data set. So, that was just the ABC thing. And then we have our particular data set, which is. Have our particular data set which is summarized by S, and what we're going to do is use the reference set to build the trees and then predict what happens on the test set. So here's just an example of how this works. This is showing the posterior for the split probability. So the prior is in gray and the posterior in yellow. And although I said the prior was uniform, there is a conditioning going on implicitly. Conditioning going on implicitly here because the process has to get to at least a thousand cells. So that's why this rate increases in the prior. But you can see there's a marked preponderance for larger values of the slip probability in this case. So that's all a bit trivial. We just have to run the thing. But what's more interesting is that the RF method produces an assessment of which of your summary. Of which of your summary statistics are useful. That is, which of them are predicting something about the posterior. And in this example, you see that the number of mutations in the sample is the most important variable. The first frequency in the site frequency spectrum is the next most, and so on. And these happen to come out sort of prettily organized, but that's so be it. So, this, as a general principle, is a wonderful way of trying to figure out, I mean, Wonderful way of trying to figure out. I mean, it's a regression principle, right? Which of your covariates are actually telling you something about the y's, the independent, the dependent variables. So this feature is very useful. So let's look at the mutation rate. So the prior was uniform. And what we learn from the posterior is that it's quite a lot smaller than the original process indicated. Indicated. So we've learned something. And in this case, as you might expect, all of the signal is coming from the number of point mutations. Here's the importance of the number of mutations in the sample. So this is all very well. It's a wonderful method. It works extremely robustly. But one thing that it does not do is deal with multi-dimensional y variables. So what we would like to do, of course, if we're interested in posterior. Course, if we're interested in posterior predictive distributions, is to say something about the joint distribution of these parameters rather than them one at a time. And that's where this beautiful method has just, we just discovered, has been written about by Sevid and others in 1922. And these are 2022, I'm sorry, got caught up with 1922s. These are called distributional random forests. And the idea. Random forests. And the idea is to develop a theory for random forests for multivariate response variables and any number of X variables you like. So these, although the word ABC was not mentioned in either of the papers, it's pretty clear that it can work beautifully for this. And I'll end in just a second with an example. So we have spent some time working on how. Working on how ABCDRF works in practice, and we're trying to write something about it at the moment. But it's the same strategy as ABCRF. You do the same thing, you generate a set of observations, you summarize them for each of the prior values, you feed that in as a reference set, and then you predict what happens. And indeed, what you see is exactly the sort of thing we saw before. So there's nothing special about that, just to show that these two methods. Just to show that these two methods do do something consistent, the two pictures at the top show a QQ plot of the split probability for random forests and distributional random forests. You might argue that's not too good. And this one is the same thing for the mutation rate, and those fit pretty much exactly. The extra bit of information you get is just the joint distribution from which you want to sample to do any number of posterior predictive things. Number of posterior predictive things afterwards. And this method, of course, provides a way of generating exactly that. So here's a 2D plot of what the joint distribution looks like. So there are a number of things in the DRF code that need fixing. One of them is variable importance, which takes three lifetimes to run. And the other thing that is typified by this example. Typified by this example is that if you make the methods more sophisticated by using SMC versions of DRF, you can make these things look exactly the same as each other for actually many less simulations. So it's quite an efficient way to do things. Anyway, with that, I'll end and thank you very much for your attention.