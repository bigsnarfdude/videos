The talk is going to be on synthesis of translation operators and execution plans for the fast multiple method. So the problem is when an integral equation representation of a system of linear PD is given, how do we synthesize low complexity translation operators and have an optimized Have an optimized execution plan. From low complexity, I mean of p to the power d, where p is the order of the expansion and d is the number of dimensions. For example, if you are given the Stokes equation and the Stokes representation with the nine Stokes lit, like how can we have an execution plan that Have an execution plan that converts these nine Stokes' representation to a four harmonic FMMs or like three biharmonic FMMs. And also how to automatically generate an FMM. So the user is only going to give us this Stokes equation and the Stokes representation. And what we're trying to do is automatically convert this to a I convert this to a four harmonic FMM or a three by harmonic FMM and then generate an FMM. So how do we obtain an optimized execution plan? First, we are going to find a scalar PD for each component. For example, in the Stokes case, we are going to look at the velocity component and see that it satisfies. That it satisfies the biharmonic PD, and then we are going to rewrite the Green's functions as a linear combination of derivatives of the base kernel, which means we are going to have 12 biharmonic FMMs going from nine Stokeslets. And then we are going to look at how to automatically reduce the number of FMMs needed, bringing us down to three bihaunic FMMs. So in order to have a low complexity translation operators for the FMM, we are going to use existing machinery like Taylor series or spherical harmonic series. To use those, we need to have a scalar PD. So in the case of the Stokes equation, we can see that the pressure term satisfies the Laplace equation and each of the velocity components satisfies the biharmonic P D. Biharmonic PDE. So, how can we go from a system of PDEs to a scale PDE automatically? First, in order to do that, we are going to write it as a learning system. And this is the, we are taking the Stokes example here. And I'm going to call these columns as W0, W1, W2, and W3. Then to Then to find a scalar PD for u0, we'll have to find a vector v0 so that that cancels out w1, w2, and w3. And then we are left with v0 transverse time w0, which is going to be our scalar PDE for the first velocity component. Note that all of these vectors are in the polynomial ring of partial x1, partial x2, and partial x1. partial x1, partial x2, and partial x3, where partial x1 is the target derivative with respect to the x1 axis. Before going further, we'll need some bits of abstract algebra. An ideal I of a ring R is an additive subgroup of R. And so for example, all polynomials generated by a finite number of polynomials. Generated by a finite number of polynomials is an ideal. And then we need to know what a group basis is. A group basis of an ideal is a subset of I and the ideal generated by the leading monomial of all the elements of G is equal to the ideal generated by the leading monomial of all elements of I. And then we are going to look at CCGs. For example, for In uh, for example, for a vector of x1 to xk, a CCG A1 to a k is something so that a1 times x1 plus a2 times x2 until ak times xk is 0. And then a minimal generating set of an ideal is a generating set such that a subset of that minimal generating set does not generate the ideal. Now, since we have V naught transpose. Since we have V0 transpose W3 equal to zero, in algebra terms, V0 is a CCG of W3 and V0 is also a CCG of W2 and W1. So in order to calculate V0, we are going to calculate the CCG modules of W1, W2, and W3 and intersect them together. So then in the case of Stokes equation, V naught comes out like this. We have a matrix M0 times a Q where. not times a q where q is uh q has any q has elements in this polynomial ring so then uh in order to find a robin basis uh what we are going to do is we are going to uh calculate n naught times w naught and then take the group new basis and then take the minimal degree polynomial so Degree polynomial. So doing this will give us a minimal degree polynomial for the scalar PD. So now we know that the Stokes PDE, each of the components in the Stokes PDE satisfies the biharmonic equation. And then now we're going to rewrite the kernel as a linear combination of derivatives of a base kernel. Of a base kernel, which in the case of Stokes is the biharmonic equation. In the case of the midline solution for half-space elasticity, we have a kernel like this, sigma 1 over x1, sigma 1, sigma 2, and sigma 3 are our strengths, densities. And we have an equation like this. And in this paper, Gimbuda said tell. Paper Gimbada Seattle proved that this kernel can be written as a linear combination of derivatives of this kernel B, which is x3 minus y3 times log r plus x3 minus r. Now the question is, can we do this automatically? Turns out we can. For example, let's take G1 and G2. Let's take G1 and G2 to be two kernels. And then we want to find a base kernel G0 and some derivative operators L and M so that G1 equals L G naught and G2 equals Mg naught. So then we solve this, instead of solving for these two, G1 and G2, L and M and G naught, we are going to solve Mg1 minus L G two and And so, we are going to solve this for derivative operators. So, instead of solving for those, we are going to expand these as coefficients of the derivative operator. So, m1, m2, m3 are going to be coefficients of the derivative operator and g1 and we are going to have derivatives of g1 and g2 up to order p. And then in order to solve this, Solve this. So we know that M1, M2, M3 are all constants. And in order to solve this, what we're going to do is we are going to sample G1 and G2 and their derivatives up to order P and then solve for this using a rank Rochellen form by reducing down to a Rochellen form. And if this And if this matrix of derivatives is not rank deficient, we are going to increase p and restart. By doing that, we are going to, we can find these coefficients mi's and li's. Now, so using that, we can find L and M. So we have G1 and G2 and L and M, but we don't have the base current. But we don't have the base kernel. So, for example, in the Stokesless case, we know that the Stokes are derivatives of the biharmonic kernel, but the Green's function for the biharmonic kernel, that we can't find analytically automatically, but we know what the Green's function is for the, in the case of the Biharm case. But for Taylor, But for Taylor series-based expansions, we find that we don't actually need G0, we only need G1 and G2 and all of their derivative. So we can actually do the FMM by just knowing that G1 and G2 are a linear combination of derivatives of a base column. For example, the Stokes Stokes case. Stokes' case x2 times x3 over r can be written as the negative partial derivative of x2 x3 of r and x1 squared over r can be written as this. So then we have rewritten the nine Stokeset FMMs into 12 biharmonic FMMs, but that's still too expensive. Expensive. And what we are going to look at next is how to reduce the number of FMMs needed. Let's take a simple example where we have a single layer plus a double layer, say a sigma plus T v, and then we have a target derivative at x1 for the single layer and a target derivative of x1 for the double layer. Naively, we might need to do four. We might need to do four FMMs for each of these terms. But for S sigma plus T mu, we can actually do this using one FMM by doing this merging at P2M and P2L stage. So at the P2M and like multiple expansion and local expansion stage, we are going to calculate these separately and then add them together and do the translations and evaluations. Translations and evaluations for one FML. Similarly, for the target derivatives, we can do this at the evaluation of target point stage. So then we calculate a sigma plus d mu. And then at the end, when we're calculating for the target points, we can take the derivatives. So then we actually need only one FMF. A slightly more complicated example from the mid-lane solution for half-space elasticity. We have these three kernels that we have, and we can write these kernels as linear derivatives of this kernel B. Now, Gemuda Settle in this paper showed that these three These three outputs can be written as these expressions. You'll see that this common expression by sigma1 plus by2 sigma2 plus by3 sigma3 depends only on the source derivatives y1, y2 and y3. So then we could calculate these at the source. And then when we are at the last step of calculating the Of calculating the expansion for the target points, we can take the x1 derivative, x2 derivative, and x3 derivative. Now, how do we do this automatically? In order to do that, we write it as a matrix equation. And then we factorize this matrix into these two matrices. And then we change the target derivative. So, target derivatives here into source derivatives. So, then we calculate this right-hand side at the source, and then we evaluate at the target for the left-hand side. Now, how do we, I'll call this a factorization, a rank-revealing factorization. Now, how do we do a rank-revealing factorization of a matrix with polynomials as Matrix with polynomials as an image. We'll have some assumptions. We'll assume that only target and source derivative transformations are there. And we'll also assume that there's only one kernel and it is translation invariant. And we'll assume that there are no composition of layer potentials involved. From rank revealing factorization, what I mean is finding matrices L and R so that M matrices L and R so that M equals L times R. And the elements of L and R are in the ring of polynomials with respect to these variables partial x1, partial x2, and partial x3. And we need the number of rows of R to be minimal. So this is an open question. So we are going to use So, we are going to use a non-optimal method, a heuristic method, to find L and R. In order to do that, what we do is we calculate the CCG module of M and transpose it. And then we see that M transpose is an element of CCG module of CCG of M transpose. So then we equate R to this and we find L as well. And we find L as well. Doing this does not guarantee that the number of rows of R is minimal, but we applied this algorithm and found that the R that we get from this algorithm matches the one found by Cambutas for the MinLini solution. So, a recap on the system of PDEs. We convert the system to a scalar PDE for each component. A scalar PDE for each component, and then we rewrite the kernels as a linear combination of derivatives of a base kernel, and then we reduce the number of FMMs using the rank rebuilding factorization. Now, for example, if you look at the Stokes and Shresset case, where there are nine Stokeslets and 27 Shreslets, there's 36 FMMs there. 36 FMMs there, and using our automated algorithm, we can reduce it down to three biharmonic equations. Whereas, in the literature, we see that it can be reduced down to four harmonic FMMs. Note that a biharmonic FMM costs roughly 50% more than a harmonic FMM. In the case of Kelvin solution for elasticity, we can get it down to 3 by how many kg. We can get it down to three biharmonic equations. And in the literature, we see that that can also be reduced down to four harmonic fmms. In the case of the mindlin solution for elasticity, it's three biharmonics plus one harmonic using our automated algorithm. And in the literature, that's eight harmonic effects. Now, so now that we have Now that we have reduced it down to three biharmonic FMMs, we need to do those three biharmonic fmms faster. So, given a scalar PDE, we need to synthesize an FMM so that it has low complexity translation operators. And there's one more condition that the expansion can be done without knowing the base kernel explicitly. For example, even if you don't know the base kernel, You don't know the base kernel of the biharmonic PD, we need to be able to do the stokes operation. Now, one way of doing that would be to use Taylor series-based expansions. Then we are going to look at translation invariant, Taylor series, translation invariant potential. And in the Taylor series, in the local expansion, we are going to have Local expansion, we are going to have derivatives as the coefficients and these monomials as the basis functions. And in multiple expansions, we are going to have the derivatives as the basis function and the monomials as the coefficients. But the issue about data series is that it's too costly because it's p to the power six for translations and it takes a p-cubed storage. But we can improve this using the PDE. So let's assume we are solving this for Helmholtz equation. So then we have that the potential g satisfies gxx plus gyy plus kappa square g equals zero. Then we have an expression for the multiple expansion like this where we Like this, where we have coefficients and derivatives of the kernel G. So using this PDE, we can replace gyy with minus gxx minus capascar g. So then we arrive at an equation so that one of the coefficients of this is zero. So we go from three coefficients in the series down to two coefficients. Series down to two coefficients. We'll call this a compressed series. Expanding this further, like we can differentiate the Helmholtz equations multiple times and get more relations. And from those, we see that if we have a triangle of derivatives, all these derivatives represented by the red dots can be represented by the derivatives. By the derivatives in the green dots. So in the biharmonic case, also, we can do the same. In 2D, this reduces the number of coefficients from p squared down to p. And in 3D, it reduces the number of coefficients from p cubed down to p squared. We'll also use some more optimizations to get faster translation. To get faster translations, we'll use recurrence relations for the Wien's functions to get amortized constant time to calculate a derivative. And we'll also use a fast Fourier transform to calculate the multipole to local translation because the matrix that we get is a recursive block double matrix. And we see that the size of the matrix is going to be of p squared log p for. P squared log P for 3D for elliptic PDs and P cubed log P for other PDs. And for local translations and multiple translations, we'll use the compressed data series. And naively, the cost is going to be P to the power 5, but we'll use temporaries and we can get the cost down to P key. So the time complication. So the time complexities look like this. So the third third row shows the time complexities for Laplace for compressed Taylor series. So we see that for local expansions and multiple evaluations, we get P squared, which is going to be the same as a spherical harmonic series. And then for P2Ms, L2Ps, M2Ms, and L2Ls, they are going to be slightly slower than the spherical harmonic series. Spherical harmonic series, but for M2L, which is one of the largest costs of the FMM, we are going to have P squared log P, which is equal to the spherical harmonic series-based expansion with FFT. Note that all the operations in the compressed TL series is exact compared to the uncompressed TL series except for M2M, where the M2M. Where the M to M has some error, but the error is equal, the order of the error is equal to the Taylor series truncation error. So we are not doing any worse than the uncompressed Taylor series. And there are some errors coming in from doing an FFT with inputs of varying magnitudes. This works for all constant coefficients elitive. Constant coefficients elitive PDEs with translation invariant kernels. And we only need the PDE and the Green's function and a way to calculate the derivatives. And we can synthesize a fast FMM for any curve. These are some of the floating point operation comps for operations P to M. For operations P2M, P2L, M2M, and M2L. And the dashed lines show the expected operation counts, and you'll see that they match closely to the practical values match closely to the theoretical bounds. So to recap the advantages of data series for expansions for Data series for expansions for system of PDEs. We see that we can generate low complexity translation operators for any kernel. And we don't need to know the base kernel. We only need the derivative operators for the kernels. And the PED is only used for compression and does not change the FMM. For example, even though the Stokelet FMM is converted to a biharmonic FMM, Converted to a piharmonic FMM. The piharmonic PDE is only used for compression. It's still the original Taylor series, but only with the compression using the biharmonic relation. So it's not a bioharmonic FMM. It's still an F stroke FMM. Just we use the bioharmonic FMM for the compression part. And the relative error with uncompressed L C D. Error with uncompressed relative error compared to the uncompressed state series is either zero or has the same order as the truncation error. So in summary, we have a kernel generic method for electric constant coefficient in a system of PDEs. We automatically reduce the number of FMM calls needed. And to do all this, we only need the PDE and the Grease function. The PDE and the Green's function for the PDE. That's it. Thanks very much, Isuru. Do we have any questions from either Zoom or Oaxaca? I have one question. I have one question if no one else has anything to add. So, for some PDEs, say, like Maxwell's equations, you can write down vector translation operators that work on vector spherical harmonics. And I don't have the calculation in my head, but is it always obvious that it's the correct thing to do to separate it into the scalar components and run scalar? It into the scalar components and run scalar FMMs versus having the unknowns be vector unknowns like current or, you know, you could do the analogous thing for Stokes, but build the FMM and optimize the FMM on top of vector translation operators as opposed to scalar ones. I'm not sure I got the question. So the question is, so in Maxwell's equations, the solution to the The solution to the PDE are vector quantities, you know, the electric field and the magnetic field. Right. And you can write down representations for those vector quantities. Right. Like the Helmholtz equation. Right. Yeah, but they're vector quantities now. Right. But so there's an outgoing expansion called the me expansion, which The Mie expansion, which is the curl of a Helmholtz potential plus something else, but the coefficients in there are coefficients in a partial wave expansion for Helmholtz. And then you can build translation operators for those coefficients, but those are coefficients in a vector representation, as opposed to what you're proposing. I think, if I have the gist correct, is separating things into their scalar components, like their X, Y, and Z components of a vector. They're X, Y, and Z components of a vector quantity, and then running scalar FMMs. Right. So the electrical engineers usually build FMMs for Maxwell's equations on top of the vector translation operators. And I don't have to, it's not obvious to me which one is faster if you really optimize things. Right. So I looked at the Maxwell equations as well. Maxwell equations as well. So from what I remember, like you can reduce it down to like three six. So there are like six outputs, right? So the six Helmos equation, Helmos FMMs, because each of the in the vector components in the in the vector In the vector components, each of those satisfies the Hermul's equation separately, independently. So we can do the scalar PDE version for each of those components. Right. And that, but that's that six, that's the naive separation, or that's your compressed separation? That's the thing that. I think that was a compressed one. I can't remember exactly if that was the optimal one or if there was a smaller number of FMMs needed. I'm sorry, I can't remember about the max amount. Okay. By the way, how does it work for oscillatory greens functions? Suppose you have. functions suppose you have a group for which they are trying to represent a represent that contains i don't know 100 wavelengths how what is going to happen then right uh so uh we uh we really don't have so we are using uh like datases and we are using some symbolics uh to get uh To get to generate these FMMs by computer editors, and we can't do non-ocelier kernels at the moment. Maybe Andres knows more about it. You will be representing, it appears you have taken the function that is basically sine 100x and calculating it and evolving. Calculating it and evaluating it at x equals one using a tether series. Right. Well, that's a little bit of a problem, isn't it? Vladimir, Andreas here. Hi. I just want to say, highly oscillatory stuff is completely out of scope for the moment. I think that's fair to say. Okay. And to perhaps add a little to what Isuru said on the previous question. Said on the previous question. The big jump in terms of complexity is to go from not having a PDE upon with which to optimize your FMM to having one. And then that's scalar or vector, who knows? And then heuristically, perhaps in terms of optimizing the translation operators, the more data you refer to inside of each little translation operator, the more you suffer. So in that sense, Greater, the more you suffer. So, in that sense, heuristically, I would say going for a scalar is probably justifiable, but either of those I would suspect is a constant factor away from each other. Okay, thanks. Any other questions? Otherwise, which on? I think it's already on. It's already on. Yeah. I mean, just to follow up on that, I mean, this algebraic technique for searching for relations and finding the optimal way to evaluate them seems very useful. And maybe you could even apply it to diagonal translation forms. I mean, there's no reason you couldn't go in the high frequency direction as well with this. I mean, it may not be, there's only, I mean, there are three high frequency equations. Three high-frequency equations, right? There's Helmholtz, Laplace, and Helmholtz, Maxwell, and Elastodynamics, right? So maybe if it would help you do that, then. Well, I'd say, Alex, there's two. Elastodynamics is Helmholtz plus Maxwell. Okay. Well, there might still be some kind of efficiency gains at the style of cutting down the number of scalar FMMs or something. Scalar FMMs or something. So, anyway, it's, you know, it could, we could make Vladimir happy eventually with this approach. And also, go ahead, Isuru. Yeah, so the parts about reducing the number of FMMs, these are all independent of the FMM, scale FMM that we are going to use. So it doesn't matter if it's the Taylor series or cerical harmonics-based FMMs. Harmonics-based FMMs. Like if you know the base kernel, for example, we know the base kernel for Helmholtz. So then we could use the machinery to reduce the number of FMMs and then use cervical harmonic series for oscillatory kernels as well. I was going to I was going to say exactly the same thing as what Izuru said. There's sort of really two parts here, and the break it down from a system to a scalar thing that's completely generic. Great. All right. Thanks, guys. So I think we'll end for today. And then the rest of everything today is confined to Oaxaca. And then we'll pick up in the same format tomorrow. The same format tomorrow for, I believe, fast track solvers, correct? Correct, fast track solvers. And Gunnar will be your leader tomorrow for that. I heard for the group photo. Thanks, Mark. Take it easy. Should we thank our speakers? Hi, guys. So, in order to get the group photo for the To get the group photo for the participants who are in Zoom, and do you want to be right now, or so? Are you going to take a snapshot? Just if you want to appear in the group photo, please turn your camera on. Let's wait some seconds. Yeah, only for the room, for the Zoom group. Yeah, that sounds good. Everyone, look at your camera, not your. Everyone look at your camera, not your screen, too. Okay, I see three, six. Okay, let's wait. Okay, you've got to give them a countdown. Okay, uh, count of three. One, two. Please smile. Please smile. Another one. One, two. And let's keep on fire. That's all. Thank you. All right, take care. 