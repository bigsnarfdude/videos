Is by Jordan Richards and oh, you have one. Okay, so we're now back to regression. So we'll be talking about high-dimensional in the regression sense, multi-regression, or spatial type of extreme like matters. Yeah, so I'm not talking about conditional extremes, which I feel is a bit of a missed opportunity because I could just write in like two slides and we'll be done in five minutes. Instead, I'm talking about deep extreme quantile regression using partially interoperable neural networks. Partially interpretable neural networks and specific applications from stream wildfires. So, just to avoid any confusion, this is wholly different work to what I talked about on Tuesday. In that setting, we had a neural network and a statistical model. We're using the networks to inference the model, but the two are separate. In this case, we have a neural network embedded within a statistical model. And for those of you that are asking why I have to make this distinction and why I have two slots on the schedule rather than the one that everybody else. The scale found than the one that everybody else has. This is joint work with Raphael, and so there's been a bit of nepotism involved in the it's not just joint work with Raphael though, and we've actually done three continental scale studies with various co-authors. The first was to wildfires in the contiguous United States, and this is actually where me and Raphael proposed the methodology. I know a lot of people in the room are familiar with this study, it's actually used data that. Let's actually use data that motivated the EVA data challenge with this data provided by Thomas. A more recent study we did was looking at Mediterranean European wildfires. This was joint work with Emmanuella and Jakob from the Helmholtz Center for Environmental Research in Leipzig in Germany. And I will be mostly, I'll be talking in general terms about the three studies, but I'll be focusing mainly on that one. Of course, we're in Europe, so it seems. In Europe, so it seems. But we also have an ongoing bit of research. This is a project spearheaded by Daniela Cisneros, who is a PhD student at CALS. We're actually looking at Australian wildfires and wildfires in Torpena as well. This is also joint work with Ashok Kahal and Luigi Lombardo at the University of Twente in Sierra in the Netherlands. And I'll mention that very briefly as well. Okay, there are three different studies, different spatial. Okay, there are three different studies, different spatial domains, but actually the same deep learning regression framework is used, albeit with different extreme value response models and neural networks. But I'll get into that. Okay, so the motivation for why we're interested in wildfires, I suppose I don't really need to motivate the interesting wildfires in this room. They're devastating and destructive and they cause damage all over the world. Last year, actually, in Europe, we saw. Actually, in Europe, we saw the second largest total burnt area on record, and that was even by mid-August, which is only about halfway through the fire season. Globally, you can say that wildfires are responsible for hundreds, if not thousands, of deaths, billions of pounds worth of damage. And it's generally accepted that climate change is going to exasperate the frequency, severity, and intensity of wildfires. Wildfires, you have this vicious feedback loop where wildfires are caused by climate change, but they also contribute to climate change as well. So, in 2021, it was estimated that wildfires globally contributed about two gigatons of carbon emissions into the atmosphere. So, clearly, there's a real need to go somewhere to mitigating the risk of particularly extreme wildfires. So, we wanted to do that by building a model that could jointly identify drivers for extreme wildfires and their occurrence. And their occurrence, but they also could be used to produce high-quality hazard maps so you could identify high-risk areas and countries. And so, the general framework then, what we're going to be interested in doing, is just estimate high quantiles of burn area. We're going to use these as some sort of proxy for wildfire or wildfire risk. So, we're going to perform extreme quantile regression for the aggregated burn area for a spatio-temporal grid box. A temporal grid box. Of course, this is an extreme workshop, so the quantiles when they're estimating and could be larger than those previously observed in the data. So non-parametric methods are likely to perform very, very badly. And instead, we're going to use parametric regression and use our astrophysically justified extreme value models that we know from extreme value theory. So, of course, we've got some classics. You've heard a bit about GED and GPD distributions at this workshop. There's some extensions, for example, the extended GPD distribution. Example, the extended GPD distribution. There's also a point-process approach, which I am sure many of you are familiar with, that sort of encapsulates both this GVD and GPD. What's really important here is we just need a parametric model that's determined by some parameter set feed. And just to illustrate the sort of data I'm working with, then this aggregated burn area, here are some observations of the log one plus burn area for August 2001 on the left and October 2017 on the right. I'm focusing on the The right. I'm focusing on these two particular mumps. On the left-hand side, this was the largest total area of the entire domain within the observation record. And the right-hand side, this had the actual largest individual value, which I believe was in the north of Portugal. Okay, so we want to do parametric extreme quantile regression. And so the first question is: what have other people done? What have other people done? Well, usually what they do is they represent our parameter set beta as either a linear or additive function of a set of predictors. For example, we'll denote here X in Rb. Of course, linear models, they do what they say on the tin. They can't capture non-linear structure and data. So if you have quite complex problems, they're not going to work very well. And we sort of expect, you know, the process that drives wildfire ignition occurrence speed. That drives wildfire ignition incurs to be quite complex. What we could do instead is use additive regression models. These can capture non-linear structuring data, don't scale very well to high dimensions. Actually, in this particular study, we have a relatively high-dimensional predictor set, where we have 38 predictors. So, we're going to use deep learning instead. Neural networks, of course, we know capture complex structure and data, scale very well to high dimensions, and facilitate very high predictive accuracy. High predictive accuracy. So, just to be fully clear, what we're doing is taking our linear, generalized linear model and replacing that linear function with a neural network. We're not the first people to do this, so I don't normally give this live, but I thought I'd give a brief sort of overview of what I feel are like the milestones in terms of the deep quantile regression literature. So, I think the first people to look at this were sort of Julie Coro and co-authors in the late 250s. Co-authors in the late 2000s who fit mixture GPD models. So these were parametric bulk distributions that had upper GPD tails. And then some work by Alex Cannon fitting GEV distributions using neural networks. And this was all implemented in this GEV CDN package. This was a long time ago, in real terms, but also very much so in deep learning terms. There's been substantial amounts of advancements in. Substantial amount of advancements in deep learning since the early 2000s. These two method optologies, and until very recently, all deep quantile regression methods have used sort of standard vanilla neural networks, densely connected multi-layered perceptrons. And these aren't very efficient. And I feel like it's only recently where people have started adopting more complicated neural networks that have efficient architectures in order to do deep quantum regression. In order to do deep quantum regression. So, for example, papers by Sebastian and Olivia Pash using LSTMs, these are long, short-term memory recurrent neural networks to fit GPDs. If we just go slightly across the border into France, there's a couple of institutes looking at using extended GPDs to do forecasting and processing for precipitation. They utilize UNIT, which is convolution-based neural network. But as far as I'm concerned, I feel like this is sort of. I feel like this is the general, this is just everybody who's doing deep quantum at the moment, but I'm sure somebody can correct me if that's wrong. So, where does our work fit into this? Well, in our US paper, what we want to do is propose a sort of general framework for deep quantile regression where you don't have to have a specific response distribution or a specific neural network that you're going to use to fit the model. And to complement that paper, we developed. And to complement that paper, we developed this R package, PINEV, and within that package, you have all of these response distribution models implemented. You can use all these, and we have a couple of different neural networks that you can play with. We're hoping to sort of introduce some new ones in the future as well, but I'll talk a bit about that a bit later on. And as I mentioned at the start, we had three different studies, and they all have different response distributions that you can play with. The first one used a point-process approach that was motivated by blended. Was motivated upon blended generalized extreme value distribution. I won't get into the details of that. The approach, the study I'll be talking about now, then this European study, this just uses old Faithful, the generalized period distribution, while our current work for Australia uses the extended generalized period distribution instead. Okay, so I've talked about the quantile regression for extreme values. Of course, that's not the end of the story. I said that we wanted to go some way to identify the drivers. To go some way to identify the drivers of wildfires, but of course, if we're using neural networks, we can't do that as standard. Neural networks are black box, it's very difficult and almost impossible to interpret their output. So, if you want to understand the drivers of risk, you can't use a neural network. So, in our first paper, we propose the partially interpretable neural network framework, which is a really, really simple concept. All you do is you split up your predictor set X. You have some predictors that you're going to model there. Predictors that you're going to model their effect using part readily interpretable functions and the rest of the predicted speed and neural network. And I'll give you the mathematical formulation for that in just a second. And a common question I'm asked is: how do you predict, you choose these interpreted predictors, these are chosen a priori based on practitioner interest or known structure in your model you want to incorporate. So, mathematically, then we are partially interpretable neural network framework. Partially interpretable neural network framework. We have our response distribution. A response distribution is this parametric statistical distribution f. It's determined by some parameter set theta. For each component of our parameter set, we split our predictor set x into two complementary subsets. We have xi, I've never been able to pronounce that correctly. Xi, which is blue, and this xn and then we represent. And then we represent the parameter as the additive sum of an interpreter, the blue function and the red function, with the range of the parameter determined by age and link function. The blue function can be linear, it can be additive, it just needs to be something that you can estimate and easily interpret. The red function is just a neural network. Okay, so we look at that. This is sort of a This is sort of a compromise between neural networks and classical regression methods, additive regression methods, but actually on the boundary of the parameter or feasible model space of those two classes. So, for example, if this was just a constant, we would just have a generalized linear model or a generalized additive model. I think Simon's talked about EVCAM, and this would be on land with premises space. If this was just a constant or zero everywhere, then we would just have a neural network. A couple of points then on this neural network. In terms of doing inference, all the parameters in the model and the neural network and the interpretable functions are all estimated simultaneously by using the R interface to Kerris. We minimize a penalized version of the negative log likelihood. You can have some smoothing penalties in there for the displients. There aren't any real fundamental restrictions on the type of neural network you can use. You should pick one depending on the mode. You should pick one depending on the modality of your data and the type of structure you want to exploit. So, throughout the three different studies, we've considered sort of standard neural networks, convolutional neural networks, procure neural networks, and most recently, graphical convolutional neural networks, which I don't think we've heard enough about actually this week. And unsurprisingly, both classical regression methods are easily outperformed by this method using the simple neural network and this pin. Simple neural network and this PIN framework provides a nice balance of interpretability and predictability. And we sort of illustrate that in our first bit of work. I won't talk about the entirety of this table. The important parts are this left-hand column. This just describes the type of model we're fitting. Of course, we have a fully linear model. This is just a generalized linear model. And these are partially interpretable neural networks here with different partially interpretable components. The best fitting model, then, is defined by this CRPS score in the right hand column. RPS score in the right-hand column, of course, the best-fitting model is a neural network. The second best-fitting model, though, is a partially interpretable neural network. And this partially interpretable neural network does substantially better in terms of its predictive capabilities than a linear model or analysis model. Can you just mention the size of the training and testing data sets in this? I not currently, I'll be honest with you. I can come back to you when. I'll be honest with you. I can come back to you and we can talk about this afterwards. The details for that are in the paper. This is from the US study, which I'm not going to talk about as much. It's of a similar size to the study that will come in, be described in the next slide. The actual application then to Mediterranean European wildfires, we're modelling monthly burn area comes from the fire CCI data generated by modus reflectance data, 20 years' worth of observations. 20 years worth of observations, June to November inclusive. There are roughly 10,000 spatial locations. And of all the space-time locations, there's 1.2 million feasible locations where a fire could potentially happen. Of course, in this grid, a fire isn't going to happen only with the meta-training scene. And all the data sets are about 100,000 non-zero values going into the models. That's the relative size of the training. The predictors then, we grab some. The predictors, then we grab some predictors from various sources. Land cover maps come from Copernicus. These describe the proportion of a spatio-temporal crystal that consists of one of 21 different types of land cover, basically water, urban areas. On the right-hand side here is grassland proportion for August 2001. Meteorological variables come from ERA5, temperature, wind speed components, and surface pressure, those sort of things. And then some orographical predictors, mean standard deviation of altitude. Standard deviation of altitude and long lap borders, we just throw into the neural network. In terms of sort of like a model overview, then we're going to model the occurrence and extreme spread of wildfires separately. We use a logistic regression model, just a simple logistic regression framework for the occurrence probability. It's a logistic regression pin, though. So it's still the same sort of framework I showed on the previous slides, but the statistical distribution is just a beneficial distribution. For extreme wildfire spread, For extreme wildfire spread, we're modelling the exceedance of burn area above some threshold U as a GPD distribution, where this exceedance threshold U is some quantile that we estimate using non-parametric quantile, deep quantile methods. This scale parameter is dependent on covariance. It's proportional to the area of the spatial grid boss, particularly the area that it can actually burn. We know the actual area of the grid boss is composed of particular length of. Composed of particular angular types that are clammable. The shape parameter is fixed over space and time, and we'll have arguments about that afterwards, I imagine, but that is a model. The empirical distribution is used for the burnt area below the threshold. We're only interesting streams, so we only have a parametric model for the proteins. We reparameterize the GPD distribution in this particular way, and in doing so, we make sure that the scale parameter is actually independent of the threshold, and so it's much easier. And so it's much easier to interpret the scale parameter as something related to wildfire severity. The shape parameter estimate, again, I mentioned splits over space and time, is around 0.3, which sounds quite heavy-tailed, but actually, relative to the other two studies of Australian wildflies and US wildflies, it's actually quite light-tailed. So, one of the interesting things we found in these studies is that burnt areas are extremely heavy-tailed, and in fact, the assumption of finite variance is very Finite variance is very unlikely to be estimated with Bernarian data. Okay, the interpretable part then, we're going to interpret the effect of vapor pressure deficit, two meter air temperature, and a three-month standardised precipitation index on P0. P0 is the probability that wildfire occurs, and sigma, which is the measure of severity of wildfires. For the neural network, then we use a five-layered convolutional neural network for Layer convolutional neural network for the probabilistic wildfire occurrence in the GPD model. We're actually just using a density connected neural network. It's quite simple, it only has 720 parameters, so relatively simple in the context of deep learning. Of course, we're using neural networks, we have a fairly robust training, tested, and validation setup to reduce overfitting and perform some sort of model and architecture comparison. We've heard a bit about Bayesian neural networks at the start of the week, but we're just going to do Start of the week, but we're just going to do a parameter and society only using a standard approach. So we just use a stationary bootstrap, and all the results I'm talking about are averages of all the model fits over those 250 bootstrap samples. Okay, to visualize the drivers of wildfire occurrence, then what we've done here is taken every spline estimate from each of our bootstrap samples, centered them in such a way that they all have the same value of this central point here. Value at this central point here, and then I've plotted functional box plots. And these are just the analog of box plots for point data to functions. The important part here is the magenta region denotes the 50% central region for all of our codes. We have then spline estimates for the effects of vapor pressure deficit, air temperature, and the air standardized precipitation index on what is the log odds of the probability of wildfire occurrence. So, to interpret this then as vapor pressure. So, to interpret this then, as vapor pressure deficit increases from zero to roughly, I don't know, about 1500, the log odds of wildfires occurring, log odds of wildfire occurrence increases, but decreases thereafter. With things like air temperature and this SPI index, we get what we expect, air temperature. As the temperature increases, the chance of the wildfire increases. And as we go from a regime where you have more rainfall, the chance of the wildfire decreases. Decrease. We've also got similar sort of results for Wi-Fi severity. So, this is their impact on the logo, sorry, the log of the sigma parameter. And I'll be honest, when you look at these, you'll notice that the zero line goes through this magenta region for the most part and sort of suggests that actually these results are insignificant. So, actually, once a wildfire has occurred, then increases in vapor pressure deficit and air temperature and its SPI. And air temperature and its SBI doesn't really have that much of an effect on the spread itself, conditional on a wildfire occurrence. We can do some sort of, I've used the term risk assessment here, I keep getting told off of this, these are hazard maps. So this on the left-hand side is log of the one plus burnt area for August 2001. And here we have this really, really lovely animation of extreme quantiles for the same month. These are estimated extreme quantiles going from, I believe, it's the 30%. Quantile is going from, I believe it's the 30% quantile to the 99.99 percentile. And we can use these if we look at particular areas that have higher magnitudes of these true quantiles, these are areas that clear more risk of mild fires in the domain. So we also did a bit of, maybe in the interest of time, I will talk about this actually, I've changed my mind. We did a bit of a look at the impacts. And we did a bit of a look at the impacts of long-term climate trends on the distribution of wildfires over Europe. So, we asked the question: what would the distribution of wildfires in August 2001 have looked like if we used the predicted air temperature values for 2020? So, we did a very, very simple study where we just estimated linear trends at each spatial location. There's a linear trend in the temperature in August specifically. Specifically, and that is what is pictured here on the slide. This would just be the regression coefficient in a linear model. And when you use these to get predicted air temperatures for 20 years in the future, we feed these into our model and we look at the changes in the probability of wildfire occurrence and severity specifically. So, here is what happens if we look at the impacts of climate change on probability of wildfire occurrence. Anywhere where there is a red colour, there is an increase. Colour is there is an increased chance due to an increased chance of wildfire occurrence due to climate change. The right-hand side is just the 95% quantile of this burnt area. The red points are where we have increases. The blue points, which there are none of, as you can see, is where we have decreases. So it's pretty worrying looking trends for the entirety of Europe. I'm going to say. Just to show some results for the essentially in the US, here is again, this is. Here is again, this is a bit of, this is extreme estimated quantiles for the US. The left-hand side is July 2007. This is actually the log of the one plus square root of burn area. Burn area for America is extremely heavy tails, so we square rooted it just to make it manageable. I think that's all I need to say on the US, let's concentrate on Europe instead. We have some very preliminary results for Australia. I'm just going to just show you what these things look like. Going to just show you what these things look like. We constructed a new data set actually for this. So rather than taking burnt area from a well-established data set, we constructed a new data set using satellite and manned aircraft images. And what we did was we calculated burnt area over regular polygons that are approximately equal population densities. And it looks sort of like this: sorry, this is the 99. Sorry, this is the 99%. This is actually from a model fit. This is the estimated 99% of the square root of burn area for January 2003. Part of this study is what we want to do is actually not just look at risk assessment for Australia as a whole, we want to do sort of local risk assessment. So we've also been looking at risk assessments in Perth and Sydney. Again, these are preliminary tools, so I won't talk too much about these. So a summary then, we propose a very flexible Then we propose a very flexible framework for fitting, particularly extreme value regression models, but actually statistical regression models in general. This F distribution I talked about doesn't have to be an extreme value distribution, but of course that is the interest in our applications. It combines the high predictive accuracy of neural networks and the interpretability of classical regression methods. The models fit extremely well to wildfire data, and we highlight that in both of the papers. And we highlight that in both of the papers. We're able to reveal new insights into the climatic drivers of extreme wildfires, and also we did a bit of a climate change impact study for European wildfires. And alongside all of this, I have this R package Pin EV. I just wanted to sort of illustrate how easy it is to fit models. You don't need to know Keras, you don't need to know TensorFlow Python. This particular line here defines what covariates go into your linear algorithm. Go into your linear additive part of your model and your neural network, and then this single line of code will build and train your neural network from scratch for you. And that's it. Here is a number of references. Okay. Daniela's paper is in draft, but will be up hopefully within the next couple of months. We have the R package, that's a podcast. And then this is the two papers that are an archive. Thank you very much. Happy to take any questions. Happy to take any questions.