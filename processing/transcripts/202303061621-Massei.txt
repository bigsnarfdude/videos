There's like like it's not necessarily backing uh there's a lot of momentum it's not clear it needs to match that's a restaurant we don't have like just remember like I know yeah I wasn't sure just that it's like some specific So, good afternoon, everyone. So, in this talk, I would like to tell you more about this joint project with Lernando Robert from the University of Pisa, based also on previous work with Daniel. And so, we got interested into providing a first algorithm, something that has a quasi-optimal complexity for this kind of structure linear system. So, we have the coefficient matrix that is a chronicle sum of Chronecker sum of the terms, so each term is of the form a certain positive definite matrix multiplied in a chronicle sense by a bunch of identities and equal to a certain right-hand side B which is potentially unstructured. And so that if you shape the problem, you obtain an equivalent tensor equation, tensor synvester equation that looks like that. So you have the solution which is an unknown dense tensors x. Unknown dense tensors x multiplied in the mode i with the matrix I, you take the sum over i, and then at the right-hand side you have this another dense tense. And so if you as the number of degrees of freedom grows, so if for example it exceeds one million, then it's hard to solve such a problem on standard computer. And usually you have to assume some further And usually, you have to assume some further structure. So, for instance, that the right-hand side is a Lo-Rank tensor, and so the solution also inherits the structure, and so you can compute a solution directly in a Lo-Rank tensor format. But this is not the case here. So, in this talk, we always assume that the right-hand side and the solution are dense, and we just assume some further structure on this coefficient aj. So, we assume that, in particular, they are directly semi-separable. They are heretically semi-separable matrices. So, in order to explain the realism, let me start with the matrix case because, of course, it's way simpler, and then I will comment on the steps you have to modify to apply to the truly tensor case. So, in the matrix case, you have a Sylvester equation, and let me recall two and one algorithms for solving this problem, which would be the building blocks of our recursive procedure. Recursive procedure. So, in the general setting, you can always diagonalize the coefficients, since they are positive definite, so you can diagonalize without danger of stability. And once they are diagonal, you can solve the matrix equation with a direct formula. And this amounts to a cubic cost in the size. Another important case instead is when the right-hand side is low-rank, because as said in the tensor case, Because, as said in the tensor case, also in matrix case, then also the solution is no matter. And then you can solve this problem with the parallel projection method or with the alternating direction implicit method. And this basically has a complexity that is proportional or log proportional to solving shifted linear system with these matrices A1 and A2. So, in particular, it is worth to spend a few more words on the structure. We assume this coefficient A1 and A2. Assume this coefficient A1 and A2. So, since we often think about this problem as coming from the discretization of Laplace-like differential operator, we often have a banded structure or more generally if you have a non-local operator like a fractional derivative, something that has a low rank of diagonal blocks. So, in particular, the key property for us is that these matrices can be split into the block uh square diagonal blocks. Diagonal blocks and low-rank anti-diagonal blocks, and these diagonal blocks retain the structure so that you can apply the splitting recursively. So, for instance, in the banded case, this would be banded as well, and you can do it again. So, if you have this nice structure, then you can rely on a smart matrix format, for instance, theoretical matrix, to store and operate with this structure. And in particular, so I will not go into details of that. So, I will not go into details of that, but the important thing to keep in mind is that we can store and compute matrix operations with a linear complexity once we have assumed this structure. So, here now I have all the ingredients to tell you what's the core observation of this algorithm. So, if you start with the Sylvester equation, so A1x plus XA2 equals B, and you apply the splitting. And you apply the splitting in the shouldn't do? Okay. And so if you apply the splitting of the coefficient, you can decompose your starting Sylvester equation into two Sylvester equations that have peculiar structures. So the first one, you just maintain the diagonal blocks of the coefficient on the same right-hand side. And the second one is that you have. And the second one is that you have the same coefficient a1 and a2. And this time you have a right-hand side which depends on the solution of the first equation, the one with diagonal plots. How do you get here? Well, just by linearity. So if you sum them up, the two, you re-obtain by linearity the initial equation, and in particular the solution of the original equation is given by x0 plus delta x. So the idea is that you can approach this equation individually, and it is convenient because in the And it is convenient because, in the first case, the problem completely decoupled into four equations of dimension and half that you can recursively solve. While the second equation has a notable property that its right-hand side is forced to have low-rank in view of the off-diagonal blocks of A1 and A2 that are low-ranked matrices. And so you can solve it, for instance, with ADI. So this trigger, of course, quite naturally at the Warren-Conquer approach where the Naturally, a divide and conquer approach where the base of the recursion, if the size of the matrices is small, you solve by diagonalization. Otherwise, you do the splitting, as said before, you solve by recursion the four matrix equations of dimension and half. You build x0, you compute a factorization of the right-hand side here, and so you need a matrix multiplication, and finally you apply your favorite Lorentz. Your favorite low-rank sover, and then you sum them up. You sum X0 and that type. So, under some assumption on, for instance, a uniform bound of the ranks of the article matrix, and the fact that you can solve this equation with low rank right-hand side with the linear logarithmic complexity and attaining a relative accuracy of epsilon, then you can show that the method has a log m square complexity. And square complexity. And moreover, that you can bound the error, the norm of the residual with something that grows as kappa times epsilon times the logarithm square, where so epsilon is the relative accuracy of the Lorentz solver and kappa instead is the condition number of the sylvester operator. So here are a couple of examples in the first one. So the equation in both cases. In both cases, this one, so A1 is equal to A2 for simplicity. But in the first example, so A1 and A2 are tradiagonal, so the 1D Laplace, or and here are truly HSS structures, so fractional Laplace operate. And we compare with the dense metal, so diagonalizing from the beginning. So dense metal scale as n cube, our metal should scale as log n times m square. And we see that, so in the bounded case, we reach a break-even point around. Case we reach a break-even point around size 2000, while in the HSS case we have to wait something in between 4000 and 8000. And here, yeah, just showing how the computational time is spent on the various tasks. So solving dense equations, solving low rank equation, building the right hand side and updating the solution, and also estimating the spectral interval, which is something that you need. Interval, which is something that you need for the ADI solver. This is how the error behaves in a function of the condition number, but I will skip this because I prefer to focus more on the tensor case. So what do you have to change in this procedure to apply to a tensor case? So you can try, of course, to mimic what we do for matrices, and you encounter two major. And you encounter two major differences, which both affect the correction equation, the one that was low rank in the matrix case. And the problem is that if you do the splitting, then you find a correction tensor equation that looks like that. And this time, the right-hand side is not row rank anymore. This is not low-rank tucker, low-rank tensor train. And this is because, in the matrix case of words, row rank and column rank are the same, but here are not. So you have something that is a tensor multiplied. Something that is a tensor multiplied a certain mode with a low-rank matrix, but you cannot say that this is low-rank in the usual formats. So the workaround we tried was to first split these into the tensor-sylvester equation just by isolating each of the terms in the right-hand side, so that now we have something that can be reshaped into a Lorentz matrix. So now you have a dense tensor multiplying the A dense tensor multiplying the mode T via Lorect matrix. And so if you reshape this problem, isolating the mode T on the left and merging all the remaining modes on the right, you now have a rectangular Sylvester equation of this size, so n times n to the d minus 1. And the right-hand side this time is a lower analysis. So you can apply ADI on this matrix. On this matrix equation, and you encounter the second issue. And the second issue is that you need to solve shift the linear system with a Kronecker sum, but this time with E minus 1 matrices. And so again, you have to reapply recursively your scheme. So you can do it, and this is what we do, and it works. But from the perspective of the theoretical analysis, this adds another layer of inexceptions, because, of course, you Error within exceptions because, of course, you cannot expect that this is a direct method for solving shifted linear system with that. So, this complicates what you can prove on the error bound. Well, let me recap. So, how the procedure works. You do the splitting, and this time you generate 2 to the B equation of dimension and half that again you rectangular Sylvester equation of this size you apply. Of this size, you apply ADI on all of them, and then at the end you sum all this D plus one contribution to get your final solution. Again, if you look into the complexity with similar assumptions as before, so for instance that you need log n iteration for the conversions of ADI and accuracy, you manage to prove a log n times n to the deconvert. log n times n to the d complexity with a square dependence on the ramp. And this time, so what is worse in view of what I said previously, the additional layer of inexactness, is that then this time this amplification factor is raised to the power b minus 1. And luckily this is not what we observe in experiments. Maybe it's quite pessimistic, but so far is the best we could prove. Best we could prove. And so let me conclude with some example, actually, not so positive example, I have to say. So here we are in 3D with equal sizes. And again, we compare with the tense method. So diagonalizing the coefficient from the beginning and solving. So something that should scale as n to the fourth. And we are not able to see the breaking employee in respect to the diagonalization method. Diagonalization method. So we uh conjectured that maybe if we could reach with our memory resources size two thousand by two thousand by two thousand we could see it, but uh this was out of our computational possibilities, let's say. In order to see a break-even point we need to consider an unbalanced situation where we fix the last two dimensions and we let the first one to grow. And here in this case, our metal scale almost linearly and we fix. Scale almost linearly, and we finally see a win around size 4000. To conclude, so we are quite happy with the matrix case. Maybe I would say that the method might be practical in the 3D case, maybe in a non-balanced equation. And I'm sure that for the greater than three diagonalization would be faster. Would be faster, but at least we managed to prove this kind of complexity. And moreover, I would like to highlight that this scheme also has the potential to exploit farther structure in the right-hand side. So, for instance, if you have block low-rank structure in your right-hand side, like in the matrix case, some more pattern like this one, where the blue blocks are dense blocks and the other are low-rank, you can. You can, and this was a previous work with Daniel, we were able to solve it with a complexity which is not proportional to the storage cost of the right-hand side, for instance. And doing this for tensor is not trivial, and this is something that we are looking into at the moment. And with this, I conclude it. So thank you for your attention. Is there any other questions? I think I guess an obvious question is how to merge it with lowering tensor formats? Yeah, yeah, the problem is here. So of course you might consider right-hand side just sub-tensors of low-rank format and then you reach the base of the recursion. Of course you solve it. What is dense you solve with the density method? What is low rank you solve with the low rank tensor method? What is low-rank to solve with the Low-Rank tensor method? But then the problem is here. So when you compute a low-rank factorization of the shape of this, so you need to multiply. So even if this is structured and then you multiply and multi by a bunch of vectors, then you're mixing slices of this tensor and everything can happen. I think you can also get three buttons. Okay, so let's thank Stephana. So what's quite a bit of one time? Like, Yeah, I was trying to figure out and then I'll leave it on the street. Yeah,