It's amazing event, it's such a such nice place. So, this is a joint work with Ron and Greta Goracci, who's University of Bolzano, and Lorenzo Trafani University, Leicester Business School, and also University of Padua. This is work in progress, so I'm showing you preliminary results. There's much to do, but I think that there are nice but I think there's there are nice results to be shown already. So that's the outline of the talk, introduction, model, estimation methodology, some simulations and conclusions. So we are talking about matrix valued time series data and where we have xt with p1 times p2 and have time direction. And we know, okay, I'm sticking this because. Okay, I'm speaking mystical. You are much more expert than me on the topic. Let me just mention that the dimension of the matrix can be as large as the sample size. So we are working in a high-dimensional matrix valid science. Factor models, okay, I'm just mentioning very, very I mean, very few papers, maybe the seminal um work Joey by Rong and co others. Joey by Rong and Gauters, these on factor models for matrix value high-dimensional tension series, and this also Laurent Methods by Lorentz and Gautros, matrix sequences. And these two works are on, I'll comment on these later on because these are two are on non-stationary factor models. So the for matrix time series, this is the usual step. So we have xp times. So we have xp times p1 times p2, and we have this the usual matrix structure. f is the factor and r and c are the factor loadings. Et is the error. So, point. Existing literature only focuses on stationary data, so either either stationary or non-stationary. So uh the the two papers I was mentioning by thousand four and uh Lorenzo and uh And Lorenzo and Mateo, Parigozzi, these are among the very few that deal with non-stationary factor models. But no results are available for matrices, either matrices or tensors. So that's where we are trying to say something. So we focus on matrix value tensors, as we said, and we estimate a model with a common stationary. With common stationary, we mix both stationary and non-stationary factors and we assume a chronicle structure in both factor structures. The main problem here is that the common stochastic trends, the nostalgia factor, cannot be estimated consistently when you have a factor structure in the error term. And this precludes estimation of the stationary component. Of the stationary component associated with common trials. And of course, standard direct estimation techniques, deep and projection-based, cannot be applied. So we propose a new way where we consistently estimate both the stationary and non-stationary patterns and both the loaded spaces of both the stationary and the non-stationary common patterns. That's the model. So in this The model. So, in this Xt is a matrix value time series that follows this model. As you may notice, we have two structures here. Well, there is one means I1, so means non-stationary, integrated of order one. And zero means stationary. So, this is the non-stationary part. So, f follows around the world, in multi-dimensional, a matrix value on the world. matrix value random work and with associated loading matrices. Random work is K R K C. On the other hand, F0 is a stationary factor. Dimension H. So H is the dimension of the zero factor and could be, like noise, it could be a stationary process with its associated loadings. And Et is the error term that same dimensionless X and with these As X and with these generic elements. So, assumptions, they are pretty much standard in the literature of non-stationary factors. So, we have the definition of the Bernoulli shift because we assume that both the error, the epsilon, remember that epsilon is the turn of the random wall here, non-stationary factor, both epsilon and stationary factor. And stationary factor vectorized can be thought of as decomposable Bernoulli shifts with some assumptions on the correlation. And then we have assumptions on the error term. This is the error term. Usual solubility assumptions. Assumptions on the loadings and also And also independence assumption between these groups, groups of errors, factors, and errors. So I'll try to give you a motivation why junction-based methods do not work when you have both a non-stationary factor and a stationary factor. Because you are treating this guy here as noise, so your UT is the sum of stationary. Is the sum of stationary parts liquid with factor structure plus nodes? So one is if you project this guy here, you multiply, pass multiply by the C1, it's like you, after projection, this is the signal after projection. So it means that you are past, you have this guy here, you are pass multiply, but this is, and you have, of course you have this guy here, but this, what is this? This is a constant. This is a constant. So, heuristically, this guy here is P2. So, it's like you are enhancing the signal. So, the signal has been boosted and looks like there is a P2 factor that inflates it. What happens to the error? The noise, okay, if the noise is weakly dependent, no problem, because this is just a sum of weakly dependent. This is just a sum of weakly dependent variables, so we can invoke a central limit theorem. And so, this guy here is a vector that grows so that the coordinates are as big as square root of P2. So, on the one hand, we have something that is inflate, it's gold, it's been boosted by P2, and what you lose is square root of P2. So, you're gaining something. But, main point is that. Okay, so project. Is that, okay, so projecting inflates the noise, but only by square root of p2, the signal-to-noise ratio gap has been widened. So this is good. But on the other hand, okay, the data is cleaner, but when this guy here is present in the noise, whether it's structure, then we have not a weakly dependent process anymore. And the above argument fails because there is no weak dependent. There is a strong dependent induced by f and this implies. And this implies that this guy is not, doesn't grow as the order of this guy is not anymore square root of P2, but it is P2. So there is no noise attenuation factor. That's the heuristic motivation why projection-based estimation fails. So what we proposed? We proposed a two-stage estimation. So first, we are estimating the non-stationary factor. Estimating the non-stationary factor per linear estimator with the projection-based method. And this is the intuitive, right? And we show and we derive a rate of convergence for these one, which is the usual rate of convergence, one over t. And then we use this to get rid of the common non-stationary component and project it and obtain the factor structure, the stationary factor structure. Structure, the stationary factor structure, both loadings and common factors. And also we get a rate of convergence for this. And then we remove the estimator stationary component back, and we estimate again the non-stationary component by refining the rate of convergence. So we have a refinement. This is kind of surprising, yes, because we are able to refine the rate over the 1 over t rate of convergence. But that's the main. But that's the main structure of layer. So that's the step one. So this is the intuitively preliminary flattened sample projected estimator. So we work with the sample covariance matrices, matrix usual, normalized by the usual factor when you deal with non-stationary factors. And then the estimators of the loadings are defined as the usual eigenvectors. Are defined as the usual eigenvectors corresponding to the largest KR or Kc eigenvalues. That's the preliminary estimators for R1 and C1. Of course, these are defined under the usual constraints. So for a serum, under our assumption, we have that estimation we have a rate of convergence 1 over t. Rate of convergence one over t, but we have also p square root of p, one square root of p. So this is uh somehow expected, right? Because we knew that when you try to estimate something non-stationary, you have this, you expect this kind of weight of convergence. So now we use this as a preliminary estimator. We don't stop here because we define the orthogonal. So in steps two three we just get rid of this part by defining the orthogonal space. Orthogonal space, a togonal projector on C and on R, on the columns and on rows with the corresponding sample versions. And we use them. Of course, these guys here, there's a problem here because these are huge, so we needed to define a selector. This is a problem we haven't dealt with yet. It can be done probably by Done probably by cost-validation, as suggested by the Journal of Econometric paper by Rong and co-authors, but we haven't dealt with this problem yet. But let's say we deal with the orthogonal space here, and we use it to obtain the complement here. So, and we use this. We form the covariance, variance covariance matrices, the sample, appropriately normalized. And the estimator of R and C are the eigenvectors corresponding to the eigenvalues, the HR and HC. Eigenvalues of these two guys here. Usual. But usual, but this time on the projected space. And we do have a theorem here. And we do have a theorem here. So these two estimators have a kind of expected rate of convergence of square root of t, but there are also p1 and p2 here. So the rate of convergence is somehow the sum of vo. Well, you have also have these guys here in the middle. So that's interesting. So we got rid of the non-stationary parts. We got rid of the non-stationary part, we were able to estimate the not the sorry, we got rid of the non-stationary part, we were able to estimate the loading, the stationary loadings, and we are also able to define a consistent estimator of the stationary factors. That's the estimator, and this is the theorem where we derive the rate of convergence. Again, rate of convergence depends on we have square root of t here, but there are also p, where p, this is the Where P, this is the minimum between P1 and B2. So there are also P in the rate of convergence. And then the last step. So we obtain it, as I say, we obtain a preliminary estimator of the non-stationary. We obtain estimation of the stationary part and then we use this stationary part back again to get the final projected estimator of the Projected estimator of the non-stationary loadings and other factors by using the filtered for the projected xt minus the stationary estimated part and we do the same procedure. We define the projected covariance matrices, we do the same procedure here, rows and columns, and of course again the projected estimators are the The projected estimators are the eigenvectors that correspond to the largest eigenvalues, as usual. And also here, we do have a rate of convergence, which is kind of interesting here. That's the rate of convergence of the non-stationary parts. Remember that the initial rate of convergence was 1 over t. And we have many terms here. For instance, we have For instance, we have, okay, this is faster than t, this is t, but we have also square root of p1, square root of p2. So it should be faster than t. So this estimator should be faster than what you would expect. Same for the columns. So that's basically the theory. That's basically the theory. That's basically the theory. And now we can define the final estimator of the factors, the factor scores, like that. And we also have a theorem for relative convergence of the factors, the non-stationary factors. Again, this is usual. This is not fast, but at least it's consistent. Something that it's not possible. Something that it's not possible to achieve in other situations. Okay, so let me show you some simulations. So we simulate from the model. So we have the non-stationary part. Okay, as I say, this is a very preliminary, so we didn't explore the simulation or all the possible scenarios. I'm showing you some of those. No stationary follows the random work. The stationary is just a white noise, a Gaussian white noise with chronic structure. But for the simulation, we assume it to be one-dimensional here. And we also assume this guy to be either one-dimensional or two by two. I'm showing you later on. So HR and HC is one, so this guy is just one-dimensional. This is the stationary part, right? Part, right? And the loadings, both stationary and non-stationary, are taken and are drawn from a uniform distribution. So A0 is a parameter for the loadings of stationary, A1 is the parameter for the loadings of the non-stationary part. So what we do have, and then we have this, so we have four cases and three sub cases for each case. And three subcases for each case. For scenarios and three subcases. The three subcases, they do have this variance. The vertices is the variance of the signal, the stationary signal, let's say, stationary function. Sigma zero is the standard deviation. Sorry. That's the sums. And then here we have the dimension of the F1. So this is the dimension of the non-stationary factor, which is. Non-stationary factor, which is one-dimensional, apart from case four, where you have two-dimensional. And then we played with the loadings. So A0, 10, it means that the loadings for R0 and C0 are simulated from a uniform distribution in between minus 10 and 10. Same for A1. And we have combined all of these with P1, which is the dimension of the. Which is the dimension of the rows, from 10 to 100 for each of these, for the combinations. P2 is kept fixed because they're being. And T is the sample size, so it's the length of the series from 20 to 200. And this is what happened. Okay, in order to of course this is well known du due to in the in the uh identification determinacy we measure the performance by using the distance between subspaces. Distance between subspaces. And this is the measure proposed by, I think, ROM proposes, yeah, between subspaces. So it's a measure that ranges between 0 and 1, so it's equal to 0 if the column spaces of 1 or 2 are the same. It's equal to 1 if they are completely orthogonal. And these are the results. So, first, I'm showing you the estimator of the non-stationary loadings for the case 1, 1. Case 11. Case one, so we have the colors give you the P1. P1 is the dimension of the row, and the symbol here is the preliminary one. Remember, the preliminary one is one that should have the rate 1 over t, and that's the triangle is the refined one. This is the refined one. So it can be seen immediately that we It can be seen immediately that we do always better than so the refined estimator always does better than the preliminary one. It's uniform. In uniform, where we have t here at the sample size. So no matter the sample size, of course it's difficult to appreciate it, but it's uniform. And this is the distance between R one and the true and the estimated subspaces. Subspaces. This is the case when sigma square sigma is 1, is the standard deviation of the constitutional component. And we have a uniform distribution in between minus 10 and 10 for both loadings. Here it's interesting because they are clearly separated. So it looks like this goes down as one over t. Down as one over t, but we have some refine, some factor that depends upon p1 and p2 that make this consistently and uniformly better. What happens when we increase this guy here? This is the variance of the signal, the signal, the stationary signal. So, if we increase it, what happens? We should expect that this complicates stuff, especially for the preliminary estimate. Especially for the linear estimator. But also for our estimator. But that depends. So by increasing the, so this is Chetalis Palibus. The only things that changes is the variance of F0. Things are less clear here, but still you have this phenomenon. You don't see any problem. And also when sigma is full, everything is less discernible, but Less discernible, but still, if you look at the numbers and the figures, we are always uniformly superior. And this is this estimation of C1 and R1. What happens to this? So this is the non-stationary part. What happens to the stationary part? So these are the three cases together. That's the estimation of that's the distance between R zero and R zero hat. The second row is for C zero. The second row is for C0. You have a rate of convergence that goes, it's not very easy to see and to understand exactly what is the law that follows, but we have a theory for that. I think I'll better be so let me let me. Let me just go over this for a moment because I want to show something on this particular two factor loadings, which is best shown on the next case. So let's have a look at case two. Case two is where it's the same as before. The only difference is that these guys are not anymore minus 10 and 10, but Minus 10 and 10, but are minus 1 and 1. So that's the signal, both signals are dampered, less strong, they are weaker. Still, you see, the preliminary is always inferior, so the preliminary is the round circle, is always inferior to the triangle for all values of P1. And this is the same if we go. It's the same if we go case 2.2 and 2.4. Even if it's less easy, it's more difficult to see it. But what is interesting here, so this is the non-stationary part, let's have a look at the stationary loadings here. And here you can appreciate, or C0, that when that here the rate of convergence clearly depends upon P, P1, okay? P1, okay, that I mentioned, because the rate of convergence of C0 has P1 in it. And here it's clearly discernible, this phenomenon. So the message, takeaway, the take-home message here, the main message is that we have a rate of convergence which is complex. It's a sum of many fact, many, many terms and not always. Not always the place part, but always sometimes are also the dimensions of the matrices play part. And then I have case three. This is interesting because here the stationary part is weaker with respect to the non-stationary part. So the non-stationary signal is ten times the non-stationary signal. So here we should expect that Should expect that the preliminary estimation is more or less the same as the final, the refined estimator. And this is exactly what happens. You see, they are close, but still, we can gain something still. You see, the triangle is always very close to the one over t. But you gain something by virtue of this structure here. Even if this factor is very weak, Is very weak with respect to the non-stationary part that here is clearly overhelming, it's clearly dominating. And so the same here, case 3.2, the variance, but the store is the same. And here this phenomenon is very, very much discernible, very much clear to see. Where you have the rate of converge that depends upon. The rate of converge that depends upon P. Last case, the fourth case is where we have two factors. This is two by two factors. So F1, so the non-stationary factor, is a two by two matrix. Again, no problems. You see, the triangles are always Are always superior to the other estimator. So, here again, no problem. And also the same phenomenon here. So, conclusions. What next? So, what next? Of course, inference for the number of factors. We haven't not derived the theorem. Derive the theory for the number of factors here, but once we get a consistent estimator for the number of factors, all the theory applied that we have derived here applies immediately and straightforwardly. So we have no problems. That's the point, but it's something that we need to work on. We need to deploy, I mean, to develop in our package or integrate into these packages, I don't know, and also think about real. Think about real applications. I think there are many applications that would benefit from this approach. For instance, economic indicators, when you have many indicators where they have a factor, a common non-stationary part, I'm thinking about GDP rather than exchange rates or many macroeconomic indicators that are known to be non-stationary. And also, there is a stationary part and also. And also real exchange rates and gift curves. But personally, I would like to apply these to some natural sciences if possible. It would be very nice. I would like it very much. Thank you very much. Questions for the speaker. I guess the purpose tries to identify common factors among non-stationary process. One of the purposes is to identify the co-integration. We think about the vector time series. They are transformed original vector. The transform original vector time series is a combination of some stationary factors and non-stationary factors plus noise. But because in the matrix decomposition, those two parts are orthogonal. You can easily identify, isolate the linear combination of original time states to be co-integrated, corresponding to the stationary market. But the way you write it here, you cannot sub. Light it here, you cannot separate the two terms. So, in a sense, you don't see the coincidence, even there is a coincidence. I see what you mean. It's easy to raise a question, I don't know how to do it. Yeah, yeah, no, it's it's it's it's uh yeah, here it's it's a metric, so it's it's everything is it's convoluted. Yeah, yeah. Uh it's yeah, I think. Uh it's yeah, I think it's definitely something we we should think about. Uh it's a very interesting comment. Thank you. Yeah. Yeah, because clearly it's yeah not possible to see it here. Yeah. But when you have a matrix structure then then how would you proceed then, right? Thank you. Questions? I have two questions. One is: so sweet or any or you didn't already ask your question about that. Plus enough sequential better and better estimation. I don't think you can get. I don't know if these are so you mean if the rates are optimal, so you cannot improve over the rates that we get over both? Right. Right. That's the question. I haven't thought about this. I doubt if you can do better than this, but it's a good question. Thank you. I don't have an answer now, but we think about it. Thank you. The other question is, maybe I overlooked this. So when you initialize from the first step, how can you make sure that you're asking the part that you're interested in? Um so monitor signal are into the R1. One that's dominating the other. First off is to ask me R1 and see what I'm gonna label aggregate. One component is the problem. What what do you mean? What what do you mean? I mean when you when you take the outer product of X T and then take the average but this is not stats right? Yeah, I know but but why the R1 and C1 will be the dominating term? I'm not going to Okay, you you don't know of course, but you're you're quite sure that it will be over fast. That it would be over fast. This works also if you don't know what is the dominating part. I mean, if F1 is dominating, then clearly more or less the two estimators are comparable, as we have shown. But if you have a noise, which has a factor structure in the noise, then Structure in the noise, then you have a refinement, and then two estimators behave differently. So, one I would say that the way the two estimators behave would give you some sort of indication on the strength of the stationary part. Thank you very much. 