And I will say that there's at least one sort of missing piece in the title. And there's, you know, it's not actually scalable smoothing. It's missing a word. It should say piecewise smoothing or targeted smoothing. And I'll explain exactly what I mean by that. But this is really work about letting us use Bayesian additive regression trees to output sort of smooth outputs. That's all we're trying to do here. We're going to use some trees and we want Here is we're going to use some trees and we want the outputs to be smooth. And this is work done by my amazing PhD students, Ryan Yee and Soam Ghosh. So, everything you like about this, it's all to their credit. The things that you think are awful, you can yell at me at lunch. So, the motivation is actually from basketball. So, my student Ryan was super interested in modeling shot chart data. And so, he looked at a bunch of his favorite players and he looked at, you know, here's Players and he looked at you know, here's a basketball court, and he said, Where did they take shots and where did they make them and where did they miss them? And I mean, from a data viz standpoint, this is awful, it's horribly overplotted, but the idea is that there are these sort of uh there's these green circles which correspond to makes, and there's these red X's that corresponds to misses, and in some regions we have really densely observed data, and for other regions, we have basically no data at all. And so, his question was: well, And so his question was, well, if he were Stephan Curry, well, maybe, or if Stefan Curry were to take a shot from right there, where we don't have any data, what's the chance that he makes it? And so he said, well, what if I just lay down a spatial smooth and just try to interpolate this and do it on a player-by-player basis? And I said, well, that's not the most compelling thing. We've got all this data from other players and presumably some of them are similar. And so can we borrow Strong? And so if you had asked me to solve this problem back in 2010, Asked me to solve this problem back in 2017, I would have said, let's just fit a hierarchical model and borrow strength across players. And back then, what I would have done is I would have tried to parameterize location using some sort of spline basis, and then maybe put some sort of exchangeable prior on those spline coefficients, and then MCMC'd my way through this. I would have tried using STAN in 2017, and it would have been real slow, but we would have done it. And then we would have sat back and said, okay, we've got smooth. Sat back and said, okay, we've got smooths for all of these field goal percentages, but it's a little bit dissatisfying because players aren't exchangeable. Stefan Curry is very different than Yannis. And so maybe I would like to use some auxiliary information that I know about these players, maybe how big they are, how fast they can run, how they're used on the court, what position they play, and really try to identify groups of similar players and only try to borrow strength within those groups. And so you can start to elaborate that spline model into maybe. To elaborate that spline model into maybe defining some sort of partial exchangeability structure, or maybe you want to introduce some other covariates that drive this sort of partition of the players. And it gets really, really hard. And in fact, it's really hard to specify the quote unquote right hierarchical model. And so now seven years later, I don't want to think anymore, especially when modeling. I'll just say throw it into a barn. Let's just train a model that says the probability of making a shot in basketball is, okay, we'll do some sort of like probing. Is, okay, we'll do some sort of like probit augmentation. So it's sort of the probit, and it's a function of the location and the player and their position and their height and their weight and maybe other covariates you want to put in there, how long they've been playing. And I really don't care about, say, the beta on the position, or I don't really care about anything except getting a nice smooth. And so I just want to throw this into a BART. I don't want to specify the functional form of the model, and it'll be nice, and there's kind of a nice perspective of bar. And there's kind of a nice perspective of Bart as if each tree in the ensemble is sort of defining a covariate-dependent partition of all of our players. It's forming sort of local averages within that. And then we're ensembling over a bunch of different X-dependent partitions. So there's kind of a nice way of adaptively borrowing strength that BART's accomplishing. And so we said, great, Ryan is a first-year PhD student. He's just going to throw all this data into BART and maybe we'll have like kind of a cute sports paper. Cute sports paper. And so he did that. And then my reaction was exactly that. I looked at these pictures and I said, this is awful. And it doesn't really show up that well here. But okay, so for Stephan Curry, he's like the greatest shooter in the game. He's a better than 50% shot basically everywhere on the court. This is not unreasonable. But then when we looked a little bit more carefully, we saw stuff like this. So what's happening here, and the color isn't really showing. Happening here, and the color isn't really showing up, is that there are some locations from which he's almost a better than 50% shooter. He takes two steps to his right. Now he's like a sub 30% shooter. And then he takes two more steps to his right. He's back to like over 50%. So there's these weird artifacts that we were getting. We just threw all this into Bart. And fundamentally, what's happening is that Bart is approximating this sort of surface using piecewise constant functions. And so you're going to get these kind of weird jumps. And so you're gonna get these kind of weird jumps and some sort of odd behavior. And we said, we really want these to be smooth. So that's when we got to work. And so this is an example of what we're really trying to do is what I think Jen Starling called targeted smoothing. So the idea is we're trying to approximate a function f, which depends on sort of two arguments, some covariates x and some other variable z. And we want this function to be structured in a way that for any choice of the x, the function as a function of z is now smooth. So an example here is. So, an example here is like given player information contained in X, we'd like the probability that they make a shot to be smooth in location in Z. Ryan and I are starting to work with some athletic trainers at Wisconsin where we're given data about all of the athletes who get injured and how they recover from surgeries, and they do a bunch of strength testing. And so, we want to know, as a function of these sort of athlete covariates, how might we get individualized recovery curves to sort of forecast when they're going to be able to return to play? So, these are sort of To return to play. So these are sort of examples of targeted smoothing in sort of one or two dimensions. And the goal of this talk is to think hard about: well, if Z is arbitrarily high dimensioned, how can we output smooth things in a BART? And fundamentally, the challenge is that vanilla BART outspoons piecewise constant functions. They will not be smooth. You'll get those sort of artifacts like we saw the last time. So this is not a new problem. There have been many approaches to getting Barts to smooth out the outputs. To smooth out the outputs, there's sort of all of the literature on treed Gaussian processes where you sort of think about a regression tree, and instead of outputting a constant, you output a realization of a GP. And so Bobby Gramasse's work and Jen Starling has a version of BART that outputs a GP. There's a group over at Maynooth in Ireland that recently came out with a nice package for doing sort of GP BART. And they're all real slow because what you're doing is, you know, you've got a large data set, you've partitioned it, and each leaf of a regression. Partitioned it, and in each leaf of a regression tree, you're training a GP in every MCMC iteration. So you've got this like cubic scaling. And in our basketball example, we had hundreds of thousands of data points. We had every shot by every player over like 15 seasons. So it was not functional. Another approach is Tony's soft part, which uses soft decision trees. And the idea is that instead of assigning any observation to a single leaf node, you sort of break each observation up into little pieces. Observation up into little pieces and assign each piece to a leaf node. And so the idea is instead of outputting a piecewise constant function, you're outputting a sort of covariate-dependent weighted average of all the leafs and leaf outputs. And softbark works real well, except it doesn't allow us to do targeted smoothing. It sort of smooths over everything. And it's also extremely, and when I say extremely, I mean unbearably slow. And there's ways to speed up his implementation, but it's still really, really slow. Implementation, but it's still really, really slow. And so, our contribution is: we're going to do BART. We're going to replace the constant function with a functional output, except we're going to use a collection of random ridge functions in the leaves. And this is going to preserve linear complexity to update each of the leaves per iteration. So we're sort of as complicated as regular Bart. And as of yesterday, we have a posterior contraction result in kind of a stylized setting of this problem where the X and the Z are sort of the same. The z are sort of the same, and you're approximating a Sobola function with piecewise smooth functions. And you have to do a little bit more work, but we're sort of happy about that. So that's what we're going to do. And so how did we get there? So here's just a very basic idea of Bart. In a regression setting, you've got an unknown function that you want to approximate. And so Bart's big idea is let's approximate this function with a piecewise constant step function. We know that these are universal function approximators. So let's just use. approximators. So let's just use a step function to approximate f and we'll just represent step functions like this one here in two dimensions using a regression tree. And so you pick a point in your space, you start at the root, you follow the decision rules, if the first coordinate is less than 0.7, you come down to the second node, and you sort of keep walking down the tree until you get to some output. Now, in practice, this is a pretty simple function. It only takes four values. In practice, if In practice, if f is complicated and has nonlinearities or interactions, you need a really deep tree to approximate it. So, the key idea of BART is to represent complicated trees that are deep and have lots of branching points using a sum of simpler trees. So, it's exploiting this idea that sums of trees are just trees and complicated trees can be represented as simpler trees. So, this is like a deep non-identifiability that we're going to exploit, and we're going to ignore it. And we're going to ignore it. So, we're going to MCMC our way through the space of these trees, and we're not going to worry about mixing. And just to get a sense of why the sum of tree things works, here's a complicated partition here on the left. You can imagine stacking these three partitions on the right, one on top of each other, and you'll recover the one on the left. So, this is the idea of Bart. F, the thing we care about, we're going to approximate as kind of an overcomplete representation with a sum of regression trees. And we're going to put a prior on those trees. And we're going to put a prior on those trees, and we're going to try to get the posterior, and we're going to run a Gibbs sampler where we update each tree one at a time. And I will lead with a caveat that the BART algorithm is very unsophisticated. By rights, it shouldn't work, but boy, is Metropolis Hastings great. So here's the idea of the vanilla BART computation. We've got a large collection of trees, say like 200 of them, and we're going to just Gibbs our way through this, cycling over one. Through this, cycling over one regression tree at a time, conditioned on all the others. And so we're going to update tree M, and I'm going to write the tree as sort of a combination of two things. There's the tree structure, so that overall graph, all of the decision rules. That's going to be in T. And then I'm going to have this mu, which is going to collect all the outputs in the leaves. And so we're going to update this pair T and mu, condition on all the other regression trees. And we're doing it in two steps. We'll update T marginally. update t marginally from its sort of conditional posterior and then conditional on on t we're going to update the leaf outputs so that first tree structure update happens with the metropolis hasting step if this is my current tree we do something real simple we either grow the tree where we pick a leaf and just tack on two new leaf nodes to it and toss in a new decision rule or we prune a tree where we take two children and kill them uh so that's that's all we're doing So that's all we're doing. We're sort of stepping our way through tree space by either randomly growing or randomly pruning. If you think about trees as step functions, we're walking around step function space, adding a step or subtracting a step. And what's critical to pull off this computation is our ability to marginalize out the leaf node parameters. So typically we put in this sort of normal regression problem, we put normal priors on the mu's and then this On the mu's, and then this turns into a giant conjugate normal normal things, you know, Bayes 101 type of calculations. And then, you know, we get a conditionally conjugate draw for the muse in sort of the vanilla part setting. So nothing too crazy. And I'll note that in the computation, when we grow, we don't do any sort of optimization like you would in cart, or we don't do any sort of searching. We just draw a random decision rule. This is like the least sophisticated. This is like the least sophisticated thing you could do. I'm just going to randomly step through TreeSpace and accept with, you know, Metropolis Hastings or not. And it seems to work really well. The acceptance rates aren't terrible. They're between 20 and 40%. And it works real well. Now, there's another perspective on this that I think is useful. And that's to visualize these tree updates as kind of Bayesian linear regression with kind of an adaptive design matrix or a random design. Of an adaptive design matrix or a random design matrix. So the idea is that you can think about looking at all of our decision rules and creating indicators. So each leaf could be written as a product of all of the indicators for the decision rules above it. And those define sort of columns of your design matrix. And the mus are sort of your slopes. So you're doing this kind of random linear regression where you're kind of first deciding, should I add or remove a predictor? And then I'll update the weights on each of those predictors. And I want you to hold on to. And I want you to hold on to that perspective for about 10 minutes. So, what are we going to do? Well, to get smooth outputs, we're just going to replace all of the mus with a linear combination of ridge functions. So, the idea is that we're going to linearly transform our z, maybe shift it, pass it through a nonlinearity, and then weight it, and then take a sum of these guys. If you stare at this, you'll see that, okay, this is like a very simple neural network that we're putting at the bottom of a tree. The bottom of a tree, and that's all we're going to do. This, at least, is going to guarantee some sort of smoothness if we pick the g in the right way. So, you can use sort of our favorite nonlinearity. So, we'll sometimes use cosines, sometimes we'll use tanches or sigmoids or ReLUs or whatever you like. And we'll specify some priors on these inner weights, omega and b. We'll specify normal priors on the betas. And then it's basically regular bar just like. So, all we're doing is changing what we're putting at the bottom of the tree. Changing what we're putting at the bottom of the tree is sort of you know combination of ridge functions. I'm going to skip the parts about the prior and explain. Yeah. Yes. Yep. So there's great question. So there's this sort of script. L, this is happening in every leaf. So in every leaf, we've got a sum of, say, three ridge functions or five ridge functions. Generally, the number of ridge functions big R, we've taken to be one or three. And because we have such a big ensemble, we're actually approximately. We're actually approximating the function with a very large number of ridge functions, one for each tree or three for each tree, and we're putting essentially independent priors across all of them. Again, like the simplest thing, don't worry about this part here, about the actual priors. They don't actually matter too much. I want to give you at least a motivation for why we're doing this. And that comes from imagine doing Gaussian process regression and using the random Fourier features trick. So the trick there is to say that GP regression. There is to say that GP regression can be approximated by linear regression with random features, where your random features are sort of random cosine evaluations. So they're just sums of these ridge functions when you choose a cosine activation and you draw this omega, this affine transformation from the spectral density of your Gaussian process. So this is really the motivation. And that's what we started with doing, where we'd sort of carefully pick these omegas from a particular spectral density so that we're at least approximating tree GP. At least approximating tree GP with a specific kernel. And then we realized the code was general enough to sort of try whatever the hell we wanted for the G and draw the omega however we wanted. All we really need to be able to do computationally is to forward sample from these priors for the outputs. We don't have to do much else. So this is the idea. And the computation, again, I don't like to think too hard. I wanted to just do as much as we could that's exactly the same as vanilla bark. Same as vanilla bark. The problem, of course, is that we're no longer conditionally conjugate. We have this complicated structure in the leaf, and I can't marginalize out the beta and the omega and the b and kind of do that mh update for the tree and then conditionally update all of my leaf parameters. So this was a little bit too bad was our was our feeling. And then we sort of thought about, well, what if we don't marginalize out all of them, but what if we out all of them, but what if we separate these leaf parameters into two groups, what I'll call the inner weights, the stuff that goes inside of our activation function, and then the outer weights. And I'll just pretend that the inner weights are just part of the tree structure, just like decision rules. And so the idea now is that we're going to then play the usual grow, prune, games, and part. We'll have a current tree, and at each of our leaves, we're going to have a collection of input weights. And when we decide to try growing the tree, I'll maybe pick a leaf node. I'll maybe pick a leaf node. I'll draw a new decision rule from the prior as sort of to decorate it with. I'll add the two new children. And then for each of them, I'll just draw a set of input weights for them. And then for pruning, I will kill off two children and then put a new set of input weights for this new leaf. And now it's just regular bark. And now I can sort of work marginally. I can come up with the joint density of tree and all of the leaf node parameters or these inner weights. I can do the material. Inner weights. I can do the Metropolis Hastings to know whether I accept or reject the grow or the prune. And then conditional on this new tree structure, I update the weights beta. And so this is where that random linear regression story comes back, because all I've done is I'm still doing random linear regression. I'm still at every step saying, should I add a feature or not? And then conditional on possibly changing the number of features, I learned the weights. All that's changed is instead of those features being indicators for being in the leaf, they're indicators of being in a leaf. They're indicators of being an elite times some nonlinear function. So it's nothing too strange, and the algorithm works. And all it requires us to be able to do is forward simulate new decision rules and forward simulate new inner weights from the prior. So we're not tuning the inner weights of these things. We're just random draws. We're really leaning on Metropolis Hastings to learn good ones. I glossed over somewhere on a slide, there was a DP floating around. On a slide, there was a DP floating around. All that was doing is so that, you know, update the trees conditional on sort of the distribution of the omegas. And maybe if we learn good omegas, we'll preferentially use them again. It's just sort of a quick little adaptation. It helps, but not by so much that I want to emphasize that. So that's what we're going to do. So how does this work? Well, I screwed up making this slide, so pretend you can't see this part. This is a piecewise smooth function, and Bart. Piecewise smooth function, and Bart does pretty well at it, right? It kind of captures all of the jumps. And if you stare at this long enough, you'll see, okay, even though this is kind of smooth, Bart has this really wiggly recovery. And up until maybe a few weeks ago, I'd say, this is pretty great. Like, I'm super thrilled when I see this. My world has changed. Now I see this, and I never want to go back because what's happening here is we're almost getting perfect recovery of the function. And this is a setting with like non-trivial. And this is a setting with like non-trivial noise, a lot of data, and you can't see it. We actually are having some artifacts of like a little bend, but that's so small, the uncertainties are really precise, and it's sort of working. And it didn't take that much longer to compute. I think it's maybe like an extra 5% overhead, and we can shave that down with some smart engineering. So, but for essentially the same computational cost, we get much better recovery. Now, I should say, this is the lowest bar that I needed to. Say, this is the lowest bar that I needed to clear, right? Piecewise smooth. That's exactly what I'm outputting. That's the truth. So it sort of gets worse from here, but this is a pretty nice place to start. So let's look at it in the basketball context. So here on the right are the sort of vanilla BART just constants. And then I think this is a specific instantiation where we used cosine activations. So that's kind of why there's some sort of weird periodic. Of why there's some sort of weird periodic artifacts, but it tends to be much smoother. And it's kind of more in line with what we wanted it to look like. Whether it's accurate or not, I mean, I'm just going to have to ask Giannis to take a shot from here and find out. He hasn't responded to my emails. But it's certainly, I would say, qualitatively better than this. And when we showed it to some folks in basketball, they said they much preferred stuff like this rather than this, which is weird and boxy and kind of non-smooth. Non-smooth. So I'm not saying that these are perfect interpolations. We're trying to make an extrapolation to an area where we've never seen a player perform before. So who knows what's right? But it's at least a little bit more sensible than what we were getting with just a regular vanilla BART. And so to tie all of this together, replacing the constants in a leaf node in a BART with random ridge functions seems to work. It doesn't seem to be that much slower. We have a posterior concentration rate in the setting. Concentration rate in the setting when the x and the z are the same. So you're splitting on x, you're dividing up your covariate space into sort of these axis parallel boxes, and in each box you're deploying a sum of ridge functions. And so it turns out that if we assume that our f is in a sort of a suitable Sobliff space, and if we make an assumption that our leaf activation is bounded and sort of failed with the right constant, this is a weak assumption because we were always using cosines or tanches or Tanches, or signs. And so, this is not a terrible thing. We can get posterior contraction rates. And the rate is basically determined by how well we can approximate these Sobla functions with ridge functions. Technically, what we end up doing is we sort of compare these sort of ridge outputs. You imagine evaluating one of those linear combinations at a particular z with a particular set of weights and comparing it to another evaluation at another. Another evaluation at another z with other weights. So we need the holderness of g, we need the boundedness at one point. We sort of do some nice concentration of sums of normals. Nothing too crazy, but the rate is ugly. And I didn't want to write it down because I'd make a mistake. So you can view this as like doing an ensemble of shallow neural networks. I have no idea if that's a useful perspective, but maybe we get the expressivity of sort of local neural networks and sort of the Local neural networks and the nice computational and uncertainty quantification properties of BART. And with that, I'll say thanks. And we'll have a pre-print up hopefully by the end of next week or shortly thereafter. So if you have questions, feel free to shoot us an email. I have no idea how we're doing on time. So I apologize if we went over. But thanks. Thank you very much, Zamir. Are there any questions? Are there any questions? It doesn't matter. I'm sorry. I actually don't have an MCMC. Oh, great. Thank you. That was very interesting. I have a basketball question. Okay. But the data plot seems to remind me of that, I don't remember what it's called, but it's the airplane image where there's basically where the data is not there. Basically, where the data is not there is the interesting part. And so, can we make a case also here that when players haven't chosen to shoot from a particular place, that actually says something about that place? Oh, absolutely. There's like a very good reason that certain players are not shooting from certain areas. And so, I don't see Armin here, but yeah, this is a massive positivity thing. So, we are making it kind of, some of these extrapolations are kind of crazy, but some of them aren't. So, if I go all the way back. But some of them aren't. So if I go all the way back, you can at least make a plausible case that somewhere in this region right here is like somewhere close is what we care about. You know, what's happening over here? Yeah, exactly. It's unclear what exactly we're trying to estimate other than I'd like a pretty picture to show a coach. Or like what we might guess if he were to take this counterfactual, if he were to shoot a lot from here, what would it look like? Yeah, because I mean, it seems like you can identify like I mean, it seems like you can identify like convex, big enough convex regions within the cloud that are sufficiently indicate that that's a distinctive choice not to shoot from that place. Yeah, of course. And then add those places you sort of want to model that into and say, well, there's a lack of confidence in this region. Yeah. I will say that the actual data analysis here could be souped up in so many interesting directions. Yeah. Thank you. Thanks, Samir. Super, super interesting, as always. So if I understand correctly, you're still using the sort of axis-aligned bins for your decision, you're like splitting into nodes. And while, okay, so you're using sort of more complicated basis functions within each cell, I would expect most of the discontinuities, most of the sort of weirdness come from the fact that your boundaries are accessible. Yep. So you're still seeing some discontinuity in your... So you're still seeing some discontinuity in your, or maybe not discontinuity, but like you're seeing some weird artifacts in your sort of newer results. And I wonder if adding some amount of like just randomness on the direction of the batteries wouldn't help get rid of that. And if that would cause your MCMC to become even more difficult than it already. So I'm so glad you asked that. And just for the audience's benefit, this wasn't a plant. So let me pitch the other project that my students are working on is to do exactly My students are working on is to do exactly what you've suggested: is like, let's do BART with you know oblique decision rules. And why we did it in two steps is because one of our new colleagues at Wisconsin has some great new results on what happens when you do regression trees with oblique rules and ridge combinations in the bottom. So, putting these two ideas together, they've got some really nice frequentist results. So, when I saw our job talk, I said, Man, I would love to get Bart to do both of these things. And so, here we are at the end of the summer, and we're And so here we are at the end of the summer and we're close to putting it all together. So absolutely, doing sort of oblique decision rules is like the next natural step. The challenging part with that and where we actually suffer a computational loss is an axis-aligned BART, what you want to do is if I'm going to grow from this blue node, it's very easy for me when they're axis parallel to figure out what is the range of decision points. But as soon as these start to be polytopes, it becomes much harder that I can give you a direction. It becomes much harder that I can give you a direction, and I really need that you to draw me a hyperplane through a particular region. And so, how do we bisect a polytope in a non-trivial way? Like, we don't want to split outside the region. And so that we ended up, the solution we ended up coming up with was solving a small linear program, saying, okay, I've got a linear function, I have linear constraints, this isn't that hard, but then there's overhead in that. So doing oblique decision rules. So doing oblique decision rules to make sure that we're not wasting an MCMC iteration by proposing absolute garbage. Like something that we know will get rejected. We don't want to do that. So trying to be smart about getting the right access oblique decision rules introduces overhead. So you could totally, and I think the code base supports it, but I don't know if we've ever tried it. So obviously we're going to soon try putting these two things together. So maybe. So, maybe the next time you see me, we'll have that result. Maybe a really quick follow-up. Do you think if you went to the opening decision rules that you could just go back to the PC functions? You could. It depends what you want to do. We have some results saying that oblique decision rules can really nail kind of curvilinear boundaries. I suspect there are going to be applications where we do actually. There are going to be applications where we do actually want something more structured in the league. Thank you. Yep. Cool. Okay, thank you, Samir. Let's thank Samir again.