Thank you for the invitation. And this talk, yeah, it's on universal approximation. Paolo was supposed to give this talk. So let me first time. Paolo, this is a joint work with Paolo. He was supposed to give the talk, but then he left. I was like, okay, I can do this. So let me start by a brief history of universal approximation, tell you what it is about, even though many of you know about the problem. And I start this. And I start this, as mentioned, the talk is about neural networks, and most of you have seen sort of a picture like this of a neural network, right? It has basically a bunch of inputs, these X. They basically next layers, the state of next layers are updated with the state of the previous layers, right? And bunch of weights, these W's and B's, right? And then they go through this activation function, and this is the state in the next layer. Is the state in the next layer? This is what's called the feed-forward network. The sigma function, I call it as activation function. There's one other notion that I mentioned in this talk, something called the width of the neural network. And this is how many nodes I use in each layer. Keep that in mind. I'll use that terminology of width. Okay? What do you call? Width, like the like width. Like the width of the like width. Yeah, width. Okay, I think. Yeah. Really? Okay, I think. Yeah. So the typical problem in neural networks is as such, right? You have a bunch of sample data. You have some input samples and some output samples. Somebody gives you these samples from a function. So there was a function that produced these x, i, y, i's for you, right? And what you want to do, you want the last layer of the neural network to approximate very well, let's say, in some sense, these, you know, the y's, basically, the f of x's, basically. Is basically the f of x is basically because this failure comes from, let's say, a function. And the typical problem in neural networks is you go after minimizing over these weights to do the best fitting into this data. It's a very typical objective, will not have anything to do with it, but I thought I'd give a short summary of what neural networks typically do. And neural networks, you know, algorithms, they typically use some gradient flow of as such to to sort of uh find these weights. To sort of find these weights. Now, what is the universal approximation problem? Which is a classical problem. So, this classical problem, you're given a function f from Rn to Rn. For the rest of my talk, take n to be n, for simplicity. And you have a compact set. And somebody gives you some desired approximation accuracy. So, this function is some function out there, and you want to come up with a neural network. With a neural network that approximates this function in some sense. And that sense could be in the sense of some Lt maybe or some L infinity even better. You want to approximate this function. Because your neural network, you should think about it as a function. It's really its output really producing a function. And you want to see if you can approximate any function, that continuous function that somebody gives you. This is the classical problem of universal approximation. This is the classical problem of universal approximation. And so, as I mentioned in the classical, classical work in this area, this number of nodes in each, because you want to approximate this function, something gotta go to infinity, right? And so the number of nodes in each layer is taken to be arbitrary. So you allow yourself to use as many nodes as you want, depending on how much accuracy people want from you, to approximate. From you to approximate. And the very classical result, which most of us know, goes back to Sivenko. And this is some stone-by-stress theorem going on est-road or something. So it's not a complicated result to prove, but it really really, in fact, you don't need many layers even, if you're in a network. But what you need is like many nodes in each layer. And there are quite a bit of extensions, and this Quite a bit of extensions, and these results sometimes are different for LP, but there's results on uniform also, when you're clinical, etc. This is not going to be too much of our focus, but I just put it here as sort of a historical note. But all the results relied, they rely on having no bound on the bit. Now, what do we know in the case that you have a bounded bit? So, as computational power became more, Computational power became more, people became interested in deeper neural networks, right? Where you rely on power of composition rather than having many nodes in one layer. You compose more. Because remember, when you move from one layer to another, you're composing these sigmas. So you're composing more. Now, so when you have bounded width, very interestingly, the results in this side of the This side of the world are not so classical. I mean, these networks are much easier to train when you don't have many nodes in layers. That's why people are interested in them. And the literature, as I said, is way less classical. Most results are LP. And most results, well, with P not being infinity, and most results are particular to a specific type of, let's say, activation function, a specific type of network. A specific type of network. And I picked two samples of the first two results in this area. As you can see, huge gap between 89 to 2017. So very recent literature. And the results I just summarized look like this. I'm going to give you L1 result. You have to take M to be equal 1. And this is the minimum width that you need, the minimum number of nodes that you need. And just as a general rule of things. As a general rule of thing to keep in mind, if you're aiming for n plus 1, you're not aiming for n. If your function goes from, let's say, Rn to Rn, there are counterexamples, you know, to Rn. So typically, people are aiming to get close to n plus 1. And this is, as I mentioned, you know, some more general results. And the big kicker in this area was when I started thinking about this with Paolo, was this result of Tiger and Theri Leons, the same Theri Leons and Strauf Path theories. Rough path theories, this very interesting result, which was really a very, very complicated paper, one of the first constructions that didn't rely on a particular activation function and had a touch of results on uniform basic approximation, which is the hard one. It's much harder to do L infinity than L P. I think I provided you now with a historical sort of set of data that you can keep in mind. Data that you can keep in mind as we go forward. These results typically apply to what's called, I don't want to get technical in there, what's called feed-forward networks, which is the networks I showed you. There are a lot of other neural networks out there. Now, closing the gap, and this is, as I said, 2020, is when I started kind of thinking about this with Pablo, where these two results, which came out at the same time, it's my result with Pablo and another group, both in Both in ICLR. And as you can see in these results, the width is getting pretty close to n plus 1. These results are more related to feed forward and our results are more general and that's what I want to tell you. But you can just basically, from now on, you can just close your eyes because I've told you what the result is going to be. It's going to be a sharp result, irrespective of activation function to some extent, on universal approximation. On universal approximation for cases when you have bounded width. Now, let me tell you how we went about this. Because now you know the result, I just want to tell you what's the machinery. So it goes as follows. It's a control theoretic perspective to inverse approximation, as promised. And this sort of perspective started with these papers in 2017, where people started looking at what's called the residual neural network. As you can see, I carry an extra. As you can see, I carry an extra memory XT in here, not a big deal really. These are the W's and the B's that I had in the update I showed you with the sigma. And, you know, the control perspective is these weights are what we're trying to say. Like, let's just think about us controlling weights. Okay? These W's and the B's. And the activation function, I'm working with vectors here. So the activation function is acting on every component. And that's what I call capital sigma. And that's what I call capital sigma. Okay, so this is the perspective that was introduced. And now, a little bit of modification that we are going to use in this work, and I will mention why I'm doing this, is we're going to throw this parameter S in here, which was not in the original update. This S is going to be either plus one or negative one, so don't worry about it. But anything you see in green, But anything you see in green is what we will select. We will select these. Think about them as control inputs. We're going to select them. Now, you can easily, so this case, by the way, right, think about them as layers of the neural network, right? And as I said, I'm interested in letting something go to infinity, and what I'm going to go let to infinity is the layers. As the layers go to infinity, you can think in continuous time, basically. In just this t, take it to be the time. Take it to be the time. So, this is the system I have. Up to this S, this should be the familiar update. This is somehow like a neural ODE, you might think about it. Now, I want to reformulate for you that problem of universal approximation. This is just a reformulation. I tried to sort of draw a picture. So, remember, you had a function and you have a compact set, a bunch of samples in this compact set. These samples are from this function, right? So, let me Right? So a bunch of x's, and I call them y or f of x, right? And so the first thing you know, this is something that typically in neural networks people call memorization, in that at the minimum, after you pick these weights, you're hoping that you can produce the data that you discovered, right? So you want to map the X's, the samples. These are my samples, my picture of my samples. You want to take them to the corresponding F. Anytime you have a question, by the way, ask me, right? So you have this sample, X, Y, Y. So you have this sample xyys. You want to make sure when you select these control inputs, they're going to map these guys to these guys. Now, what's the issue here? The big issue here is you have a large sample, okay, but your control, which is this SWB pair, does not depend on the sample. Typically in control, they tell us, hey, here's an input, here's an initial condition, take it to a finite condition. But I have an ensemble that I want to move, and my control doesn't depend. Move and my control doesn't depend on the niche position. So, you know, in the same line with the talks in the morning, this has an ensemble feeling to it. Like we already see it has an ensemble feeling to it. But this ensemble feeling is you have an ensemble of points rather than having your systems depending on some parameter. And if you're familiar with this set of work of Aikashev and Sarashev, they've talked about this in fact. They've talked about this, in fact, this beautiful paper on control of a group of diffeomorphism, and this is the paper on control and the space of ensemble of points, which is exactly this setup that was mentioned today in the talk. So it's really ensemble feeding into this problem because you have to drive an ensemble. Just one controller doesn't depend on the points. So that's a difficulty of the problem. Now, and remember, this is step number one. And remember, this is step number one. I did not tell you that if I do this, you're going to get uniform approximation, but you at least reproduce the sample points. Okay, so this is a step number one. I'll get to step number two in due time. So just for convenience, I'm going to lamp up my initial conditions in one vector, okay? Like you can think of it as column or matrix or whatever. And basically, these are the data. This is the data that's going to map to here. Data. That's going to map to here. This is what I know about the function. And I just basically vector up this one system that I have into. It's easier to sort of talk about this than vector. But the problem is the same. And so now the machinery in front of us is kind of clear what we're going to use because we have this nonlinear system, right? We know the control inputs. And so the machinery we're going to use, of course, is The machinery we're going to use, of course, is sort of some geometry control machinery to sort of see what we need from this activation function, let's say, for us to have enough expressivity, so to say. Now, the typical thing is, you know, remember my system, it has these W's and the V's and the S that I can pick, right? As it's typically in control, we just make a selection of piecewise first and foremost. Of piecewise faster controls. I'm not going to exactly tell you what I'm selecting, but after you make this selection, you end up typically with a family of sort of vector fields, right? And this family of vector fields, you want to study, which will set orbits of this family. And as now I can kind of come to the, well, the portion that's so important for me was the addition of this S. Because as there are lots of experts in the audience, if I don't, then I don't. In the audience, if I don't, then I don't have this S, I'm out of this case that's called the symmetry case, because typically, if I want to have a vector field forward, I want to also have a negative of it backward. So that's why this S plus 1 and minus 1, having both in a set of vector fields, helps me to not get into the craziness of difficult side of controlability, basically, right? And so, what you do, you basically want to compute after selecting these proper sort of piecewise controls. Proper sort of piecewise control inputs, you want to compute sort of the Lie algebra, right? And this is the Lie derivatives you're going to compute. And even by the definition of Lie derivatives, you must expect that whatever condition I'm going to give you, I'm hiding a lot of things because of the turning stop under the rug, but I think all of you are going to expect to see derivatives of sigma showing up. Right? Because you have the sigma, you take, you compute these derivatives. You take, you compute this derivative sigma version. Now, a good selection, a good selection, if you do a very good selection and some tedious calculation, you are going to face sort of your rank condition because you have a rank condition for this L-ange problem, right? This rank condition is going to boil down to something of this form, okay? Just keep an eye on this, right? This is one sigma derivative of it up to derivative of some order of it, right? Some order of it, right? You want this thing to have rank n. So it turns out, after computing this, doing this tedious calculation, this is what you're going to get. And now I want to tell you like a little lemma, okay, which is kind of cute. It's known. Most of you know where I'm going with this, but it's a cute thing, okay? So take like a function sigma, okay, and suppose this function c, sorry, function c satisfies a different. A differential equation with quadratic right-hand side. Okay, just keep it in mind. Quadratic right-hand side. This A2 cannot be zero. Okay, so you have a differential equation, quadratic right-hand side. What is nice is when you write up this, this is our matrix, guys. You write up this matrix, right? This matrix has a determinant not zero, okay, basically, if and only if this thing is not zero. Okay, and if you really think about this, I mean, I think most of you can. This, I mean, I think you most of you can say this has a Vandermont feeling to it, right? It's just a differential sort of cohesion type of a Vandermont kind of condition. So this gives us a lead because it's telling us, hey, you wanted this rank condition, here's what you're going to put. You're going to ask for this sigma to satisfy the differential equation with quadratic variance. Okay, and now let's see where I go with this. So there it goes. There's our sigma. Okay? And I'm going to ask for it to satisfy this equation. Going to ask for it to satisfy this equation. You may complain because you may say, Well, there's so many argistication functions people work with. You came up with this condition. Would any of them satisfy this? And so here's a table for you. I was pleasantly surprised when I saw this. Maybe somebody knew this. But in fact, we can even have a high-order derivative of sigma satisfying this equation. So this is a list. So, this is a list of functions that people use. Hyperbolic tangent. I mean, Raylu is not differentiable, but you can run some. Sorry? From my course of ODEs, you know, like there is just one function, this logistic function. That's all. Everything else is unbounded. I mean, there is if you have the rest by consider, it's just one function essentially. Unless it's unbounded, it will be just logistic. The logistic function is not bad for instance. Isn't but for instance, they're the same. Hyperbolic clan, it's the same, yes, that's right, that's right. And you know, the soft plus is what they use. But the nice thing is something like radio function. You can sort of approximate it by, because you see, remember, I can have even higher order differential equations. Because I don't need sigma itself to satisfy it, but maybe higher order root of sigma that I satisfied also. This one? Oh, because as I mentioned. Oh, because as I mentioned, you see, a higher order derivative of sigma is okay if it satisfies. It's a little more general than just itself satisfying. But typically, these are the functions that people use in neural networks, right? They either use Raylu or they just logistic, something like that. I mean, the condition, remember that I arrived, is a sufficient condition. It's just because of my choice of inputs, et cetera. You could probably come up with some other conditions. probably come up with some product conditions, but this kind of did the job in that it had enough of interesting functions in it. So anyways, given this assumption on sigma, then the result is kind of clear. What it's going to be is that as far as this function sigma satisfies that condition that I just wrote, you basically are somewhat controllable in a sub-manifold that's avoiding. Sub-manifold that's avoiding that zero set of the determinant, remember? Because my determinant could still be zero in some location, and those points are kind of reasonable to exclude because these are distinct points, right? That they should be mapped to distinct. If the function has to, if it maps two things to the same thing, then two distinct things to one point, then I have a hope. Okay, so this is basically step number one of the result. Of the result. But the thing to keep in mind about this as step number one is sort of this nice unifying condition on the activation functions that people use at the end of satisfying this equation. Is that the only condition? Because sigma constant satisfy this to the sigma constant would satisfy this. Is that true? So this is. Why would it satisfy this? The left hand side is zero, right? Left-hand side is zero, right? But am I missing something? How would the right-hand side be zero? So the let yeah, I wouldn't, I so but you're going to get one constant sigma, right? Depending on A0, A1, and A2. Yeah. There might be some other condition. It's, I mean, it just basically, let me just think about that. Basically, let me just think about that. What you're saying? Just a constant that satisfied. Constant satisfied this question? Something is kind of off. Anyways, so let me actually talk about the second step of this. Because in the first step, what we have achieved is we've managed to bring this point to the final point. As I said, you think about the reproducing of the data. Now, to get universal approximation, you want to make To get universal approximation, you want to make sure the points in between those samples, they don't move far away. Because if they move far away, then you have a problem. Right? Because you really, really, what you want to get, you want to get close on the whole set. The current result we have is on the samples. We move the samples, but we don't know what happens in between. And so the question is, can we say something about this? Now, this comes the hard part of this work. This comes the hard part of this work, okay, is how can we sort of guarantee universal approximation? And the big sort of thing that we used here is monotonicity. And Sauber yesterday gave a talk about monotonicity, right? And the function is monotone if it preserves some sort of order, right, in the following sense. And so one of the ideas that we have in mind. So, one of the ideas that we had in mind was, you know, basically, if we had some monotonicity of the flow, maybe we could get some control over how far these points go. It's a wishful thinking. And to sort of have evidence for it, is that if you have a continuous function on a compact set, and you have basically a fine enough samples, think about this as a bunch of cubes, right? And if you basically Basically, you know, you have basically suppose that phi is some monotone function that satisfies this L infinity bound on the samples, just on the samples, meaning on the boundaries of these cubes. Then, you know, because of the compactness, you can quickly get sort of a bound on the whole set. This is basically just using modular subcontinuity, a couple inequalities. So, this is basically, this is what we had. So, this is basically what we had in mind. We had in mind that, okay, we know what the samples are, what would be the thing that wraps the thing up? Well, if you had monogenicity, very wishful thinking to hope that the flow of sort of what neural network generates would be quantum, but at least this was an evidence that if we could ensure some monotonicity of generated flow, then we would be okay to go. But as I said, this is visual thinking. But as I said, this is visual thinking. So the first step that we did was: what if the function that they gave us itself was monotonic? Because remember, we're approximating a function. This function is supposed to be only continuous at the beginning of my talk. But for a second, suppose that the function I knew additionally it's monotonic. Yeah, and I'm going to think about it right now, but it is. We needed it in this. We can get rid of it because you know later. And so, this result, which is quite a technical result, that we developed was that if your function happens to be basically monotone itself, then we can do a nice selection of these inputs, okay, such that using the previous construction that I showed you, we sort of keep some monotonicity on the flow, and that allows us to get a universal approximation result. Okay, again, I'm not going through. Result. Again, I'm not going through the details of this result, but basically, this was our first step. Our first step was: okay, we have this memorization result. Now we additionally, suppose our function is monotone. It turns out we can generate a monotone flow and then everything goes through. But this is kind of a silly result because, you know, most functions somebody gives you are not monotone. And so what do you do? Yeah. So T will be a diffeomorphism, right? Yes, C, so T. And would the S be? The F itself is basically just a monotone function. So you can approximate any monotone analytic function, arbitrarily closed, DLFT, no compact set, but if you want it. Yes, yes. Yes, that's worked. And this has to really, the construction is very like detailed. And analyticity, for instance, comes in because we do this thing in patches and we want to make sure. We do this thing in patches and we want to make sure we have sort of a finite number of switches, this kind of stuff. So it's a very tedious construction, this construction. And as I mentioned, it relies in really heavily used monotonicity of F. Now, the problem, however, and one of the things, guys, that I want to reiterate here is if you really think about this as what the neural network produces, this is the width, you should think about it as being N, which is not. Think about it as being n, which is not what's expected. As I said, the minimum you expect here is n plus 1. So it's a little surprising, but the assumption is too much. The assumption is the function itself is monotone. Now, what happens when the function that somebody has given you is not monotone itself? Sauber talked about this yesterday. And what he mentioned was, well, I'm going to use some embedding. I take the function, I embed it to a larger space, so that in there, I'm going to get 1.6. I'm going to get 1.6. So we're going to use some embedding, but because of the very particular way of our construction, because you're just dealing with neural networks and not general sodium function, it turns out that we can get away, as I mentioned, with this function f, right? We can get away by just adding one dimension. And in fact, we can use linear injection and projections to bring this function that's not monotone from here to this function of. Non-monotone from here to this function up here that's monotone. Typically, you need 2n for this, but you can get a bit more because of the particular form of the problem. Now, then what we do is we basically apply the previous result to this extension and the analysis, if you can get rid of it just by approximation of this continuous function. And so, the result, really, what it says is you can basically approximate. Approximate discontinuous function, okay, but you need a neural network that has n plus one. This capital is n plus one, n plus one node in each layer. Basically, the punchline here to keep in mind is that the bit that we need is actually n plus one. And of course, there's quite a few issues with the way sort of we treated the problem because I Sort of retreated the problem because I showed you these cubes that are very important in this approximation. We don't have any geometry or anything to the data. If the sample data that somebody would give you, which most of the time is the case, would be for instance on a low-dimensional space, we wouldn't know how to deal with the problem. And basically, this summarizes what I wanted to talk about. This is the reference, and there was a follow-up work by Agrishev and Sarchev, which they actually And Sarshev, which they actually use the machinery that they developed in the 2020 papers to basically give the setup from that perspective. So it's also an interesting one. I think I'm going to stop right here. All right, questions for what I thought. Yes. So you went to your model, so I'm just trying to follow your steps. You went to the model by passing to a continuous limit. So in the discrete time case, or the discrete index case, I guess because it's an index, it's not time, is you've got this S, which is plus 1 and minus 1. One minus one. And I'm thinking, you know, maybe is that like inhibitory or excitatory neurons? But then when you go to the continuous limit, it's just piecewise over pieces, it's plus one. Yeah, exactly. Over pieces plus one, that other piece is my next one. Yes. Anything that we plus one any piece that we plus we pick, right, we also throw a piece with a negative peak. At least with a negative in x. Any vector field we pick, right, we throw in a negative of an inexport. That's really for that. Because the symmetry, you know, if there was a stronger control result that we could use, then we wouldn't need this. But it's just that to have the symmetry in the set of vector fields. You see what I mean? Right? Because basically. Because you're using the bracket type. Yeah, exactly. And here, if you don't have both the negative, you know, forward and backward, it's. You know, forward and backward, it's always a problem in Coinbroke. This is kind of doing Charles theorem. Yeah, absolutely. This is really Chow's theorem, easy case. The symmetry case is there is Book of Georgia, which you can just open it, and this symmetry case is exactly there. So that's the result being the same. The set of vector fields is symmetric. In that Z and negative Z or in the set of vector fields. So if I think about the approximating systems, but how can I think about this? I mean But how can I think about this? I mean, because I've got, suppose I want to confine myself to the best I can do with ten layers. That's fine. So we have like a version of this work in discrete where we give sort of bounds because people are interested to know, for instance, if I give you an epsilon, what's the number of layers you need to get that. So we have a we have a result. We have a we have a result in a discrete case. The bounds are not great. They're kinda exponential looking. So They're kind of exponential looking, so people don't like exponentially. Well, yes. I mean, these have worked mostly theory now. It's not something that people use, it's just. I was going to ask you. So the real ResNet doesn't have the S, right? Yes, it doesn't. So if you add S, do you think it becomes better? Like if you add S. It becomes better for anything. It doesn't seem to need it. No, this is artificial, really, just to make our life easier. Otherwise, it's not. It's function. Otherwise, it's not the vision. Yes? I think I might have just say the answer, but your trick to turn a non-monotone function into a monotone one? Does that always work? Oh, you can always embed to 2n, as was mentioned by Saugar the other day. But here you can get away with less. Compactness comes into place, you can really throw in sort of a kappa term that dominates stuff, etc., etc. And that allows you to get away with less. That allows you to get away with less, but 2n, yes, that's what Sor was talking about. Exactly. And that exactly corresponds to that extra sort of dimension that you need in the number of nodes in each layer. So then it becomes a reach. If you don't have the S to turn both the signs, and it becomes a reachability problem. Exactly. And I didn't want to deal with that. That's as you know, because they're much more experts dealing with these things and they haven't settled it yet, so They haven't settled it yet, so when they settle it, I think because problems are very difficult. So a little bit of cheating, let's call it this way. But that, you know, then the next question after that is, which subclass of functions can be a function? Oh, that's a very good question. And I I don't know the answer to it. No, I mean the results of uh Results of Terry Jones, right? That shows that, but that result is just by hand construction. The way these guys prove these results, guys, you give them a function and they're going to build a network for you, like literally. Like, here's, we do this, then we connect this thing to this, we connect that thing. So they do prove these results, but not using control. Sorry, I'm just going to stop. All right, so is that possible? Thank you very much. Thank you very much. Constant would not give us the intimators of the I'm thinking some columns are zero. Yeah, the solution to the O E, that's what I mean. I don't mean like peak one point sigma right there. I think in the real world it doesn't have it has more s and have two. It does have s, it has both two. We really are controlling artificial there is a reason they put but minus apparently in practice too. So it's not really double even most science. Double sigma w yeah. Oh, it has plus and minus. Alright, so we can perhaps for a discussion later. It can probably break and so as continued. The next speaker is familiar, so it is all. Cool. Thank you. And thanks to the organizers for inviting me and giving me a chance to speak here. This is actually my first time interacting with the larger control computer. So I come from a more differential geometry perspective. So I come from more of a differential geometry perspective, from like Cartan type approach to things. And it's been a lot of fun, and I've been realizing that there's still a lot of control that I definitely did not even know existed. So it's been fun. Okay, so my talk here, I don't know if you can read this, it looks a lot better on my laptop, using symmetry to construct dynamic feedback linearizations. So this is a pretty well studied field. Studied field, okay. This notion of dynamic feedback is sort of like the best, the next best thing to a system that fails to be static feedback linearizing. And so this idea went back to 1981 from Sing. A dynamic feedback for a system x dot equals f of t xu. So this sort of control system. A dynamic feedback of this is an augmentation of the system where you add additional state variables and dynamics for those state variables y. For those state variables y, and new control variables which we'll call w, and a relation among the old control variables u and the new control variables w, and also as well as the new and old state variables. Notice that the old control number of old control variables is Rm, and we'll demand that the number of possible new control variables is at least larger than that, greater than or equal to that same number. Okay, so there's been a lot of work that is related to this. That is related, modeled, really well-studied related concepts. There's differential flatness and endogenous feedback, which are the same thing: lead back luna and defeides, absolute equivalents coming from more also the carton-type direction. And it's just, there have been so much literature on this topic. So here's sort of an incomplete list of names, but plenty of people who have touched this field in one way or another, at least that I know about doing my literature search when I was making this talk. But I apologize. But I apologize if I've forgotten anyone who's also featured in this and was not on the short list here. Yeah, so really, what I'm going to be talking about today is a way that's closer to doing some things involving kind of more with the different flatness type constructions. However, I would like to point out some ways in which this framework could actually get us to the full dynamic feedback linearization sort of viewpoint. So the first thing I need to address for dynamic feedback linearization is one Is one is there's a notion of a regular dynamic feedback that requires that the number of new controls W is in agreement with the number of old controls. And it satisfies something called the solution correspondence condition. Basically, if you have a trajectory to the original control system, it needs to lift up to at least one trajectory for that larger dynamic system. It does not have to be unique, though. That's sort of a key point. So, this definition here says that the system one Definition here says that the system one, the original control system, is dynamic feedback linearizable if it admits a regular dynamic feedback with the property that that larger augmented system is itself static feedback linearizable. What's curious is that when you have a property like this, it turns out that you can always write the state variables, x of t and the controls in terms of some arbitrary functions and their derivatives. Derivatives. Now, something important to note here is that this is not necessarily quite different to flood necessarily because you don't actually have to have dependence per se on the zeroth order things. In fact, the difference really between dynamic feedback linearizability and the flatness is that that failure, the uniqueness of lifting of trajectories up to the larger thing is captured when you don't have it. Is captured when you don't have dependence on the lowest order function derivatives. So there's some basically constants of integration that could handle that. Okay, cool. So sort of the two basic examples. This one is incredibly well known, this first one, which is partial prolongation. So some number of your controls, some i's, but not necessarily all of them, if you basically consider those to be new state variables, then you can sort of quote unquote differentiate k times. Of quote-unquote differentiate k times for each one of those, and you get this integrator chain, essentially. This is the classic example of a dynamic feedback linearization. You can call it dynamic feedback by partial prolongation. And one that's, to my knowledge, I don't know if this is actually anywhere in the literature, but it turns out there's another type which is just something called an integral extension from a Paffian systems and exterior virtual systems standpoint. But writing it just in terms But writing it just in terms of equations like this, you'll notice that we're not adding on derivatives of anything, we're actually adding on, in a certain sense, antiderivatives. So that's kind of a curious thing, and I'm not even going to write the slide because I'm going to say it necessarily, but I'm pretty sure that these two examples are what constitute all dynamic feedback linear predictions. Pretty sure. I don't have a proof of that terms. Okay. So let me basically choose an example. 