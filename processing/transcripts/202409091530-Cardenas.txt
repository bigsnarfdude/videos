Yeah, so that's what I was mentioning, that this started as a summer project with Trinidad Novoa, which is a former student of mine, and Paul and Farnes. So this was back right before the pandemics. So we couldn't push it too much further after the pandemics. So the idea or the goal that we had was to learn a little bit of how hard. Had was to learn a little bit of how hard is to learn with machine learning, with neural networks, the kinetic energy density functionality. So we know it's hard. Paul shows you how hard it is for actual molecules. So this is our goal. We want to feed a neural network with the scriptures and see how hard it is to train them to get the actual value of the positive kinetic energy density. So we are not going to put the Taplasian. So, we are not going to put the Taplacian in our model. So, we are going to use the positive definite version of the kinetic energy density. And we are interested in two, mainly in two things, is non-locality, because we know that it's a highly non-local functional, but also in the influence of the number of elections, because there are discontinuity with the numbers of lectures. So, to do that, we choose a very simple prototypical. Prototypal model, which is the double world potential in one dimension. So the first part is just working in one dimension. And why we use this functional or this potential is because with, yeah, like in 2018 with Julia Contreras, who I guess is in the audience now, we showed that this simple potential recovers a lot of molecularity in the sense that in 1D you can In the sense that in 1D, you can recover some features of chemical bonding, and you can think about this potential as a diatomic molecule in one dimension. So we use 900 different potentials, including 15 number of occupations, that's the number of electrons from 2 to 30. 30. And this is the data set. So, look how diverse it is, especially when you increase either on the left for a given number of electrons the strength of the potential or on the right, which is perhaps more interesting if you keep a fixed potential and then you increase the number of electrons, how the magnitude of the kinetic energy density changes rapidly with the number of electrons. So, given that. So, given that, what we use was a neural network. We optimize, this is what we use as parameters for hyperparameters for the network. We use as a loss function, the mean score error, and we optimize the hyperparameters, not all the hyperparameters, that basically the number of hidden layers and the number of neurons were optimized with variation of optimization. So, and believe me, there are quite. So, and believe me, they are quite deep for this very simple system. So, as the scriptures, here is the trick. So, we have a few tests in two dimensions, but I'm mostly going to focus on one dimension. So, in one dimension, we use the density. This rho to the power of three is just Thomas-Permi in one dimension. Then the second-order deliberates, and so on, so forth, right? And this is basically. And this is basically something similar to five seconds in one dimension. In three dimensions, we use first, second, third, fourth order deliveries of the ogrammes of the density. And that's for the local descriptors, but we were interested in including non-locality. So, as Paul mentioned, there are beautiful results of the kinetic energy density function. The kinetic energy density functional in terms of moments of the electron density. So, as a non-local description, we use this function here. Oh, sorry. This function here, which is a convoluted version of the moments, the k-moment of the electron density. And I think that for me, the inspiration, well, Paul has more insights on this, but for me, the inspiration in this description is in this. In this description, is in this work by Genesco, where he shows that a very simulated electron delocalization range function, which is a function of that has the same form of the descriptor is the density matrix and something that is a exponential composition of the electron density, recover pictures of non-locality, for instance, for different values of u, which is in the scale parameters that gives you like the range of non-locality. Like the range of node locality around a given point, and you can recover with this electron the localization range pictures like the one of the electron localization function, the gradient of the density, which Juli has used for non-convolution interaction, and so on and so forth. So it seems like there are some electronic structural information in this convoluting moments of the electron density. So in our for the scripture, we use different values of k, which is two. Different values of k, which is different values of different orders of the moments of the electron density, and different values of u, which is the scale of non-locality around a given point. So, yep. So, but we also did a trick, and we didn't use just, for instance, as the top thermosphere in one dimension, we just rescale. Rescale also the descriptor so that they have units of energy, so the same units as the local kinetic energy density. So yeah, these are the non-local descriptors. So we use up to second moment of the density and we multiply this by the correct order of the density so it gives us the right units of energy per volume, right? And we did the same for two dimensions, for actual models. For two dimensions, for actual molecules. So these are the results. So this is we use as a logistic functional RELU. And so this is the architect. This is when we say local is we fit the neural network only with the local descriptors and non-localists we include the non-local descriptors obviously. So the architecture is parametrized evaluation optimization. And here is the Mihana. And here is the mean average error in atomic units, whatever atomic units means for this one-dimensional system. So, is basically half an atomic unit when we fit only the network with local descriptor, or 0.3% of error in the total, the kinetic energy density on average. And when you include non-locality, you reduce that error by a factor of four or five, depending on the system. Four or five, depending on the system you focus. But so non-locality is important to recover or to reduce the error of the prediction. And that can be seen here more easily. So in these plots, you have the prediction versus the labels, the actual non-interacting kinetic energy density of the system, which you can compute with the accuracy you want. UN and so for the semi-local model and for the non-local model. So, one can see is that including the non-locality is reducing the error in the sectors where the kinetic energy density is largest. So, here is a distribution of an histogram of the errors. So, in this sector of the largest error, you reduce them with the null locality. With the null locality. So it seems that in this model, it's key to use no locality to reduce those largest errors. Okay, not just in the average, but in the sector of the larger, you are making the largest error. So this is how the kinetic energy density is fit for every potential. Every looks pretty nice. It can show you a thousand of thousands of these plots. But if one looks But if one looks carefully, there are systems where there are huge errors. So, for instance, for this potential, whatever it means, 0.5 is just the depth of the potential. And four and two electrons. So, I can understand two electrons because even in one dimension, you still have the five seconds exact for two electrons, right? So, what this plot is showing you is that it's hard for the algorithm. Is that it's hard for the algorithm to learn that it must ignore all inputs when the number of electrons is stood. So if the number of electrons is stood, the algorithm should learn to ignore all terms because pi is secure exact. We did not impose that on purpose to see how hard it is for the algorithm to learn that, basically to learn this continuity. But also there are points where you have small numbers of elections and it gets very difficult to learn the continuity. It gets very difficult to learn the kinetic energy density, and it's because, in this system, when you have a small number of electrons, so you are in the bottom of the potential, and you're far from the homogeneous electrograds or regime, which you basically will achieve if you increase the number of electrons, and you are in the upper part of the potential. If you are in the upper part of the potential, you're basically in a box because the electrons are not interacting in this model. And we also And we also test how good is not the local kinetic energy density, but its integration, the kinetic energy, basically. And it's not that bad. Even with the non-local potentials model, you have errors of 3%, which is huge if you extrapolate these two molecules of 0.4 atomic units. Again, I'm not quite sure what atomic units are here because this is the inside of a molecule. Because this is the insight of a molecule is not in my hair for these systems. But you can see here a drastic reduction in the error in the kinetic energy when you include the non-local trends in the network. So that's training the network. So then we wanted to learn a little bit from the machine. So it's machine learning, but you can also learn from the machine a little bit. So what we did was to assess the importance of every description in explaining the result. Scripture in explaining the results of the fitting of the network. So there is this trick that we learn, and is that if you construct a score matrix, which is just the product of the matrix of the weights between layers in your neural network, in your network. So you have the weights between layer L and L minus one, and you multiply all of them. So with that, you can define the relative importance of a feature or the script. Of a feature or the scripture, because here, if you have one output, it's just the output is the kinetic and the local kinetic energy density. This phi would be a vector with as many entries as inputs you have. So, if you take one of the inputs and divide it by the sum of the norm of the other inputs, you will get the relative importance of the descriptor. So, this is based in a method called randomized CY method. So, what a description would be important to explain the results if the relative importance is large. But not only that, you want that the way the future is explaining part of the results, you want it to be statistically significant, that you're not doing by chance, basically. So, what we did is this trick. So, you have Is this trick? So you have your network, or you have your input, your descriptors, you have your labels, your actual values. So if you shuffle randomly the outputs and you retrain the network again, you will get a new model, right? This is a nonsense model somehow. So you can compute the distributions of relative importance for each descriptor. So what I'm showing you here is So, what I'm showing you here is, for instance, for the bisecret term, the second order in one dimension, the analogy. So, no, no, this is bisecker, but the second order in the middle of the second order gradient of the density as an input. So you compute a distribution of the relative importance. So, you want your relative importance to be large, but you also want it to be statistically significant. And you can compute kind of p-value. And you can compute kind of p-value. So basically, you want that your actual value, the one you optimize for your network, be as far as the mean of this randomized or shuffle output distribution. Because if they are too close, then basically what it's saying is that the actual optimized parameters for your network explain as well as the random pick of. As well as the random pick of the value. So you want p is small, which means that this point, the red point, the actual value is far from the mean. And it's interesting to see that Thomas Fermi, which is the first order in the dispensation, seems to be not very significant statistically. And the same happens for Pfisaker. Okay, so this table summarizes this. So you want this RI relative importance. This Ri relative importance to be large, and you want the p-value to be small. And it's interesting to see that Thomas Fermi doesn't have a very large relative importance, and the p-value is large. So it seems not to be significantly statistically at least. And the same for PhiSecre. And it seems that the network is learning from the other descriptors that one would expect not to be that important as this one that at least Pfisaker is a bound. This one that at least five circuit is a bond for two electrons. And one should be careful because the bonds one knows for 3D, they all don't apply on one dimensions. Especially the bonds that you use in which to deduce, you use the Colomb kernel, because the Coulomb kernel in 1D is not the same. So, and here we also could be able to find like the length of the non-locality that. The non-locality that explains most of the results. So, in bold, we have all the inputs of non-local descriptors, and which we consider to be the more important thing in the data. And if you retrain the neural network with only those scriptures which are important or are significant or that have a large relative importance, you will get basically the same results, dropping out all the rest of the scripture. Of the scripture. So that was the story for 1D. And then we wanted to move to actual molecules. And we could do a little bit. So there is this huge GDB713 database that has in the order of 14,000 molecules. And we pick 50 molecules, which contains as a training set. They contain only hydrogen, carbon, nitrogen, and oxygen. Hydrogen and oxygen. And we did the same. We did basically the same procedure we did in 1D, but we did it out in molecules, the Gaussian and all the coding was done in Chem tools, basically to computer descriptors, which is a wonderful code maintained by Fernas. And so this are those, yeah, but this is the distribution of. Yeah, but this is the distribution of the inputs in molecules. So the density, the gradient of the densities, and so on and so forth, the kinetic energy densities. And one can initially see is that even for these molecules, the diversity or the values, the histograms of the values of the descriptors are not as diverse as in the 1D case. So it seems that that 1D case we work on seems to be harder. Work on seems to be harder to have more diversity in the values. So, for molecules, we do the same. We optimize the structure, the architecture of the number of layers and the number of neurons. We use this tangent logistic function as an activation function. And here are the results. So, if you use only local descriptors for this architecture, which is the optimized one, your mean average error is in the order of three atomic units, which Three atomic units, which watching the numbers of poles is not that bad for molecules. And it's important to say that we did not use pseudo-potential. So we just made all electrons calculations and we put the numbers from the all electron calculations in every point in the space, including the core of the atoms in the molecule. And that's an error of around 1% in the non-kinetic, in the non-interacting kinetic energy density. Interacting kinetic energy density. So I would expect this value to be around 1% of the total energy as well. And if you include, if one includes the non-locality, so one reduces this by at least a factor of five, I would say. So again, it's more or less the same factor in reduction of the error as in the one-dimensional case. And there are also the same features. are also the same features so if you have here the semi-local the non-local model and you have the the predicted value the actual value and you have the distribution of the error in the kinetic energy density the non-locality is basically reducing in an order of magnitude or reducing the largest error is taking care of the places where you can you make the the largest error and here are the results for these four molecules not seen in it in the the test set Needed in the test set is only five molecules. So you can see that Thomas Fermi is bad. This must be a mistake, but Thomas Fermit is bad. If you train the non-local model, it proves a little better. I mean, this is the label. This is the con-sham kinetic energy density for dump molecules. So if you include non-local, if you use the non-local value, If you use the non-local value model, you get decent values which improve a lot if you include non-locality, but still they are far one atomic unit from the actual value. They are far, but not that far. Look at this number for this molecule. This is a third, I would say, or half atomic units in error. And there is some, at least, hope with this model, because if one Because if one looks at where one is making the biggest mistake in the prediction in the space, so in here, this plot is just, or this molecule is just the molecular plane. So we're plotting the error as a function of the position. And here in this log scale, you clearly see that the largest error we are making, we are making it in the core of the island. So I think we were too optimistic. We were too optimistic in not using local pseudo-potential to describe the core of the island. So I believe that, or my feeling is that if one uses, as most people do, non-local, local potentials, local pseudo-potentials for the core of the atrons, the error can be drastically reduced. So yeah, that's basically what I want to tell you. Want to tell you the so the summary of the story is that if one is trying to use neural networks to predict the non-local kinetic energy density, one must include non-locality in the model. That in our systems, non-locality reduces the error by a factor of five and even more in the case of the molecules. And that maybe this relative. And that maybe this relative importance and descriptor could be useful to guiding the creation of the du or of analytical expressions for the kinetic energy density. Yeah, so with that, I'm happy to answer your questions.