Sorry. So in this room I'm asked, so how many of you actually had have had a above zero experience in risk prediction modeling work? So how many of you have had experience in risk prediction related work? Oh, okay. So then I'll go a little bit slow then. So, see, I was like following what Tanya talked about yesterday. So, the first part is, okay, so the part about the structure of a compelling research idea. I will try to make it sound compelling. So, the first part is why a problem and it exists, and why do we care? So, this is the first part. So, when we actually develop these prediction models, Devil risk prediction models. A question that routinely we ask is that: so, for a set of predictors, if we put it into the model, are they useful? Are they useful in terms of increasing the performance of the risk prediction? So, then to do this, for our statistical work, so we develop first a base model without those predictors, and then we develop an enriched model with those predictors, and then we compare the performance two models. We compare the performance of two models according to a metric that we are interested in, for example, area and the ROC curve, and the positive predicted value, two positive rate, and so on. So this is a routine task. But when the data we have on the kind of predictor that we are trying to evaluate for added value is not ideal, then the evaluation becomes challenging. So let me actually just use an example to talk about this idea. So breast cancer risk prediction. So breast cancer risk prediction tool, BCRAT, that's actually a standard risk prediction model for predicted the episode risk, accumulated instruction in the survival setting, that a woman with developed breast cancer during a specified age period. So the standard risk predictor include age at first level birth, age at minority, the number of breast biophysis that a woman ever has. Biophysis that a woman ever had representing the health or the breast disease history and the number of first-degree relatives who ever had breast cancer. So these are the standard predictors that INCO has been included in this standard BCRET model. So now we want to see actually whether adding the breath density and the polygenic risk score into the model can increase the predictable accuracy of the model. So breath density, usually Breath density, usually so too commonly used the measurement into the percentage of mammogram density. For those who have familiar mammogram, that's a percentage of dense area on the mammogram image. So also for other image modalities, so they can also measure the volume match density and the dense area and so on. So those are the markers have been of interest. So then the data So then the data we have access to for evaluating the added value of breast density and the polygenic risk for increasing the, for improving the predictive performance of a PCRET come from a PEM medicine dial bank. So the woman, the data we extracted including women, around 11,000 women, whose age is between 40 to 84 years. Between 40 to 84 years. Just a brief history of Pet Medicine Biobank. At some point, around 15 years ago, so we decided to store the blood sample into the, we stored the blood sample and created this panel medicine biolog. So the director of the Biow Bank actually is an established cardiovascular disease researcher. So as a result, so the people, the patients who are included in BioBank, Who were included in Biobank, usually they had all sorts of cardiovascular health. So later, when investigators tried to use the resource from Biobank as an obligation for using the data, they were obligated to recruit the patients, other patients into Biobank. For example, if the investigator does research on rheumatoid arthritis, then the investigator would contribute to more patients who have data condition. So now you can imagine that. you can imagine that so for patients in this or bank so they they are actually have much higher combined comorbidity conditions compared to regular women. So just to clarify, so this model is used for the US general women population. So the goal for us to is to evaluate added value breast identity and the polygenic score for prediction for For prediction, for the prediction in the general US woman population. So, PEM Medicine by Outbank is a heavily biased sample compared to the US woman population. Let's look at the data to show this point. So, just a look at the distribution of the several standard predictors. So, PMBB, Penn Medicine Biobank. So, then the distribution for the US general women population was estimated from national health interviews. From a national health interview survey. So, if you look at the age distribution, the woman in the pamphlet and bio tended to be older, and they tended to have the first live birth later in life or remain than laborers. And then the age and the manner, the distribution was similar, it was similar to Pan Baobank and the general women population. But then, also the Pan Medicine Bio Bank tended to have more. Baobank tended to have more breast disease and they had a strong family history. So, then, because this, so then, if we actually want to use this pan biobank to evaluate the added value of the best density and the polygenic risk for prediction, then the data we have is biased. It does not represent the target population, that i.e., we are in i.e., the US general woman population. US general women population. But our goal is actually to do this unbiased assessment. But so to do this, we needed to unbiased risk estimates by the standard model, as a BCRET, the unbiased model, by the updated model that included the new predictors of interest. So the naive evaluation, the added value with biased data is biased. And so in summary, when we want to do that model updating the evaluation, we must recognize that this. We must recognize the discrepancy between the data we have and the targeted population that we are interested in. So, do you have access to those data sources? I'm getting to that point. Yes, but that's really the key question, actually. Okay. So, so I'm looking at whether I finished the first point. Whether I finished the first point yet. So now we're talking about unbiased risk estimates. So when we have a model, to evaluate unbiased methods, we do it through assessing the model calibration. So one of the most popular approach, of course, is the Holtzman-Lamosha goodness fitter test. So when you have a risk model, so when you have a model, then for everybody in your data, you generate a In your data, you generate a risk score and then you specify the risk interval. With which risk interval, you calculate the expected risk, you actually also calculate the observed risk. So if the O and the E are close across all the risk intervals, you see actually your model is well calibrated. So if your model is well calibrated, then we are confident that our risk estimates actually is unbiased. But when we actually evaluate But when we actually evaluate the model, for the increase in model accuracy, the metrics that we usually look at are the increase in area and the ROC curve, the positive predicted value, and the other metrics. But the problem is, unless we have unbiased risk estimates, estimates for those metrics do not make sense. So for this reason, we want to actually develop aggregate. Develop updated model, including the new predictors we are interested in, but we want to make sure that the updated model generates unbiased risk estimates as quantified by the good calibration. So I finished the first point. Okay, so this is our problem. So this is the general idea of our approach. So now this actually, I'm ready to ask to answer Chibi's question as well. Question as well. So, what do we have? So, the data we have is from PenMedicine Biobank. It's a biased sample. That's the best we can have. And then, what other tools do we have? So, from the targeted population, we do have information. So, the first piece of information is the model itself, the BCRET, the model, the standard model that have been used. So, that model, since that was developed more than 30 years ago, that model has been extensively validated in different cohorts. Validated in different cohorts. So then the model was considered very well calibrated in the U.S. general women population, in the general women population. So that means, so we have a base model that we know well calibrated in the target population. So if you do not have this base model, then we would assume that you have a large data set that includes the basic data that allow you to develop a well-calibrated risk model. Calibrate risk model. And if you have to do it, if you do have to develop from the scratch your base model, then you can literally use any modeling technique. For example, of course, we can use our classical logistic regression model, and we can also use virtual machine learning algorithms and so on. So then, for us, our approach is that we have this based model, and then during the model development, during the development of the new model, include the new risk predictors. We want to actually. Predictors. We want to actually enforce the calibration. We want our new model to have the same calibration standard as the base model. So the base model is well calibrated. And if our new model has a similar characteristic, calibration characteristics as a base model, we would say that our new model is also calibrated. So, of course, there is no free lunch. To do our method, we assume that we have access. We assume that we have access to the distribution of the baseline standard predictors in the targeted population. So, for the breast cancer example, we have access to the National Health Interview Survey data for the form that we estimate those distributions. We just look at that. And we have to assume that the relationship between the new predictor, so breast density and the polygenic score, and the standard predictors is similar between the bias data we have and the target population. We have and the type of population we are interested in. Okay, so just some shorter notation. So we distinguish the study source of population and the targeted population. So the study, so from the study source of population, so we have YSC, we have those predictors, but the data we have from the target population that are against the general women, U.S. women population as an example, we have no. The example, we have a known distribution of standard predictors we call X, and we have a calibrated based model based only on X, again, BCIT model in the example. So then, just an example, then targeted population is the general US women population, women in the US PMBB, that's our source data population. So, or, so as away from best category example, the target population could be just a time medicine whole year. Could be just a panis, the whole EHR. That's a much, much bigger data set than the PMBBA itself. PMBBA as the biofuel is a much smaller subset. So then in the targeted population, the data distribution, probability distribution is different from the source population. And we have a data ID from source population, not from that targeted population. So we do not assume we will. We do not assume we will use the individual level data from the targeted population. Okay. So then we have been working with the logistic regression model. So just to say, okay, so we don't have very aggressive goal to develop a large model. Our goal is simply just to incorporate this new predictor represented by Z into a model, into a logistical regression model. So we do not claim this is a correct model, it's a working model for prediction. Prediction. So, just to summarize the framework, so we assume that the condition distribution of new predictors given the standard predictors are the same between the source and targeted population. And then, but we allow that the condition relationship that we want to develop the outcome variable, we want to predict, give them all the predictors. We don't need this. We recognize this condition. We recognize this condition distribution is different between the source and target population. So we require that the underlying distribution, well, we also allow the underlying distribution of the standard predictor to be different between the two populations. So another, so we, in the end, we enforce a parametrical model for the new predictor, even the standard predictor, using parametric distribution for that. We need that. So then that's the price we have to pay. I have a quick question for these assumptions. So can you give us some ideas why is it reasonable to assume that? To be the same targeted populations for my applications. So this one. Yes. So so that's a very good question. For example, like a example like so why do we have to do that so first we have to that's a part the data does not have that information for example so if we we say the this distribution the new predicted address that he given us then the predictors it could be different between the target and the source of population but because from the target population we do not have any information about the C. So there is no way for us to assess that. So is this reasonable? Is reasonable. So, I would say it really depends on study context. So, in the specific data setting, we really have to think about whether it's reasonable or not. So, now this is the key part of the key idea of our proposal. So, first, let's look at what it means by the base model. It's well calibrated. Model is well calibrated. So, this is how we set up the Hosman-Lamato test statistic, right? So, this is the base model, Swiss baseline risk interval. So, this is the expected risk based on this model for people who risk for into this risk interval is a probability under this model, the y given one, the probability of being a case given that. Being a case, given that a woman's risk, a person's risk falls into this risk interval. So this can be computed, of course, from this model. And so this is the E. So if you have access to a data set, unbiased data set, then this is the observed risk. Then the total number of observed number within this risk interval, the total events within this risk interval, the ratio of exactly the observed risk. So we look at, we've We first specify all these risky intervals. So we compute O and the E within all the risky intervals and then summarize the OE values together to see that whether O is sufficiently close to E. E is sufficiently close to O. So if they are close, we see that this model is well calibrated. So we know that our condition is that we know this base model is only X, is well calibrated. O and E are sufficiently closed. O and E are sufficiently close. So now comes to our idea. So if we were able to get a biased data set from the target population, of course, we can do the same thing. But the problem is we do not do this. So I did not define this notation, this X T. So safe space is X, both X, and C is expanded. So based on the new model, we would not be able to do this kind of OER. Be able to do this kind of OE assessment. So, since we cannot do this kind of validation, so our idea is that let's enforce the calibration during the model building stage. So, at the time of model building, we already enforce the calibration so that later we don't have to worry about the unbiased risk of risk prediction. So, then since we have a base model, we know that actually base model generates risk that actually unbiased. So, now during the model development process, let's actually match. Process, let's actually match the two sets of risk, I think, by the original model and the updated model. So, by matching, the matching mathematically, this is how we express it. So, that the expected risk from this new model, the difference between the expected risk, the difference between the expected risk of the new model and the older model, the difference is actually small. Half small, actually, we want. Half small, actually, we want to be is again up to us, the user's judgment. If we set the limit very large, that means the constraint does not have any effect. So, if we actually let the constraint to be, this limit to be very small, then we actually trust this original model more so that we want to rely on this base model more to actually enforce the calibration RNU model. So, this is up to user deferred. So, this is up to user definition. So, then we have depending on how many risk intervals you will create, then we have that many constraints for the model development. So, then now this is a final core part. So, again, now I talked about why and the background. So, opportunity space. So, now actually, according to Tanya, we will. So according to Tanya, we will go to results. But here, with the space, actually, of course, I want to add one item to find the right collaboration. So in my case, the right collaboration is actually yeah. So at that point, so I was stuck. I thought, okay, so now this is what we want to do. How do we do it? And so now actually, so this is what Yang told us, this is how we did it. So this is a likelihood function for our data. Likelihood function for our data, observe the source individual data. So this is just standard likelihood function for our distribution model, IED data. So to actually enforce this inequality constraint, we actually use this technique called select parameters to convert it into two equality constraints. Then we can continue to our optimization. So I won't go into details. So the rest is that. Details. So the result is that we maximize this likelihood function under these constraints. So the estimates we get is the estimate we get would be what we want. So then just a toy example to show how this constraint actually works. So let's say we don't even have C, we just find a figure A simple distribution model with one predictor in it. Then we have two parameters. Then we have two parameters: the intercept parameter, and this one single logo ratio parameter. So, this blue is the parameter space for one, and this blue is a parameter space in the target population, and this yellow, this orange is a parameter space in the source population. So, if you see X wave, the number. So if you see actually if the naive estimates ignoring this kind of bias something, the estimate would be somewhere here, and then the true value in the target population would be somewhere here. So then through the constraint, we were able to pull this estimate closer to the true value in the targeted population. So because of that, we ended up having a model that helped improve the calibration. So again, I will just be. So again, I will just be quick. So we showed that in the end. So in the end, the parameter, what do we get after it? So actually, the parameter value we get, not just sample-wise, converged to an estimate. That estimate actually minimizes the callback label information between these two populations, the target and the source population. How much time do I have? Time do I have? So, four, so three. Oh, okay, good. So, so then just think about what we get after this constraint maximization. So, the first scenario, let's say the first scenario, you got a lucky. So, the source of population actually is identical to the target population. So, what do we, so in this situation, so intuitively, the estimate we get should actually converge to the parameter values in the target population. Values in the target population. So that's indeed the case. So, but through the constraint, because through the constraint, we are assuming we know a little bit more about the data. So, we will get the constraint as in as we get, we will have a smaller standard error than just simply analyze the data in the standard way. But of course, like this would be a good proof-of-concept scenario. And in reality, we wouldn't bother to actually do the constraint maximization just for the sake of increase. Just for the sake of increasing the statistical efficiency, I would say. So, the second scenario, of course, is the case scenario that we are interested in. The targeted population and the source of populations do differ. So, in this scenario, then the parameter items we get are ultimately converged to a value that minimizes the co-backer labeler information criterion among all the parameters that satisfy the proposed constraints. Constructs. And so, in the end, through the minimization, we were able to actually really make a use of the IID data from the biased source population. So, after faking this model, and we are actually able to assess the added value by looking at the increase in the criteria of the interest, for example, increase in the area and the ROC curve, and increase in the Curve and increasing in the 2 positive degree value, and so on. Just this is because, like, so with this value, with a known distribution from the targeted population, with the estimated conditional distribution of the new condition predictors and the model that we fit. So, now with all these elements, we can actually get the estimate of the full risk distribution. So, the risk distribution, the distribution of this risk in the Distribution of its risk in the targeted population. So, with this, literally, we can calculate any summary quantity we want for this model. And then just now, just a summarization of summary of what we do use this constraint maximum likelihood method when we analyze the data. So, first, from the targeted population of interest, we obtain the distribution for the standard predictors, and then we actually make sure. And then we actually make sure that we have this, of course, we need this calibrated base model that based on standard predictors. And then to implement this constraint of MLE, we need to define our constraint. So first we will want to actually decide the risk intervals, how we are going to really cut the risk whole risk interval into small risk intervals. interval into small risk intervals so that actually within those smaller risk intervals we want to inform we will enforce the calibration. So in a minute through example I will show you how to do that and then within each interval we will calculate expected risk based on the standard predictors and then we also need to specify a parametric model for the new predictors we are interested in. So with all this and through the optimization, then we can actually fit the model. Can actually fit in the model. So now go back to our example of breast cancer risk prediction. So the outcome is breast cancer within a five-year period so that we can just look at 0, 1, whether actually a woman had breast cancer or not. So the stat predictors, age item, first letter of birth, magnetic, number of previous breast biophysis, number of first-degree relatives with breast cancer. So in the original Breast cancer. So, in the original PCIT model, everything was categorized. So, we just followed the same convention, categorized everything. So, this C, we just look at one single predictor, it's a percentage of mammographic breast density. And the distribution of these standard predictors came from the National Health Interview Survey, as I showed earlier. And the base model was that of breast cancer risk assessment to PCIAG. And then we fit out the distribution. And then we fit the distribution model for y given all the predictors. And then for the percentage memory graphic density, because it's heavily enriched for zero values, so in the end we used truncated a lot of normal distribution for this. So for this distribution per C, we did do a lot of model checking, comparing different options. Finally, we found this actually, this time period of normal distribution had a best fit. But going back to Thibaut's original question, To Tibo's original question, whether we feel this actually, this model, after fitting this model, whether we ended up having something that's actually consistent with the U.S. gender woman population, we are not able to check. So then this for the data, so the pen data in the end, so we actually considered the pen data I talked about earlier, but just to show, to check how well our To check how well our method performs, we actually created a sub-sample from this whole sample just to make the data more biased, to see actually with even more biased data, whether our way of fitting the model would actually be similar, whether our method would be similar in exactly. So then, for risk interval, for creating risk interval to enforce the constraints, we consider the two approaches. Considered two approaches. So, one approach is actually we just put the Mr. Cattle point in the quantiles, in the four quantiles. The other one, so when we use these models in the healthcare setting, usually we want to use them to identify women who have a high risk of breast cancer. So, as a result, the high-risk region is of the most highest importance. So, because that we actually look at the option, what if actually we put more constraints in the high-risk region? Put up more constraints in the high-risk region so that actually we ensure the calibration in the high-risk region is better because that's the region that we are using. So, this we call CMOL1, that we equally space the risk cutter of points, or we actually put the risk cutter points actually more intensely in the high-risk region. Okay, so here is the result. So, now you fitting, just a standard literature version on the data directed fitting, and then And then this so this is the R method with the first constraint, this actually the CMRE2, the R method using the highest region constraints. So if you see actually our constraints actually are different from 9U 15, so the same actually happen also with a further bias of sample. Our estimates are different. But the question is actually the difference actually. Actually, does difference actually mean anything better? So, this is how we look at that. So, we look at okay, so does our model generate better risk estimates if we look at a subgroup of women defined by two strong predictors, number of graphic hypotheses and the number of first-degree relatives. So, then this PCRET again is our standard model, is our model. Model is our model, it's our template model to look at. So, these risk estimates are presumably unbiased. So, if you look at a naive fitting, when we use the whole sample size, a whole sample, so there is some difference. So, usually the naive fitting, the expected values are much larger, can be larger than the standard values. So, these models are not calibrated. So, our method. Calibrate. So, our method actually, the estimates are generally closer to the standard template estimates. So, if we look at the sub-sample, this is further biased. So, actually, the advantage of our method actually is more clear. So, the naive fitting, those estimates, actually, the number of predicted risks, the number of predicted predicted cases actually. Predicted cases actually in the PEM or bank sample is much larger than the template prediction. So, using our approach, the atoms actually are much closer. So, that says actually our model is much better calibrated. Sorry, I have a question. So, you might have talked about this earlier, but in the UFEN data, did you only have Caucasian women or did you have a mix? Mixed, but in this example, we only worked with Caucasian women. Use the second sample, which only worked with Caucasian. Okay, so like the estimation and then the application, everything only caucus. Okay, yes, thank you. So then this information types of cancer when it was looked down or compared to the prediction, that's, I think, that that must be the important one. Yeah, absolutely right. So for Right, so for currently we look at the composite and the points. So we include both invasive breast cancer and ductocasinoma in side tube. But so we did not look at the breast cancer subtypes. So basically, at this point, I don't think there is a commonly accepted model yet for predicting the risk of breast cancer subtypes, like you said. Risk is good influencer. This could be different, but uh you're absolutely right. And maybe, you know, with just level, you're just you know, some of them are predicted, you know, under predicted, and some of them are overpredicted, right? So, so, of course, also like a different type of breast cancer subtypes. So, so, so, just as a single factor, they have different predictive values called different breast cancer subtypes. So, we have a Types. So we have my collaborator, one of my collaborators have some data on that. But the actual model that can be usable for clinical counseling is not there yet. So then this is some additional data to show the effectiveness of our method for correcting the bias in risk estimates. So this is the x-axis actually. The x-axis actually expects risk from our model. So, this y-axis expects risk from the base model. So, this base model is actually give us a bias risk estimates. So, we expect if our model works well, so all the points should fall onto this diagonal line. So, the further away, the worse the naive model, the fitted model would perform. So, then without this one, so look at this. So look at the basic one. So naive thinking of the model, now actually the risk estimates are not close to their basic predictors. Yes. So then our method, so after the correction, so all the points actually are getting much closer to the diagonal line. Okay. So then this is just some quick simulation data. In the first scenario, again, proof of concept that the target population, the source of That the target population, the source of population are identical. So, in this population, in this setting, we expect that our method produces generate unbiased risk guidance. Yes, that's exactly the case. And the standard errors are smaller than that. So, this also gives us some confidence about our method. So, then I have two scenarios. These are two scenarios is actually about Actually, about so the source of population is different from the targeted population. To actually set that up, we actually first want the distribution parameter, target and source of population, intercept parameters different, or both the intercept parameter and the altitude parameter are different. So, in both situations, so our estimates actually are different than the naive estimates. So, again, look at the figure also. Again, look at the figure. Also, the figure now shows whether the difference means something actually better. So, then compare all the figures. So, in this first setting, in this setting, so this is standard fitting. So, this point is away from horizontal line. Our method was able to pull it back. And so, also the same. R method is pulling it back. And the standard method actually people buy styles, right? So, all the data shows that actually our method of developing the calibrated risk prediction model actually works well. So, it does give us daily estimates that are closer than what we would expect when we have better data from the target population. Okay, so then just a quick conclusion. So, our method has been effective. And so, but one question we were not able to really address theoretically is that. Theoretically, that so we see the source and the target population actually can be different, but we were not able to quantify how far they can be in order for our method to be still useful. So, if you think about the integral breast cancer example, for example, so if I want to, if the data come from Asian women, so now my target population is still US woman population, and then can I still do that? So, it's a total population, are the two populations too far? So, I would say it's too far, but in we are. So I wouldn't say it's true, but in reality, I wouldn't say this rarely happens. I would say the investigators always try to get the best data possible they can get and to do the work. So hopefully the source and the target population will not be too fast. But our method is still useful. So then this method I can see a lot of applications. So in a setting that I work a lot on, it's like the electronic health record. So now actually I have to recognize. Now actually, it has recognized that very often for interns physical modeling, very often we need to actually get the data outside of EHR. So then it requires effort so that the data from external sources outside the EHR is very often limited. So to combine a limited data source with a large amount of EHR data, that actually the the two sources also can be different. So this is a method that can be applied. So this is a method that can be applied really to generate useful models for the population that we are interested in. So that's it. So this is a work that was led by a former student in my group. And then this actually the method of development I heavily relied on Yan Yang to actually work with us on this. Thank you very much. Time for one or two questions. Great time. I have a similar weird setting where I have like a nuisance model.