Same is here. Uh so that doesn't work just usual keyboard I suppose. Ah. Yes, good. All right. Ah, okay, so what's the setting? So we are in the standard control difference agent setting. Okay, so you have you know the X Y pin graph paths, F s F sufficient smooth so that every everything makes sense. Everything makes sense. So, what do I mean by impress problem? Well, in general, we want to extract information about the control from observations about the response. So, one case where this has been done is by Ismail and Yosa. So, they looked at solutions, so response to different listing conditions. So, several response and try to reconstruct X, but X together with higher But X together with higher level signature for X. So what I'm looking at is quite, it's much more straightforward. It has some sense. I just want to, you know, given Y, I want to know X in path space. So if I could have continuous Y, then in some ways this problem is trivial, trivial or impossible, trivial when this is invertible, the atom map is invertible, basically. map is invertible, basically the inversion map becomes an integration. So from continuous y, you can get x exactly. But of course this is the real world. We never have continuous observations. So really the case I'm looking at is a case where you observe the response on a partition of the interval 0t and you want to reconstruct the path x. Again, I mean, if you just leave it Again, I mean, if you just leave it at that, find X, it's not unique. There are many paths, X, that can drive the system and have responses going through the observations. So I want this uniqueness, so I'm going to impose some further conditions. So I'm going to say, okay, I want to find a piecewise linear path X on the same partition. I'm saying it a bit hesitantly because, well, being the same partition will really depend on X. Partition will really depend on F, whether you have this matching degrees of freedom. But let's say that M and D are the same, F is invertible, so then it's really on the same partition. And yeah, so this is the problem. So you have the observations of the partition, you want to find the piecewise lemopath on the same partition. Ideally, what one would like to What one would like to do is actually find the piecewise linear random path, possibly, on an arbitrary fine partition. And then, you know, this is with the idea that actually X is not piecewise linear, it's a stochastic process. So when I'm looking for this piecewise linear path, it's only an approximation. But for this talk, I'll just stick to this very simple problem of trying to find this path, piecewise linear path XT. This path, piecewise linear path XT. So, why do I care about that? Alright, the first is, you know, okay, you observe, sometimes you want more information about, you know, the control, so you don't know much, so what you do is you reconstruct the control and study the properties. But my real motivation is computing the likelihood of the observations. And the reason is: so, ideally, I want to compute the likelihood of the discrete observations given a model. Given the model, and that's for any general stochastic process X. At the moment, think of it not as any stochastic process, a random piecewise linear path X. And I know the distribution, but it's very hard to go from the distribution of X to the distribution of Y and work with this directly. So, what I do instead is say, okay, let me try to, assuming I know the model, let me try to find the piecewise the realization of the piecewise. The piecewise the realization of the piecewise linear path X that would be driving the noise if these were indeed my observations and the model. And so this is the inverse, the solution to the inverse problem. And then because I know the distribution of that, and I can write down the likelihood here, times of course you have to have this Jacobian correction. So if I could have this piecewise lining path, the solution I was able to I was able to construct the solution of the p-square of the inverse problem, then I could write down the likelihood. Of course, in reality, well, the x is not going to be a random piecewise in the path, it's going to be some sort of stochastic process. So this is not the quality, it's an approximation. So I approximate the likelihood using this method. Again, assuming that this is a piecewise driven pathway. That this is a piecewise path, but this is an approximation. But I can control the distance, the error, approximation error, because under certain conditions the likelihood, you can show that the likelihood is going to be continuous in the p-variation topology. So as long as my piecewise linear path, so the piecewise linear interpolations of x converge in p-variation, then I can control this error. So I get a generic variation. So I get a generic, very generic, very general way of approximating the likelihood by going through x. By solving first for x, I know the distribution, so I can. So here you're that you have a decent expression to the right-hand side. It's fine because everything is discrete. So everything is, you know, everything is discretized, everything is discrete. There are issues when you take the limit, but leave it. But leave it aside for now. So everything is finite-dimensional. Here. So this is Jacobian, is the derivative with respect to the y's. Oh, right, it's the partition. So so so the L the first thing we do is the path. In the first argument is the path X. So I invert the so I construct the piecewise linear path, yes. Flag the piecewise linear path, yes. Then the one that you're conditioning on is the derivative of that path. So I'm conditioning here, F, so the conditioning, the information about the model comes into the inversito map. I use it to reconstruct the path. Okay, so this is the idea. Okay, so this is Okay, so this is just trying to imagine other cases where you might want to reconstruct X. It could be that you want to learn a system, but you don't know the input either, but you can apply the input to a known system, then you can use the output of a known system to go back to the input and use input and output to know the systems. But really, my motivation is this, is construct the likelihood. This is all discrete, it's finite-dimensional. I can write that when you actually use When you actually use that in the limit, yes, you have to be careful and normalize appropriately so that things make sense. And would you like to reconstruct your X thanks to this likely good expression? That's my main motivation, but it sort of takes a life of its own. So, yeah. Okay, so it's actually a very simple problem. So, you know, in each segment, you have a In each segment, you have now the x is linear. So, really, reconstructing the piecewise linear path is equivalent to finding all the gradients and all the segments. In each segment, you replace it, so you have an ODE. ODE, you know, the gradient comes here, so initial conditions is first observation, the last observation is the next observation, is the final initial condition. So, you can compute all these gradients by just solving this system. By just solving this system. So, this is your generic ODE. If we assume we know the solution, capital F, which depends on initial conditions and C, we can solve this system with respect to CK, and that gives us the gradients, and then we are done. Okay, I'm going to go through this quite quickly, so this is just about uniqueness. You can show that this is going to be unique if the rank is basically. The rung is well basically x and y have the same dimension and f is invertible. You know, you can generalize, it could be that you have a drift, then you don't, like x, that's not the problem. Again, you know, you just subtract it and then you want to find that x. So you can handle cases where coordinates of x are known. Or if indeed the x and y have different dimensions, you know, it's an imposed assumption that x is a piece. Assumption that x is a piecewise trigger on that partition, you can play with the partition to match the degrees of freedom you get uniqueness. So, there isn't, I mean, the main thing here is to that you can get uniqueness by playing around with this. All right, so we assume existence and uniqueness to that system. It's all about solving the system to get the gradients from the gradients to the piecewise path. The thing, of course, is that you never know the solution. You rarely know an analytic solution to this system, so you have to have approximations. The most well-known approximation for computing, solving an equation is the Newton Rafson. It could be anything. So Newton Rafson just gives you this sequence of numbers. Each time you update, and then eventually, quite fast, you converge to the solution of the system. And we also know that these errors. And we also know that these errors, so the the error in the computation of the gradient is going to disappear very fast. So and if Newton Russian doesn't work, there are many algorithms out there and you use something that converges. So it all looks fine, except now we have two, I mean, if you think back of the initial reason for doing that, which was to compute the likelihood, there was already this hidden assumption that I want to take the limit with respect. I want to take the limit with respect to delta. So now I have another limit with respect to n, and to be able to control both these errors, one, the approximation of the likelihood, and the other, the approximation of the solution to the system, I really need the solution to that to convert uniformly with respect to the Lso, the other thing is, I don't, I mean, I said, okay, if you find the gradient C, then you have found that your piece was. And C, then you have found that your piecewise linear path, but it's not the error on the C that you are interested in, it's the error on the paths. So for each, so at some point you stop, you have your approximate solution Cn, then you construct the path XTN, which is piecewise linear, and that's your best guess for your piecewise linear control. Control. So, what happens when you don't look at the errors for C, but the pathways error is: yes, in the first segment, you've done very well computing your C, so you are very close. Similarly, the gradients are all very well approximated, but each time, because of the continuity of X, you start a bit off. So, initial conditions change because you have to move things around. And these little errors accumulate. So, when now you look at the So, when now you look at the error in terms of p-variation of your approximate path, it's all pieceways linear, it's all very easy to handle. But now it's the p-variation error of the approximate path, pieceways linear path and the actual one, you get that actually with pretty high probability it can explode. So, this is not good enough. Especially when we want actually to have this error converging to zero uniformly with respect to delta. So, this is what you want. So, you want to find a different approximation scheme that converges, where the p-variation distance between your approximate piecewise linear path and the actual one converges to zero uniformly in depth. And the issue why the previous method didn't work is not Newton rational, is the fact that we were just looking at each segment separately. It's the fact that it's Separately, it's the fact that it's localized. So, to avoid this issue and achieve this, we really have to work on the path space. So, I restate now the problem in the path space. So, okay, my control differential equations is a map. It's a little map between rough paths. I want to find this inverse little map. And really, the inverse problem becomes finding. Becomes finding, you can restate it as finding a pair of paths x, y, on the whole interval, that satisfy these three conditions. So y is the response to x through the rhythm map, x is piecewise linear on the partition, and y on the partition takes the values of the observations. It's very easy to find a pair that satisfies any two of the conditions. Say, you know, you take a piecewise linear, okay. Okay, take these two. Yeah, any piecewise in a path, compute y as the response. You have two of them. But it's not easy to have all three conditions satisfied. Actually, there will be a single, it's going to end the only there will be a single pair that will have this property. So I've stated now the problem in I've stated now the problem in terms of the paths, and I want to go a step further and state the same problem in terms of the signatures. So I want to find a pair of path signatures that satisfy the following conditions. So now I'm matching this. So the first one just says, okay, one is the date of response. So y is the response to x. You can write that in terms of the. That in terms of the signatures, that's fine. Now, we have these two conditions, and we keep saying that the signature is invariant to time parametrization, and that's true. But actually, in the sense of matching events in time, this is quite sensitive. So the piecewise linear. So the piecewise linear path on, when we say piecewise linear path on T, it could be, I mean, I can reparameterize the partition, but the important thing is that between the points of the observations, whatever the times p are, x has to be linear. So there is this coupling of information that's local. It's it's not exactly specific times, but actually it's it's it's i local information about events. Local information about events. So the way you can translate that is: okay, so X is piecewise linear on D, and I really want it to be piecewise linear, so linear on these segments that correspond to the observations. So the way I state of it, okay, the logarithm of the signature on the segment is zero everywhere for all levels except the first one. So that's a linear path. So it's fully characterized. So it's fully characterized by the increment. And similarly, y going through the observations as stated as the logarithm of the signature of y, the increment is actually agreeing with the difference in the observations. So I have a way now to state my inverse problem in terms of the signatures. And it's global in the sense that it reaches the whole path at the same time, so it avoids this problem of trying to solve the problem. Of trying to solve for the segments locally. But at the same time, it's local in the sense that it can keep this local information about how the events coupled very well. And this is really a nice thing about signature, that it can keep both global information about the path, but also local in that sense. I mean, if you were to say, go through Fourier transfer, you would lose that you can do good. Do it. Okay, so based on that, the idea is now to create this algorithm where every time, so you know, you start with a pair of signatures that satisfy two of the conditions, every time you correct for the third condition. And then of course by correcting the third condition you're doing something else. But you keep going through this loop, and each time you correct, you correct as little as possible. Or let as little as possible. So you get the following mark. I'll call it. So let's say you just, I mean, initially, so initial path has to go through observation, just take a piecewise interpolation of your points. So that's the path going through observations. Then you have to have a, so you have to have that map relationship. So you just invert. So this is continuous. Continuous, it's a continuous path, I mean vertically by integration, you get another continuous path, x, and now y and xn satisfy two of the conditions. But this is not going to be piecewise linear. You want to change this path as little as possible in the signature so that it is piecewise linear. So it has to have the property that each segment, the log signature is zero everywhere except for the first letter. Zero everywhere except for the first level. So, of course, the obvious thing is to take the log signature on each segment, make it zero everywhere, keep the same first level. So, what you are really doing is just piecewise linear decolution. So, you go from this xn to x hat n, and now x hat n and yn satisfy again two of the conditions. This goes to observations, this is piecewise linear, but they don't have the one is not the response to the other. One is not the response to the other. So you have to take again the ITO map here to get a new path. That's the response of your x tilde n. So now again, you have this pair satisfies two of the conditions. This is piecewise linear. This is the response, but this will not necessarily go through the observations. How do you make it go through the observations by changing the signature as little as possible? Well, what you do is you just add QG. is you just add, you manipulate time and you add a tree-like connection to the observations. So that doesn't change the global signature, but it is going to change the signature locally so that it satisfies these local conditions. So just to go through the algorithm kind of once again sketch away, so you start with a piecewise during the path, you integrate to get an X. To get an x, these points, so you have to have these points matched to this. This is where y corresponds to the observations. So when you linearize, it has to be up to that point. And I've done it against time, it didn't have to be time. So you take the piecewise linear approximation, then again you take the eto map, that gives you another y. This y is not going to go through the observations. To go through the observations. What you do is you add these three-like connections, so now it does go through the observations. Okay, you lose the condition that one is the response of the other, so you take the inverse Ito map. And what happens here is actually the Ito and the inverse Ito are going to cancel locally except for the three line bits. So this piecewise linear bit you get is exactly the same. You get is exactly the same as this, but you get this extra again tree-like things as inverse map, ito map of the tree-like connections. So now this is your new X. So in some sense, you're comparing the solution of your equation with the Yes. So you yeah, so you you solve each time, you compare to the solution. Compare to the solution, then you match it, then you go back to the path, you linearize it. So you keep doing that. So here again, you want the piecewise linear path, but it's, as I said, so you are matching now this point, the point where your y goes through the observation, to this point. So the piecewise linear interpolation has to go through through that. I think the cover markets a piece. Okay, I don't know. So you keep going like that, and the hope is that you converge. And you know, it looks pretty messy, but actually, it comes down to something quite simple. You can express in terms of how you update the gradients. So C are the gradients. And okay, so this is the initial step where you get. The initial step where you get them by integrating the piecewise linear interpolation. And this is how you correct. And it's these little bits that I said here. So you linearize and get these little things. And let me go to the next slide. So if you were to compare it to Newton-Russon, you would get this sort of correction. With this, you get this sort of correction. Well, you both add a correction. Both add a correction for the end point and subtract the previous correction from the previous point. And this is important because intuitively, if I was to think straight from the, you know, from the end on the path and say, okay, how am I going to do that so that it's also x is continuous? I would have said, okay, I'm going to find C, and then this is what happens here. The next correction, I'm going to start using not the real observation, but what would have Observation, but what would have been the observation based on my correction so far? Yeah, five minutes. Five minutes. Okay, that's fine. But what's really logical, and I would not have thought of it intuitively, is that you also take away this. And this is really because of the tree-like correction. So not only you add this correction at the end, but you take away the correction from the starting point. And we keep saying that basically. And you know, we keep saying that basically we treat the tree-like the tree-like excursions as nuisance, but actually, there could be cases where they can be very useful. You can add them without altering the signature, but they can give you local properties for the path that you actually want. So, okay, so this is the signature, and almost done. So, here's some simulations. It's some emission. It's some initial numerical experiments, but we need a bit more investigation. So, this is the model. It has a drift, so it's really one-dimensional. We're trying to solve for x here, parameters. So, x is random, but it's bounded variation. So, these are done by a collaborator with Joe Cop Manchester. And he computed the one variation and two variation between the piecewise linear approximation to the inverse. Approximation to the inverse control and the actual control, which is piecemeal linear. And this is what you get when, so this is a number of iterations, this is what you get with neutral graphs, and this is very much because all these little small mistakes accumulate. This is what you get with this. And this is the same for the two variation, it does a bit better. To variation, it does a bit better because it's bound to variation, so things scale differently. But again, it's almost hard to believe how fast this converges. Not hard to believe it, but actually we also have a theorem. So it is possible to prove under assumptions that you get this convergence in B-variation uniform with respect to delta. So the idea of the proof, I'm almost done, is so Almost done. Usually, in these sort of things, you want to create some sort of contraction. Only in this case, it's two steps, which makes it a bit more complicated. So, you have to estimate first how the correction error evolves in each of the steps. So, how correction error in X depends on the previous correction error, correction in Y, and then how the correction in Y dependent on the previous correction in X. And then finally, you get And then finally you get how correction in well going to be Y at the set of points dependent on the previous correction in Y. And this gives you not exactly contraction, but you can get the contraction by then getting the the inquiry. How am I doing time? Two minutes, okay, I can do that. Two minutes, okay, I can do that. Okay, so quickly, so, okay. So, correction in X, you know, it's this little, these two bits I said about. So, this is actually integration and it's this straightforward. You can write down the correction in X in terms of the corrections in Y. And this is really the initial condition, and this is the one in the last segment. Correction in Y is Is these two little bits, but you can also think of them as the path that goes through the observations minus the path that doesn't. That was the solution to the previous case. And that's actually quite helpful because now you have a difference in beto maps and you extend them and again you can get an estimate that looks like that. So this, so the correction segment k for n plus 1 is actually going For n plus 1 is actually going to depend on all accumulative corrections at that iteration up to k minus 1 and then the x correction in the last segment. So this is really because of the difference in initial conditions. And then you can use this estimate to get the bound for each of the corrections k in terms of again all the accumulated errors and the one coming from the previous operation and then from And then from that you can get a bound for the p-variation by using some sort of induction on the integral, so 0 to m delta r to capital M. Alright, so I am done. And really the message here, there are two messages and a challenge. The message is what I said before is that, you know, the three line corrections, there are instances where they can actually be a very powerful tool when we want to actually Want to actually not change the sequence route, but impose some sort of local properties to your path. And the challenge is, so this is a proof assuming, so going back to path, so the whole conception was with the signature space, but and then interpreted to piecewise linear paths. In theory, it doesn't have to be piecewise linear paths, it could be anything. And what I would really like to show is that the convergence Is that the convergence of the algorithm in terms of the signatures? Yeah, that makes me. Thank you.