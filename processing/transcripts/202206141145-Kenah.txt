Yeah, so it was very exciting to see the interesting. There we go. So it's really interesting to see the forecasting work today. I'm going to talk about something that I think is a major gap in the response to COVID-19 and possibly to future epidemics. And that's the role of household studies and high-resolution studies of the transmission of infectious diseases. Transmission of infectious diseases. So I'm going to whine about methods currently in use and then recommend other methods or methods to supplement the methods that have played a prominent role in the COVID-19 pandemic. So if you look at the scientific literature or even the popular press, so epidemic curves in RT have played a major role. So much of the analysis of the COVID-19 pandemic has been based on time series of incidents. On time series of incidents of incident cases in a region or a nation. And these products, these data are widely available, so it's natural for people to gravitate towards using them. And they're often used to estimate a generation interval distribution or an effective reproduction number based on a generation interval distribution. Oh, sure. So almost all of the methods or variations of a Walling Methods are variations of a Walling and Tunis paper from 2004 or a Corietal paper from 2013. And the generation, I'm not sure why, but the generation intervals are often assumed to have a gamma distribution where the parameters have been estimated from known transmission pairs. So there are several problems with this. So these approaches generally always assume mass action. There's no reason to think that known transmission pairs are representative of all transmission pairs or that the generation interval distribution is unaffected. The generation angible distribution is unaffected by characteristics of the pair. So, in a pair ij could be characteristics of i, characteristics of j are sort of pairwise characteristics. Typically, even in principle, only the forward and backward generation interval distributions can be measured, but what we actually need for these methods to work is something called the intrinsic generation interval distribution. So, there's a mismatch between what we need and what we can measure. There's reporting delays, changing availability of testing, et cetera, that complicate the relationship between the incidence curve that we see. Between the incidence curve that we see and the true incidence of infection. And then it's difficult to incorporate heterogeneity and susceptibility, and it's even more difficult to incorporate heterogeneity and infectiousness. And even when this is possible, most approaches just assume homogeneity. So another thing that's played a prominent role are agent-based and other complex models. Many of them are adapted from influenza models, and these can encode much more realistic assumptions about human behavior. You can go to school, you can go to work. Human behavior. You can go to school, you can go to work, you can do other complex things. They have many parameters. They typically do not generate a likelihood that can be used to fit them to data and account rigorously for uncertainty. And I think it's really important. So my PhD is in epidemiology, and I've been thinking a lot recently about how that's sort of rare in infection. And I think there's a perspective from regular epidemiology that's very valuable. Regular epidemiology that's very valuable here. So, and that is a model can fit data extremely well, but not accurately represent the effect of an intervention. Those are two different things. And I think the complexity of infectious disease epidemiology can sort of cloud these issues. So, it's good to have an example from regular epidemiology. So, these agent-based models are often used to estimate or assess the impact of an intervention through scenario modeling. We're doing cognitive. Scenario modeling, we're doing causal inference in that case. And we're subject to the laws of probability and statistics and causal inference. So, for example, you can accurately predict that someone who consumes a lot of beta-carotene has a low risk of cancer. That is a true prediction, and that's perfectly good as a prediction. So, there was so much evidence, and there was such a plausible biological mechanism. If you look at your Hill criteria. If you look at your Hill criteria, every box was checked for beta-carotene preventing cancer. So they did a trial. These were high-risk, I think they were men, I'm not entirely sure, but these were high-risk individuals for lung cancer. And they did a trial, a randomized trial of beta-carotene supplementation called the Carrot trial. It had to be halted in 1996 because the individuals on the supplement had so much more cancer than the people randomized to placebo. The people randomized to placebo. So, any machine learning algorithm unleashed upon the data in 1981 would have come to the conclusion that consuming beta-carotene reduces your risk of cancer. It does not, in fact, happen. So, this is, you know, this is a very much simpler than an infectious disease situation, but you've got the same thing. You can have a model that fits the data beautifully and then does not tell you what happens when you do. Tell you what happens when you do an intervention. So, estimating the effect of an intervention requires control of confounding and selection bias. It does not matter what kind of model you use, it requires that. That requires measurement of covariates and then stratification, regression, weighting, et cetera. So now, I fought the law and the law won. So, infectious disease data, it's only about infected people. We don't have person time. We don't have person time, you know, we don't have denominators. We don't have person time at risk of infection. So, if you think about the two most basic measures of disease occurrence from epidemiology, you can't calculate them. So, risk requires a denominator, the number of individuals at risk. An incidence rate requires person time at risk. We can't calculate these at any scale with the data that's typically used in infectious disease epidemiology. So, we have no denominators, we have no person time at risk. It can be difficult or impossible to interpret associations between individual characteristics and the onset of infection. So, if I look at my line list of infected people and I see some characteristic that's predominant, does that mean that those individuals are more susceptible to infection? Or does it mean that more individuals with that characteristic were exposed to infection? You can't tell. You just can't. It doesn't matter what you do to. It doesn't matter what you do to model that data. This is information that you don't have. So, the problem gets worse when you consider infectiousness, and susceptibility is much easier to handle than infectiousness. And then, if you add in subclinical infections who might not even make it into the data set and other considerations that are really vital to the construction of accurate models of transmission. So, and then this another thing: so, the type of model you use really doesn't matter here. There are things you can't say. You can't say. And without adequate control of confounding and selection data, bigger data can just be a faster ticket to nowhere. So the size of the data set does not necessarily help you. So as in the rest of epidemiology, there are things that have to be measured no matter what model you use. And I think that's a perspective that is often missing from infectious disease modeling. So, and I don't want to disparage anybody else's work. So I'll disparage my own and give some examples. And give some examples. So, this is, just as a LARC, I had likelihoods for data that I had likelihoods for network-based models, and I had a likelihood that was derived from those for mass action models. And I thought, well, why don't I generate data from a network-based model, then throw out everything I know about the network and analyze it using a mass action likelihood? Now, I promise you, this mass action likelihood worked when the data was generated by a mass. When the data was generated by a mass action model, it does not work. So these are true R0 and estimated R0s. It does not work when you use a mass action likelihood for data generated by a network-based model. So these 95% confident interval coverage probabilities were 0.185 for R0 and 0.004 for beta. These were terrible. These are bad coverage probabilities. So for the network. So, for the network-based estimates, everything was above 0.949. These likelihoods work, but you have to be making some assumptions that are correct. So, we had a, you know, in the COVID-19 pandemic, we had a well-validated age-stratified model of COVID-19 transmission in Ohio. And as another experiment, we modified it to include ascertative mixing among vaccinated and unvaccinated groups. And there was sort of a And there, there was sort of a political, you know, there was a social-political divide, geographic divide. And there are all these divisions between people getting vaccinated and people not getting vaccinated. It seemed logical that there might be assertative mixing. So we modified this model. So rho equals one corresponds to no assertative mixing, and rho equals zero means the two populations are completely isolated from each other. And we'd fit it to age-stratified incidence data and then estimate the effective reproduction number as the spectral radius of the next generation. Is the spectral radius of the next generation matrix? So you can see, so we have a posterior distribution for estimates of R0 on the left. And then on the right, we have the fit to the incidence data from Ohio. There's no visible difference to the fit to the incidence data from Ohio, but the distributions of all R0 are all not really overlapping. So this same incidence data is consistent with many different values of R0. Different values of R0. And given what we know, we can't tell how much. Wassier did a literature search for asertative mixing between vaccination groups, and we came up with nothing. Right. So the row, so this is row, this is no ascertative mixing. And then as we go up, it's more assertative. The way we parametrized it maybe wasn't the best, but the lower row means more assertative mixing. So row of zero means they're isolated from each other. means they're isolated from each other. So here, you know, and we did this, we were sort of suspicious about how high estimates of R0 were for some of these variants of COVID and that maybe accounting for ascertative mixing would make them lower. And that in fact is what happened. But this is purely an experiment. We don't know how much assertative mixing there was or was not among the vaccination groups. Posterior. Yeah. And so the Yeah, and so the main point is: even though there's no visible difference in the fit of the model, the distribution of R0 is very different from model to model. Right. Or, you know, it's, you know, we just don't know. You don't know where you are along here. You have to, you have to do some measurement. You have to do some measurement to figure out that problem. So, I think we have some great modeling efforts. I don't mean to disparage the forecasting efforts or the agent-based modeling efforts, but I think in a sense, they're built on sand because we don't have the measurement and the causal inference that's needed to support them. So, bedrock is like as with the presentation about Kermik and McKendrick, the history of epidemiology has some solutions to this problem. So, and that's household studies of trans. And that's household studies of transmission. So these were a central part of infectious disease epidemiology, which was a central part of epidemiology in the early 20th century. So they were often used to estimate the secondary attack risk, which is sort of the probability of transmission from an infected household member to a given susceptible, and not to at least one household member, but to a given household member. So, and these were developed by Charles V. Chapin and Wadehampton Frost. Chapin and Wadehampton Frost. And Wadehampton Frost is a major figure in the development of epidemiology in the 20th century. And they used it for diphtheria, scarlet fever, influenza, and tuberculosis. And it's also been used to study the transmission of many other diseases through the 20th century. But they've died out and they sort of, you know, they were common in the early 20th century. And I think through the 60s, 70s, 80s, they mostly died out. They're still done. And there have been studies of. There have been studies of COVID-19, but if you compare them to the gigantic number of papers, it's dust. It's a rounding error in the literature. Every time I say households, you think of any close contact setting. So these could be congregate living facilities, prisons, hospitals, classrooms, and workplaces. And one of the bitterest experiences we had in COVID-19 was trying to get prison data and then being shut down at the last minute. I also made a doomed. I also made a doomed and desperate attempt to get data from the Diamond Princess cruise ship, but it never worked out. And that would have been a gold mine of information about transmission, but the way it was analyzed, it didn't work out that way. So household studies of transmission, they give you unique insights into the natural history of disease. You are getting clinical and subclinical infections, so you can measure the relationship between symptoms and infectiousness. You can get immune and serological measurements. You can get immune and serological measurements before and after infection, and you can relate those to individual characteristics. And you can even do this for multiple pathogens at the same time. So, there was a series of studies called the Virus Watch studies in Louisiana, New York, and Seattle that actually did giant panel, you know, to the extent that they could then, they did these panels for respiratory and enteric viruses in these households. So, you know, it's almost like Nielsen families or something like that. So, the other thing you can do in a close The other thing you can do in a close in a household study that you can't do elsewhere is you can jointly estimate effects of covariates on susceptibility to infection and on infectiousness after infection. And joint estimation of those effects is really important. If you look at only susceptibility, you can get bias across the null even when an exposure is randomized. So you really have to be looking at both at the same time. And I think household studies also are probably the most reliable. Probably the most reliable. Oh, so bias across the null means it looks like the risk ratio goes one way, but the causal risk ratio goes the opposite way. So it looks like something's harmful, but it's actually beneficial or vice versa. So bias across the null is bad. So household studies are also probably the most reliable setting to track the evolution of transmissibility and to look at the effects of changes in behavior and to look at how those affect susceptibility versus infectiousness. For instance, with Susceptibility versus infectiousness. For instance, with masks, are you wearing a mask to protect yourself from infection or to protect other people from being infected by you? You can look at that question in high resolution. So, but you can snatch defeat from the jaws of victory. So, household studies are done rarely, and most of those conducted are analyzed incorrectly. So, in most cases, I would say it's closer to 100%, but I'm saying over 90%. Saying over 90%. Once you have a primary case in a household, they attribute all subsequent infections to that person. So then they estimate the secondary attack risk using a logistic regression or occasionally Poisson. Epidemiologists like zero-inflated Poisson models for binary outcomes for reasons that mystify me. But you see both of those models used. And this approach, it ignores multiple generations of infection within the household. It also ignores the ongoing risk of infection. The ongoing risk of infection from outside the household. And you can do simulation studies and see that this is, these are very serious sources of bias. So, the COVID-19 pandemic has seen very few household studies and almost all of them have been analyzed using logistic regression. So, I submitted a paper once about why you shouldn't do this. And then, American Journal of Epidemiology said, nobody does this, but they still do. That was like 10 years ago. So, but there is a logic to this. There's a logic. But there is a logic to this, there's a logic to why you might ignore these things, and that's because transmission probabilities are usually small, five or ten percent. And so, a chain of length k is going to have a probability that looks like p to the k, and that decays really fast. But people are forgetting about the fact that the number of paths of a given length can be quite large. So, this is a number of susceptibles and then path length. So, you see, if you have a large household, 10 susceptibles, and a path of length five, you have thousands. And a path of length five, you have thousands of paths. So the actual decay of these trends, of this probability of being infected through multiple generations, decays much slower than it seems. So, and this actually has an effect. So, the secondary attack risk is on the bottom here. So, that's the true secondary attack risk. And then if we look at the sort of the final attackers, the proportion of susceptibles in the household who are infected, it's much larger than the secondary attack risk. So, that's why you can't just attribute. That's why you can't just attribute all infections to the primary case. And then, one, people realize that there's a problem with this, but they think they can fix the problem by changing the variance. So you get like a robust, generalized estimating equations or a random effects estimate of the variance. And this shows from simulations what happens to your confidence interval coverage probabilities when you do that. The adjustment for the variance, it does improve the coverage probability. It has to because it's making. Coverage problem, it has to because it's making the confidence interval wider, but it does very little to improve the 95% confidence interval coverage problem. So the black lines here have the adjusted variance, and the gray lines are the unadjusted variance. It just doesn't work. You have a bias point estimate, and it doesn't matter what you do to the variance. It's not going to fix the problem. And most secondary tack risks are on that 5% to 10%, but it seems like some variants of COVID could be. It seems like some variants of COVID could be 20% or above. So you could have really, really low coverage probability. Okay, so how, you know, I think this setting is really important, but it's also important to have the right analytical tools. And these are rarely used. So we're taking methods from chronic disease epidemiology and trying to fit them into infectious disease epidemiology. So, but I think the So, but I think the generation interval with a slight modification that actually works quite well. So, generation intervals are sort of an attempt to use a failure time distribution to summarize disease transmission. But they have this requirement that you have to have transmitted disease. And that assumption creates all kinds of chaos. And that's where these things like generation interval contraction, I have a whole talks where I whine about generation intervals and the chaos that ensues from this assumption. But if you just drop. This assumption. But if you just drop that, all kinds you know, you go through the Lalis and Wonderland door into all kinds of possibilities for analyzing these studies. So I call these things contact intervals. I'm the only one who uses them, so I can call them whatever I want. So I call them contact intervals. And the contact interval is from I to J is the time from the onset of infectiousness in I until infectious contact with J, whether or not that causes infection. And that's the difference from the And that's the difference from the generation people: it's whether or not it causes infection. And infectious contact, I'm defining infectious contact to be sufficient to infect J if J is susceptible. So when I mix infectious contact with J, either J will be infected or J was already infected by somebody else. Okay, so but this contact interval distribution actually, you know, I think it has a deep relationship to the transmission kernel that Odo was talking about. That Odo was talking about yesterday. So, the probability of infectious contact from I to J comes from the survival function of the contact interval distribution. So, you know, and if I and J are members of the same household, that's the household SAR. You can also define secondary attack risks in other settings, such as congregate living, you know, nursing homes, prisons, hospital wards, things like that. So, the survival function is really useful. The hazard function is also really useful. So, the hazard function is telling about the instantaneous. Is telling about the instantaneous infection of an infected person. So, and this gives us instantaneous infections as a function of the infectious age. So, that's the time since onset of infectiousness. And a scaled version of this is often called the infectiousness profile. Now, the infectiousness, people really want this to be a probability density function. So they scale it to have an integral of one. But what you're actually interested in is the hazard function. It doesn't, you don't actually want it to be a probability. Actually, I want it to be a probability density function. It's properly interpreted as a hazard function. Okay, so, and if you look at any stochastic epidemic model, you can find a contact. It has to specify a contact interval distribution. So a network-based Kermit-McKendrick model, you have a constant hazard of infectious contact with infected neighbors, so you have an exponential beta contact interval distribution. In a mass In a mass action model, you have to take account of the fact that the hazard of infectious contact between two individuals is scaled. But when you crank through all of the math, which I won't inflict on you here, but you get that the basic reproduction, so the basic reproduction number is governed by the cumulative hazard function of the contact interval distribution. So in networks, the R0 is governed by the degree distribution of the network and the survival function. Of the network and the survival function of the contact interval. In a mass action model, it's governed by the cumulative hazard function of the contact interval distribution. So, and you can use this to do things like come up with a new derivation of the fact that R0 is beta over gamma in the classic Kermit mass action, Kermit-McKendrick model. So, one thing I think that's been a very useful contribution to the literature is the idea of intrinsic generation intervals. Intrinsic generation interval. So that's these are the this is the generation interval that you need in a mass action model to get all of these renewal equation approaches to work. But it's not actually helpful, I think, to think of it as a generation interval distribution. So it's a contact interval distribution. So in these two papers, Walling and Lipsich 2007 and Chomperdon and Dushoff 2015, they both say that the intrinsic generation interval distribution in the Kermit-McKendrick model is exponential beta. But if you actually Is exponential beta. But if you actually keep track, you know, if you actually crank through what the distribution of generation intervals is, it's this very strange distribution with a, it looks actually a little bit like an exponential distribution, but it's not. So I think it's not helpful to think of this intrinsic generation interval distribution as a generation interval distribution, because you're not conditioning on the transmission of infection. When you transmit, condition on the transmission of infection, you get a very Transmission of infection, you get a very different distribution. But there's a deep relationship between this idea of an intrinsic contact interval distribution, intrinsic generation interval distribution in a mass action model and the contact interval distribution. Okay, so how do you estimate the contact interval distribution? You have to deal with observation and sensoring. So it's a big survival analysis problem. But luckily, there's a lot of machinery that's available for us to deal with this problem. Available for us to deal with this problem. So if there's aye becoming infectious and Jerry recovering and that observation ending. So if I is infectious and doesn't infect, you know, and J is infected while I is still infectious, we have either an observed contact interval, if I infects J, or a censored one otherwise. If I recovers before J is infected, you know the contact interval is censored. And if observation ends before I recovers or J. Before I recovers or Jay is infected, then you also have a censored contact interval. So you can prove that if infectious periods in the end of observation independently censor the contact intervals, everything boils down to a standard survival analysis problem. So you can get this distribution parametrically or non-parametrically. You can also use the contact interval distribution to write likelihoods. So this is a household of people who got infected in alphabetical order. You can see when they get infected. They become and they get infected, and when they have removal times, we don't have a latent period, so people become infectious right away. So, and the likelihood for each possible trend. So, you know, I have A and then B and then C getting infected. So, there's two possible transmission trees. A infects both B and C, or A infects B and then B infects C. Now, each of those comes with a likelihood contribution. And if I don't see who infects whom, I add up the likelihoods for all of the possible transmission trees, which turns out to be easy. Which turns out to be easy due to a some product decomposition, even if there are millions or billions of transmission trees. So, and in discrete time, this likelihood turns into the chain binomial model that's been used for some household studies. So, because we can write a likelihood, we can also write regression models. So, there's sort of two classes of regression models that we've been working on. One is a parametric model. It's an accelerated failure time model where you're using rate ratios. Failure time model where you're using rate ratios, and another one is a semi-parametric model where you're using hazard ratios. So, the basic structure is the same. And I just want these are joint work with Wassir, Zeynabtiello, and Yusuf Shorkar. And you have an internal transmission model for within the household, and then you have a baseline rate parameter, and then a rate ratio. And you also have an external transmission model, which has a different external. Transmission model, which has a different external rate, it can even have a different distribution. There's no limitation that the distributions that you're using in the two models are the same. And it also has a rate ratio. And you can combine the two models and fit them simultaneously. You actually have to do that to avoid bias. So there's a lot of bookkeeping. So if covariates could be in both models or in only one of the models, and so you, if a covariate is in only one of the models, you set it to zero in the other model. And then your data set looks like you have. Model. And then your data set looks like you have rows that represent pairs within the household. And then you have rows that represent a pair where the infectious person in the pair is the external world. It's an environmental or community source. And then you have an external row indicator. And once you do that, everything looks exactly like a regular regression model. So just in the interest of time, I'll skip through this. So there's a simulation study we did, and we did four different epidemiologic study designs. Designs. And the basic message is that the point estimates work, the interval estimates work. And when you see who infected whom, it turns out the sort of italic study designs are ones that you shouldn't do. They're bad study designs. But when you see who infected whom, you can get away with murder in your epidemiologic study design. You still get good point, you know, for rate ratios, you still get good point and interval estimates. Good point and interval estimates. I really wasn't expecting that result. I think it's actually strange and a little disturbing. But it shows you that any information you gather about who infected whom, such as from pathogen genome sequences, can be extremely useful for both reducing bias and improving precision. So, it helps, and the more data, the better. If you're missing sequences, you'd have to crunch through it with some sort of a data augmentation and MCMC. But basically, when somebody is infected, you'd want to try. But you don't have, things work actually quite well when you don't see who infected whom. So, when you have nothing, it's okay. It's just any little bit, anything you can do to sort of trim down the number of possible transmission trees will help you. And that could be, that could, I think genome sequences are probably the most promising. I think genome sequences are probably the most promising avenue for doing that. Assuming you don't know the common person, are you doing this over the course of an entire academic simulation? Oh, no, we just stop at one week. So these simulations had 300 households and we just stopped things when we got to 500. So you're not assuming anything. Maybe it was over, maybe it's not. It doesn't really matter because you're handling censoring in the correct way. Handling censoring in the correct way. Okay, but I guess what I'm wondering is if you do this very, very early in an outbreak, it may be a reasonable substance that there's always one external infection to outbreak. Right. Early in an epidemic, you might be able to ignore the external transmission. When you see who infects whom, you actually can ignore external risk of infection. And it's really amazing. But when you don't see who infected whom, the constraints on the epidemiology, you actually have to be using good epidemiologic study to. Be using good epidemiologic study design. So, I'm not recommending using bad epidemiologic study design, but I think this result shows that there might be ways to develop efficient study designs for the analysis of transmission in households in the same way that case control studies were derived from cohort studies using concepts from survival analysis. Okay, so this model produces valid point and interval estimates whether or not we see who infects whom is observed. Valid study designs are critical. Valid study designs are critical when we don't see who infects whom, but when we do, we can estimate rate ratios surprisingly well. When household transmission is rapid, we don't have much data for estimating the external hazard of infection. So there's a role for enroll. It might be good to enroll control households where we don't yet have a primary case to get more data about the external hazard of infection. And then with little basis for choosing a failure time distribution, and we don't know what. Failure time distribution, and we don't know what these distributions look like for almost any infectious disease. So, a semi-parametric extension is useful. So, that's this semi-parametric relative risk regression. So, it extends a regression model that I worked on in 2015 to include both an internal and an external model. And you do all of the same tricks with covariates that you do in the accelerated failure time model. And so, here, a couple of analyses I wanted to show based on. A couple of analyses I wanted to show based on these models are Los Angeles County household influenza data from the swine flu pandemic. I don't have, you know, people really don't do these studies. This is the only household data I have. So they just tracked households for influenza A, H1N1. They measured covariates like antiviral prophylaxis and age and things like that. We had 299 individuals in 58 households. Individuals and 58 households. We had 99 influenza A cases, of which 62 were classified as primary cases. So we only had about 36 infections. So this is a very small set of infections. So I wasn't expecting to see much in here. But we could use all of the regular tricks. This looks just like a regression model and behaves just like a regression model. So you could use things like AIC to make choices about which distributions to use. And then we could get parameters. So these are. And then we could get parameterized. So these are log rate ratios. So, what these are saying is adults appear to be more infectious, because that's a positive coefficient, and less susceptible than children. But though, you know, it's not statistically significant, and it turns out to be sensitive to natural history assumptions. Antiviral prophylaxis had reduced people's susceptibility to infection, it did not appear to affect. Infection. It did not appear to affect infectiousness after people were infected. And that was statistically significant and not sensitive to natural history assumptions. And then what we're showing down here, when we ignore external infection, we actually lose a lot of power. So we have a better fitting model when we account for external infection. So that's an important thing to do. And then you could use these parameter estimates to calculate secondary attack risks from children to untreated children. To untreated children and children on prophylaxis or adults, and you get walled confidence intervals for them. And this is a really interesting exercise. I did a similar exercise for in Ohio State. They did regular testing of all the students. And I did an analysis like this of transmission in all of the dorms. And you could see things like how much faster transmission was within a room than it was between students who lived on the same floor, but not in the same room. Room. So you could see differences between in-person classes and classes that were hybrid or classes that were online. So it was like having this x-ray machine to look at transmission on campus. It was very exciting and hopefully someday it's publishable. So we've already gone through some of this. Antiviro, it reduces susceptibility to infection. It becomes a stronger result when we account for external infection. You know, missing data, so that was brought up. It needs to be, I did not do anything. To be hand, I did not do anything principled or sophisticated to handle missing data here, but that's a common problem. And data augmentation in MCMC, I think, is the way forward on this. And none of this required that we observe who infected whom. We did not, but partial information can increase precision and reduce bias. And then another analysis is influenza transmission from pigs to ferrets. So they had two different vaccines they were testing, and they had pigs. And they had pigs in cages, and then susceptible ferrets in cages adjacent to the pigs. And some of the cages were like next to the pig, and some were like three feet away from the pigs. And so we analyzed this data with a log-logistic pairwise accelerated failure time model. And we could calculate rate ratio. So we had like naive versus ferrets that receive killed vaccine. We had a rate ratio of 0.88. Live versus naive, we had 0.66. And then we had contact. And then we had contact groups. So it didn't appear to matter that much whether you were right next to the pig or a little bit further away from the pig. And then we could translate these rate ratios into the so log logistic distribution has this special thing where the odds ratio is constant. The hazard ratios or risk ratios are time dependent. So we just estimated vaccine efficacy using the odds ratio. For the killed, we got a 0.61 with a wide confidence interval. For the live vaccine, it Well, for the live vaccine, it looked like it truly was an effective vaccine for the ferrets. So that's good news if you're a ferret. And one thing I have to say, like the people who work with animal diseases do a lot of things right that I think we don't get right in human diseases. They have the same people who did this, they go to these swine shows and they track transmission in all of those pigs and they save that data and it's just sitting there. And it's just sitting there. And I'm really excited to take a look at that data with this. And, you know, I think we just need more high-resolution studies of transmission among humans to support the more sophisticated, you know, to support complex models that we use for forecasting or for scenarios. You know, I think that's what I mean when I say we're built on sand. Like these models are not being adequately supported by on-the-ground epidemiology and causal inference. And causal inference. So, for this research, I think there's three primary directions: improving model fitting and inference, including partial information from pathogen phylogenies. There's been a little bit of progress on that, but there's a lot more to do. And then developing novel epidemiologic study designs. And I think that thing where you see who infected whom and you get away with murder as far as epidemiologic methods is concerned, that's promising. So, but all of these methods depend on improved data. Methods depend on improved data collection. We have to actually go out and do these studies and capture people who are exposed to infection but not infected. And that just doesn't happen most of the time. So I think this is an unrecognized but critical gap in our pandemic response. This has been severely neglected in the COVID-19 pandemic, and I see almost no recognition of this problem. So I think the forecasting and predictive inference is very useful. And we worked on that with the Ohio Department of Health. They're looking at hospital burden and ICU. Looking at hospital burden and ICU admissions, and it's very important. But I think strategic victory against novel pathogens requires that we understand the mechanisms of transmission. And that requires causal inference, and that requires these types of careful study designs and measurement. And I think just like weather forecasts, complex population-level models to guide policy, they'll get better if they're based on accurate estimates of causal effects. So I think there's nothing wrong with the models that are getting used, but I think. Models that are getting used. But I think they don't have the foundations in accurate estimates of causal effects that they really need to realize that potential. Oh, that's going backwards. Okay, so now why aren't household studies done? They're hard. So we have one study in Haiti that's trying to do household studies of cholera transmission and a study in Columbus that's trying to look at household transmission of COVID-19. Recruitment is a big problem and you often don't recruit the whole household. Problem and you often don't recruit the whole household. When you're missing people in the household, you get time-dependent confounding. Ensuring a high frequency of testing and accurate collection of symptom data and demographic data is challenging. So one solution that we've thought about is maybe having home testing kits that can be stored in a refrigerator and like mobile apps that can collect data. And then obtaining control households to accurately estimate the external hazard of infection is difficult. But something like a test-negative design. So you have a primary case that you've tested, and if they're positive, You've tested, and if they're positive, you keep the household. And then, if you're negative, you that's an opportunity to have a control household. And, but one, I think the overriding point is we don't good at these studies because we don't do them. It's complicated and you have to do it in order to become good at it. And every influenza season is an opportunity to practice. And we're going to be living with COVID. That's another opportunity to practice. So there needs to be sort of a commitment to, you know, it's just like we couldn't go out and build the Saturn V again. Couldn't go out and build the Saturn V again. Like a lot of that knowledge has been lost, and household studies are in the same position, but it's worth the effort to get them back. So, and then the last thing, this is a buildup to Wasser and Greg's presentations. So, and that's dynamical survival analysis. So, that extends pairwise survival analysis from sort of households and close contact groups to population level models. And I think it's potentially very useful for things like forecasting. Like, like forecasting. And both models use contact intervals. So they have a root. You know, so household studies based on pairwise survival analysis and DSA models have a method of communication so they can they can inform each other. And the DSA methods can make both mass action assumptions or allow people to live on a configuration model network. So in combination, I think they provide a unified multi-scale approach to monitoring and analyzing epidemics. But Wassier and Greg will have more to say about that. And so there's funding. And so there's there's funding and a disclaimer and thank you very much.