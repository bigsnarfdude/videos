Thank you for arranging this meeting and inviting me. You've assorted a collection of varied and interesting people. So I'm from the Department of Mathematics. We all have to make that clear at this meeting. And this is joint work with Aria Isolis from Cambridge. So I hope that some of you will find this interesting and stimulating. Is interesting and stimulating, and that you'll come to me and talk to me about it and Aria as well. If you don't find it stimulating and interesting, please come and talk to me anyway and tell me what does stimulate and interest you. I'll be very open-minded. Okay, so I'm going to explain what T systems are, so don't worry if you don't know what that means. But I hope everybody here knows what I mean by the numerical solution of Schrodinger equations. As a point of departure, let's look at semi-classical Schrodinger equation for a single particle. Schrodinger equation for a single particle. Okay, so the solution is a function of a real variable x and time t. There's an initial condition that's prescribed that is smooth and square integrable. It's all standard stuff. We've given a potential that's smooth and real. It could depend on the wave function as well, potentially. But there's also a small parameter there. So we'll There's also a small parameter there. So we'll just call it epsilon. You might know the order of magnitude of this epsilon for any particular application you have in mind, but it's a small number that can get in the way of calculations, or at least make them tricky. Okay, and just as these things are very pretty, let's just look at how these systems evolve. Systems evolve. This is a standard harmonic potential, just x squared over 2. And you see this is a kind of a wave packet. It's highly oscillatory. That's what I want to point out from this. It's highly oscillatory. But at some times, it's not oscillatory. So it can be oscillatory, and then at some times, it's not. You can have a range of wavelengths and frequencies. And this In this situation, it's been trapped by the quadratic potential well. So, we all know that the Born interpretation of this is that the absolute value squared is the probability density of the position of that particle. And so, it needs to maintain that the integral of that is one for its state of the probability density. So, the solution operator is unitary. There's also symplectic or Hamiltonian structures. Also, symplectic or Hamiltonian structure is worth noting. And as I said, this high oscillation that really can just be non-oscillatory all the way up to epsilon to the minus one order oscillation. So one way to get this is from WKD analysis. Okay. So if we want to solve this numerically, something that we've heard about already quite a lot is spectral methods. Already, quite a lot is spectral methods. And what this involves is a basis of square integral functions, and you have as your anzats a linear combination of these basis functions where the coefficients are going to depend on time. So this is exact, it's an infinite series. It's exact if this is a complete basis or the space of functions. Or the space of functions. And so, you know, completely exactly, there's no approximation yet. These coefficients satisfy an ODE. So all these differential operators and multiplication by potential operators, they get turned into infinite-dimensional matrices. And these coefficients, which are square-summable sequences, evolve in time. And these matrices, you can have one that's d squared. You can have one that's d squared, which is coming from a matrix D, which encodes the expansion coefficients of the derivatives of these basis functions, and a matrix V, which encodes the expansion of V times a basis function. These linear operations, so you get, that's why you get matrices here. And then if you want to make a numerical solution out of this, you need to project it somehow onto a Project it somehow onto a finite dimensional subspace. So, what we've talked about so far is Galurkin projection, or Petrov-Galurkin projection more generally, or you can project using co-location, where you prescribe the solution to be satisfied at a set of n plus one points. So, just to be even more complicated. So, just to be even more concrete, the standard thing to do is the Fourier basis, right? So, here we use the Fourier basis of these exponentials, as we're all familiar. And you pick some equally spaced points in a domain from 0 to 1. And if you want to find those coefficients by co-location, you can use an FFT to do that very quickly. So, this is order n log n operations to obtain those coefficients for the initial condition. And then, if you Condition. And then, if you want to evolve it, this is just a linear system of equations with a constant matrix H here. So it's the discretization for the Fourier basis is also very easy. So this differentiation matrix, the matrix that encodes the derivative, is diagonal with these imaginary integer values. The derivative of Fourier basis is just a multiple of its. Basis is just a multiple of itself. And in a collocation method, this multiplication by V matrix, you would take coefficients, you would do an inverse FFT to get values, you would multiply those values by the values of the potential, and then you would FFT again to get back to coefficient space. In the end, just want to compute an exponential. And In this situation, the matrix H is Hermitian. Because this D was skew Hermitian, you're squaring it, and this is a real potential. So it means that you naturally get a unitary evolution, very nice properties. We said that there's going to be high isolation in these problems, like epsilon to the minus 1. In these problems, like epsilon to the minus one, properties of the Fourier basis means that you should expect your number of necessary modes to scale like one over that frequency. So you'll have big O of epsilon to minus one modes. This is just in one dimension, just from the oscillation. So of course, if you're going to generalize and have multi-dimensional situations, then this number of degrees of freedom can get even larger. These number of degrees of freedom can get even larger. But just from the oscillation, you get some uncomfortable, comfortably high number of uh degrees of freedom, depending on your tolerances. So, one approach, given that n needs to be very large, if you want to be as efficient as possible and you want to preserve this unitarity, one approach is splitting methods. So, this operator naturally splits. Operator naturally splits into the kinetic part and the potential part. So this kinetic part was diagonal. So we could solve that system by itself. And this potential part is diagonalizable by an explicit similar transformation, the fast Fourier transform. form. And so the basic idea of splitting, as I'm sure most of us are aware, is that this exponential of H, ITH, it's approximately the same as if you did the exponential of one part, the potential part, for example, and the and then it's flashing, which probably means it's running out of batteries. It's flashing, which probably means it's running out of batteries or something. Yeah. Okay. Just approximately equal to the splitting. The strand splitting is where you split it with half, whole, half. This is an extra order of accuracy over that leech rotted basis splitting. And then if you want to go to higher order, things get a bit tricky, right? Yeah. So this is the So, this is the strang splitting. But if you want to do so-called symmetric Zassenau splittings to get higher order, then you take your strang splitting, you split the middle one, then you'll have a correction term in the middle which involves multiplication operators of derivatives of the potential. But this is one way of efficiently getting a fourth-order method. And this W2, you can apply it to a vector in n-log n operations still, so you're utilizing all the structure you have. Very non-trivial, but all this is this made so that it's a Hermitian matrix. Hermitian matrix. Quite a few leaps for imagination were made in this paper by Barney, Silas, Popolnitska, and Singh. More details on Singh's PhD thesis about how to get these symmetric saturnal scriptures to work. And the idea about how to actually compute exponentials is not clear. You can use a free loss subspace method, and the sort. Subspace method and the sort of the size of it, the fact that this is actually quite a small matrix in norm, allows you to compute that exponential very quickly. Very few sort of base situations are needed. See a Hochrook and Lubich Simon paper about that. Trusted. Okay, let's look at some more of these simulations. They're always very pretty. So, this is a free particle, so the Particles to the potential zero. Oh! What happened there? Yeah, so there's periodic boundary conditions, right? So that's one of the disadvantages of using this Fourier makes you get all these lovely structure preservation properties, very fast methods, but if your particle is not confined, like that first example, it's eventually going to reach the boundary. It's eventually going to reach the boundary, or you're going to have to figure out how big to take this window to approximate your solution to avoid this unphysical behavior. Let's consider, say, Diristle boundary conditions instead. I mean it's similar kinds of issues. I mean, this is a part of the debots really, but Bots really, but it's not going to solve the problem of what happens when you hit the boundary. So putting divisive boundary conditions is not a naive solution, and it's not going to help in terms of what happens if your solution hits the boundary. But in this simulation, I've used the method I'm going to show you today where the solution has been done on the whole real line, there's no real hard boundary. So, this is just an example of tunnelling, where there's a potential that's green that's quite large, but the solution can tunnel through despite the large potential there. And the solution will go off to infinity in each direction. So, how did you solve this model? I'm going to show you. It's called suspense. Okay, so can I have some models? Okay, so can I have some more quick questions? Yeah, please, if you go back, you mentioned this Daisenhouse quick. So does it have some advantages compared to this Exuzuki or Triple Jump composition? Yes. You saw me hesitate. Yes, okay, so for this is a very small matrix. So the perturbation that you Matrix. So the perturbation is a very, very small perturbation mesh. So if you're doing a Suzuki, you can sort of go forward and practice through the simulation. Well, this is a forward evolution in time, and this is a forward evolution in time. And this is just a correction term. So it's it's more optimal in that sense that we're not overshooting. We're speaking the collection term. Is it more complicated than the potential of the kinetic energy? Is it more uh to evaluate the correction green term W to is it more complicated than the potential correct? I mean this is the explicit um expression. So this is the multiplication operator for the first derivative of the tension squared. This is the multiplication operator for the second derivative of the potential. So you need to know. Derivative of potential. So you need to know those two things to make this work. Is this a Suzuki chip? Is it the same or a Suzuki chip? No, this is very different to Suzuki. No, this is commutator. Yeah, it's not a Suzuki chip. No, this is very different. This is from the commutators. This is from commutators. But the commutators have all been on role to make it. No, chin and chin. But the commutator was an alternative function. The commutator was a multiplicative function at that point, right? But also, the commentator is done in infinite dimensions here. So, the commentator is done in infinite dimensions and then reduced. Because you have simplifications happening where the commentator between a potential and momentum is a multiple of the derivative of the potential. You utilize all that rather than physicists, I think, from XRAS made this differently the same economy. I'm not aware of that word. Because I know exactly what you are talking about. And in Jin and Chen, this correction is a multiplicative function. So exponentiation of this is just exponentiation of the exponentiation of diagonal matrix, each in H. And here it is different. It is not a multiplicative function, it has a gradient side. So exponentiation of this Exponentiation of this is difficult, but the advantage is that this W2 is an operator of a very small size. And this is why you can use current current. This is what I said. Just repeating what I said currently. Yes, yes, I am repeating. And for this reason, we can draw for subsistence methods. And exactly Macu said that we need a And exactly what you said, that we need uh two or three operations. No, so this is the yeah yeah, you set this idea. And then the last question is also a little bit provocative. Is that you motivated it by having a semicolon which I think is small? So why would it use all these methods in the first place? I think here it'd be interesting to go from semi-classical, for example, hydro wave packets or something. Example hydrogen packets or something, but you have a basis which is very localized. Right, so this is a let's say if you have a three-dimensional problem on a grid, it's doable, but if you decrease epsilon, it becomes more and more complicated, it will increase easier if epsilon is not small. So I'm just showing you the hard situation. Hard situation. The smaller the epsilon, the faster it converges. Because it's an asymptotic expansion. Yeah, we can discuss it. And it is even if you are not sure. What converges faster? The expansion. Because you can go on turning the handle to have higher and higher orders. With chin and chen, you can go have this 4 compatible. And here, you see this W2 square stack epsilon minus 1T cube. And W3. cube and w3 will say that epsilon to minus 1 t quantity so you can go further with ease because you would have even less the key thing which i said before was that this is a this is a smaller the correction gets smaller and smaller replay for emotes and two months well that's in this example yes but the rest of the talk is not going to talk about forums this is not the main topic of my talk This is not the main topic of my talk. This is part of the introduction as something that other people are doing. And I'm not sure if you're not. Because you have error, always having one order excellent. Yeah, but it only reduces it by one. But it's just important to say that what you think might be a second order method reduces to first order, depending on the size of epsilon. Okay, so maybe I will take a voice as HR. Let us remove this discussion because it is very. Let us move this discussion because it is very interesting. My people are interested after this. And let's continue. Yes, because we have to go with the scale. But it's an interesting subject and I involve myself. Yeah, okay, thank you. So your first thought, if you say, well, let just extend to the whole real line, the natural thing that people would suggest is Hermit functions. Okay, we're all familiar Herme polynomials are orthogonal with respect to a Gaussian weight, and if you use a square root of Gaussian weight, Square root Gaussian weight on the polynomial, you get Hermic functions. And these also have some nice properties. So I want to point out that they have this sparse differentiation relation. The derivative of a Hermic function is a linear combination of the one index above and one index below. So it's not quite the diagonal differentiation relationship you had for Fourier. This is a tri-diagonal relationship. So if you're going to have a differentiation relationship, Relationship. So, if you're going to have a differentiation matrix here, it'll be tridiagonal. But it'll also be skewed permission. So, these numbers here, the way they're related, implies that it's actually a skewed emission tridiagonal matrix. So, not quite Fourier, but it's got this going for it. But I don't think it's got much else going for it as a basis. What's wrong with the beat? Well, there's no fast and stable transform to go from expansion. Transform to go from expansion coefficients and to values onto some grid. So like the FFT. So if you want to have something stable, the best that's really known is order n squared algorithms. So that's a disadvantage. Also, the approximation properties are poor. It's a complete basis in L2 on the real line. It's been said before here. But actually, if you just consider an example of a wave packet, this is a This is a Gaussian function times pi an oscillatory function. Then the number of modes you need, the number of Hermit modes, is order omega squared, where omega is the frequency. So this is really poor compared to Fourier. So we have a paper in constructive approximation on approximation of wave packets on the real line, comparing Hemet and Fourier and some other functions that I'm going to discuss in a moment. We'll discuss it in a moment. And we get we prove we've proved some asymptotic formulas for these these these uh expansion coefficients and basically show how how poorly Hermic functions perform whenever you get say the the location incorrectly or you get you get multiple high frequencies in the function that you you don't know a priori and could perhaps match and that basically punishes you if you don't know enough about your system. Enough about your system. Here are just some expansion coefficients that the main thing is that it's the frequency squared, not the frequency. So they're not very easy to use for a general purpose running a solid. The aim of this talk is to see: can you find a different basis that could emulate the success of Fourier basis, but it's a space on the real line. It spaces on the real line. The things that the Fourier basis enjoys, that we would like to enjoy as well, is orthonormality. This implies some stability and also implies that if you have the unitarity on the coefficient, you're going to have unitarity on the solution and continuous norm. Diagonal skew emission differentiation matrix, fast transform. Fast transform between values and coefficients is very helpful, the FFT. And also this approximation properties that the number of modes you need is proportional to the frequency of the solution rather than the frequency squared. So more specifically, our desiderata is what we've since started to call a t-system. So what we want to do is to characterize functions. Characterize functions that satisfy this sparse derivative relation. So it's not like Fourier, where you have a diagonal relation, it's this tri-diagonal relation where the derivative is an expansion in the neighbouring basis elements and the element itself. And we want it to be scoopermitian in order to have unitary evolutions. Evolutions. Okay, so this is just a desiderata. Looks like a hard problem, but we can actually characterize it. We want them to be off the normal as well. And we want, hopefully, to have some kind of fast transform to get between values on a grid and coefficients, so then our co-location methods can be done very quickly. And the answer, because Aria was asking this at several conferences about a decade ago. Conferences about a decade ago and beyond, all the way up until I guess 2017, when two of my favourite things were floating around in my head. So the Fourier transform, it's sad to call that one of my favourite things, but it is, and orthogonal polynomials. So I was thinking a lot about these two things and it turned out that these two things, if you smush them together, it solves the question of these T systems. The question of these T-systems. So, orthogonal polynomials, you can think like the Hemi polynomials or Legendre polynomials. They're orthogonal with respect to constant weight or Chebyshev polynomials, Laguerre polynomials. We're all familiar with orthogonal polynomials like these. If you're interested in physics, because they come up a lot. So, here's the characterization. So, if you want to have an orthonormal set that's at To have an orthonormal set that satisfies this sparse differentiation relation, all you need to do is take a Fourier transform of an orthogonal polynomial, really an orthonormal polynomial, that's orthonormal with respect to some g squared, where g is some function that you can, I suppose, choose. To make sure that this integral is well defined for when these Well defined for when these polynomial degrees get larger and larger, you want to make sure that G has some decay properties, like some exponential decay, for example. You can also have some phase adjustments to the basis as well. This is a characterization. These are all the functions that do that. We have a couple of papers, twenty eighteen, twenty nineteen. And we can also characterize when this these functions will be dense and square integral functions. And square integral functions. And basically, you want this w to have support on the whole realm. These are called t-systems. They're the only way to satisfy this sparse differentiation relation, which could have some computational advantages. I think, in the interest of time, I'm not going to go through a sketch proof of efficiency, but it basically comes from a But it basically comes from a three-term relationship that orthogonal polynomials satisfy, and you're transferring it to a differentiation relationship. So if you take the example of Legendre polynomials, this T-system characterization says if I take a Fourier transform of Legendre polynomials, they should satisfy a tridiagonal differentiation relation. And what you get if you take the Fourier transform. And what you get if you take the Fourier transform of the genre is Bessel functions, spherical Bessel functions. And then it was already known that they do satisfy this barse tridiagonal differentiation relation. So we were happy with that, we're on the right track. It seems to be interesting. And if you put in heavy polynomials, they're eigenfunctions of the Fourier transform, I mean functions. So Transform home functions. So you get back home functions. So homic functions are a sort of a fixed point of this transformation from orthogonal polynomials to these t systems. They actually are t systems themselves. Of course, obviously this was already known. The next family of polynomials you might think of is like air polynomials. So they're orthogonal with respect to this exponential weight on the half-line. The half-line. And if you take this Fourier transform of those weighted orthogonal polynomials, you actually get a nice rational function. Cool. And our theory says that these rational functions, they should satisfy a sparse differentiation relation. So you have to take negative energy. So, you have to take negative n as well if you want to have a complete basis, but they do satisfy this simple tridiagonal differentiation relation. And it turns out that these functions were already known. It was sort of disappointing, but sort of good to see that these functions were known already. So, they were invented independently in 1926 by Markus Detakanaka, and of course, Savina. And of course, Savina had used them since in signal processing. And then several people, notably Christoph and Boyd, had used these functions for spectral methods. And these are really cool functions. They've got a nice exact formula. Thing was sort of difficult. And actually, if you think about this rational function, This rational function, the absolute value of this number here is always 1. So it suggests that perhaps you could do a variable transformation. You could set x is a half tan theta over 2, and that makes this thing here e to the i theta. You all do a variable transformation, and what you get is the coefficients of a function in this basis, they are They are Fourier coefficients of this mapped and weighted function. The long and short of that is that you can transform between coefficients and values using FFTs. These functions have fast transforms to speed up collocation methods. And what do they look like? Oscillatory. Oscillatory, increasingly oscillatory functions. Bases often look like this, expressive spectral bases. And in this paper recently, Eastern, Lung and Webb, we looked at these wave packets and we looked at expanding them in the Hermite basis, expanding the wave. Expanding the wave packet in the Malcolmist-Takaraka basis, and we have a very nice expression for how the Malpus-Takanaka coefficients behave. And the key thing is that you get an n over omega term in the expansion coefficients. We got a very complicated expression for the Hermite coefficients, but that had an n over omega squared term in it. So the Hermite coefficients, you need omega squared terms to resolve that. terms to resolve that, but with Markus Nakanaka you need omega terms to resolve it. Quite similar to Fourier. So the sort of resolution power of these functions also seems to be more favourable than Hermines. Here's just a picture of some coefficients. If you start to increase the frequency, the coefficients go down exponentially, but the rate, of course, increases. But the rate, of course, increases linearly with, well, decreases linearly with omega. So if you have a larger omega, these cohesions will decay more slowly. But I mean, if you only want, I don't know, 10 to the minus 5 digits, you don't need that many. And also, if you want 10 to the minus 15, you also don't need a crazy amount. So this is how I made this figure here. Let's watch it again. This is just using the Markus Takenaka. Just using the Mankostakanaka basis. And it works on the whole real line. So there is technically a boundary. These coefficients have to decay, like 1 over x as you go away to infinity. But it's a very soft boundary. It's like the accuracy does sort of get worse as you go further away from zero. But it's not like Fourier, where you have a hard periodic boundary. And it's not like Hermit, where it's a very Like her meat, where it's a very quick, soft banding that's you know, it's super exponentially decaying. So it's more forgiving as you go out away from the initial window you were interested in. And of course, this resolution power is superior as well. So we also found some other bases. I don't think I've got much time to go through them, but we also found some. Through them, but we also found some bases that are in terms of Jacobi polynomials and tanch. And these also have a linear dependence on the frequency. But they're also, these functions also, these also decay exponentially, so they're also unforgiving like Hammond if you stray away from the window where you've scaled it. But I still wouldn't recommend. But I still wouldn't recommend these or their approximation properties. But they have this tridiagonal differentiation relation. They come from continuous Hahn orthogonal polynomials. So now I'll just tell you a little bit about what we're currently working on. So one thing that we realized on a wonderful trip to Poland to visit Carolina was that these T systems they live in. They live in Krylov subspaces. But they're Krylov subspaces of the derivative operator. So you have your phi 0, your first basis function, and the next one is in the span of phi 0 and the derivative of phi 0. And the next one is in the second derivative of phi 0, first derivative of so on. So it's because if you want to get to phi n plus 1, To get to phi n plus 1, you're looking at the derivative of phi n and some other previous phi n's. It means that we should be able to use Kwilov subspace theory to construct these functions and get a bit more than what we've got so far, where we've been looking at orthogonal polynomials. But also, we should be able to analyze convergence theorem. And also, you know, if it's evolving in a cooler subspace, you can use a lactose. In a querulous subspace, you can use a Lactus algorithm to generate these functions. So, in preparation, we've got a theorem that says your basis, if you have an inner product where you have an integration by parts property, and your phi naught is not a solution of a homogeneous constant coefficient of E, then this algorithm will give you a T system. It's off the norm. It's often off. I'm aware that time is running out, but what we're also interested in is the Hamiltonian of a Schrodinger equation. If you can make a basis that is orthonormal with respect to this sesquilinear form, and if you use a gloping spectral method, you should be able to preserve that Hamiltonian. So you could generate these. So, you could generate these using a differential line audio. So, if you're interested in this sort of thing, please come and chat to me or to Aria. I'm running out of time, so I'll just go to the summary. So, T systems are orthogonal bases and their differentiation relationships are tri-diagonal. And we have a very simple characterization for L23. Very simple characterization for L2 on the real line functions: that you take orphanomal polynomials, you multiply them by some function g, take the Fourier transform, and that's what guarantees this T-system property. And a very promising basis, which has been used in the past, is the Malka-Stakanaka basis. And in terms of the approximation properties, we have a really nice paper comparing different ways. Paper comparing different ways to approximate wave packets on the whole real line. And currently, we're looking at differential quilogue methods. And on Friday, Aria will talk about something called W systems. Trying to get the whole alphabet at some point, but so far we've only got three letters. Thank you very much. We spent a lot of time on the discussion on time. And are there any questions about space sensitivization? I actually have two questions, short one and a longer one. The short one is letter T and T system that stands for tri-diagonal. We also had eight systems where the differentiation matrix is Hessen, though. Okay, so here comes the longer one. In the introduction, you said potential could also depend on the solution itself, such that you have a non-linear system. When you're facing Linear system. Then you're facing more challenges, for example, the linear combinations inside a non-linear T, how do you propagate non-linear parts? And also, well, we have resonances, because I have oscillations coupled through a non-linear T, so you have to use the resonances. And if the non-linear T is not polynomial chemistry, can you see something about how to do that? But you know, I mean, you can use Magnus type integrators. My suggestion, but I've not thought into it. Carolina, Pernafsing, and I will very, very soon complete the paper. We are doing exactly the same thing. We are essentially led by Jason House and this sort of stuff to solve non-internal problems. I apologize for the discussion errors. For me, it's very interesting and actually, and it's actually most of your presentation is about different signals. I wanted to say that this is going to be, I think, very useful. Because we always have a problem with the boundary cognitions. Even if you want to solve, let's say, the split operator free has some dissociated system. So you either need a, you know, you need a bit. You need it to create or use this uh absorbing theory boundary condition and then have to work a lot with this. So I think this is r this is really interesting. And yeah, I didn't know. Maybe I have a couple questions. So how does it work with the, you know, how does it scale with the epsilon? Is it also the same refactor or I haven't heard before one epoch? So it's it's it's it's like one over epsilon with these Malkus Takanaka functions. Um Kanaka functions. So this omega the omega in in in in this is is supposed to be analogous to having one over epsilon. So omega is supposed to be like one over epsilon. So that this is one this was a complete surprise. So it's really pleasant surprise that that the resolution power is is just like Fourier. Thank you very much for the presentation. Thank you very much for the presentation. So, if I understand correctly, you were defining this instant for the whole real life, right? And the argumentation is that, okay, the Schrodinger equation sometimes is defined in the whole real line. But in some cases, we have the, for example, the radial Schr√∂dinger equation, which is defined from zero to infinity. Could you construct certain functions in that domain? Principle. I'll show you why. So, we started to explore these systems. We started to explore these systems using orthogonal polynomials and Fourier transforms, but then more recently we have this differential Lancas algorithm, which doesn't need any Fourier. You can generate the functions just with this iteration. All you need is these inner products, products of inner iteration. So in theory, this does not depend on the space. This could be on any domain. This could even be in high dimensional domain and you're looking at a partial derivative operator. A partial derivative operator. So this this is extremely general now. And in theory, we could have come up with this first and then and and never looked at the Fourier transform orthogonal polynomials. We probably would have noticed it later. But yeah, in principle, this could be used on radial domains or boundary conditions or anything that we haven't got around to it yet. This is current work. This is in prep. Thank you for your question. I take your question. Prime theorem for, let's say, a more general initial condition? Something... I mean, you have that weight package which is multiplied with the cosine, and I can see why you do it. But I mean, from a numerical or let's say practical point of view, it would also be interesting to see more general More general initial conditions, right? This is open questions. The approximation theory of these functions is difficult. So, Christian Lubich has done quite a lot of work on the Hamid functions. So, there's a theory you can have where the order of convergence depends on how many powers of the annihilation operator you can have on the function. So, p plus x. But for mother. But for Manushtakanaka, there's open questions about how the approximation theory works. There's a lot of surprises. So we stuck to this example as a sort of stereotypical way of encoding frequency and shifts and stretches to at least get a handle on a specific example. This is all we've got that is indicating. Dating that we get linear dependent on the frequency. So, any work in that direction would be it is quite amazing that convergence of autogonal sequences for analytic functions in bounded intervals has been completely resolved one hundred and ten years ago by Bernstein Analysts. On infinite intervals, there is no convergence below the constant. Convergence theory, for example. And I talked to just about every leading approximation theorist in the world about it. They all agree there is no approximation theory on the real line of Afghanistan. Okay, the last one, the short one. So you are mentioning that the Hamit polynomials, they are problematic because they decay too fast. And so in all the cases, the basis is fixed. Is fixed. It's time independent. Or not. So, for example, if you could move somehow the permit bases would be fixed. So, yeah, so people make permit bases work, right? Lots of people are able to fit them to their problem, with if they know where their solution is confined. But the what I'm having in mind is is situations where you you don't have that information. And so it's just hermit functions are very unforgiving if you get it wrong. Functions are very unforgiving if you get it wrong. But yeah, if you do know a window where your solution is defined, and you can it could move, then that