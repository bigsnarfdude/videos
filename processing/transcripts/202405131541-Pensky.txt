Good afternoon, everyone. I would like to thank the organizers for inviting me here. I'm very humbled to be here. Like, we have three fantastic speakers already, and we are going to have two more, all from extremely famous universities. Uh, who knows where the University of Central Florida is? Yeah, I did two. So it is Mickey Morris University, Orlando, Florida. Orlando, Florida, all Disney parks. So, I am going to talk today about multiplex networks, about clustering and inference in those networks. And this is an outline of my talk. So, we will talk about the model, the particular cases that already are really big, which are algorithms, theoretical results, simulation, and real data examples if we have time. So, multiplex steps. So, multiplex networks are networks basically as a collection of networks, and there are many ways of modeling networks like this. But we are going to consider the so-called multi-layer networks, so we have the same nodes in every layer. So, you can think of it as in Genivera Allen one modality collection. So, we are looking at one modality, etc. And there are, of course, many examples of this, brain networks being one of the examples. You can look at trading networks and so on and so forth. So, we are actually going to use the generalized dot product graph model. We are going to start with this that Jessun Jin just talked about. So, in the generalized graph, Gravel dot random dot product model matrix P is modeled like this. So he was talking about basically this model. So you have U, you have matrix Q, which is K times K, and U is basically like the normal matrix. And if you do the S V D of Q, you can write P in this form, but X is just a matrix. Just a matrix. And you can go from one representation to the other easily. And IPQ is exactly this diagonal matrix with plus ones and minus ones on the diagonal. So this allows to consider all the models. Now and this is exactly the advantage. The GR DPG network is very flexible and it actually models all of the above. Models all of the above. All four models that just Sean talked about: the popularity adjusting block model. It's just a matter of adjusting representation. And that's why, kind of, if you go with this model, you don't have to choose. But it is exactly what again Jessun talked about. What about the sign? It is very hard to say to It is very hard to satisfy the condition that you have matrix P is indeed matrix of probabilities. It is not hard to make it between negative 1 and 1. This is trivial. But making it non-negative is a challenge. And that's why, if you look at all the simulation studies in the literature that I saw so far, they actually simulate one of the four models that you have. That you were saw in the previous talk. So there are two solutions. One solution we saw in the previous talk, let's look at the conditions when it is indeed probability. Or why don't we just add the sign? Because actually, the sign is very often indeed present. When you have a stock, it can go up and down if you model it. Model it starting with some level. If you have actually brain networks, in Genivera's talk, they were weighted networks, but when you make them into, I mean, it was a very tense earth numbers, but when people make it into a network, very often they just remove the sides. It is either one or negative one. But why? You can have positive and negative connection in the brain. Negative connection in the brain, same as in the social network. But unlike in the social network, where there is this balanced theory where you have each triangle have the product of the science is positive. It is not true actually in biological networks. I took a brain network and I just started looking at triaments, and you have some proportion of triangles. Proportion of triangles which have three negative signs or one negative sign and two positive signs. Because there is no reason why you have this balanced theory in the biological networks, which brings us to the sign generalized dot product graph mode. So we still want to have ranked K model. So P is still like this, or like this. But we will assume now that. We will assume now that P is between negative 1 and 1. And what we observe? We observe a connection and we observe the sign. So sine is terrorist. Social scientists will say that it is possibly sticking, that you can have positive and negative zero, whatever. This is not for modeling social networks. Social network is a different piece. This is more for modeling biological networks, where scientists means say very often networks are. Very often, networks are coming from correlations. So, sign means basically the nature of connection, which we also can kind of observe and we sometimes observe. And of course, everything here, because of this formulation, if you want to just have a generalized product brand model, everything I'm talking about today will work. So it is not like it will work for sign-up, not work. It's not like it will work for sign and not work for the without sign. It will work with signs and without signs. Now, I said that I will be talking about the multiplex model. So, now let us consider what multiplex model we will look at. So, we are going to look at variant diverse formulation. So, we have L capital layers and L types, and we consider that the number of layers. And we consider that the number of layers is reasonably large. So you are not interested in the case where L is five or ten. So you have large L, large L, and large large L and large L, large number of nodes and large number of layers, small number of types and small number and small rank. So the matrix of connection probabilities we are modeling like this. So Um is So UM is the basis of the group M of the group of layers. So, and I will give you special types and you will understand what it refers to. So, in each group you have a build subspace structure. So, if you think in SBL, you have the same community structure. Or you have similar kind of mixture probability. Mixing probabilities. But Q, which this is the individual basically parameter. So QL is the parameter of the individual. So each individual in the network, each layer of the network can have this Q which is completely different. We are not assuming that there is any connection between them, except that everything is similar. And you can actually very easily extend the tool. Very easily extended to non-symmetric case. I am just considering symmetric because this is the more usual formulation. So, layers can be partitioned into groups, such that layers in the same group are embedded in the same space. And to my knowledge, it is the most general model-based multiplex network model. I mean, of course, there are infinite number of models, but if you want to have some kind of red K model, Some kind of REPK model, and here you allow even the sign. If it is something is the ones, negative ones, of course, if you have normal distribution, whatever, it will be somewhat different, then it is probably the most checked. So you are observing the adjacency tensor, and it has either ones or negative ones, and you would You would have several problems, sub-problems. Here you would like to recover the layer clustering function. Why? Because if you know groups of layers, you can then pull the information on each group of layers together, and it is always good to do it. And then we would like, of course, to recover the subspaces UM, because if we recover UM well, and especially actually in the paper, I have In the paper, I have also the results of recovery into infinity mode. Then you can recover your QL and then you can do subject analysis. And of course, if you would like to have fast convergence rates, I'm a mathematics department, so I have to do this. So this model actually is the generalization of the model that I will have in my. Of the model that I have been working for a while with my PhD student, Joshua Bank. And we were considering exactly the same model minus the signs. So we also wanted... So the only difference here was that we did not have the sign. Everything was non-negative and we assumed that matrices are such that you have everything non-negative, as they usually do GRT. And the problems were exactly the same as here. were exactly the same as he. Now this model in itself is generalization of a whole lot of models. First there is the common subspace independent H Causier model which is just basically the same model actually not as it's just dimple GR DPG they don't assume the sign so they have Don't assume the side, so they have only one matrix U and they would like to recover. So you estimate matrix U and then you estimate each URL, but I'm not going to talk about that. There is a separate normality and you try to speak. Now the other things, of course, there is a huge multitude of the models where you assume that each layer follows the SBM. So this is another kind of level down. Level down. So, if you have SBM, your matrix of probabilities has this form. So, we assume that communities are the same in each group of layers, and matrices of connections, BL, are subject-specific and don't have any restrictions. And this, of course, is a particular case because your ZM can be easily reduced to Um by just multiplied. So, UM by just multiplying by the name of the there are several models that are particular cases of this SBM-based model. One is the mixture multilayer stochastic block model, which was published by Don Shea in 2021, an excellent paper. And it basically reduces to this model if you have BR. This model, if you have BL equals to BN. And then, and so you need to have SBM and you need to have BL of only M types. So you have basically K M types of SBMs. You have L capital types of SBMs because all our DLs are different. And then there is a huge number. And then there is a huge number of papers with persistent communities where they assume that my exist the same. So it is basically the scoshing model and as we have said. So these are particular cases and there are particular cases of these particular cases, but I think it is enough of particular cases. So why do we consider such a diverse model when there are so many models Many models already there are, and adding sites. Just because we just want to kill two birds with one store and consider almost everything, anything possible, as was suggested, but if I go. So let's do inference and then from there we can go and do something else afterwards. So the main motivation for doing something as diverse is you have. Is we have many few assumptions. In fact, our assumptions only that we can partition methods and partition layers. And why? Because there are actually many applications when you want to partition layers. Because you want to partition individuals according to something. But you don't know which structure this something refers to. So we assume that we can have different also ambient structures. Also, ambient structures for different types of things. So you would say, and how can we get such a network? So let us look just at very generic way of generating such a network because each of the four cases that we saw in the previous talk, you have the way of kind of generating it. So we assume that there are group members. Assume that there are group memberships are the group of layers are generated by multinomial distribution. So each of the groups of layers can be any of the types with some probability pi m small. We generate matrices xn because we need to generate matrices u manage groups of layers. So we generate matrices xm according to some distribution. And just the support And the chance support will be somewhere in between negative one and one. And we have some mean and covariance matrix. And then we have matrices B L they can be generated in any way possible. They are not assuming anything about them, so you just have matrices B L. And then if you have the S V D of X M in this form, then you can write In this form, then you can write your QL like this. So you have UM, and then QL is basically just related to UM. So here, I mean, to do analysis, we make some assumptions. So vectors, the groups of layers are balanced. The rank of each groups of layers is of the same order of magnitude. Covariance matrix. Covariance matrices. Now, this is an assumption, L3, is that covariance matrices have eigenvarius bounded above and below. Remember that K is small. We are considering the case that K is 2, 3, 4, 5, and that's why this is not a wild assumption. But you will see that in two particular cases, say SBM and mixed membership, it is not satisfied. And mixed membership, it is not satisfied. I will return. The number of groups of layers grows at most polynomially then. Why? Because we will need unit bounds. It is again a very mild assumption because it is polynomial growth and we have sparsity factor left. And sparsity factor we put, of course, in the matrix SPL and we assume that the sparsity factors of the same. The sparsity factor is of the same order of magnitude everywhere in the colonel. If sparsity factor is different, then theoretical analysis of it. And possibly the other is. So simple examples, Trunkian normal distribution, where we just take, generate a normal vector as then to take each row as its reduction to reduction to uh L to non being one and we take and then we generate n layers for matrix X1, X2 and so on. Now example 2, we can have multinomial distribution, but we take only k first entries instead of k plus one. So we Instead of k plus one. So we do remove the last one. And you will say, hmm, it is SBL, but you are removing the last immunity. So again, I will do that. So the last one is Diriclet distribution, which is needs membership model, and we are again removing the last committee to make matrix sigma not being imposed. And if you want, of course, to have anything with different degrees, you need to have generate theta. I'm sorry. You need to generate your vector theta according to some distribution, and depending on how diverse the distribution is, you probably will have different therapeutic properties. Okay, so now I have to tell what we did in the previous work. So, in the previous work, we We basically used k-means clustering for a proxy matrix. So it turns out that if your matrix PL has this form and you take the S V D of P L, then your UM and the S V D of P of your P L are the same up to some rotation. Now you don't know this rotation, it's too expensive. Rotation, it's too expensive to find this rotation. But if you take this, this depends only on the group of layers M. You don't need to find the rotation. You have the invariant quantity. And actually, in the case when you have symmetric networks, if it is symmetric, you have to use both. But you can actually take only n times n plus 1 over two values because it is you. Because it is, you have the C. You have basically two symmetric matrices. You are not interested in the both. So the algorithm is as simple as it can be. Find the matrices, you do the S V D of A, you do the vectorized versions, apply the S V D to the columns of this matrix very fast because you need to have only leading singular vectors and then leading singular vectors and then cluster them by k-means. It is very simple, very scalew scalable algorithm. You don't know of course Km. You don't assume that Kms are the same, but there is a saving race to you. We basically assume that you know Km. So each network when we do this when we estimate these matrices, we take the run of each separate uh network. Each separate network. Because otherwise, if you assume that you know them and your ramps are in groups of laser, two, three, four, good luck, which is for this network, two, three, or four. And if they are the same, of course, it's much easier, but we don't assume. Okay, so these are the error rates. So the between the clustering error, we assume that error errors are larger than the log, and it is much stronger. The rogue and it is much stronger condition than in the for the twist algorithm or don't cheer, but we have much more complicated setting. And this is the between layer clustering error, which is not so good. And the most upsetting thing here is that when we look at the subspace estimation error, this one, basically the layer clustering error is dominant. If we assume that m is equal to 1, we have Equal to one, we have in POSI model they have this. And we have actually a sum of letters, which is it. So, what is the idea of how to improve it? So, let me just peddle for the idea. So, assume that we have support of F belongs to the unit boom just to make life easier, or it can be L1, or it can be cube. It is since kms are small. Since kms are small and bounded above by k, it is just a matter of some factor of k. So it does not matter. So covariance matrix is sigma m, but the means are zero. So construct a sample covariance matrix. Then ul is actually given by this. It will be the orthogonal, the eigenvectors of an orthogonal space. And this, and you can easily show that this is true. Easy to show that this is true. So each row has a small L to column. And then if we consider these vectors theta L, then if they are in the same group of layers, then the scalar product is here. But if they are in a different group of layers, then the scalar product is very small. So if we took the matrix of scalar products, this will allow us perfect cluster. This will allow us perfect clustering. We will say, so what? If I knew exactly the means of the vectors in k-means clustering, I would cluster it perfectly well, too, right? So what is so special? And in addition, so these vectors, of course, are unknown. But in addition, the mean is not equal to zero. So what are you talking about, right? It turns out that it can be a just. And first, Adjust. Of course, of course, the mean is not equal to zero. The means are mu m, but you know they can be easily estimated by the mean, right? And which is even better, if you look at this, basically, if you would like to remove the mean, we just need to basically, if you have a projection operator on the space of constant matrices, then matrices, then you will basically have your xm, the S V D of X M, and the mean will pretty much will be close to zero because the mean will be zero, the expectation is not zero, but the mean and expectation are very close to each other because m is large. So it does not cause a big error. And then if we take matrix psi, then we will have Matrix psi, then we will have this property that we had before. It does not matter that we don't have zero mean. And oops, and then of course we can instead of P we can use A, so we form matrices A tilde L where we project them on the space orthogonal of the L Orthogonal of the constant. We find the S V D's of the first K L vectors. Then we construct scalar products. So basically after that you can use simul definite programming to find the matrix of similarity matrix Y hat and then we can cluster the rows of similarity matrix. Of simulators. But in general, this almost like thresholding. It just works better when you have these intermediate steps. So now you will ask me, and what about the SBM and the mixed membership model? We assumed this, but this is not true. In those models, xm times vector of ones will be vector of 1. So obviously your Your covariance matrix has rank which is one less. But assumption, so our assumption actually A3 fails, it doesn't work here. But the technical theory still works well. Because, because think about what we did. We projected it on the orthogonal space, so we removed this vector. It is not there anymore. So what we need now. So, what we need now is just after we project it, reduce that. And you remember that basically rank we are anyway finding empirically. So, when if we suspect that it may be SBM or mixed membership model, basically just you're when you're looking at the rank of your layers, just projected on the orthogonals. Projected on the orthogonals, on basically use the orthogonal projection, find their end, and it will give the true one if after that. And then the theory will be exactly the same, because everything will be repeat. So, simulations, this is just the comparison with our previous technique. So, the dashed lines are the previous technique, so they work. So, it works better. This is in different simulation settings. Okay, now back to the science. Where do the science help? So, the science, in some applications, as I said, science are removed at the stage of modeling data. It is actually not unknown. And it is known that it is bad, but people still are doing it. You can find papers where they would take data. Would take data and when they model network, they remove the signs and model them as zero lots. So it is actually not a good idea. You can just model them as plus one, minus one with zero, and then you actually gain a lot of traction. So this is a very simple example. We generated a network. So here we generated entries of BL. So we are not actually assuming that the Actually, assuming that our networks are assorted, they are assortative networks, these assortative, they are all sorts because we just generate the entries of the others uniform random numbers. So you can have anything. Some will be assortative, some disasterative. Anything can cover, basically. And here I'm showing you results when you take the Then you take the entries between negative 0.2 and 0.2. And if you keep the signs, you practically don't have array. If you don't keep the signs, this is how the array behaves. Actually, if you keep the sign, it works with 0.005. It is really sparse network. If you remove the signs or generate it without signs, the network is sparse and The network this sparse and this diverse, even of five months. So, uh, I don't know whether okay, real data example. Please don't hold me to high standards. It is a very small, illustrative real data example with very small subset of the data, just to illustrate the idea. Obviously, which should and can be extended and cannot compare even remotely to what you Even remotely, to what you did with your talk. So we looked at one autism brain image data exchange, and the first set of this IPD, it is autism spectral disorder, 18 individuals with autism and maintain controls, 116 brain regions, 145 time instances. Instances. So extremely small n here, and sorry, extremely small l and n is 116. So the goal is to try to separate the layers. So we process the fMRI signals. These are a REST fMRI. We use WC4 wavelets, we threshold soft thresholding. Softish holding, and then we reduced the number of brain regions. Actually, there are papers associated with this data cell, and they said we cannot find anything. But they, of course, have not read the paper of, I don't know whether they pronounced it correctly, Jin Chin, Fan, who said that if you don't do model selection, it is as good as pure, yes. And majority of brain regions, of course, are not, have absolutely nothing to do with autism. They're responsible for your eating, grieving. For your eating, breathing, moving, and what's not. Because autism is very high-level disease, so we actually used 30% of highest correlations by absolute value. We used 10 capital equals 2, 2, K equals 3, and then we repeated our response or about 100 simulation ranks. So we obtained 15% clustering error when you keep the size of that. So it was these are. So, it was disappointing effort. So, if you keep the same, it is about 15% error. And if you don't keep it twice as much, the variances are, standard deviations are very similar. And the interesting thing is that you have average number of edges in two groups are almost the same, but you have a different number of negative edges. So, the negative edges are almost 13. Are almost 13 on the average for one group and about a little more than 8 on the other. And then we evaluated how your signs are not and without signs your spectral distances are between subspaces, and you have better results if you keep the signs. And this is just one picture. So this is the brain network when we have the signs, red for positive, blue for negative. And this is what you will see if This is what you will see if you don't keep the science. So it is the same picture, but you can see that it gives you some information. And I think just I think it was your paper where you examined this sign theory and said that in majority of situations it is not true. So these are the references. The references, and you know, today it is Monday the 13th. So, I know that in the Western Hemisphere, Friday the 13th is considered to be a very unlucky day. Actually, in Russia, it is Monday the 13th, because maybe people are not working so hard, but they drink very hard on the weekend, so all accidents happen on Monday. So the only thing worse than Friday the thirteenth is Monday the thirteenth. 13th is one benefit. But you know, not for everyone. Like this cat said that for him, 13th has always been lucky. If you can count, there are 13 tigers here. And you can always turn your luck around. Okay, so that's all. And I hope that the 13th, Monday, the 13th today will be a lucky day for you guys. I'm wondering whether you also studied at the estimation error of the UN. Yes, yes. We have results from estimation errors of UN, of course. Actually, there should be a slide. Actually, there should be a slide where I think I skipped it by mistake. Let me show you. I have a slide on estimation error UL. Ah, here. This is the slide. So if assumptions hold, then uniformly you have this condition. You have this very high probability between layer class and error zero. And this is the error in Freudinius norm. We also have the error. Norm, we also have the error in the correctional norm and to infinity norm on the paper. I just put it, I don't know how I skipped the slide and did not show you. Yeah, these are the results. So you have very and the thing is that you have under this condition, which is much stronger than condition in your paper, you have n rho n times l greater than log n to some power. But you have But you have something comparable because this is just a small term. It is basically this comes from the fact that our mean is not here. I actually would love to talk to you about this problem and some other things, maybe later if you have time when you feel less judged by it. Thank you for the question. Yeah, I don't know how I strip this one. And this is actually the main reason why we are doing this. You have between layer clustering is practically under this condition almost perfect, and you have good subspace estimation error, and it is even better into infinity and all. Basically, and then you have you can do this basic. Do this basically central limit theorem, recover your QLs that are subject-related parameters, and then you can do inference on the subjects. I mean, you don't do anything about QL in the paper because it would be completely side project. I mean, it is not of the interest in these regards, but it is a natural extension to both of them. Any other questions? All right, if not, then thanks to being here again. I think we didn't have a two-minute break because we're being all early.