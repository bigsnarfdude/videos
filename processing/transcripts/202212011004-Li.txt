Thank you very much for the introduction and uh thank you for the organizers to organize this wonderful workshop. Uh so today let me talk a little bit I would say half numeric and half rigorous stuff. So it's like how can we use the coupling method to detect the underlying dynamics? So I mean it's it's more like the multi-scale problem of the coupling time distribution. Time distribution instead of the multi-scale dynamics itself. So it's like I want to use the multiple time scale of the coupling time distribution to detect some underlying dynamics. And you will see some interesting application in computer needs. So first, some very brief overview about what my group does in the recent few years. So basically the one focus of my work in the The focus of my work in the recent few years is about something I call it data-driven computing. So, the idea is that we have some simulation data plus X. This X, it could be some traditional PDSOR or could be some modern deep learning approach or it could be something else like a coupling technique. So, we did lots of work about how to solve a Frog-Planck equation. Basically, from Basically, from low to very high dimension, and how do you carry out the sensitivity analysis against some small perturbations? And also the QSD that we just mentioned, what does QSD look like, and how sensitive QSD is against all kinds of multiplications. And also some neuroscience work about the use of the deep learning to basically make a surrogate of biological neural networks. Uh, real veroticle neural network. And in this talk, let me talk about the detection of underlying dynamics by using the coupling technique. So, the idea is that, okay, so I run the coupling simulation and I collect lots of data for the coupling type for different magnitudes of noise. And then I want to use this data to detect the dynamics by using the different time scales of the Different time scales of the coupling root. So let's go into a little bit more details. So the idea is very simple. So assuming that there is a dynamical system here, it could be some simple iterative mapping or could be ODE. And the dimension could be low or could be very high. Like when you train an artificial neural network, then you are working in a dimension, maybe a few meetings. Okay. So the idea is the following. Okay. So the idea is the following. So let's add a small noise with magnitude epsilon. And then we know that so under some mild assumption about the dissipation, so there should be invariant probability measures if you have some good dissipation at infinity. And then we also know that, yeah, so unless you really cook out the dynamics carefully, otherwise you should expect some exponential coverages. Some exponential convergence to this steady state. So I let this rate of convergence to be R of epsilon. And it turns out that this relation of R epsilon versus epsilon, that's very important. So basically, you change epsilon a few times, and then you collect this R epsilon. And this is the scale of R epsilon. Actually, it should be scale of R epsilon versus epsilon. It reveals lots of the underlying. It reveals lots of the underlying dynamics. That's interesting because we are living in a 3D universe and it's difficult for us to imagine very high dimensions, but this provides some way for us to understand what happens when the dimension goes really high. But here is one difficulty that so how can we get a sharp bond of epsilon? I believe that's a relevant topic for many applications. For example, in this workshop, I have heard a few times. Uh in this workshop I have heard a few times that uh I need the uh spectrum of focus bank operators and like that. And uh uh in other more applied uh scenarios so this is this guy is extremely uh important if you want to do any kind of sensitivity analysis. So anyway, that's like a difficulty. So how can we really accurately capture this speed of convergence or maybe at least give some estimation? Because this is like Mention because this is like some eigenvalue problem. And if you do some traditional PD, then probably you can only do a simple 2D problem, at most 3D. So if you have some higher dimension problems, then there's very little hope. So our approach is that we use a coupling. So we use something I call it a numerical coupling method. So what is a coupling? Coupling is like, okay, so if I have one multiple person. If I have one multiple process, I call it a phi in some phase space, so that's uh just one process. And a coupling is like uh have a pair of processes on the product space, such that each marginal distribution represents one realization, with I mean, of course, with different initial distributions. And then they represent just random. It does not have to be independent, but I mean, of course. But I mean, over independent coupling is one kind of coupling. So I run them until they meet at somewhere. And if they meet, then I couple them together and they will never separate. So that's the rule. And the time when they first meet, that's called the coupling time. So I think that's kind of a well known thing is that uh so the uh T V norm is kind of controlled by the prob probability that you have not coupled yet. That you have not coupled yet. And actually, does not have to be TV norm. If it's a W1 norm, you can also, if you, sorry, if it's W1 distance, it's also controlled by the coupling time. So the coupling time basically tells you that how far these two distributions are at a particular time t. And more interesting is that the coupling inequality could be an equality. Inequality could be an equality for some coupling. So basically, there exists an optimal coupling in such length. So the probability that you have a non-coupled is exactly equal to the total vibration distance. I mean, of course, this optimal coupling may not be marketed, which means you have to have some knowledge about the future to construct an optimal coupling. And the existence of an honest Existence of an honest optimal coupling kind of is solved in some scenarios, but for the most generic setting, I believe it is remotely okay. So anyway, that's the idea. So which means the coupling can actually tell us that, yeah, so if you run some coupling and you get some good coupling time distribution, then it gives you a lower bound of this geometric equality rate, this R x. This order. And then you can also use coupling to get some upper bound. Why? Because you can split the phase space into two parts, A and B. It could be time dependent. And because we know that there exists one optimal coupling, and you separate the phase space into A and B. So I don't know what the optimal coupling is unless I know the future. But if this optimal coupling is a very important thing, If this optimal coupling wants to couple, it has to cross the boundary. So it has to cross the boundary jump from A to B. Okay, so the minimum time of the minimum time for the first axis either X either X leaves A or Y leaves B. So this gives you an upper bound of this geometric equal distribution. It basically tells you that, okay, so the speed of conversion cannot be faster than this and cannot be, sorry, cannot be slower than it cannot be slower than this and cannot be faster than this. Just like some theoretical foundation. And another difficulty is that how can we implement this numerical Uh implement this uh numerical component. Because uh uh as I said this is a this work is half uh rigorous and half uh uh computational. So uh the idea is that of course the every numerical simulation comes with error. So which means if you just run the naive coupling method no matter how you couple you have the risk of near missing each other because of the errors it could be like the theoretical process already meet but the numerical process because of the error if you just Because of the error, we just nearly miss each other. And also, many theoretical coupling methods are not very robust. So, the solution is that you have to switch at some point, switch to a more robust way of coupling. So, that's called the maximum coupling. So, the idea of maximum coupling is very simple. It's that at a suitable time, you want to switch to this maximum coupling and only compare the probability density function at the next step. Density function as the next step and couple the overlapping part. For example, if you have two density functions, one is blue, one is red, then you want to couple the azure part, which is the overlapping part. I mean, of course, it doesn't work if they are two apart away, but you can use some other coupling technique to bring them together. And then you run the and then you compare the probability against the function. And this maximum coupling is robust against some. Coupling is robust against some small perturbations, so which means it makes the numerical coupling to be robust. And let's take a warm-up example. I mean, of course, just 1D system. It's not like the super difficult high-dimensional dynamics. But let's just take this just take a look at the graduate or maybe even undergraduate textbook to see this. Language textbook to say this. I have some 1D iterative mapping. And let's see that the first one is like some well-mixed map because you have this 2x and then you move everything by 1. And the second one is that it's erodic, but it's not mixing. So they say it's like some directional rotation. And the third one is bad. It's like a logistic map. And you have a stable periodic orbit that you are jumping between. Periodic orbits that you are jumping between two points. Okay, so let me add some small noise and let's couple the trajectories and let's switch to the maximum coupling when the distances are really small. And then let's take a look at how is the exponential tail of this coupling time distribution change with epsilon. So we can actually show you the picture later on, but the result is that so for the first Result is that so for the first one, our epsilon versus epsilon is linear. The second one is quadratic, this last one is exponential. And the heuristics is very simple. The first one, the dynamics itself helps you to mix the two trajectories. So obviously you have lots of chance to see them come together. And when they are together, they can be coupled. The second one is like it's not mixing, it's equal. It's not mixing, it's ergodic, which means you have to use the noise. You have to let your noise to drive it to trajectories together and make them couple. But the third one is that you have a stable periodical orbits, which means the noise has to make a trajectory to cross some barrier in order to couple. And of course, that's much slower. So you can see the remarks. So the first one, when it's well mixed. When it's well mixed, you can see that the speed of coupling gets slower, but not dramatically. And then you take a linear alterpation, you can see that it is roughly linear. And the second one is much slower because you have to let your noise drive two trajectories together. But because it is irrational rotation, so there is no resistance basically. So it's slower, but still okay. And the third one is like it's really slow. I mean, look at this time scale. Look at this time scale. Look at the time scale here. So you have here I calculated two things. I mean, one is this coupling time, and the second one is the minimum of two first exit times. You can see that both of them are scaling like, so I mean you can see that both of them are scaling like exponential slopes. You can see that, yeah, so this one is the, look at this n, it's like magnitude of 10 to the power of 12. It gets dramatically dramatically. Power of 5. It gets dramatically, dramatically slow. And then you take a look at it as well. So you can say that, oh, it's this is like the natural log of the slope times epsilon squared versus epsilon squared and you got to be there. So it's like exponential of some constant divided by epsilon slope. So you can see that this scaling of uh uh component time distribution actually tell you uh what happens at the uh underlying dynamics. Underlying dynamics. So after this warm-up example, let's try to go higher dimension and I want to do some interesting declaration. So when we have a stochastic differential equation, so how do we couple the stochastic differential equations? So that's some continuous time mark process and there is transition kernel and we all know that the Oriela-Variyama scheme is very simple but it's consistent. So it's numerical anyway, so let's just cover the numerical trajectory. So the idea of coupling stochastic differential equation is that I want to make use of the symmetry of the Browning motion. And I want to make sure that actually there are a few ways to do it. You can just set the noise to be independent. But if they are independent, it only works in 1D. In higher dimension, it doesn't work. In higher dimension, it does not work. Or you can let them be synchronized, use the same noise. It works in some very special scenario when you know that you have a good random attractor. When you know that if you work on the random dynamical system, you follow one trajectory and then you convert to some random attractor. Then that's the scenario that the synchronized component work, which means you let your two trajectories follow the same noise. Noise, sometimes it works. And more generic settings, the reflection coupling works better. For example, you want to mirror in the random terms. If the first trajectory, the noise term goes to left, then the noise goes to right, then you want the noise term in the second trajectory to go to the left. So you want to mirror them. And this is called the reflection coupling. Actually, it is known that the Actually, it is known that the reflection coupling is optimal in some scenarios. So, anyway, the strategy is simple that we have some mixed coupling, which means at first run either reflection coupling or synchronized coupling if you know that there is a random attractor, or independent coupling if the dimension is really low. Until you see these two trajectories are really close to each other, and then I switch to the maximum coupling. The mapping coupling, I want to compare the density function. And if a success, then okay, you're coupled, otherwise, it would just step away. And we proved that the way of mixed coupling is basically consistent with the numerical coupling time to is not very far away from the true coupling time, toll I mean is not very far away from the true coupling time for any. True carving time for any finite T. So we do have this consistency. So and then let's so basically some summary that well so for the lower bound we run the mix the coupling repeatedly and you collect like maybe a few hundred or maybe like a few thousand of data and then you can plot this coupling time you have a distribution and then you can plot it in the log linear Plot it in the log linear plot, then you have a slope. You can see the slope of this probability. And this slope is basically telling you that's like your geometric ego density rate. So you get some lower bound of this R oxen. And how about the upper bound? So the upper bound is like you want to have some good division of the phase space into two parts. And so the general rule is that you want to make the transition between Transition between A and B to be as difficult as possible. For example, if it's double weight potential, then you want to divide at the peak of the barrier. And then you simulate and you capture the minimum of these two first passage tanks and you do it repeatedly. And then you do the same thing, that you plot it in low linear plot and you especially this loop and then select some hopper bot. So basically there are the It tells us some information about the underlying dynamics, and actually, there are two strategies of doing that. So, one strategy, I would say, it probably has more theoretical value. Is that you run this? There are some people, it's like a run synchronized coupling using identical noise. So, when the noise is identical, it means that the It means that the underlying dynamics provides the force that drives two trajectories together. And you can do it in two different scenarios. One is that you have a negative Lyapunov exponent. And basically, it's like you have a good random attractor. You have a way to connect this Lyapunov exponent and the special gap of the infinitesimal generator. And the infinite tensimal generator. And the other way is that if it's super chaotic, and you have some good estimate of this, something called the EOC chaos, and that also provides you some way to estimate some connection between the spectrogap of the infinite testimonial generator and the dynamics when you just follow the trajectory of one noise. And the second strategy is the strategy that I'm Is the strategy that I'm following here? So let's run the reflection coupling using the symmetric analysis. And of course, we change it to the maximum coupling when they are really close to each other. The reason is that the reflection coupling is kind of optimal for a class of SDEs. Basically, you have some good gradient flow is optimal or near optimal. And this gives some a good estimate of this conversion. A good estimate of this convergence in many applications. Okay, so let's go to the things, the more exciting things, and let's try to see how can we use noise to component to detect the underlying dynamics. So this is a little bit similar to the old problem: that can you hear the shape of a drum? Like if 50 years old problem, so we know that when we knock a drum and we hear this. Knock a drum and we hear this sound. So it's basically like how can we determine some geometry just based on this spectrum. And we use this idea. We use this idea because the copper engine basically gives you some lower bound of this. So it's difficult to know the shape of some high-dimensional landscape. And you are seeing this everywhere in deep learning, in molecular dynamics, and in basic. In molecular dynamics and in Bayesian inference. So the idea is that let's hit this landscape by injecting some small noise to this gradient flow. And then let's capture the coupling time distribution at different magnitudes of noise. And then the scaling of this coupling time can tell us some interesting information about this underlying high-dimensional landscape. So the idea is the following. So we have some stochastic. Have some stochastic gradient flow. So you have the gradient flow, and then I have some additive noise. And then you just run this mixed coupling for a few different extremes. And for each coupling, you want to collect maybe a few hundred or even more trajectories so that you can have some distribution for coupling time. And then you want to estimate this tile distribution by plotting things in a log linear plot. And then you can see that actually these are absolutely strongly, strongly related. Absolutely strongly, strongly related to the landscape of this potential. So the result is the following. So if there is one global minimum, like you have and everything converts to the bottom, then actually this or epsilon does not change with epsilon. So it's like the component gets slower with the noise gets smaller, but the tail does not change. To change. And more importantly, this R epsilon approximates the least eigenvalue of the Hessian matrix at the moment. But when you have many local middlemarks, then it's like you're going to the scenario of this periodic orbits. It's like the noise has to cross some barrier in order to make things coupled. So you can see that it really is really slow. And this R decays exponentially fast for small. Exponentially fast for smaller epsilon. And more interestingly, is that you take some linear actual attributes of this guy, then it can actually approximate the maximum height of the barrier. So you may have a very rough landscape. But this linear articulation of this log of coupling grid can give you some approximation of the maximum. Approximation of the maximum height of the barrier. So, I think I don't want to delay a coffee break, so let me just go to the numerical result. So, this is joint work with Shiro and Moni, actually. Okay, so basically, you have this idea that if there is one local minimum, then you are going to basically the linear local dynamics when you have small. And you have, if there are. And you have, if there are many local minimums, it's like you go to the first exit problem because at least one trajectory has to cross a barrier for you to couple that. And the idea of proof is that you have some basically random sum of random variables because for each coupling you may fail. And you fail, you have to start over. So then we go to the numerical result. So if there is one global minimum, you can say that the coupling time can get slower, but the 10 can get slower, but if you measure the tail of this common time, you can see that they are identical, identical, identical. So that's the dimension from 2 to 8. I just show this one with something that I know the hash matrix at the bottom. And if you have more than one locomotive, like this double wave potential, then you can see that the component time gets slower, slower, and very slow. However, if you measure this low, However, if you measure this log of coupling width and you do some linear actual pollution, you can see that this intercept basically overlaps with the theoretical height of this barrier. This is telling you that you can actually use this way to detect where the whole height is the maximum barrier. So let's go to deep learning. So one very interesting and very difficult problem is that what does the landscape of a loss function look like? A no expansion looks like when you train some artificial neural network. As I said, we are living in a 3D universe, so our brain is not trained to imagine super high dimension. So here I just do some illustrative example, but actually for larger settings, it also works. So I have a very small three-layers artificial neural network. And there's only two hidden layers. So I changed the settings. Change the settings. So, small network means you only have four and a three new room, medium means you have 20-10, large means you have 20-20. So, turns out that well I can have the loss function, loss surface, and then I use the coupling technique to detect the landscape. And it turns out that, yes, so the smaller the network is, then the higher the barrier is between local minimums. So, I don't know how many local minimums there are, I don't know where they are, but I know. I don't know where they are, but I know that if the network is small, then you have some high barrier between them. And if the network is high, then the bottom of this neural network is the bottom of this neural network loss function is very flat. You cannot see very high barriers there. So this is consistent with many other literatures saying that if your neural network is over-parametrized enough, means that the number of known parameters is bigger than your training size. Another parameter is bigger than your training set, then the bottom of lost surface is flatter and you have better ability for the generalization. So that's the result. You can see that for this small neural network, that's the ground truth. And for this small neural network, it's like you can have some bad locomotives with poor training results, and you can have some good ones. And there are actually some pretty high barriers between them. But if the size goes larger, such that you go to the overall. Other such that you go to the over-parametric domain, then we run this for thousands of times and we have never seen any bad local mainland. So basically, all local minus has very nice performance. Okay, and that's the that's our detection of the coupling time. You can see that small network, so basically small network means, I mean, look at this scale. So small network means that you have slower results of coupling. Coupling and then you take some linear altruism, you can see that also the height of barrier I detect is much higher than when the neural network goes larger. Okay, so I think I have used my time and I would like to acknowledge my collaborators, Mo Li from Jonga Tech and my previous postdoc, Jiao Yu, he's working in Shanghai Tech, and also Shi Ru, North Li Shu is working in Jimmy University in China. So thank you very much. So thank you very much. You have multiple slides that detail probabilities of the cutback time somehow are exponential. What are the assumptions on your systems that you can guarantee this? Well, yeah, here we have to assume that you have this exponential triop, but it's really Tail, but the reality is that you have to very carefully cook up the dynamics to get no exponential tail. So, I think for the SD, I only know one example. I only know one example that you can really have some sub-x, sub-exponential tail. It's very difficult to create a weapon. So, yeah, you're clear. You're right that I have to assume that this to be I have to assume this tile to be exponential. But the reality is that you have to create some very special mechanism for the trajectory to get stuck somewhere in order to avoid exponential rate of equal density. So I think I only know one example where they have lots of rotors and then they can create some scenarios and to get the polynomial rate of sorry, they get the sub-external rate of polynomials. Is the system still geometrically ergolic? Is that you describe still geometrically ergolic? Which one? No, it's not. It's not. So that's that's why I mean you have to uh really carefully uh engineer some example in order to see that there exists some problems that are not uh geometrically equal to yes. So we have to assume it. But uh the reality is that so 99.9% of the problem. No, 99, 49% of the system that you meet is how it's all magical equalists. So in the SD case that you had, I thought it was additive modes, right? So there, I think that Dublin's conditions for geometric characteristics are fulfilled, right? And you can get explicit estimates of all the constants and its connections and so on. Constants and these connects and so on. Do you have a comparison of what you get with those? So I would say that if it's all done to Roger 1, it's almost optimal. But otherwise, I don't know. I mean, w w what we say is that it's the the reflection covering uh comes very fast, but uh we have no result of saying that it is. No results seen that it is optimal, but from what we see, then at least it's close to optimal. I mean, if it's all up to otherwise, I'm quite confident that it's either optimal or it's very close to optimal.   