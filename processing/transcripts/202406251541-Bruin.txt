Are we are we ready? Ready? You're ready? I'm ready. You're ready. I don't think Needs needs introduction right now because everybody knows and the title is written right there, so please just go ahead. So, please just go ahead and get on. Okay, yeah. First of all, thanks to the organizers to invite me to this workshop. I guess for lots of people here, it's not the kind of workshop that they usually go to because it's really a little bit of a different topic. I usually would call myself more of a number theorist, perhaps an arithmetic geometer. Geometer. So the geometry is really in aid of the number theory, usually. And the things I'll be talking about here really got developed because I wanted to use them for arithmetic questions about algebraic curves. And what I'll be talking about is hopefully, well, certainly, a little bit of a lighter theme after our very earnest discussions earlier this. This afternoon. And well, I think for geometers, I mean, algebraic Riemann surfaces, probably about the lightest and most enjoyable topic you can find. It's the origin of algebraic geometry, basically. And it has its origins in the analytic theory and does. And that's what I'll be mainly talking about. So I'll go through a bunch of structures that are important and then allow you to link the analytic theory of great Riemann surfaces to the algebra of algebraic curves. And this is interesting to have available for computations if you want to do some experiments and things like that. Do some experiments and things like that, and that's what we'll be looking for. In the process of this, the contributions to the mathematics of it are certainly modest on our side. The people displayed here for some part, Alexander Zotin worked on this for an undergrad project. Fujian Gau was also an undergrad working on it, as was Also, an undergrad working on it as was Sora. Disney Hawk did a Google Summer of code project for Sage, implemented a whole bunch of things. So this is one of the nice things about Riemann surfaces. There's a whole bunch of very interesting mathematics that's accessible almost at entry level. Complications quickly come in. But okay, so the main thing that I'll be advertising is talking about is how these functionalities are now available in Sage. So let's first do a quick start guide. So at this point, as quick start guides are supposed to be, you don't really understand what's going on. You just see that there's an answer that comes out, and then let's sort of picture interest, and then we can see how. Picture interest, and then we can see how it works. We can do this actually for real. I do have the example here. I'll zoom that in a bit. Is that an okay font size for people? Can you read that? Yeah? Okay. Let's go with that. Okay, so what we'll do is we do this. This is available in Sage right away. If you have a modern version from 9.something, certainly in Sage 10, it's all available. So, we're going to define affine space, two-dimensional. We're going to define a plane curve in it. So, this is some smooth plane cortic. We ask for the curve of it, and then we ask the corresponding Riemann surface of it. So, that's basically the complex points of this plane curve projectively closed, desingularized. Well, there's not that much to designularize here. Not much to be singularized here. But then, with the structure of a complex manifold, and we're going to use a precision, now we're going to work with this numerically, a precision of 100 bits. This is actually an important point of this implementation. It's arbitrary precision. So you're not limited to system flows, just doubles. Just doubles. For the applications that I'm using for this, that's actually a very important ingredient because I want to recognize rational numbers at the end. And 52 bits is usually not quite enough for that. Okay, so we can get the Riemann matrix. For display purposes, I just changed it to 20-bit numbers here. It looked a little bit long. So, as you can see, now it nicely fits on the line. So, you get a Fits on the line. So you get a well, the Riemann matrix is a three by three matrix. And it's, as we'll, I'll explain it later, it basically defines a six-dimensional full-ranked lattice inside C cubed in this case, Z-lattice. Okay, so it's this matrix we have, and here we have another, as it turns out, also genus 3 curve. This is a hyper elliptic one. Curve. This is a hyper elliptic one. Andrew Sutherland passed this curve on to me because these two end up having isotonous Jacobians. You can check that in all kinds of ways. You can look at their L-series and see that they're the same. Then you know that they're isogenous, but you don't know what the isogeny is. This can actually do that, although it's not proven. Actually, we do that, although it's not proven in this case. We do a little bit of computational work. You said you don't know what the isogeny is. So, whereas this one will define. Yeah, it's right there. It's right there, exactly. So, what we do here is same thing, we define the corresponding Riemann surface, and now we ask for the homomorphism module between them. It is just a rank one module. It is just a rank one module, but I select one of them. And well, that'll be just, well, it'll be a morphism between lettuces. So, in order to sort of know what the structure of that is, you just look at this myth form. And here you see it's one four. So that gives you the kind of isogeny that is actually available between them. So, well, the title had rigorous in it, so I should probably. Had rigorous in it, so I should probably say what is rigorous about the computations. The period matrix here that was actually computed with certified homotopy continuation. The integration also has certified error bounds, although it's not quite going to all the details of getting the very last bit correct. Here, certainly. Correct here, certainly, because well, we actually chopped up the precision, but it certainly has the property that as you crank up the precision, you will get better results. And in principle, ignoring the small round of errors that you get from repeated adding, which is just precision loss in the arithmetic, you should have non-digits. So, yeah. You do a little arithmetic with ball fields, like complex ball fields. I can talk about that later where that is problematic. You don't want to. You can use balls and your ticket. It tracks your errors, basically. Yeah, that gets you very, very bad results. Interval over the wheels. Yeah. That gets you very bad results. Because you need to do neutron iteration. But there's, I mean, people do certainly I mean, people do certification in that. I mean, you just have to do outer approximations to your balls. Yeah, so I have not looked into a posteriori certifications. So the tracking is certified. The tracking is certified, yeah, absolutely. So at least we know that we're integrating on the right sheets. But okay. So that was this example. Okay, so the overview of the talk is: I'll quickly go over complete Riemann surfaces, part of the story that's associated with them. We'll look at how to, well, the ingredients of then computing Riemann matrices. There's a homology base that you have to find, so we use rigorous homology continuation for that. You need to determine a cohomology. To determine a cohomology basis, that's actually an exact arithmetic computation. We have nothing to contribute there, basically. And then you compute your Riemann matrix through rigorous numerical integration. Other people have looked at that. We don't prove any new main theorems about it. But what is new there is you need some bounds on the variation of your integrance. And we weren't able to find. And we weren't able to find that for arbitrary plane curves in the literature. So that required a little bit of work. We've said what it's like. The rigorous numerical integration. I'll get to that. And that's basically where any claims to rigor end. As I've demonstrated, there's some capability here to find homomorphism modules between complex tori and And there's some other word here as oh, I didn't even put in here that we do have a computation of Abo Jacobi maps now as well, thanks to the work that Ben Hog did. And then the other bit is numerically evaluating data functions with characteristics and derivatives and using those for reconstructing curves from period matrices. That's not rigorous. That's purely heuristic. That's purely heuristic. You don't have height bounds on the coefficients. And you're just very, very confident of the answers that you get back because the number of bits that you find in your integers in the end is so much smaller than the number of bits that you did your reconstruction from. So, okay. So, when you say determining a holomorphic basis, typically people construct holomorphic forms on homology basis. Do you also get the second type? Do you also get the second type, I mean the rest of the forms as well? We don't bother. Because you care about a C basis for hormones rather than a Q bar basis. You don't have complex conjugation otherwise. Oh, so we basically compute the differentials as a function field over an exact field, over Q, for instance. So we have that. But we don't have the remaining parts. Particles. The objects are not defined on the future. Doesn't make sense. But anyway, okay, if you don't believe it. I think it's there, but because we know how much contiguation works. No, it's just that the basis for your whole module will not be a Q-bar basis for the other breakground module. The holomorphics will be defined over Q, but their complex conjugation will no longer be elements in the asymptotic round. You will complete the basis in this complex numbers. That's true. Okay, so I was just wondering whether you worried about this. It doesn't come up for what we've been doing. Okay, so an algebraic curve, in our case, the models that we use. In our case, the models that we use will just be plain models. The models that we allow are allowed to have singularities, but we do mean a designularized projective curve with them. So once you have such an algebraic curve, the C points will form a Riemann surface. So it'll naturally have a complex manifold structure of dimension two, and there'll be some holes in it, which will be. So as you can see on this picture, the genus 2. On this picture, the genus II Riemann surface. If you want to write down a homology basis for it, well, there's different ways of walking around. And there's some extra structure on it. Because as you can see here, you actually get an alternating signed pairing on it from the intersection pairing because there's an orientation to this thing. So you can sort of assign a sign to each intersection point, and it's all just. Point and this all just corresponds to the capitaling that you get on the homology. Second homology, in this case, is just Z. So what we can do is we can find a homology basis that actually is symplectic. That's to say, you have G cycles and then another G cycle such that they each pair to one pairwise and the rest all pairs trivially. Trivially, and there's a collomology, there's a cotangent sheaf. So that's a rank G vector space. And in classical complex analysis, then finds an analytical description of a pixel of S via integration in the following way. So you have your homology, you have your cohomology, and what you can then do is: well, you have differential forms. Well, you have differential forms, you have paths, so the natural thing is to integrate them against each other. So you just do all of them. That gets you, well, a matrix of vectors. And from the construction that you get here is basically, well, its span is a Z module inside the dual of the differential specifics. Those paths. Those paths integrate against them, integrate against those points, and then give you C vectors. And then the Abu Jacobi theorem tells you that if you, for instance, take a degree zero divisor, that's just the difference of points, you can choose a path from one to the other in the appropriate direction, and then you can integrate along that path your differentials. Now, you started with a divisor, and you have a division. You started with a divisor and you have chosen a path that can affect the value of your integral, but your differentials are regular, so it's actually limited how much it can affect that. It actually is only affected up to homology. So you get a well-defined map into the values that your homology gives, because you can change your paths with your homology basis. That's going to affect the values, and that's all that happens to it. And that's all that happens to it. So you do get a well-defined map into C2G model that lattice. And Albert Jacobian theorem tells you that that's actually an isomorphism. It's exactly the principal divisors that will integrate two lattice values. So that gives you an analytic description of the Jacobian as a complex torus. And in our And in order to compute these period matrix, the philosophy used here is a very naive one. Just use the definition. There's way more advanced methods. Emre has worked on one, for instance, where he views the period matrix actually as a function of the curve on a family, and then you trick the way that it varies as you vary the curve, and that allowed him to. A curve and that allowed him to basically by a homotopy continuation type argument to compute period matrices. Also, cases where the integration is a very painful business, but it also works for Kirch. It's very interesting because it has a very different complexity profile from this method. But this is a method, I mean, it's certainly not new. It goes back to, for sure, To first sure Van Hui and the Koenig, who implemented this in Maple, I think, around 2000 somewhere. Okay, but so the approaches we take are homology basis. That's a little bit further. I'll address that a little bit further. We use rigorous homotopy continuation for that. We determine our cohomology basis. That's basically function field arithmetic. That's one way to get these. Or if your singularities are relatively nice, then you can use Baker's theorem in some cases to just write down a basis for these functions without too much effort. And then once you have that, you can. And then, once you have that, you can use numerical integration to just compute these values here, because now you have everything that's specifically described. And then the output's going to be: well, I did this big period matrix, the whole thing there. Or we can realize that, well, we had some freedom in basis choices here. And these differentials here, that was just a C vector space. So we have a whole GLGC acting on. G C acting on these matrices on this side. So we could actually just multiply by the inverse of this to force this to be the identity matrix, and then we're left with what's called the Riemann matrix on the other side, which has all kinds of, well, satisfies the Riemann relations. So for different purposes, you might still need this for certain properties, particularly if you're interested in If you're interested in recovering arithmetic data on your environment or for other questions, for instance, if you compute theta functions, you're probably just interested in doing the functions. Okay, so homotopy generators, that's perhaps the most enjoyable part of this whole argument. So, what we have is we represent have is we represent our curve as a well as a plane curve. In particular, we consider it as a degree end cover of just the Riemann sphere. And well then there's a critical set, so we just throw in infinity for good measure so that we can just work with an F model. Then there's the branch points and there might be some singularities. And outside that, this is really just an unramified This is really just an unramified pattern. So, well, homotopy then works very nicely. We can write down homotopy generators for this punched Fieman socker that just loops around every hole. And if we lift that, we get homotopy generators for the cover. And well, once we add in those points, I mean, that might only reduce the homotopy. That might only reduce the homotopy. So we're definitely getting a homotopy generation in that way. If we're integrating these things, well, we'll have to sort of continue whatever differentials are doing, which is easy to do along unramified paths. So what you want to do for your integrals, that's one way, is actually stay away from those critical points. Well, the best way to stay away from them is to just take a variety of. Stay away from them is to just take a Voronoi decomposition around these points, where, well, you need to do a little bit of work because you don't want to go all the way out to infinity, so you introduce a few extra points that sort of specify for you what is going to be the area at infinity. But that's just going to be sort of a fairly well-spaced circle around it. And then for the rest, you just take a Riemann or Voronoi cells and Voronoi cells. And well, as you can see, you can extract a holotopy generating set from just these Voronoi cells. The cells themselves are exactly the loops around your critical points. So, what you can then do is you can use a complex root finder, and each of those points, so that gives you an x value, just get the n y values that belong to it. Y values that belong to it. They're n distinct values, so you can tell them apart. You just label each of them, and then you use certified continuation to just walk from one point to the other. That's going to tell you how your different labels connect up between them. And you can figure out how it works, how it will connect up around the entire thing. And then you can And then you can, well, that basically, well, this gives you a graph, right? You're in the plane. So this now lifts N21 to another graph. And a cycle basis of that graph contains homotopy generators for your Riemann surface. Well, you can actually figure out what the intersection. What the intersection pairing does on that, because locally you can see how they intersect. Do they intersect like the x-axis and the y-axis in the complex plane? Then they intersect with multiplicity plus one. If it's opposite, it's minus one. So you can just figure out how that works. And then there's a nice algorithm going back to Frobenius, which basically just crunched meat for sympathetic pairings that allows you to find a sympactic basis. And it's a theorem that that gives you now your homology. Now, your homolog that gives you your homology basis. So, once you've done the continuations, it's easy to extract a homology basis from this whole story. Is that a part which is certainly everything is repeated on this basis? Yes, yes, yeah, yeah. Because what we do for, and our approach for certified continuation is a very naive one, but it works, well, it is the bottleneck actually for a lot of the companies. Actually, for a lot of the competition. Because the problem is, if two of those points lie awfully close together, you're going to have to take very, very small steps in order to certify your moment, what we do is we just take steps that are small enough so that the different branches vary by less than what would Than what would make it possible to confuse them. And we do that again and again and again and again. There might be higher performance continuation strategies that work better for that. Although, yeah, for the cohomology basis, we're not doing anything particular there. For a smooth projection, Anything particularly there. For a smooth projective curve, the basis would just correspond to D minus 3, the D minus 3 twist of the structure sheaf. And there's the other cases in other situations. So that I do have a little bit of an illustration for that. So here we... So here we exactly the graph that we had before. That's this graph. So it's this is Klein's the Klein cortic. So it has a nice set of seven critical points. Well, an eighth one at infinity. You have eight right there. Seven, eight, nine, nine, I guess, maybe. And to give And to give you a bit of an idea of what this lifted graph looks like, this actually gives you a 3D image of the thing. So why does that work before? Huh. It doesn't work anymore. That's very unfortunate. That's not working. No, no, no, no, no, no, no. It has completed. No, it it has the ground. Okay, well it would give you a nice 3D image. It doesn't really tell you anything. It's just a little bit. It's not circle I think. Oh! No, that's an information thing here. So it seems that it is running. Yeah, it is MSP NC, maybe it says It's I don't yeah no okay this is a this is a if you change the font size maybe it yeah yeah but it's no uh viewer but is it Jmail JS more. I don't know which one. JS was set. It could be JS mode. But it's saying it doesn't like viewer as an argument. Oh, uh yeah, okay. I guess what I mean. Take the odd output and then show them. And for Tachyon, I don't think it's because Tachyon will work, but I don't think that gives a yeah, you can't see the paths because that doesn't do yeah, okay, yeah, and 3D is the And 3D is difficult for SagePrim. It was good for a while and now it's not anymore. Okay. So yeah, that's... Yeah, it's nice to have a 3D image that you can rotate around. It doesn't really tell you anything. What you're going to get is a graph that has three nodes above each of these at each point, and then they are connected, because there's an edge here. They are connect because there's an edge here, they're going to be connected pairwise, so and that fits together somehow. And if you walk around on it, you'll figure out that there's monogamy. And you said you so this gives you more generators than the base number of bases in general. Right, yes. The binuses to extract the basis. Yeah, whatever it is. It's not the generators. Get you used to the fact that it's perfect. Okay. And also the von Hui, the Koning implementation, used that in the end as well. They use a different way of finding these integration paths. Perfect. Okay. You construct. And then instead of attaching I thought you would attach two facets inside this graph to kill certain groups. But in fact, you don't do this, you just look at the section. That's a very fast combinatorial thing. The hard part is to integrations. Um or it's no, it's the continuation at this point. We haven't even integrated it this part. This point. We haven't even integrated this part. But then you have the path, so now you can just compute your piece. Okay, so that gets us to the rigorous numerical integration part. So the standard setting for numerical integration is you take an integral from minus 1 to 1 is the usual standard path. And in this case, we have an algebraic integrant. It's the holomorphic form on your differential. Differential. So, in our case, well, the differential actually is holomorphic on the entire Riemann surface. It's just there's these apparent problematic points at these critical points, either because it's branch points or singularities in the model. But if you take, I mean, on a disk where the thing is holomorphic, you can write down, like Tay Varian Cauchy, a bound on A bound on the variation that that integrant will have in terms of well the derivative at the point together with a bound on the variation of the function. So that seems circular, but it gets you a better result here because, well, you know that the values are roots of this polynomial where your z is in a certain circle. So you can use things like Fujiwara as well. So you can use things like Fujimara's bound on roots of complex polynomials to bound the size of your integrant here, and you can use that to get a bound on how much your integrant varies. And that's an ingredient for a very nice result. Not due to us, it goes back a long time. Considering ellipsoid, and now Ellipsoid, and now if you have your integrand that is homomorphic on that ellipsoid, so r here is a measure of the eccentricity. As r gets smaller, you get a skinnier and skinnier ellipse around your path. So, as you let r go to infinity, your ellipse will be, will basically approach the path. If you look at the formula here. Higher. So if you're holomorphic on this ellipsoid around your integration path, then the nth order Gaussian-Jeanre quadrature of your integrant of your integral will actually only deviate by the toll provided that you use more integration points than Use more integration points than this. Important part here is so the several ingredients show up here. The important part is this R here, which shows you that choosing R small is very, very expensive. So you want to use reasonable values of R, which means you want to have fat ellipses around your paths. So if you go to an integration path from minus one to one, and say this point here is one of those critical points, the red one here, so that's a point that should not lie in your ellipse. So that limits you what you can do with your ellipse. So now you basically have two choices. You could either take an ellipse that's skinny enough to avoid that point. Avoid that point and do it with one integral, or you split up your time in several paths so that you can get nice round ellipsis, but now you have to do multiple integrals. So, the very definite result that you get from this estimate is that you definitely want to split up your integration path. Gaussian genre integration really wants a holomorphic team. Yeah, yeah, yeah. Yeah, so we yeah, for periods we never run into them. We're actively avoiding that. So our approach here is to just keep bugging. Our approach here is to just keep bisecting the paths while it keeps reducing the total number of predicted function evaluations that you have to do. And then that's the integral that we, those are the integrals that we compute. So the important part there is this is not just sort of an estimate. If you look at the values that your integrals give, you sort of trying to do it this way really requires more integration points to get an accurate To get accurate results, that this does. So it's real. It's not just something that's an artifact of the way investment. So that was sort of the most recent improvement of the integration. Previously, we were using just the heuristic integration method. So here in is they're equally space points. And so the genre. Okay, so it is adapted. Okay, so it is adapted. So, yeah, it's on each of those integrations. So, here we're integrating, yeah. Getting from minus, so okay, these are the center points of your segments. So your integration points like here from minus one, and then here. This is, yeah, this is good. This results from a bisection. Yeah, and on each of those intervals, you just do a calculus running to each. So you use the same eccentricity all the time. Basically. Basically, yeah. Reuse the point. Yeah. But yeah, that's another advantage. So the work that went So, the work that went into this here was basically that for arbitrary plane curves, we needed this estimate. It's not deep method, it's basically Calvo. Well, complex. Complex variables, course. And then sort of plugging it into this to get certified integration results. That's pretty standard by now. Okay. So, as an application, So, as an application for computation of homomorphisms between complex tori, so in this case we have as inputs two complex tori. They don't have to be of exactly the same size. And a homomorphism between them basically amounts to having matrices like this, such that they are related. Are related by this by left multiplication here, that by right multiplication on the other side. So, via coordinate transformations, you can basically simplify these things a little bit. You then set R to be a certain matrix. You end up with an equation that looks like this. So, tau 1 and tau 2 are the Riemann matrices associated to these period matrices. And you're looking, you're basically solving. And you're looking, you're basically solving for integers that satisfy this relation. So, this would be a G2 by G2 matrix of complex numbers. This is a G1 by G1 matrix of complex numbers. And you're looking for, well, integers that satisfy this relation. And if you're looking for specific integers, you're looking for small integers, right? For large enough. Integers, right? For large enough precision, there are small integers. So that is something that we can do thanks to LLL, because LLL is good in finding small integer approximate relations between vectors of real numbers. So you just expand everything. You run LLL to translate this into finding short vectors in a lettuce. You find them, and you sort of use the ones that you think are surprising. The ones that you think are surprisingly small, that's the usual thing, and you assume that that's going to be the syntactic homomorphisms that you're, or the homomorphisms that you're looking for. And with a little bit of extra work, you can even do this on for J1 equal to J2. There's the Riccati Rossetti evolution that allows you to compare that. To express what it means for the thing to be a symplectic homomorphism, and then there's only finitely many of them because you end up enumerating only vectors from your module that are within a certain compact ball. Are there a priority balanced on the integers? No. But I thought it's okay. It's very, very unlikely because these are transformations on a homotopy basis, on a homology basis. So there's no guarantees, but in practice, they're not. Yeah. So that's not a problem in practice, but yeah, there's no practice. Yeah, no, in in in practice recognizing the small it's it's pretty clear. It's pretty clear. So, the question was: that if you have to play curves such that it approves misogynists, do we have a bound on? The bound will depend on the basis that you've chosen, in particular the long jesus. But yeah, but he has an essentially analogical choice. You know, if he does it that way, it's not going straight. I mean, that's the issue. You need a definition of something like a height for the homology basis to decide that you've chosen a good homology basis, maybe a length, whatever that means. And then maybe with respect to this, you can define a height of diseogeny. And these books. It's very interesting. Those parts are what do we okay? These things you see all the time, and it's very interesting. This is kind of like a deep data thing, but you see. Like a big data thing. You see it all the time. Either if you do a massive computation, you break your hypothesis, or maybe it's true and there's an interesting theorem. So you mean the interesting theory in this case, just to be explicit about it, would be this bound for the degrees of the isotope. As we try to argue, before we theorize, we should make sure that there's more data. So, I can give you an example here. I mean, point cordic, the first thing you want to look at is its holomorphism group, obviously. So, here you can compute exactly these symplectic isomorphisms. And we find that the endomorphism, so this is the length of the endomorphism basis. So, the homomorphism module from this thing to itself. This thing to itself is a rank 18z module. That's what you get from this. I guess I can, I currently don't want to show you all of them because that's going to look pretty horrible, but I can look, I can give you one of them or something like that. Yeah, yeah, the first one is kind of seen. That's sort of precisely what we're doing. So these are homomorphisms, right? They're not necessarily automorphisms, but we basically Necessarily automorphisms, but we basically want to. I mean, this gives you a matrix algebra, it's got to have units in it, those are automorphisms, and then there's the extra property that they have to be symplectic for them to actually be holomorphisms of the curve. Okay, so we did compute the symplectic holomorphisms, so it's done that computation already. That's just a vector enumeration via. So there's 336 of them. 336 of them, that's twice the number because you get the minus ones as well. So it's found all of them. If you look at the cohomology basis with respect to which this is represented, you can see that this is 1 over a certain denominator, y over a certain denominator, x over a certain denominator. So to get your Your representation of the otomorphisms as otomorphisms on the canonical model, basically, in this permutation. So we want to change that permutation to correspond to our choice of coordinates here. We want to have x, y, 1. We do that. We get here. And then one of the matrices, for instance, if you can. And then, one of the matrices, for instance, that you get, so this is just one of them, it's got this structure. And then, if you apply that matrix to this vector x, y, z and substitute that back into your formula, then indeed you're getting an automorphism of the curve. We get the same equation back. So, because this curve is already given in this canonical model, the representation that you compute That you compute with again recognizing you do some well algebraic number recognition here. And you might wonder what the A is there. So the base ring of this matrix is a number field generated by the 14th root of units. So that's the A here. It's basically just a minus A as 7. So you can do this. So, and you can do this for the other ones as well. So, as an example of how you can sort of recover data that's interesting from an arithmetic point of view already, from analytic data. Okay. So, that's the automorphism. So, we've come through that now. So, we do have Albujacobi maps as well. As well. So that is really what you can do. If you take a degree zero divisor, you can look at that and split up in paths, you can integrate, and that will give you a value that's well defined up to the period lattice. So go here, and this is the work of Disney Hogg. So we can do this entire computation here. So we take a genus 2 hyper elliptic curve. 2 hyper elliptic curve here. This is certainly not the best way of computing period matrix for a hyper elliptic curve. You can do that in much better ways. But you can get a plane model. It still works. We can take one point, we need to take its place, and another point, interpret it as a place, and then we can construct a divisor that's the difference of them. And then we can ask for the Abu Jacobi map of that divisor and also. Of that divisor and also value of twice that divisor. As you might notice, these are branch points of the curve. So if you multiply them by 2, they're linearly equivalent. It's one of the hyper elliptic fibers. So 2 times D is going to be a principal divisor. So the algo Jacobi map of that had better be a value in the lattice. So we can check that here. We can reduce. Here we can reduce the value of the period lattice so you get a representative modulo lattice. And you might see here it's a purely imaginary value, in fact. This is close enough to zero that you should consider that zero with the precision that we're working with here, I think. And this is just the standard precision, this is 53 bits. And if we did another one, that you should consider that zero number one. That's zero here. So that's sort of a rough rough verification of this. In this case, because we're integrating two integration points, we need to use other integration schemes for which we don't have rigorous values. So, in this case, the values that you're seeing here, you should not consider them certified. But that's not. But that's not, I mean, in practice, they're fine. In principle, you could go to your borrowing, or you could have constructed a boron line where p1 and p2 are positive vertices you have to go to. Yeah. And then take your. Yeah, so yeah, so what it does in practice is it takes these values and finds on this Voronoi skeleton a convenient point to go to. Sort of it just computes that extra path integral, does that for the other one as well, and for the rest it can work along its And for the rest, it can work along. The integrals get already drawn. I think we need one to this one and one to that one. But why did you say this was not rigorous now? The younger one. Because it's not using DAS measure integration, so we don't have a guarantee. Just what's it double exponential integration scheme? And we just use adaptive, so we just look at how the value is changing and extrapolate on that. So in practice, that works extremely well. I have a theorem you want to say for. Ethereum you want to say for this kind of yeah okay okay well you can you can put a pull request on save right yeah okay there's other ways of solving this problem you could you could because you can move the divisor right you don't have to represent this divisor class by this divisor you put a rational class yeah Yeah. Um okay, so that was for Al Jacobi maps onto theta functions so here we really don't have anything mathematical to contribute. It was just something where we felt it was convenient because we were not able to find an implementation. We were not able to find an implementation that had all the features that we wanted. And those features are to have, well, rigorous summation. Most of them have that. But we wanted arbitrary precision. That already cuts down on the possibilities you can have. So we do use the BRFR library in a relatively optimized way. It's vectorized for derivatives. So if you want to compute. Derivatives. So, if you want to compute a whole bunch of derivatives at the same point for your theta function, then that's a lot faster. Because if you look at the expressions for those, there's a large common expression in them. There's only a little bit of extra terms. So, as you'll see, that's actually exactly the application that we have here. And we also made sure to have Ziggler reduction for the lettuce to get more convenient lattice that actually actively. That actually actively tries to preserve precision because there's a bunch of other implementations that we tried that worked iteratively on float matrices, inverting matrices all that time. So then you just really lose precision like crazy. What you should do is do your work iteratively, but collect your work in an integer matrix and then only do one inversion. More complicated, but it wasn't done in other things before. Wasn't done any other things before. So, as an example, this is not yet standard integrated, but it is a package that's easy to install. You can just pull it from GitHub and then it installs relatively easily. Okay, so what we do here is we take a hyper elliptic genus 2 curve. Genus 2 curve, and we can compute its period matrix. We split up the matrix in its first bit and its second bit, and we can do the inverse of the first bit so that you can get the Riemann matrix. We can ask for the odd theta characteristics, and we can And we can ask for, so here I ask for the Riemann theta function of twice the Riemann matrix. So that's a different lattice. This is a lattice of an isogenous abelian variety. But in genus 2, that's again a Jacobian. So what would be nice is to now recover the curve. Of which curve is this thing a Jacobian? thing a Jacobian, well, there's a theorem that you can reconstruct, well, certainly that you can get the bias truss points as the, as projective points basically, as the derivatives of the theta functions with all theta characteristics evaluated at 01. So that is what this does. This theta function here takes a characteristic, you give it which derivatives you want to have. You give it which derivatives you want to have, so that's the derivative with respect to the zeroth variable, and the derivative with respect to the second variable. And then we have to multiply that whole thing with AI to make sure that we're representing this thing with respect to our original basis, where the thing is actually algebraic. We can compute that vector. Is this the second, so you take two derivatives? Yeah, so you get a vector of derivatives, yeah. These are first-order derivatives. First order derivatives. I mean, these are odd data characters, so these are odd functions. So their first derivatives are even functions, so its value at zero does not have to be small. Emmanu theta is a class, and it keeps computations so that next are. Yeah, yeah, there are some data structures in it that are, yeah. But it is a class that's made for doing point evaluations of your. Evaluations of your theta functions with characteristics and with derivatives. You can take also higher-order derivatives. That's why this is a list of lists. And if you do that, you can now recognize, so now we basically just reconstruct our sex stick polynomial by using these values, take the quotients, take them as roots. Take them as roots, and then if I affect the isogeneous? Yes, yeah. So if you, I mean, in the case of genus two curve, two isogenes, or rich low isogenies, we have a nice closed formula, so you don't need this analytic stuff. You could just compute this thing straight up from this. But in this case, you're recovering it from analytic data. From analytic data. And then, well, to check that they are actually isomorphic, we can, or isogenous. We can again do the same thing as we did before, compute a homomorphism basis. And we are getting a homomorphism. So if you were to compute the Smith normal form of this, it would be 112. So this is true. Uh this is sort of it's again, it's not a proof, but in this case there is an algebraic wave thing. Okay, so uh how are we doing for time? Okay, this is this is taking more time for me. Uh um perhaps more spectacularly, or at least sort of it's always a bit of a miracle to see these algebraic things reconstructed from analytic data. Analytic data. And in a less trivial situation, you can actually do the same thing for a genus 3. Earth, so the smooth plane cortic. So there's a little bit of code here. We have a smooth plane cordic here. This is a little bit of an awful equation, but just compute. There's a few seconds. There it is. We extract the all theta characteristics again and make our theta function. And the thing here is, well, a plane quartic has 28 bitangents. And actually, the coordinates of those bitangents occur as gradients of the theta functions with off theta curves. So there's 28 of those. There's 28 of those. So you can just compute those vectors, re-normalize, and recognize them. This particular curve was actually constructed to have 28 rational bitangents. So we can, I mean, in the construction, I knew which bitangents those were, but we can reconstruct them now from the analytical data as well. These vectors here. So the coefficients of the DNF forms. Yes, these pieces. Yeah, exactly. That's what it's about. Yeah, exactly. That's what it's so we do the same thing as before. We basically take a two-isogenous abelian variety. We just multiply the Riemann matrix by two, and we do exactly the same computations. And we can look at the fields of definitions of, again, well, a coordinate of these. Of these bitangents, and then we see that they come in conjugate groups of four. So this is the minimal polynomial over q of one of the coefficients in each of those vectors. And actually, if you just a little bit of Galois theory, this is what you should get. But more importantly, perhaps, Bradia has a nice article where he gives a He gives a formula to reconstruct quartic explicitly from these bitangents. He takes the Steiner system and the appropriate way. He takes some determinants and things like that. So that's this whole story here. And then you do get a thankcordic out of that, this one. And if we want to compare this. Compare this with this was brought up that SIGH is good at communicating with other computer algebra systems. So in say in magma, there's actually a routine to minimize and reduce than quartic using the GL3Z action, GL3Q action for the minimization for the reduction. So we can run that. So we can run that. So we convert the polynomial to one in magma, call this magma routine on it, and then convert it back to sage. And that just runs without any problem. I mean, you only get barely a nicer looking equation out of the units. And as it turns out, this one's actually more poorly conditioned for computing the period. So I will not use it for the next step here. So in order to verify that we have That we have probably an isogenous abelian variety here. We do the same thing as before. This takes a little bit more time. Because this one is not so nicely conditioned. There we go. So a little bit more time. 30 digits. And then you do get, indeed, what is a two-way switching. But there's an extra part. But there's an extra part here, and that makes this question perhaps a little bit more interesting why these kinds of computations are now interesting, because something like a formula like Vichelov estroges doesn't exist for a genus to be curves. And there's actually a relatively good reason for that, and that is that, well, over an algebraically closed field, basically all obedient, certainly all irreducible, Irreducible, abelian three-folds are Jacobians if they're principally polarized. But over Q, that's no longer true, because abelian varieties over Q allow for twists, because there's a minus one holomorphism, and generally that's not allowed for the curves, so there's sort of not enough curves around to give you Jacobians. And it is indeed the case that, well, we have two genus three curves now. Three curves now that have Jacobians that are isogenous over C, but they don't need to be that over Q. And well, sort of, this is a little bit of circumstantial evidence for this. The discriminants of these curves, let's say the primes of bad reduction for these plate quartics are not the same. The second one we computed here has a whole bunch of extra primes here. What? What? Also, to yeah, um, this I didn't ask for the integral normalization here, so I'm not sure that this one has good reduction from just this table. Now, that doesn't tell the whole story because the curve could have bad reduction, and the Benin variety could still have good reduction. So, this doesn't really tell the entire story. We could compute some L-functions in reductions and see the In reductions and see that they don't agree. But this is actually an extra, another part that you'd have to do if you want to get these results arithmetic. You actually would want to say not, I thought it was. Exactly. Yes, yeah, yeah. And this is circumstantial evidence for it. It doesn't quite prove it, but. But you have the basis over the homology basis as well, so it's G by G matrix, which is the pound of Q bar, and you could check. Q bar, and you could check. I mean, that's not a property point Q that would be your evidence as well. I mean, you're showing us only the integral part of the isogeny, but there's also the homology part. Yeah. You have these three major things. Those, well, you, I guess, uh, see them as floating point first, but you can reconstruct those big numbers at the first point. And this will tell you, okay, I don't have these ideas. I don't define it. I don't no, no, because the I don't know, because the okay, so the reconstruction of the curve from this, I mean, in principle, I have an abelian variant that is esogenous to the Jacobian of my first genus III curve. It's just not the Jacobian of this genus III curve that I've reconstructed. Because the reconstruction, the formula from Guernea, ignores quadratic twist. That formula is not arithmetic, it's just reconstructing. It's just reconstructing from an original data. The first one, the curve, isomorphism, and then the curve is another Jacobian that's complex isomorphic to this. And these last two are just protected twists, so the data to differentiate between them is just the number of modules squares. Just a number of modules squares. So that's an extra step you can have to do. And this sort of gives an indication of what kind of prime factors could be in there. So if you compute enough data functions of reductions of these curves, you can just solve for what that factor is. Yeah. Okay, I'll leave it at that. Thank you for your attention. There is something that I do want to. There is something that I do want to highlight the other packages that are there, because this is not the only option for, and these different packages have different sort of sets of functionality available. I think all the isogenies you construct can now be proved to be rigorous. Work of Costa, John Warren, John. Yeah yeah yeah p I think they use this numerical methods first.