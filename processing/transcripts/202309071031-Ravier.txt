Great. I actually never had any particular exposure to specifically sort of like Robiocide outside of Cryo EM. So actually seeing what you're all talking about, I actually changed my talk a little bit specifically to that because I didn't realize that there were enough people that were focused on registration problems. But the general ethos of it should be pretty much the same. Does this work? Does this put the right thing? So I'll try to go with some motivation here. I'm going to be a sort of a little. What's the motivation here? I'm going to be a sort of a little mum on the details because, frankly speaking, I think actually getting into the weed kind of obfuscates a lot of things. But here's sort of a different problem of a different twist. So I've worked a lot with anthropologists for a while, specifically in terms of better understanding the fossil record, trying to understand how evolution has worked, how species have spread, how things have just changed over time. And there's a particular problem of interest involving a couple of giant gaps that we've. Involving a couple of giant gaps that we actually have in sort of the lineage of primates. In recent memory, there was one discovery in 2019 of the first time that, well, just basically like part of a monkey jaw with teeth still intact, dated to 22 million years ago, which was significant because honestly, this is a field with very low sample size. They only had two teeth 12 million years prior to poor. And basically, that's one of the big challenges of this. You get what you dig up, and you dig up what you. What you dig up, and you dig up what you dig up, and you deal with whatever laws that you have to deal with in order to dig things up if you actually want to pay to digitize. We're not going to talk about that. I'm going to go through sort of this particular annoying little lump that I've worked with for the past couple years. I say that because this has sort of consistently challenged things and honestly done a lot to frustrate me in terms of thinking that I was done with certain aspects of my dissertation that I wasn't. That I wasn't. I'll be talking about that. But the ultimate goal here is that we have this little molar range group, okay? It's from a particular region of Africa, specifically in Kenya. It's known to belong to a particular group of primates, which has a couple different subgroups, two to four. It's actually kind of controversial and political, but we're just going to support for this. And ultimately, the question that we're going to try to answer in this is: can we determine which of the infra orders this Which of the infraorders the specimen is closest to, and how, in a non-parametric fashion? I emphasize non-parametric specifically because I honestly hate using parametric tests in any particular capacity. I have seen, I've been involved in some sort of like legal cases specifically where people try to go about using distributional assumptions just to have this US Supreme Court just kind of like strike me down and say, you wasted a lot of time and money on that. So I'd rather not risk that, and I'd rather people just be able to apply this stuff off the shelves for sick. Apply this stuff off the shelves specifically. But the goal here is: let's find out a little bit more of that. And I'm not going to get into the biology of this, I'm not the person to do so. But this is kind of a particular overview of the challenges when working in anthropology. Everything that you want to use doesn't work, or at least not in the particular way that you want to. Sort of the big offender that we actually have here, classic, well, usual Kendall, Procrustis, what have you, methods for statistical shape analysis tend to fail in this region, in this. Region in this area because you end up having things of reasonably high variation. So, to give you this particular example, on A, you'll see on the thing marked A on the top left area you'll see things that align correctly after doing some bodstandard ICP type thing. And you'll see another one in B, which doesn't align correctly. They're actually just kind of flipped over. This happens a lot. And we actually do have a couple cases where, at least in the past, Cases where, at least in the past couple of years, this is not just an artifact of ICP being a non-converging thing. We actually did have a global solution for the percrustis problem that did yield non-biological results. And this kind of keeps perpetuating in various forms. We have registration methods that would normally work for things that are developed that end up failing because I have some underlying non-isometry that gets in there and just something gets screwed up in the optimization. I have statistical methods in the case my registration is good. Methods in the case, my registration is good, that yield absolutely meaningless things, and I need a particular different way to solve stuff than what people would normally go about doing. So this actually started sort of back in long before I was a part of this, but there began to, the one solution method that did end up working was adding a little bit of manifold learning into everything. Okay, so here's my dissimilar pair that I have up there on the upper right. If I, rather than actually If I, rather than actually trying to just flat out try to align the really two different things, if I align a bunch of stuff, and then compose alignments on intermediary paths, going from similar thing to similar thing to similar thing to similar thing, I have my biological alignment. Okay, we can formalize this a little bit using some manifold terminology, blah, blah, blah, connections, parallel transport, what have you. What can we get if we keep using this language? If we keep using this language specifically, while simultaneously keeping compute costs cheap, because the people who want to ultimately use this stuff don't necessarily have much in the way of hardware. GPUs are absolutely not a reasonable expectation. Good processors, up until recently, not a reasonable expectation. RAM outside of special systems, not a reasonable expectation. You have limited things that we want to build. Okay? So, as I said a couple slides ago, I'm going to focus this. As I said a couple slides ago, I'm going to focus specifically on registration. And again, I want to statistics about manifold moving least squares at the end. I'll try to speed through some of this because it's a lot to talk about, but hopefully it's all enlightening. So the problem of interest that I have here, given a collection of shapes, I want to find unambiguous, consistent corresponding points between each pair of shapes. And when I say consistent, I mean if I have, say, like a billion shapes, and you point to one point on one of the shapes, you assign us a theme so you can On one of the shapes, you assign us to him, so you point it to that same corresponding point on every other shape. There is absolutely no argument in terms of what you need. Okay? Really, this is kind of a necessity at the end of the day if you want to do anything reasonably statistical, because I need unambiguous things to compare to and actually do statistics on. So that's why I'm enforcing this. There's a lot of different ways that people have gone to solve this. There's some work out of Leo Divis's group, that was out of Leo Divis's group. That was actually a Debussy proof. I forget the lead on that specifically, where you can close all of this whole consistency constraint in terms of an optimization problem. I have challenges specifically in some of the services that I work with in terms of some optima things that you want to work not actually working. So hopefully we can get something like that. But in this area, what counts as good is not really well defined because we don't actually have a lot of reference to what. We don't actually have a lot of ground fruits to put on. It's a very painful process to actually get ground fruit that they can go about doing other things and actually advancing anthropologists can go about doing other things. And we need their help, but that's not going to get in paper. So we got what we got. So one potential solution that has been worked on for a while is something involving a minimum spanning tree. So the underlying idea that I presented before. Underlying idea that I presented before, okay, so like I could align shapes reasonably well if I compose transformations along intermediaries. I can do the same thing with correspondences. So I have a bunch of different. So this is a diagram out of a PNAS paper in like 2011, which basically shows that, well, if I try to map A to B directly, then I get a different way than going between a bunch of different intermediaries. And it turns out by going through a bunch of different intermediaries, there's some underlying biological reason for that one little circle by Mark. Reason for that one little circle landmark that you see right there. That's better. Not a biologist, so won't go about saying that, but I'm going to run with it. So the nice thing about trees or minimum expanding trees is that, you know, that's an attempt to go about establishing consistency because I have an unambiguous way when I have a tree of going from point A to B, if I'm working under some underlying graph model, with a minimum part being try and sort of always go along similar things. Always go along similar things as much as possible to go about getting sort of the thing that I want, which is good correspondences. They're pretty bad. There have been instances where, I mean, this is sort of just a particular example where I'm removing one particular shape or one particular node in something, I believe, right about node 27, which is going to be sort of on the right thing, right there specifically where the underlying tree topology changes. Tree topology changes, and that's not something they're very keen on because interpretation the anthropologists, because at some underlying point that ends up screwing up their analysis and they have to go about doing a bunch of things and they end up complaining that they have just things that aren't done well. I get it. Not ideal given low sample size and they frequently dig stuff up. So I want some sort of better alternative here in terms of going about doing everything that I want to do. That slide actually should not have been there. Slide actually should not have been there whatsoever. Sorry. Nor should that one. I apologize. So kind of like running with this underlying thing here is that rather than actually fixing myself to a particular tree, I'm going to soften things a little bit. I'm going to actually start working on measures of path space and graphs specifically. There's no reason why I should particularly say that one, either ACB. That one, either ACB or ADEB is better or worse unless I have external bio, or unless I have other biological information here. So I can just construct a probability measure of domain being all possible maps that perhaps I pre-computed. Very computationally and feasible to do so. Blows up rather quick, combinatorial stuff. But if you select things intelligently, you can make this really simple and really quick to do in practice. So here's the This. So here's the basic setup. I'm going to assume I have some underlying shapes, meshes, what have you. I have some underlying correspondences from I to J. I have some notion of distance here, and that can be one of many different things specifically. Here I'm going to use something that we call the continuous percussive stance. The actual paper limits ourselves to sort of looking over the formal maps or rather area of preserving diffeomorphisms. Here I'm just going to sort of ignore. Here, I'm just going to sort of ignore that part and just kind of say, like, over any reasonable homeomorphism, if you morphism, what have you, I'm going to go about finding the map that makes things sort of best aligned and minimizes that underlying energy. So, I have whatever these things I have that are computed well or not, and I want to make things that are better and more consistent in that process. Okay, so going with this graph analogy that I've sort of been talking about, but not necessarily formalizing, I have a graph. Formalizing. I have a graph where the vertices correspond to individual meshes. My edges correspond to pairwise correspondences and distances. Any path between two vertices is a correspondence between the two meshes. You got by just composing the underlying correspondences that you have. And I can build a gib on correspondences between each individual points by just taking some squares distances along the edges, blah, blah, blah, blah, blah. Nothing fancy. Now, here's some language that I like a lot, and is something That I like a lot, and is something that just kind of is sort of really important to sort of take home because it's something a little different that comes up in a lot of this field. Though I know that some of you here are kind of adjacent to this and already know the general idea. We can think of the collection of surfaces that we have as samples from something called fiber bundle. Those of you who've never heard of the term fiber bundle before, that's a manifold that's locally a product. It could be a product. Biggest counterexample generally is a Nubia strip. Biggest counter example generally is a Nudia strip. It's non-orientable, so it can't be a product space. In our situation, each surface is a point on this underlying base manifold, tooth manifold, home manifold, shape manifold, whatever. The fiber is the Riemann surface with the same topology of the surfaces. So I'm always assuming that I'm working on the same stuff. Parallel transport is a funny thing. I mean, I'm going to be showing you sort of the normal way to think. I mean, I'm going to be showing you sort of the normal way to think about parallel transport in just a second. It's actually, I'll actually start with that. Whenever people say parallel transport, they usually think about sort of just going along and taking some sort of underlying vector at a tangent point on a manifold and just propagating it along some path and solving some differential equation and doing all that stuff. And you get different things but different paths, and that's called holonomy right there. The fact that they don't align, and holonomies are. The fact that they don't align, and polynomial is garbage. But there's actually a really more natural thing. I actually find it more natural to think of this in terms of really more abstract things. Parallel transports are homeomorphisms. Like physically, if you go on MCAT Lab category theory website specifically, they start off by sort of defining parallel transports and define them as selections of homeomorphisms, diffeomorphisms, what happens. Arrows. Makes a lot more sense than sort of like building intuition in that particular way. That's just me, I'm biased. That's just me. I'm biased, and you're letting me talk so I get to just refuse. Anyway, back to this. So, we're going to make one underlying assumption that good registrations that we have are going to correspond to something like GUD6 and shapespace. Nature is efficient, what have you. Biologists seem to be happy with that. And that particular means that that means that all of these massive amounts of possible correspondences that I have can mostly be tossed out. Because there's a lot of ways. Because there's a lot of ways to go about making something taking a path that makes it not anything like a GOTC. It adds extra holino, it adds some extra crap that doesn't necessarily need to be in there, and you can, frankly speaking, just proof. Okay? We can throw away all loops, but we can do a little bit better than that. We're going to get rid of some inefficient paths, and by inefficient, I mean I'm going to restrict myself to things that always go towards the thing I want. I always, between any two individual points I have, so my P and Q specifically. Have so my P and Q specifically. I allow a connection here. I hit an adjacency matrix by basically allowing a connection if I'm always going away from the point that I'm starting out with and I'm always going towards the point that I want to go to. In effect, you can actually go about showing it's not really anything that hard. I think I found it in some sort of differential geometry textbook. That this actually gets you to a pretty easy bound in terms of curvature. Pretty easy bound in terms of curvature, which would, or some sort of like bound on holonomy, effect, practical effect of holinomy, uh, or differences in parallel transport, rather, that ultimately goes away. You can start phrasing things in terms of some sort of underlying variational problem. Don't need to get into that there. But point being, this is kind of an efficient space that I can implement and practice it with a depth first search. It takes just a couple seconds. To get from that, I'm going to have this little tiny That I'm going to have this little tiny point right here. Basically, what I'm always going to do here is put a measure around each individual vertex I have of interest. Okay, and I'm going to propagate it along my, like use my Gibbs measure in order to transfer that to whatever particular other shape I have. And actually, I'm going to end up doing that a bunch. So I have something that might be just a little Dirac delta there, and it just kind of propagates on my efficient paths to some larger region-ish specifically. But that's fine. Specific, but that's fine. I'm going to sort of trim that down. So, our pipeline is called SAMS, gives a particular procedure that we have in order to do this. So, first thing, I'm going to extract some very salient landmarks that I have here using Gaussian process regression. This was something that was done by my old lab, Rodobishi's lab, in 2019. Basically, it's a way of doing sort of farthest point sampling on manifolds while sort of emphasizing things that I want curvature. So, I'm going to put That I want curvature. So I'm going to put little distributions on all of those, and I'm going to propagate them all simultaneously. I'm going to go one way, and I'm going to go the other way. And you can see, alright, well, that's a lot of things, and we're going to sort of go about parsing. But what I want to do there is basically take a look to see what happens if I go from surface A to surface B and I get a particular good matching there-ish in terms of pick your fingers. Pick your favorite statistic. And I do from B to A, and I get some sort of mutual matching there. I'm going to omit those specifically. And that trims my however many landmarks that I have down to a much smaller number. And I want to point out specifically, note that this little tiny cusp that I have up here, specifically on the left, has no particular corresponding matches on there. Those are the things that I want, and frankly speaking, those are the things that caused a lot of problems to registration for us in the past. Yes? The past. Yes, question. Sorry, how is the propagation done? Basically, so you take it, in practice, I'm going to put a little Dirac delta on each individual landmark, and I'm going to propagate it along each of the individual paths and weight it based on the underlying on the GIF measure that I have specific. Right? So then I have another distribution on here, and I'm going to just sort of match up divergence, whatever, blah, blah, blah. So, I'm going to go a little bit quick here. There is an underlying interpolation step. There are actually two works that we use. For topological spheres, we use hydropolic orbifold embeddings. It does actually have some underlying injectivity guarantees in terms of the registration surrounding the landmarks, which is part of the nice way. Also, it actually has a reasonably fast implementation. Otherwise, we're going to use a somewhat simpler version that was proposed actually in the Gaussian process paper that I just didn't mention here. Paper that I just didn't mention here. I can get consistency of things by just the fact that I'm just choosing a temple that I'm mapping everything to and then reparametrizing everything in a nice way so that I can actually do statistics. I have code here, but before I do anything else, I just sort of want to say this was a really cool validation. We did actually plug these particular registrations into about like a set of 500. Well, we registered about like 500 or some primate molars that supposedly. Molders that supposedly could accurately sample all primates at the time. I don't know if anything has changed since then. I don't know what people have dug up. But we use those correspondences, excuse me, segmentation, via spectral clustering, and something called horizontal diffusion maps, extension of diffusion maps that incorporates any sort of correspondence information. And anthropologists love them. This was sort of unexpected. I didn't really do anything fancy here. I think we use just like five-dimensional spectrums. Dimensional spectral clustering in terms of that, and it apparently solved a lot of problems. You can hopefully see and hear all the cores, all the colors are sort of corresponding to one another, and maybe you can see some underlying variation of the actual shapes that kind of say that, well, these are a little bit different. I actually do have the entire file for anyone here to just kind of like click around on my computer and swirl around to see sort of what I mean. But that was telling us we were doing something. That's all the registration stuff specifically. Any questions so far? It's a lot of stuff in a very short amount. Okay, cool. Now, all right, back to this underlying problem. I want to use these registrations in order to build some fancy non-parametric tests in order to go about solving this. Okay? Now, we did try some underlying methods that are used in neuroimaging based on outlier testing, on T-statistics, what have you. On T-statistics, what have you. You can get some p-values and heat maps that illustrate what regions are really different from one another. I had a case study for my dissertation involving some fancy thing on the right from species Tylargenia asiatica, for which, as far as I'm aware, that's the only digitized specimen in the world, and I'm not necessarily sure exactly if any of the other ones are any good. Where the anthropologists just wanted to go about seeing, all right, is this thing similar? Seeing, all right, is this thing similar or different from any other thing? And we tried doing these particular results and got absolutely nothing of value. Most of the p-values we computed were uncomfortably close to machine precision with false positive correction. I don't like it. After some further discussion with them and Barack Sober, my main collaborator on this part, we kind of realized that the global alignment of everything was sort of screwing everything else up and we had to do something. Of screwing everything else up, and we had to do something a little bit different. So we chose to extract patches instead. By patch, I just simply mean just some underlying subsurface that I can do in a consistent way because I have consistent registration type. So I'm going to take for each surface, I'm going to sort of extract a little tiny patch around it. Not necessarily this anisotropic thing, but usually just a little isotropic part. Significantly lowers the dimensionality. I don't think we actually have any sort of like patches in patch space that are over dimension four. Space that are over dimension four, based on some of the tests that we did. And because we only have a small part of the surface, that prevents the global alignment issue that we had before. Okay, now we want to build statistical tests on pack space, but we're left with an underlying problem. These technically do exist, but they're really, really, really hard because at some particular point, I have to do a computational logarithm, like a real logarithm, a geometry logarithm. I don't want to do that. So we're So, we're assuming we don't have any underlying good model for this. So, MMLS comes in specifically because it's designed to, given a high-dimensional point cloud, for which you can assume that these patches are reasonably high-dimensional, I can get an approximating manifold by basically just find some local coordinate system by minimizing this particular energy functional right here. I can fit a local polynomial around those learning coordinates. I can basically, I'm building an atlas based in a very data-driven fashion. Of fashion. Okay? So I have some underlying manifold that I can go about learning. That's great. Now, so if I'm going to work in sort of the, I don't want to put a distribution on anything, well, I want to try to restrict myself to very basic things. And what I'm going to do here is, well, see what I can play around with just a pure geometry game. This is a really simple thing in order to go about proving. If you have a bunch of IAD samples from the same distribution on a metric space, then the distances between them are going to then the distances between them are going to converge to zero as case as a okay let me show you yeah the minimum distance will converge to zero as things get larger probability that's not really that hard for proof so the idea that I'm going to run with here I'm going to measure likelihood of the tooth in question that I'm interested in whether it belongs to a given group based on the p-values of bootstrapped distributions of distances away from the tooth okay so I have some data I'm going to bootstrap the distribution I'm going to bootstrap that distribution. There are ways you can kind of do it somewhat robustly, even though it's a stream-value thing, so whatever. I'm going to skip over that. I'm going to observe the underlying p-value, and hopefully that's going to work. There is an underlying question of whether or not we get reliable distances. Yes, we do. We actually do believe, we're able to show specifically that if you go about reconstructing the Riemannian metric on a Riemannian manifold via MLS, you actually get something that looks quite nice. That kind of looks quite nice. For those who are familiar with sub-alev things, this kind of looks like the sub-alevin equivalent. This does look like exponents are in the right place for sub-Alev sharpness estimates, and we haven't gone about doing that, but we do believe this to sort of be true. So we are working with something that's quite reasonable, or at least we feel it's quite reasonable. Okay, so I have this entire pipeline. I'm going to do some significance testing based on the Wilcoxon-Rank sign test, not the sign. Based on the Wilcoxon ranked sign test, not the sum test, as I mentioned in my abstract specifically. And I'm basically just going to do things ranking once. So I'm going to actually present a couple things right now. One, can I go about distinguishing whether or not this thing is far enough, whether or not there's any real difference in terms of this special thing, the special tooth being far enough away from one of the four infraorders that I'm working on or not? Is there any separation between those? Can I actually do heat maps? Do heat maps. Gonna hope, and I wouldn't be talking about it if I didn't actually have something here. So it turns out, so you can view on this right, I have a bunch of sort of heat maps. Each of the individual columns corresponds to a particular different infraorder that I'm looking at. The left two, the tooth in question, seems to be a fair bit closer to the left two than the right two. I can say that with significance. I forget what the exact p-value is, but this is kind of like. Kind of like 10 to the negative 6 or something like that. So reasonably significant. Things I'm happy to go about saying are significant in this case. And you can actually go about using some notion of evidence. So I'm going to integrate some underlying function of the p-value of each vertex on the mesh in order to try to get an idea of what the regions of differences actually are based on the correspondences. So as you'll note here, compared to the other ones I showed you. Here compared to the other ones I showed you, and not everything's all lit up. I actually have very good distinct regions of blue on pretty much everything. You don't necessarily have to worry about going from the rows up and down, that's basically me just varying with its parameter and then the last construction. Basically, it shows that you get the same difference, same basic shape here, and you get the underlying robustness. But the thing that they actually seem to care about is this. I've noticed over time as I work with more people how More people, how important thought plots are. For grad students and postdocs who aren't, thought plots are really important. So there was one particular test that they liked a lot, or thing that they just wanted done. We're going to run a patch-wise comp. So each info order that we have consists of multiple genera. I'm only using three here because we were flat out told not to care about the fourth because there's no way that the actual is going to be anything meaningful, so fine, less work for me. Fine, less work for me. But I'm going to run a patchwise comparison between each individual, between the tooth of interest and every other tooth in the sample by some one-app procedure. Each path, I'm going to get more bootstrap distributions. I'm going to rank which is more similar to the remaining specimens in the genera, the tooth of interest or the tooth that was left out. I'm going to compute the ratio of number of patches where the tooth of interest is more similar over the number of patches. And I'm going to make the spots plot relative to each of the individual specimens. And the only thing I can go about telling you right here. And the only thing I can go about telling you right here, specifically, the thing that matters is that the two box plots that are high in those particular categories are really, really, really biologically interesting. And I'm going to zip my mouth and not say anything more to get myself control. So yeah, that's cool. Again, that's kind of a lot here, and I know that. Somehow, I managed to finish before time was up. Hopefully, what this showed you specifically is that I can add a little. Showed you specifically is that I can add a little tiny bit of geometry here in different ways than before, and I get a lot of stuff out that's actually really useful and quite general. I have my alternative solution to this partial landmark matching problem that kind of screwed us over for a while, but now we seem to actually be able to move past and do what drastic things are. I have this new fancy statistical outlier detection test. Not really that fancy, I don't think, specifically, but hopefully power is there. I want to emphasize specifically, just because I'm working on Genus Zero things. Just because I'm working on Genus Zero things does not mean the ideas don't generalize specifically. I'm trying to keep language as general as possible. I really don't even want to use much, particularly money and metrics if at all possible. So if you have a correspondence problem, similar things can come up. Right now we are trying to clean up the code base so that way people can actually use it. We do have a two-sample testing analog of the outlier stuff that gets a little bit weird because everything's modulized spaces. You can't get rid of the modulized spaces, and I have no idea how that kind of happening. And I have no idea how that ended up happening, but that's counted in my life. And we're going to start doing some data-driven excellence at some particular point 3s in the parameter thesis, and I should stop talking to you. Thank you. I wasn't sure whether that you register everything to a single template, but my follow-up question to that is, is it possible, based on your registration, to reconstruct the evolutionary Construct the evolutionary means? Second question, we are working on that specifically. The challenge there is that I actually have a figure from a job talk. If you do things in a really naive way, again, because of this whole global registration high variation thing, things end up getting really washed out if you actually try to go about reconstructing specimens. There was talk originally about using some of the patch stuff that Barack and I have been working on. That Barack and I have been working on in order to do that, but that's been kind of slow. To your first question, yes, it's only single templates so far specifically. We take the Frochet mean of the underlying sample just as a proxy, but we are also trying to figure out ways of doing that, of trying to do some KV English flavored things so that way we can do multiple individual templates and sew things together that way, because there's. That way, because there's with that high variation, you really do need multiple times. But this works fine for you. So any comments on how this could have for phylogenetic tree reconstruction? Yeah, actually, so we actually do have a paper out sort of on this. It uses some very standard things in genetics specifically, where basically you go about treating the other. But basically, you go about treating the underlying shapes that are registered now, since everything's consistent as high-dimensional vectors, and you can just use whatever things that actually happen in genetics. You actually do get things that are considered like sort of phylogenetic meanish shapes. There are some sort of resolution issues, and sort of the thing I'm trying to push people on is, you know, if I go back to actually, you know, I don't want to take up much more time, but if I go back to these sort of segments here. But if I go back to these sort of segments here, these segments are going to be really low-dimensional specifically, and I'm going to actually be able to get very reasonable. Things won't be washed out if I restrict myself to segments. So we'll have to glue things together, but in a certain dry. I'd love to just small about the pressure. Yeah, absolutely. I'll just talk about one. Yes. And then I'll find the button. On top, top left. Okay, we can. Can you hear me? Yeah, we see your presentation. We start the recording. Okay, that's great. Hello, everyone. Thank you everyone for turning. I also would like to thank the organizer for this amazing work.