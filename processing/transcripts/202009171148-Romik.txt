Thanks, Omer, and thanks, James and Valiant. It's great to see everyone. So I'll be talking about some new results related to the oriented swap process, which is based on joint work with several people who are listed below. And I think some of them are here today. We're here today. So, let me start with an outline of what I'll be talking about. So, basically, there are some fun random processes, the oriented swap process, and several other interesting sort of processes with a combinatorial flavor. They are related to random sorting networks, which were mentioned at some point by other people, and random young tableau. Here are some relevant pictures. Some relevant pictures that some of you might have seen. And the main new results that I'll be talking about are some interesting identities. I mean, there are identities in distribution between different random variables related to some of these processes. And they are actually related to what we call the termination times, or you can, it goes by several other names of different processes. Says, and these are precise identities, so like finite n, they are sort of about there's a parameter n, and for every n, some identity holds. But both the motivation and an application of these results are to derive some new asymptotic results, which are also quite interesting. And the techniques that come up are sort of an interesting combination of probabilistic techniques. Interesting combination of probabilistic techniques and combinatorial techniques, like things related to the RSK correspondence, the Burge correspondence, the Edelman-Green correspondence. So things that come up in algebraic combinatorics, but end up being relevant here. And at the very end, I mean, so the math is interesting, but I thought the story of how this work came about is also interesting, at least to me. Also, interesting at least to me, hopefully, to some of you. So, I think there'll be some meta-lessons about math research that I'll mention at the end, which you may or may not find interesting. So let's get started. And I'll start with one of those identities that I mentioned that I think is probably the most striking, or at least the easiest to explain. And I mentioned the oriented swap process. I mentioned the oriented swap process, and I'll define what that means in a minute. But for now, let's just think of two graphs which are shown here. One of them, the one on the left, is called the permutahedron. So it's basically the set of vertices are just the permutations of order n, in this case for n equals 4. And they are connected by edges. And so it's basically the Cayley graph of the symmetric group with the Group with the adjacent transpositions as generators. So it's somewhat standard. So a lot of you probably know about it. And it's called the primutahedron. It can be thought of geometrically as a polytope. In this case, I'm thinking of it as a directed graph. So basically, we have the identity permutation at the bottom. At the top, we have what we call the reverse permutation with the numbers arranged in decreasing order. And moves correspond to performing adjacent swaps. And it's done. To performing adjacent swaps, and it's done in a directed way so that you're always trying to get towards the reverse permutations and away from the identical permutations. Then the second graph is a graph of Young diagrams. So it's a subgraph of the graph of all Young diagrams, which is known as Young's graph. In this case, I'm restricting my attention to Young diagrams that are contained in the staircase shape. So I denote this by y of delta n. And again, I'll go more into detail. I'll go more into detail in a bit. So now we have these two graphs. And as I said, we're interested primarily, we're doing a random walk on this graph, and it's a continuous time simple random walk. So in both cases, you start at the bottom of the graph and you start a simple random walk on this graph as a directed graph in continuous time, in the sort of the obvious way of doing it. And I denote by xn and yn the time. Denote by xn and yn the time that it takes each of these random walks to terminate, meaning to reach the top. And you know, looking at these two graphs, it's not clear that there's much of a connection between them, but the phenomena that I want to discuss is that xn and yn are actually equal in distribution for all n. So this started out as a conjecture due to myself with Elia BC, Fabio Kunden, and Shane Gibbons. Kunden and Shane Gibbons. And later, fairly recently, a few months ago, in a separate work with Alexei Bufedov and Vadim Gorin, we figured out how to prove that. So it's now a theorem. So I said it's a mysterious phenomenon. I mean, it's now a little bit less mysterious because it's a theorem, but there's still an element of mystery to it, which I hope I'll be able to convince you. There's still more to understand about it. So that's, you know. So that's one striking example of the kind of thing that's going on. Now, let me discuss one cool application of this result, which is again an asymptotic result. So what I called the random walk on the graph PN is precisely what we more technically call the oriented swap process. And this was first studied in a paper of mine with Omer Angel and Andrew Holroyd in 2009. Andrew Holroyd in 2009. And we worked out various results about this process. But one of the results that are relevant to the current discussion is we figured out the law of large numbers for this random variable xn, the absorbing time of the process. So xn is approximately 2n asymptotically, in the sense that xn divided by 2n converges to 1 in probability. But in that paper, we posed an open problem of finding the more precise asymptotics. The more precise asymptotics. So, in other words, the order of the fluctuations and the limiting law of Xn. So, that was an open problem that I personally was very interested in throughout the years. And now we can solve this open problem thanks to this result from the previous slide. And the result says that the limiting law is the Tracy-Widham GOE distribution. So, if you do xn minus 2n, you divide by 2n to the 1. 2n to the 1/3, this converges to F1, the Tracy-Widham distribution with beta equals 1. Why? Where does this come from? Well, I mentioned that Xn and Yn are equal in distribution, and for Yn, it was already known that we have this limiting law, thanks to the work of several people. I mean, this has most recently BC and Ziguras, but before that. Us, but before that, this goes back to work of Baikon Reigns and Matsumoto, Ferrari, Sponge, and maybe someone else. So this was a known result. So we get that for free as a limiting law for Xn once we prove this identity and distribution between Xn and Yn. So it's a satisfying resolution to that open problem from the 2009 paper. 2009 paper. Now, let me backtrack a bit, and I want to tell you about more identities, but before I do that, I have to explain a bit of background. So I want to discuss two families of combinatorial objects that are involved here in the story. So one of them is staircase-shaped Young tableau. So we have one example of such a thing on the right. You have the staircase-shaped Young diagram, which is the Young. shape young diagram which is the young diagram n minus one n minus two down to one and I denote by s y t of delta n the set of standard young tableau of that shape which are the fillings of the numbers one through n choose two in the boxes of the diagram where both the rows and the columns are in increasing order. So those are staircase shape young to blow. The second family of communitarial objects are sorting Family of communitarial objects are sorting networks because the oriented swap process is a random process, but it takes values basically in a set of sorting networks. So here on the right-hand side, you have a picture of a sorting network of order six. So it's basically a path in the permutahedron, or it's a way of representing a path in the permutahedron connecting the identity permutation and the reverse permutation. So here we have on the left, you can think of the permutation, the identity permutation, it's the numbers one. The identity permutation is the numbers one through six, and from if you read this from bottom to top, then you get them in increasing order. But then on the right-hand side, you get them in decreasing order. And the diagram shows the evolution of these permutations as you perform adjacent swaps. So the horizontal axis here is the time axis. The vertical axis can be thought of as the space axis in this permutation picture. Okay, so some basic facts about these objects are: first of all, that they are equinumerous, meaning there's exactly as many sorting networks of order n as there are staircase-shaped young tableau of order n. So those two sets have the same cardinality. That was a result of Stanley from 84. If you want to know this is actually the number of elements, there's a nice formula for it, but that's not really relevant. But that's not really relevant for our purposes. And then a bit later, Edelman and Green found an explicit bijection between these two sets. So that kind of explains Stanley's result in a more explicit way than his proof, which was kind of algebraic in nature. So this bijection is now known as the Edelman-Green correspondence. And for example, I have these two example objects representing each of the families and The families, and they are actually related under this bijection. So, this sorting network is the image under Edelman Green of this staircase shape Young Tableau. So, that's a bit of background. And now I want to add probability to the picture. So, to add some randomness, so there's a very interesting random process that takes values in young diagrams, and that's called the corner growth process. Growth process. It's very heavily studied, so there's dozens or maybe even hundreds of papers about it. I think it goes back. I mean, the original one is due to Rost in 1980. And you can think of it as a simple random walk on the Young graph. So you start with the empty Young diagram. You start adding boxes one at a time and you do it in a random place. So each time you add the next box in one of the places where you are allowed. Uh, in one of the places where you are allowed to do that, so in other words, it's just a simple random walk on the young graph. Now, for what I want to talk about, I actually don't care about anything that happens outside the staircase shape delta n. So you can think of my version of the process as a kind of a projection or push forward of the corner growth process under the map that forgets everything outside the staircase shape. But I'll still call it the corner growth. Shape, but I'll still call it the cornerbrow process, and I, yeah, so then in the sorting networks picture, we the oriented swap process is a way of generating a random sorting network by doing a random walk on the permutahedron. So again, you start in the identity element, then you start performing adjacent swaps, and each time you do it, the position of the next swap is chosen uniformly at random. Swap is chosen uniformly at random from the set of swaps that you are allowed to perform at that point. And that's what we call the oriented swap process. The term random sorting networks is a more general one because there are other families of random sorting networks that have been studied. They were mentioned by Zach a couple of days ago. And just to emphasize, again, I always think of these processes as continuous time processes. As continuous time processes. There's a natural way of doing this by having each transition edge in the graph. You associate with each such edge an exponential random variable that tells you how long do you need to wait to cross that edge. And all of those waiting times are IID. And then you cross whichever edge has the smaller waiting time whenever you're at a node in the graph. So that gives you a natural time parametrization for these two processes. And it's under that parametrization that the identity xn equal to yn holds. I mean, if I were to use discrete time parametrization, the identity would still hold, but it would be completely trivial because the times are constant. Okay. Now, so I mentioned the identity xn equal in distribution to yn. xn equals in distribution to yn, but there's actually a stronger, sort of a strengthened or generalized version of that, which has that identity as a corollary. And it's an identity between two random vectors. And the components of those vectors are what we call finishing times because they represent times not when the whole process is terminated, but when something, some aspect of the process terminated or finished. So I'm going to define these two processes. So, I'm going to define these two processes. I call them Un and Vn, sorry, two vectors. So, Un, and they both have n minus one coordinates each. So, U n, there's one associated with the oriented swap process, and there's one associated with the corner growth process. So, un of k, the kth coordinate of un would measure the last time in the oriented swap process when we saw a swap between two adjacent positions. Between two adjacent positions k and k plus one. So for every k, there's going to be a last time when you see that happen. And after that, you know, all the swaps that you see after that will be in different pairs of positions. So, and then obviously when all the un of k's have happened for all the k's, that's when the process terminates. And then for v n of k, it's actually simpler. I define vn of k to be the time when the kth bar K to be the time when the kth box along the diagonal of the staircase shape was filled. And again, when all of those things have happened for all the k'th, the process has terminated. So what can we say about these two vectors? Well, here's just an example to illustrate it. I mean, to really illustrate it, I would need to show you an instance of both of these processes, and that instance will come with a continuous time parametrization, as I said. Parametrization, as I said, but I don't want to bother with that, so I'll show something simpler with a discrete time parametrization. So I'll take the previous examples and I'll just interpret it as a realization of those two processes. So you have the staircase shape Young Tableau, where I now interpret these integers inside the tableau as representing times when something happened. And similarly, I have the sorting network, and I'll just interpret time in the discrete, sort of naive, discrete time. Sort of naive discrete time parametrization. And with those parametrizations, both the vectors u n and vn are given by 10, 13, 15, 14, 11. So with the staircase-shaped young table, you can see that these are simply the numbers that are written along the diagonal here. So that's obvious. With the sorting networks, you have to kind of stare at the picture for a bit to convince yourself that that's what the vector is. But trust me, I just, I don't want to waste time on this, but you can verify it yourself. Time on this, but you can verify it yourself. Now, you know, you notice that in this case, it is actually the case that u n is equal to vn. That's sort of a fairly trivial combinatorial observation. It's simply a feature of the Edelman-Green correspondence that maps this young tableau to this sorting network. That it will map the five numbers along the diagonal into the appropriate things in the sorting networks picture. So that's like a little lemma. So that's like a little lemma that you can prove. It's not very difficult. But now, going back to the continuous time picture, oh, I have this energy-saving light in my office. Okay, in the continuous time picture, we have the following conjecture, which is that it's still the case that Un and Vn are equal in distribution to each other. But that does not follow from Edelman-Green. I mean, Edelman. Follow from Edelman Green. I mean, Edelman Green will map staircase-shaped Young Tableau to sorting networks, but it does not push forward the measure on the corner growth process into the correct measure on oriented swap process. So that's why it's a conjecture and not a theorem, because we don't know how to prove it. But it's a conjecture with the, we have sort of, I would say, an overwhelming amount of evidence for this conjecture being true. So I'm convinced that it's true. I'm convinced that it's true. What are some initial observations about this conjecture? First of all, as I mentioned, it implies the other result about xn being equal to yn in the distribution because xn and yn are simply the maxima of the coordinates of these two vectors, u n and vn. So if you knew this conjecture, it would immediately imply that result. That result is now a theorem, but we proved it in a different way, not using this conjecture. So that's sort of one. So, that's sort of one sanity check that we have: it implies something that we know to be true. Another sanity check is that it implies another thing that we know to be true, which is that each coordinate of un is equal in distribution to the respective coordinate of vn. So, and that's something that we know and we understand quite well from the paper with Angel and Horror. From the paper with Angel and Horoid. I wouldn't say it's a trivial fact, but it's a well-understood fact. And not very difficult once you understand what's going on there. So of course, the statement that the UN is equal to the distribution jointly with VN as a joint distribution, that's a much stronger statement. And that's why we don't know how to prove it. It's sort of, it's a new feature, a new phenomenon that has. Feature, a new phenomenon that has not yet been really figured out. Okay. Can I ask a question? Yeah, sure. James, do you have a question? Would you expect that there's a coupling between the two processes that respects the Edelman-Green correspondence and also the quality of these vectors at the same time? Well, as I said, the Edelman-Green process does not give you a coupling. So I think you would have to have a new. I think you would have to have a new type of map of some sort, maybe some kind of probabilistic version of Edelman Green that is not like a not a map in the sense of combinatorics, like a function that maps x to y, but a coupling of measures. Yeah, I mean a coupling under which the final state, well, yeah, the coupling under which the state of the sort of network picture and the. Yeah, I mean, it's a natural question to ask, and I've asked myself the same question. Question to ask, and I've asked myself the same question. I spent quite a long time trying to think of such a mapping, and I just couldn't figure out anything like that. I mean, yes, one would expect that if there is an equality in distribution, there might be a coupling that explains it. But as of now, I don't have any idea how such a coupling would look like. Okay. So how did we approach? So we thought quite hard about this conjecture. About this conjecture, we found a few things to say about it. In particular, we verified it for small values of n, you know, just to convince ourselves that it's true. How do you even approach such, you know, even verifying it for n equals 4 is not exactly a trivial thing to do. For n equals 3, you can do it, you know, in a quick calculation, but then it becomes more and more complicated. So, one of the things that we did that I think is interesting is we reformulated this as a purely combinatorial. As a purely combinatorial statement. So, you know, right now it's a probabilistic identity between two random vectors. So to verify it, you have to work out the joint densities. And it gets kind of thorny to write down those densities. So if you take a Fourier transform or a characteristic function, things become a bit simpler. And then we arrived at basically an algebraic identity that is equivalent to the conjecture. So that's what I want to show now. So that's what I want to show now. And I won't go into all the details because it's quite a complicated identity after all. So it will take me a bit of time to explain everything. I'll just give a quick rundown of what's happening here. But basically, think of it, it's an identity of two generating functions. So the identity says that fn is equal to g n, where fn and g n are both generating functions with n minus one variables. They are rational functions. They are rational functions of the variables x1 through xn minus 1, but they are also vector-valued. And in fact, they are sort of live in a vector space spanned by permutations of order n minus 1. So they have n minus 1 factorial components, which is quite a lot of components. That has to do with the fact that these random vectors, when you write down their joint densities, you get n minus 1 factorial cases depending on the ordering of the variable. Depending on the ordering of the variables. So, again, they're complicated random vectors. And then, so the formula to compute this Fn, you have to do a sum over all staircase shape Young Tableau of order n. And the thing that you're summing is a pair of things called the generating factor multiplied by another thing called the finishing permutation. So that's sort of the bin that it gets put in. You know, the core, it tells you sigma. It tells you sigma t tells you which component of the vector it goes into. And similarly, on the GN side, you're summing now over the set of sorting networks. And the thing you're summing is called the generating factor. And it gets multiplied by a permutation that tells you, you know, this is the component of the vector that it goes into. So you can think of it as a weighted version of Stanley's result about the two sets being equinumerous. Equinumerous. Now the two sets have a slightly more detailed identity relating them to each other, where the generating function generated by Young Tableau is the same as the one generated by sorting networks. So it's a weird identity. I mean, I've shown it to a bunch of people who, you know, have worked on Young Tableau, like Curtis Green, people who are interested in Young Tableau. And no one has quite No one has quite seen, no one has seen anything quite like it, but they all think it's very interesting. So here's a small example, just a numerical example to show you with the same sorting network and Young Tableau that were in the previous example, the permutation sigma t and pi s are 1, 3, 5, 4, 2. And that comes from this vector 10, 13, 15, 14, 11 that I mentioned earlier. 15, 14, 11 that I mentioned earlier, you just remember the ordering of those five numbers and you forget what the numbers were. That gives you sigma t and pi s. And those two are equal to each other, again, because of the Adelman-Green correspondence. So that's not mysterious. But then the generating factors F sub T and G sub S are actually different from each other. That kind of makes sense because the graphs that we're doing random walks on are different graphs. Random walks on are different graphs. So there's no reason a priori to expect that there will be such a relationship between random walks on one graph and the other. Even knowing that the Edelon green map exists doesn't really help you because these factors are just not the same. It's only when you sum things over all the T's and over all the S's that things magically sort of get aligned together to create this identity. So that's what we know. That's what we know now. Another thing that you can do, once you have this identity, you can now just program everything into Mathematica and just explicitly compute these functions fn and gn for small values of n. And you get this result, which says that we verify this identity for n up to 6. It's nice. It gives us a fair amount of confidence that this is a real thing. A real thing for n equals six, you're basically summing over 292,000 of these objects. So, you know, it's not a coincidence, it couldn't happen by accident. There's obviously, it's obviously going to straight through for higher values of n. So that's good. Now I want to move on and discuss a third identity. So there's more identities still. More identities still to explain. So now we are interested in this fact that un is equal to vn, but we discover that there's yet a third vector, wn, with the same distribution as un and vn. And this wn is actually closely related to vn and not obvious what the connection to un is. So that's why that part is still a conjecture. But the identity between vn and wn is also quite Between Vn and Wn is also quite interesting in its own right. So I want to explain that. How do we define Wn? Well, first of all, I'm going to redefine Vn to set things up in a way that makes the idea behind Wn more apparent. So first of all, I'm going to look at VN and I want to give a new description of it or a new interpretation of it in terms of last passage percolation. So I know some of you know a lot about last passage. Some of you know a lot about last passage proclation, but let me just remind basic definitions. So, the last passage procolation between two points in Z2, so you labeled AB and CD. So, here's a picture. You have AB here and CD here. You look at all the monotone paths connecting AB to C D. And for each such path, you sum a bunch of random weights along the path. Of random weights along the path. So you have a random environment here of IID exponential waiting times or weights. And you take the maximum over all such paths, and that gives you a random variable. And that random variable is called the last passage percolation time between AB and CD. And now with that definition, you can rewrite the definition of Vn. And this is standard. I won't explain it, but it's a well-known fact. Explain it, but it's a well-known fact that what I earlier called Vn can be now written as a vector of last passage proclaim. Actually, don't read this line, just look at the picture. The picture explains it much better. You're doing a last passage proclaim from the corner of the, I mean, from the origin or 1,1 of the Young diagram to the corner points. And you put all of those last massage proclamation points at time. Those last less percolation points, times you package them in a vector, and you call that Vn. Now, once you do that, I'm now ready to define Wn, and it's just an analogous description where you flip each of those origin and ending points of each of those last passage proclaim times, you flip it vertically to get something slightly different. Different. And now you take all of those last password propulsion times and you package them into a vector and you call that vector Wn. So why is that an interesting thing to do? It's hard to motivate it a priori, but the result sort of gives you an after-the-fact justification for why it was an interesting thing to do. Because it turns out that those two vectors are related to each other in a nice way. Okay, well, so our theorem says that they are equal in distribution. So Wn gives you yet another vector with the same distribution. And it's not at all obvious that that's the case. It's a theorem, so we know how to prove it, but it's not an obvious theorem. So I'll explain, I'll give a sketch of how we proved that result, but just again, let me mention as a sanity check, the fact that each, that the marginals. The fact that each that the marginals, the one-dimensional marginals of these vectors are equal in distribution, in this case, it's completely trivial. It's just the obvious symmetry of the definition. Another thing I want to mention before I go into the proof is I want to go into the story a little bit of how this was discovered. So I call this an accidental prophecy. So I wrote a book about longest increasing sub-sequences. About longest increasing sub-sequences. And in that book, I have an exercise where I basically ask people to prove that UN is equal in distribution to Wn. Now, at the time when I wrote this, I was convinced that this was true, and I even thought that I knew how to prove it just because I thought it followed from the same argument that was used to prove the equivalence of the marginals. But in hindsight, I realized I was wrong about that. It's not at all obvious. At all obvious. So, what is in my book as a conjecture, as an exercise is now a conjecture. But somehow I accidentally made this prediction. I didn't even formulate it as a conjecture. I just thought it was true that this identity held. And that was basically the starting point of this whole work. If it weren't for this accidental prediction, I don't know if if we'd ever stumbled upon this result. Stumbled upon this result, but it was because I suggested this to Eliobisi and the other collaborators as a starting sort of way to approach the problem that I think they ended up discovering this result about Vn and Wn and also concluding that Un had the same distribution. So I think that's a cool story. Let me go to the proof now. I won't go into the details, but it basically boils down to a certain symmetry or duality relationships between two correspondences in algebraic combinatorics, which are the RSK and Burge correspondences. Now, let me be a bit more specific. First of all, I'm sure everybody's heard of RSK. I mean, Burge is actually a less well-known correspondence, and I hadn't heard of it until last year. Until last year. And the specific version of these correspondences that we use is a slightly non-standard one. So it was studied by Kratenthaler and more recently by BC with some, with O'Connell and Ziguras. But because of this duality, basically what follows from that, and it's not a difficult argument once you set things up correctly, is that for general Young diagram lambda, if you have a rand, you start with a random. If you have a random, you start with a random tableau of shape lambda, where the entries are IID, either geometric or exponential variables. And then you compute the output of both of those maps, RSK and Burge. So you get, which are also tableau of shape lambda. So we call them RSK of X and Burge of X. And because of this duality relationship, it's not difficult to show that those two tableaux are equidistributed. And then the result about Vn and Wn comes. About Vn and Wn comes about because, in the specific case of a staircase-shaped young tableau, Vn and W are simply the restriction to the corner entries of those two output tableau, RSK of X and verge of X. That's actually a nice place where sort of quantities that are of interest to combinatorialists have been studied pretty much independently by probabilists, you know, under the name last passage percolation. But there's strong connections between last passage percolation and Faster proclation and some of those things like RSK. So that's pretty much the way the proof works. Now, recently we realized there is actually a second way to prove it. So I'll just sketch this completely separate proof. And the proof follows in this case from a much more general result that was proved by Duncan Duverne recently. It's a kind of invariance principle that he discovered in last passage percolation. Passage percolation, and that actually is one of a family of such results that have appeared recently. So, there's another related invariance principle that was proved recently by Borodin, Gorin, and Wheeler. So, I won't go into the details, but the general idea is Duncan proved that you have a vector of last passage proclation times, and it will be equal in distribution to another vector of last passage proclation times, where what you do is you keep most of the pairs of keep most of the pairs of points AIBI and CIDI, you keep most of them the same, but the last one or any of the any one, it doesn't matter, the order, you shift by some fixed amount, alpha, beta. You shift both the origin and the end point of it. So under certain conditions that have to be met, and it's not always the case, but once these conditions hold, it can be shown that there's identity in the distribution. And then our result about In distribution. And then our result about Vn and Wn we realized actually follows in a pretty straightforward way by iteratively applying this invariance result. But that's a completely separate proof from the one using RSK and Burge. So I think it's kind of cool that the same result can be approached sort of in two different ways. Any questions about that? I have one more thing to discuss and then I'll wrap it up. So it's I have about five more minutes, right? About five more minutes, right? I have a quick question. Yeah, hey, Dan. So, is there an analog of WN for the in the world of the oriented short poses? Not that I'm aware. Yeah, I don't know what the analog would be. It's an interesting question. Your call can't be taken at the moment. Okay, so let's. So let's move on to the last thing I wanted to discuss basically. But my screen got stuck for some reason. Looks fine. Okay. So yeah, like I mentioned with Bufettov and Gorin, we managed to prove this identity about Xn and Yn. And that was using the invariance principle of Borden, Gorin, and Wheeler. I think I'll skip the details. My software is sort of doing weird things right now. I don't know why. Okay, so we use an invariance principle that is also very general. It's stated for something called the colored stochastic six-vertext model. In a certain limiting case, it degenerates to the multicolored TASEP, and the multicolored TASEP and the multicolor TASEP is one way of representing the oriented swap process. So, you know, a bunch of hocus-pocus using different mappings and things like that. And Vadim Gorin and Bufetov, I think, worked out pretty much all the details for this. So I didn't have to do very much other than tell them about this problem. But it was nice, and we resolved it. And we resolved the problem, but we didn't actually prove the full conjecture. So we only proved what I denote by star here: you know, that basically the joint distribution function of u n and v n along the diagonals are equal. And that's enough to conclude that the maxima of those two vectors are equal in distribution. But it leaves open, as I said, the method doesn't work to prove anything stronger than that. Doesn't work to prove anything stronger than that. So the full conjecture remains open. And that's both good and bad. It's bad because I would have liked to prove it, but it's good because there's still more left to do for other people. And it also suggests that there's actually possibly new phenomena that have not yet been understood about the multicolored TESEP, various symmetries that might be of interest. So there might be additional results left to discover there. Left to discover there. I had some mathematical things to show, but I think I'll skip that. So let me summarize, and I'll go now to the meta story about the research. So as I was saying, in 2008, I asked this question about the absorbing time XN. And I thought this was a fascinating question. I told about it to many people. I wrote it in my book. I wrote it in grant proposals and so on. And I somehow. So on, and I somehow kept having this nagging feeling that it's an interesting question. It wasn't until 2018 when I met Elia BC and Fabio Kunden at the conference in Dublin, and I told this story to them, told this question to them, like I told it to many people. I think it was during the hike. And eventually, this led to this solution because a year later, they started working on this as a summer project with an undergraduate named Shane. With an undergraduate named Shane Gibbons, and I told him this accidental prediction that I made, which I had at the time I still believe that I know how to prove it to get them started. And so this led to basically the stuff that I was telling you about. And I have to say, the result that we discovered really exceeded our expectations in the following sense: that we started with a question about asymptotics, but we ended up discovering exactly. But we ended up discovering exact identities, which, depending on your point of view, I think they are more interesting than the asymptotic result. And they are not yet even fully understood. So there's still more left to understand there. Now, what are the lessons here? I think, I mean, this is a story, but what are the lessons? So, the lesson number one is that even if you don't care about asymptotics, you should care about asymptotics because often asymptotics is a Is a good motivation for working on things and it leads us to discover things that are unrelated to asymptotics. So that's sort of at the mathematical level. But at the meta-mathematical level, lesson number two is that when you have a cool problem, keep plugging away at it and keep telling people about it. And if you wait long enough, you know, sometimes it will pay off. And I found this to be the case. This was an example. I found this to be the case. This was an example, but it's happened to me different times over the years. And lesson number three is that conferences are very useful and also going on hikes. So this is a hint to the organizers that we'd like to have an actual conference with the hike at some point. So I think I'll stop here. Thank you. Thank you. So we can all unmute and Unmute and thank them for the talk. And we can have some questions 