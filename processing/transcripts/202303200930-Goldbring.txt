All right, well, thank you very much. First, I want to thank the organizers for the invitation, first and foremost, and for thinking of me to give this talk, but also for all their patience and hard work during the original planning of this meeting and then the re-planning of this meeting. It's been a lot of work on their part, but I'm very grateful for everything they've done and for putting. Grateful for everything they've done and for putting this together, and I'm very excited for hopefully a very productive week. Okay, I also want to ask a favor of the audience, please, please, please interrupt me often with lots of questions. I don't want to just be talking to my computer for an hour, so please, it'll make me feel like I'm there if you interrupt me with questions or comments. I should say that I decided to focus on. I decided to focus on just the basic mechanics of continuous logic and didn't really include anything on anything effective, no computable structure theory for metric structures here, just for time purposes. And I wanted to make sure I covered the essentials of continuous logic. So what I did this morning is, since I forgot my Dropbox password, I put links to some of my I put links to some of my papers that are more on the effective side of things in the overleaf folder. So if you click on those links, you could see more of some of my work on the effective side of continuous model theory. All right, I guess I'll get started then. So borrowing an idea from my co-author Brad Hart, I want to motivate the syntax and semantics of Syntax and semantics of continuous logic using metric ultra products, which probably isn't the most relevant thing for this meeting, but I do really think it is one explanation for why the definitions are the way that they are. Okay, so let's focus on the following situation. I'll have a family of metric spaces of uniformly bounded diameters. All the metric spaces. Bounded diameters. All the metric spaces have, there's a single bound on all the diameters of the metric spaces in my family. For simplicity, let's say that that bound is one. And let's take an ultrafilter U on this index set I. Okay, then I'm going to define a distance on elements of the Cartesian product where I'll look at their coordinate-wise distance, and then I'll take the ultra limit of that family. So, in case you've never seen an ultra limit before, it's the Ultra limit before, it's the unique real number so that you give me any epsilon, and the set of indices for which these coordinate-wise distances are within epsilon is a large set of indices according to the ultra filter. And this exists by our uniform boundedness assumption, and it's unique as well. It's easy to check. But unfortunately, this is not a metric in general. It's just a pseudo-metric. It's very easy to see. It's very easy to see, just like when we talked about classical ultra products, two things can be equal in two elements of the Cartesian product could be equal on almost every coordinate without actually being equal all the time. And so what do we do when we form a classical ultra product? We mod out by that equivalence relation. So we have to do something similar here. We have this pseudometric, which could give the two elements of the Cartesian product distance zero without actually. Distance zero without actually being equal. So we quotient by that. It's the usual quotienting procedure for a pseudometric to obtain an actual metric. And we call that the metric ultra product of the family of metrics, the family of metric spaces with respect to the ultrafilter U. And I think at some point I'll need terminology for the equivalence class of a sequence. I'll just put a subscript U for the equivalence class of the sequence. Sequence and just like with the classical ultra product construction, if all my metric spaces are the same, then we call the ultra product the ultra power of that particular metric space with respect to u. And you have a diagonal embedding of the original metric space into its ultra power by just taking an element to the equivalence class of the sequence that is constantly that element. And we have a nice isometric embedding. Isometric embedding then of the original metric space into its metric ultra power. Okay, are there any questions about this? Okay, now as logicians, we're not going to just want to be thinking about metric spaces, families of metric spaces as is. That'd be like thinking just about sets with equality. We want to consider additional. We want to consider additional structure. And so, for example, what if on my metric space I have a distinguished function for some reason? I would like to possibly interpret that function on the metric ultra power or otherwise said slightly differently. Can I extend this function to the metric ultra power viewing the original? The original M as a subset, a subspace of the metric ultra power in the most naive way, the point-wise way. So, if you give me an element A sub u of the metric ultra power, what should happen when I plug it into this function? What should the definition be? It should be f of the sequence, which by that I mean just the sequence f of AI. And then take the equivalence class of that sequence. Sequence. Is that even a well-defined thing? If two elements A and B of the Cartesian product represent the same element of the ultra power, do F of A and F of B represent the same element of the ultra power? Only then can you make such a definition. It's very easy to see you can do this if F is uniformly continuous. That's no problem exercise. But it's actually also necessary. So how do you see? Necessary. So, how do you see that? If f is not uniformly continuous, what does that mean? That means there's some epsilon so that I can find for every i, integer i, pairs a i and b i. Sorry, that's a typo. That should just say d of a i, b i. So the distance between a i and b i is less than one over i, yet f of a i and f of b i are at least epsilon apart. Now, as long as you Now, as long as U is non-principal, if U is a principal ultrafilter, the metric ultra power is just M again, and that's boring. But as long as U is a non-principal ultrafilter, I guess I'm technically assuming here U is on the natural numbers, a slightly more technical hypothesis. I guess countable incompleteness or something would be necessary for a general index set. But let's just pretend that our index set is countable for the moment. Index that is countable for the moment. Then A and B would represent the same element of the metric ultra power because for almost every index, the distance would be less than epsilon for a given epsilon. Yet F of A and F of B represent different elements of the ultra power because they're always epsilon apart on every single index. So you can't do this. So a necessary and sufficient condition for extending your distinguished function to the ultra power is that it's uniform. To the ultra-power is that it's uniformly continuous. And if you want to take this one step further and not assume that you have a single metric space, but rather a family of metric spaces, each of which have a distinguished function, and you'd like to take the pointwise definition on the Cartesian product and extend that to the ultra product, then not only will the FIs have to be uniformly continuous, but they will have to be continuous. But they will have to be so uniformly, okay? Meaning the same relation, you give me an epsilon, I give you a delta, that witnesses uniform continuity, that there must be a single such choice of delta for epsilon that works for all of the FIs simultaneously by the exact same argument. Okay, so if we're envisioning a family of metric structures, which we're about Of metric structures, which we're about to define, as metric spaces equipped with extra structure, then those distinguished functions are going to have to, if we're going to want to take their ultra product, they're going to have to be uniformly, uniformly continuous. Are there any questions about this slide? Technically, that's only if and only if you're quantifying over like all ultra filters, right? Because if it's just one particular ultrafilter, you'll If it's just one particular ultrafilter, you only need it on a big set. I don't follow. Can you say it again? So say that you have half of your structures are bad, like they're very badly, not uniformly continuous, but that half is not in the ultra filter. I mean, you can always ignore what happens on an ultra filter small set, right? So you might. Set, right? So you might as well, yeah. So, right. Yeah, but of course, when you're doing model theory, you're always going to want to be considering the ability to take any ultra product. Right, yeah, exactly. Yeah, yeah. Okay, great. Good. Any other questions or comments? Okay, so let's take that as our motivation for the syntax and semantics then. So what does a continuous language or continuous signature do? Signature does so it has a lot more obligations than a classical language or signature, basically for the reasons that I just described. So first of all, we were talking about a family of metric spaces that were uniformly bounded. I supposed it was one just for simplicity, but there was no reason it had to be one. Now, regardless, our language should specify. Our language should specify a bound, which, once we go to interpret the language, our structures better have diameter bounded by the bound that the language gives us so that we can consider the ultra product. Okay, now just like in classical logic, we'll have a set of function symbols. Okay, and for each function symbol, besides the language having to provide an arity, which is something they had to do in classical logic as well. It had to do in classical logic as well. We now have to prescribe a function which we call a uniform continuity modulus, which tells us, you give me epsilon, this is a delta that witnesses the uniform continuity of F for that given epsilon. This is exactly what we were talking about on the previous slide. If I'm going to want to take an ultra product, I need them all to be uniformly, uniformly continuous. And so that's what this uniform. And so that's what this uniform continuity modulus is specifying for this particular distinguished function. This includes the case of constant symbols, which have already zero, and then there is no uniform continuity modulus requirement. Okay, now what's really new is we're going to have, well, maybe I have to say before I say what's new, what's not new, just like in classical logic. What's not new, just like in classical logic, we're gonna have predicate symbols or relation symbols, whatever you wanna call them. I kind of prefer predicate symbols for continuous logic because, as we'll see, they're not actually gonna be relations. Now, for each predicate symbol, the signature provides, once again, an arity, just like in classical logic. Of course, the arity can't be zero now. And once again, it's going to provide a uniform continuity modulus. Now, that may seem strange. Now, that may seem strange because we're used to thinking of predicate symbols as distinguished subsets of our structure or Cartesian powers of our structure. Here, a predicate symbol is actually going to be a function again. What kind of function is it going to be? Well, if we think of the metric as the analog of equality, then the metric is a real-valued function. function. And in general, that's what our predicate symbols are going to be. They're going to be real valued functions. And we are going to want them, just like with function symbols, to be uniformly continuous in a manner specified by the language. And so that's once again what this language then has to do. It has to give us a uniform continuity modulus for each predicate symbol. Okay, and we'll have a bit more to say about this. And we'll have a bit more to say about this. I'll make one slight comment. You might say, why do our predicate symbols have to be real-valued? Why couldn't they be complex valued, or why couldn't they take values in other, let's say, compact Hausdorff spaces? And the answer is they could, and you would get no greater expressive power. So this has been observed by various different people. I should say in the original continuous model theory of Cheng and Kiesler back in the 60s, that's what they did. That's what they did. But it was sort of very unwieldy and it didn't quite catch on. There's a very nice note by Atai Benyakov several months ago on the archive explaining how this version of continuous logic that I'm presenting gives you the same exact expressive power as allowing more general values for your predicates. And we will, for various reasons that we'll see, we'll want Various reasons that we'll see, we'll want our predicates to be uniformly bounded. So their images aren't going to be just real value, but they're going to actually have to take values in some compact interval in the reals. Just like our metric had to be bounded, our predicate symbols are going to have to be bounded. And so the language is going to tell us a bound on them. Just like in logic, in classical logic, you always include equality as a Quality as a distinguished predicate symbol, at least I do. The metric is always for us going to be a distinguished binary predicate symbol. And it's straightforward but annoying to extend this to the many sorted situation. But there's really nothing more complicated about doing that. Isaac, why are you moduli from the unit interval to the unit interval when you are allowing arbitrary bounds on the metric? On the metric because uniform continuity is always about: if I want the values to be this close, then the inputs should be this close. And closeness is about small stuff, right? So there's never a small epsilons and deltas. So there's really no loss in just saying, if I give you an epsilon less than one, what delta less than one do I need in order to have my uniform continuity satisfied? My uniform continuity satisfied. And actually, with a little bit of effort, you can require these things to be continuous, these moduli. And that will actually be useful for some technical reasons later. Are there any other questions? Awesome. Okay. So then what does an L structure do? You have this language L, an L structure has to interpret the symbols. So it should interpret. So, it should interpret first and foremost the underlying. It's not an underlying set anymore, like we used to have in classical logic. It's an underlying metric space whose diameter is bounded by the bound that the language provides. And actually, we require it to be complete. As we all know from working analysis, complete spaces are the best. And that's you always want to be working in a complete space. But you might wonder. Space. But you might wonder: is that like cheating? Are we losing anything by requiring our metric structures to have as their underlying metric space a complete metric space? And the answer is no. In fact, requiring them to be metric spaces and not just merely pseudo-metric spaces is also no loss of generality. There's a little procedure, a little check. If you give me a metric structure whose underlying metric space isn't actually a complete metric space, but just Isn't actually a complete metric space, but just merely a pseudo-metric space. There's a mod-out and complete procedure which will give you a structure that I'm about to define and which is bi-interpretable with the original version. And so this is really no loss of generality. People sometimes call those pre-structures. They arise naturally in analysis, but the first thing you do is you immediately mod out and complete. So might as well just start out with. Might as well just start out with this to get going. Okay, and then you give me a function symbol, I have to interpret it. I interpret it as a function on the appropriate Cartesian power of the thumbulite space, and it should be uniformly continuous as witnessed by the modulus that the language provides, which means if you give me two tuples of the appropriate arity whose distance is less than delta f of epsilon, and by that I mean coordinate-wise, each of the distances. Wise, each of the distances in each coordinate is less than that. Then the distance between the outputs is, you'll notice less than or equal to epsilon. So there's a slight inequality, pun intended, between the left and right-hand sides of this implication. One quick explanation for why that is, is that if you think about an ultra product, if each of the structures in the ultra product satisfied this implication with both Satisfied this implication with both inequalities being strict, then all you could conclude in the ultra product is that it satisfies this implication where the conclusion is a weak inequality. So that's a little, you wouldn't want the individual structures to satisfy the strong version and the ultra product only to satisfy the weak version. But if you change it to this implication, now there's no loss, there's no asymmetry anymore. There's no asymmetry anymore. Okay, and then predicate symbols, you interpret them similarly. They take their values in the interval in the real specified by the language. And once again, they're uniformly continuous as witnessed by the modulus that the language gave us. Where now, of course, we're using the usual metric on the real. All right. Any questions? Any questions? Yeah. Why is it important that the language specifies the bounds instead of the structures specifying the bounds? Well, because like you imagine yourself, you're doing model theory and you have a bunch of structures and you want to take their ultra product. And as we talked about at the beginning, if they're not uniformly continuous in a uniform manner, you're not going to be able to take their ultra product. Not going to be able to take their ultra product. So you could say, well, all of the structures have to have the same modulus of uniform continuity, but that's basically that should be part of saying they're all L structures. If I say they're all L structures for the same language L, then that entails I can take their ultra product and get an L structure again. It entails this uniform, uniform continuity. Other questions? Wondering if there's an example coming sometime soon. Indeed. Aha. Yes. Good. Thank you. That was planned. All right. So I have a couple examples. So let me just remind you what a Bonach space is. For me, my Bonach spaces are always. For me, my Bonach spaces are always complex. So, this is a complex vector space equipped with a norm. And when you look at the metric that the norm gives you, that's a complete metric. Okay, so we'd want to talk about Bonach spaces in continuous logic. And uh-oh, the metric on a Bonach space is unbounded. Almost all interesting metric spaces are unbounded. Let me just make that as a definitive statement. And so what And so, what do we do with that? Okay, there's a couple of approaches that are more or less equivalent. I usually work with the latter in what I do, but let me tell you about the former because it's a bit easier. You only work with the unit ball. Unless your metric space is, excuse me, unless you're for general metric spaces, this is not true that all the interesting information is contained in the unit ball. But for phonic spaces, But for phonic spaces, it's absolutely true that studying the unit ball is more or less equivalent to studying the entire thing. Okay, so the metric on the unit ball is bounded by two, in fact. Fantastic. I can't quite add and multiply by scalars freely because I'll leave the unit ball. So you study rounded combinations. You take these combinations alpha x plus beta y that are guaranteed to land in the unit ball. To land in the unit ball. So alpha plus beta, absolute alpha plus absolute beta is at most one. We would want a constant symbol for zero, and we would have a predicate symbol for the norm, and that would take values actually in zero, one. The way I set it up, you know, we said it would between minus one and one, but in fact, it'd be between zero and one. That's one possibility. The other possibility is what I usually do. What I usually do out in nature, I would work in a many-sorted structure, one sort for each ball, BN, which I'm going to think of as the ball of radius n around the origin. And I believe I coined this term, but I may be wrong. I like to think of this as the dissection of the Bonach space because you're taking this poor Bonach space and you're ripping it apart one sort for every ball. Now things get annoying if you are going to be. Are you going to be pedantic about keeping track of exactly all the different symbols? For example, I would have one symbol for addition of elements from the n-ball times the n-ball. That answer is going to land in the 2n ball. So I would have one such symbol plus sub n for every n. Similarly, for multiplication by 4, I would need one. Sorry, I dropped the annoying subscript. I'm already, you know, in practice, I'm already, you know, in practice, one never writes them, but when you're presenting it for the first time, you should. Multiplication by four, you're going to have one such symbol for every n. You give me something in the n ball, the answer is going to lie in the four n ball. You would have one constant symbol, zero sub n for each n, and one norm symbol for each n with bound n. And then you would want some inclusion mappings between the balls. You would want a mapping from the m ball to the n ball when m is less than n. When m is less than n. Of course, in practice, you would never write all these subscripts and you forget about this, but you definitely have to keep in mind that you're doing that when you're talking about the model theory or the computable structure theory of such things. And for all these symbols in either presentation, there's obvious uniform continuity moduli for all of these functions and predicate symbols. So your language would specify that. Questions? Questions? So, any particular reason why choose multiplication by four in China? No, no, no, sorry. You have one symbol for every complex lambda. I'm just giving you an example. But for every comp, for every, and if you're like, wow, that's a lot of function symbols. Okay, then just you could deal with like rational, rationals plus rational times i. That would give you a countable collection of symbols, which are dense. Symbols which are dense, and it would be good enough for the model theory. But no, you really need one for every such lambda. That's why I wrote EG, just giving you some examples. Other questions? Yes. These two approaches, could you give an idea of when you might prefer one over the other, or is that coming up? Or did I? No, I also. I no, I always work in the second. I just wanted to make clear that if you're uncomfortable with the many sorted, then you can work with the just the unit ball approach. But for me, I always work in the many sorted. Thanks. No problem. And there's some expansions of Bonach spaces that you could consider. For example, Hilbert spaces. So Hilbert space is a vector space equipped with. Space is a vector space equipped with an inner product. And when you go from the inner product to the norm, from the norm to the metric, that's a complete metric. So you could consider those as structures. Now, small issue, the inner product is complex valued. So technically, you would need predicate symbols for the real and imaginary parts. If you're in the unit ball approach, you would want to say those are in minus one. You would want to say those are in minus one, one. If you're in the many-sorted approach, you would need one such pair of symbols for each n. Thinking about restricting inputs to the n-ball, and then their bounds would be minus nn. And once again, there's obvious uniform continuity moduli for the inner product. You know that the inner product of this pair and that pair are going to be close if the constituent entries are close. Entries are close. It's an easy exercise in Hilbert space. So that's one example of an expansion of a Bonach space. Another example that's near and dear to my heart are C-star algebras. So these are Bonach spaces equipped with extra algebraic structures. They have a multiplication operation and they have a adjoint operation, multiplication being binary, adjoint being unitary. Being binary, adjoint being unary, and the key axiom is the C star equality that the norm of x star x is the same as norm of x squared. You would need a binary symbol for multiplication or a bunch of such symbols in the many sorted approach. You would need a unary function symbol for the adjoint operation or one such symbol for each sort in the many sorted approach. Sometimes people restrict attention to unary functions. Sometimes people restrict attention to unital C-star algebras. They demand that there's a multiplicative unit, and then you might consider adding a constant for that. And once again, there are obvious uniform continuity moduli. All right. So now let's do some formulae. You define terms exactly as in classical logic. You close variables and constants under function symbols. And you define terms. And you define formulae by induction on complexity. So if I have a bunch of terms, I can plug them into a predicate symbol and I'd get an atomic formula. And that includes the distance between two terms, because remember, the distance, the metric symbol is always a distinguished predicate symbol for us. And if I have n formulae, I would want to be able to plug them into an n-ary connective. Into an N-ary connective, and we're very generous in continuous logic. We allow any continuous function from Rn to R for any n to be a connective. So I could plug phi1 and phi n into u and I get a new formula. Okay, now when people first see this, they go, whoa, that's a lot of connectives. And okay, but here's how we calm them down. We say, first of all, in classical logic, we all know that with a All know that with a very limited number of classical connectives, in fact, with the Scheffer stroke a single one, you can generate any function from 0, 1 to the end to 0, 1. So here we're not so fortunate. Well, we kind of are. I'll say something in a second. But we'd want the analogous thing to be true. You'd want to be able to express any function from r to the end to r, not just arbitrary functions. This is analysis, after all. Functions. This is analysis, after all, so continuous ones. As we'll see in a second, this is actually not generous enough. We're going to want some infinitary connectives later on, and we'll discuss that. But for the moment, these are our connectives. And then we have two quote-unquote quantifiers. You don't have to be emotionally attached to thinking of them as quantifiers, but they kind of play the role of quantifiers. If I have a formula, I can talk about the soup of a variable for that formula. Soup of a variable for that formula and the inf of that formula with respect to that variable. And as I said, it seems like there's a lot of connectives, but actually there is a very small, finite set of connectives that generate a collection of formulae that are dense in a certain natural sense in the set of all formulae. And in fact, then you have an effectively enumerable collection of such formulae. And for our considerations, that's important to know. Important to know. So, as long as you well, provided your language itself, like the non-logical symbols, can be effectively enumerated, then you can effectively enumerate the L formula appearing in this dense set. And that's often good enough for all purposes. But as we're going to, we see right now, just with the distance between two terms, that's an atomic formula. It's not true or false when I plug in a tuple for the. When I plug in a tuple for the variables, it's going to be a number. And that's a new feature to continuous logic: that the interpretation of a formula in a structure with some tuple plugged in for the variables is going to be a number. So that's what we're going to do. We're going to interpret formulae. So let's suppose I have an L structure M and I have a term. You interpret it just as in classical logic. But it's important to notice for later purposes that. Notice for later purposes that when you go and do that, you actually have a uniformly continuous function now. The interpretation of the term is a uniformly continuous function, and it's uniformly continuous uniformly over all L structures, or as we like to say in the business, it's the uniform continuity of TDF is known to the language. You can predict using the uniform continuity moduli of the function symbols, the uniform continuity. Symbols, the uniform, a uniform continuity modulus for a given term. Okay, and then you interpret formulae inductively in the obvious way. So here's an example. So suppose I have soup over x phi xy, and I plug in a tuple A for Y. How do I interpret that in the structure? I interpret phi M B A for every B. A for every B in M, where I'm assuming inductively I know how to do that, and I take the supremum of all such things. And you may go, wait a second, why is that supremum not infinity? One thing you're doing along the way as you're interpreting these formulae is you're also proving that all of these formulae are bounded and uniformly so over all structures. In as I said, like the bound is known to the language. So, in other words, there's a positive real r sub phi, so that in every structure, the value of the function phi m is contained in the interval minus r phi r phi. So the soup in the above display exists. And in the exact same way, each of these interpretations are uniformly continuous in a way known to the language. In a way, known to the language. All right, maybe now is a good time to pause and ask: are there any questions? Okay, so let's see. So, let's get used to this. So, as I said, unlike classical logic, if I have a formula and a tuple from a structure, and I interpret the formula. And I interpret the formula in the structure evaluated at that tuple. I'm not getting true or false anymore. I'm getting some number in the interval minus r phi r phi. So to assert something, you actually need to say you need to set a formula equal to zero. Excuse me, set a formula equal to a number. So for example, if I have two terms, t1 and t2, and a structure m, and I plug in some tuple. Structure M, and I plug in some tuple A for the variables. That's an atomic formula. And I can ask whether or not the atomic formula is zero or not. And that's clearly the same as saying that the two terms, excuse me, the what happens when I evaluate the two terms on the input A, I get are the same element of M. So to assert these two terms are equal on this given input is the same as setting the atomic formula. As studying the atomic formula d t1a t2a equal to zero. For this reason, and I guess, yeah, for this reason, let's say zero is often considered to be the default truth value in continuous logic. You don't have to think of values as formulas as truth values. You don't have to. But if you want to, it kind of might bother some computer scientists, but then we think of zero as the default truth value. As the default truth value, in which case we might use the familiar satisfaction symbol and write m models phi of a to mean phi m of a is zero. But don't freak out, like zero is completely an arbitrary choice. To say phi m of a is zero is the same as saying if I look at the formula, oh, sorry, typo, sorry, massive typo. Phi m of a equals r is what I meant to say. Excuse me, phi m of a equals r. That's a equals r that's equivalent to what you see on the right hand side now let's dissect that for a second phi is a formula the function you give me a number and i calculate absolute value that number minus r that's a continuous function on the reals so i can plug phi into it in other words the the formula absolute phi minus r that is really Absolute phi minus r, that is really a formula again. And it evaluates to zero on a precisely when phi m of a is r. So I can express any truth value I want using zero. So it kind of just makes life convenient to restrict attention to zero. As in classical logic, a sentence is a formula without free variables, and a theory is just a set. And a theory is just a set of sentences. And again, we say T is satisfiable if there's a structure that satisfies all the sentences in the theory. Conversely, we can talk about the theory of a structure. It's just all the sentences that evaluate to zero in that structure. You then know what every sentence evaluates to by this procedure I mentioned a few bullets. By this procedure, I mentioned a few bullets above. So the theory, even though it focuses on which sentences get value zero, it really understands the truth values of all sentences. So just going back to that, my first reaction about to assert something one sets a formula equal to a number, my reaction was, couldn't you set a formula to be less than one half or something like that? But with all of this, it basically comes to the same thing. The same thing that's right. Well, not less than a half, but less than or equal to a half. Okay, right, yes. Yeah, you could say, um, right, you could say the maximum of my formula and a half is a half. Yeah, right. Okay. And then you can similarly talk about one formula being less than or equal to another formula on an input. Same thing. Okay, so just to give you an idea, we can axiomatize, let's say, the many-sorted approach to Bonach spaces using these ideas. Let me give you some sample axioms. If you wrote them all down, it would drive you nuts. But here's some sample axioms. So here is a distributive law, one of the distributive laws. And if you're, I'm purposely being pedantic and writing all the subscripts, but this is how you would express that multiplication by four. Express that multiplication by four distributes over addition for four elements of the n-ball. Okay, you immediately stop doing this once you start writing papers, but when you first see this, this is technically what you have to do. You would want some axioms connecting the metric and the norm. You would want to say the distance between x and zero is the norm of x. Okay, and one such axiom for each. And one such axiom for each ball. And then you would want to say that the sorts BN are precisely the elements of norm at most n. And this is a little delicate, right? You can have an element of BM. And if its norm is at most n, it better be there because it was the image under the canonical inclusion of the n-ball in the m-ball of something. Okay, and so here's. Okay, and so here's the continuous logic way of doing that. So you see this dot minus symbol there. This is just truncated subtraction. So if it becomes negative, you just set it to be zero. So what if this soup is zero? What does that mean? It means this is why we think of soup sometimes as universal quantifier. It would mean for every x, this is zero, because both things in the min are non-negative. So if its value So, if its value, if the soup is zero, it means every single value is zero. So, for every x, this is zero. What does that mean? Well, if the min is zero and it wasn't because of the first coordinate, that would mean the norm of x is less than n. So, if the norm of x is less strictly less than n, what I'm saying is that there should be an element y of the n-ball whose image under Whose image under the canonical inclusion is X. This is not, that's not exactly what this says because infs are infs, right? So that doesn't mean they're exactly a Y. That means there are Y's that do this approximately. So there's a little bit of a completeness argument to tell you that you could actually get a Y to do it. But morally, I mean, this is what it's saying. This is one of the little occupational hazards. Occupational hazards in continuous logic, ifs are only approximate existential quantifiers. And so this axiom looks like it works and it does, but you have to double check that it does. And this is where completeness of the sorts comes to the rescue. Okay. One thing you're hopefully taking away from this slide is that axiomatizing elementary classes in continuous logic is a chore. A chore. In classical logic, if you're trying to study something from a model theoretic perspective, step zero is to write it down in the appropriate language, and that's almost always obvious and said in one sentence, and then you move on to more serious matters. And continuous logic, I've written papers about here's how you present something as a metric structure. Like it's usually a non-trivial endeavor. Okay. Once we've got all this down, other model theoretic notions come for free. What are embeddings? They are functions that commute with the non-logical symbols. In particular, embeddings are isometric embeddings because they come. When I say commute, I shouldn't say commute in the case of predicate symbols. If you do the predicate symbol or you do the embedding and then you do the predicate symbol, you get the same answer. Do the predicate symbol, you get the same answer for the predicate that is the metric, that's just an isometric embedding. Obvious notion of isomorphism, a surjective embedding. Elementarily equivalent structures, they give all sentences the same values, or equivalently, they have the same theories as I defined them before. And of course, isomorphism implies elementary equivalence. What are elementary embeddings? They preserve the truth of all formulae. Okay, so if I evaluate a formula in M on a tuple, or I evaluate it on N, in N on the image of the tuple, same answer. Substructure, elementary substructure, you have all the natural model theoretic things. Um okay, any questions at this point? Okay, um it's not super relevant for this conference, but based on the beginning of the talk, I figured I would mention there's a wash theorem. Kind of we motivated the notion of a language so that I could take the ultra product of a family of structures and get a structure again. So it's the language. So, it's the language allows you to do that. And then, the natural version of Wash's theorem is not a formula is true in the ultra product, if and only if it's true in almost every structure, but rather, how do I compute the value of a formula in an ultra product? I take the ultra limit of the values in the constituent members of the ultra product. And as usual, this gives you that the diagram. And as usual, this gives you that the diagonal embedding is an elementary embedding. It's a nice exercise to prove the Wash theorem. It's an exercise to test that you understand how all this works. Okay. Let me say a couple of things about compactness and completeness. So there's a compactness theorem, which is actually comes in a slightly stronger form than you might guess. If you have a set of If you have a set of sentences, satisfiable and finitely satisfiable are the same, but actually it's a little bit better than that. You can get satisfiable and approximately finitely satisfiable are the same, which means instead of saying sigma is zero for finitely many sigmas, you can actually say sigma less than or equal to one over n for finitely many sigmas and some space. Many sigmas and some specific n. If you could satisfy those, then you can satisfy all of sigma. And this is very useful in practice. Right? In analysis, you can hardly ever do things on the nose, but you can do them approximately. Just as in classical logic, you can prove this using ultra products, or it'll follow from the completeness theorem that I'm going to mention right now. So, Benyakov and Peterson developed a proof system for continuous logic. I won't tell you how it works at all, but I'll just tell you all the great things that we're used to in classical logic remain true here. So, for example, if I have an RE collection of formulae, then the set of consequences is Re again. They proved a soundness and completeness theorem. Gamma is consistent, which means it doesn't prove everything. Which means it doesn't prove everything precisely when it's satisfiable. And an important corollary of all that, which has been really useful in the computable structure theory that we've been up to, is what they call a Pavelka-style completeness, which says if I have a set of sentences gamma and some other sentence sigma, then the soup of the interpretations of sigma. Interpretations of sigma in all, oops, sorry, another typo there. That should say M models gamma, excuse me. The soup of sigma of m as m models gamma. So the biggest truth value sigma could take over models of gamma is the inf of all the, let's say, rational r's for which gamma proves sigma is less than or equal to r. Right? If gamma proves sigma is less than equal to equal equal If gamma proves sigma is less than or equal to r, then in any model of gamma, sigma better be less than or equal to r. Okay, but this Pavelka-style completeness says there's no gap. The soup of all the values in models of gamma is precisely the inf of all the reals for which gamma can prove sigma is less than or equal to r. Isaac, I have a question about your second bullet point. You've only got recursive enumerability for Sort of approximate consequences, not exact consequences. No, so there's no such when you the set of consequences is the set of consequences, period. When you say what you're thinking of is like in this Pavelka-style completeness, right, you might think of sigma as an approximate consequence when gamma proves sigma is less than or equal to r rather than just proving sigma, right? But no, the set of consequences is just like in classical. The set of consequences is just like in classical logic. It's the set of things that gamma can prove using the proof system. Oh, oh, okay. Yeah. Right. Okay. Right. All right. So in the last little time, little bit of time, let me just talk about some of the nuances. All the things I talked about before are, they may seem strange at first glance. They may seem strange at first glance, but once you get used to this, they're like, oh, yeah, I would just take this classical thing and I would plug it into this machine and I get the continuous logic version and no problem. What I want to talk about in the remaining time are some of the nuances that are really specific to continuous logic and are important. And they've been important in our applications and in some of our computable structure theory applications. And really understanding the nitty-gritty of how. And really understanding the nitty-gritty of how they worked was essential. So, I want to first talk about a generalized notion of formula. As I said, it seems like we had a ton of formula before. It turns out that's not enough formulae for what we want to do. So if I have a signature L and a theory T, there's a natural notion of the distance between two formulae with the same free variable. What do you do? You just look at all models of your theory and all. Of your theory and all inputs from the model, and ask what's the difference of the two formulae on those inputs, and you take the biggest difference. So, two formulae should be close, roughly speaking, if in all models, they almost they give you almost the same value. Right? So, this rho t being small should tell you phi and psi are like almost equivalent in models of t. In fact, phi and psi are t equivalent in the obvious. Are t equivalent in the obvious sense, meaning in every model of t they interpret to be the exact same formula, exact same function, precisely when this rho t gives them value zero. So this rho t is a pseudometric, and two things are equivalent in the pseudometric precisely when they're t equivalent. And as we always do, we always identify t equivalent formulae. In other words, we would mod out by this pseudometric. By this pseudometric. But this is analysis. We always want complete stuff. So I would want a complete space, a complete metric space. The collection of formulae modded out by T equivalents isn't going to be complete in general. So we complete it. And the elements of the completion are what we call T formulae. Okay, so they are just limits of what you might call. Of what you might call naive formulae, or basic formulae, the formulae that I introduced earlier. To give you an example, here's something you can do now. You can take weighted sums, countable weighted sums, a formulae. That won't be an actual formula like I defined earlier, but it will be a T formula. Actually, T here is not important. It could be the empty theory, for example, in this example. And you can interpret T. And you can interpret T formulae in any model. What is a T formula? It's a limit of a Cauchy sequence. You could just evaluate the elements of the Cauchy sequence and take the limit. And there's some routine things to check that this is well defined, independent of which Cauchy sequence you chose when you viewed this as an element of the completion. And once again, T formulae will be uniformly continuous in a way known to the language. In a way known to the language. Oh, sorry, in a way known to T, right? Because T is telling you how this sequence is Cauchy. Okay. So that's one thing that's new to continuous logic is you're going to want to complete your collection of formulae relative to some background theory T. Okay, that's one nuance. A much more subtle nuance is what Um, subtle nuance is what's the correct notion of definable set in continuous logic. Just to be clear, when I'm talking about definable set right now, I'm talking about the viewpoint of definable sets as like a functor on models of a theory. You give me a theory, sorry, you give me a model of the theory, I'm gonna tell you the definable set corresponding to that model, right? Just um. Right, just um, right, so really like zero definable sets. There's a version relative to a structure, but I just want to talk about this version for a second. So, if you thought about it for two seconds, here's what you might think. You might think, well, Isaac just told me if I know when a formula evaluates to zero, I know everything there is to know about everything. So why don't I just do this? You give me a formula. And Isaac just told me formulae aren't really the right thing to talk about. Aren't really the right thing to talk about. T formulae are the completion of the collection of formulae. So, why don't I just take one of those things and ask on a given model when is it zero? Take the zero set of that thing. That should be a definable set. Then, if I want to know when it's a half, you know, that's morally equivalent, and that would also be a definable set. While it is true, definable sets are of that. Definable sets are of that form, that's actually not the right notion of definable set in general. It's not robust enough. In particular, you might want to do the following thing. Take a T formula and quantify over such a thing and quantify some of the variables over such a thing. The remaining object, whatever it is, and whatever. Object, whatever it is, and whatever free variables you didn't quantify over, you would want that to be a T formula again. Naively speaking, if you quantify over a definable set, the resulting thing should be a formula again. In classical logic, you don't even think twice about this. This is obvious. But it turns out with what I just wrote down here, that's not going to be true in general. So let me just mention the main theorem, which ties all this together. Which ties all this together. So look at bullet one. This is like what you would want to be true for a definable set. If you have a t formula in some variables x and y, you would want to be able to soup over inf over the x variables ranging not over the entire structure, but just over your divinable set. And you would want that thing to be a t formula again, one of these completions of naive formula. Completions of naive formula. So that is what we would call a definable set, and it's equivalent to measuring the distance to X, to that being a T formula. For time purposes, we can ignore item three. If we have some time in the discussion, we can go back to it. But item four really should strike you as, oh, yeah, that's obviously something I would want. It commutes with ultra products. It commutes with ultra products. If I ask what does the definable set associate to an ultra product, it should be the ultra product of what the definable set associates to the individual structures. That's equivalent. So these are what you call the t-definable sets. And what's so this notion existed, and classical logicians scoffed at it and said, you guys are crazy. This is way too complicated. You guys are crazy. This is way too complicated. This can't be the right notion of definable say. But our lessons have taught us in talking with mainstream mathematicians like operator algebraists, for example, they do this on a daily basis and get paid for it. We're clearly on the right track in terms of the notion of definable set. And importantly, for the computable structure theory aspects that have come up in connection with operator algebras. Roz, being very careful and paying attention to how this works in an effective situation, came up to pay huge dividends in a way that I could talk about at some point during the week. In any event, this is the right thing. And yeah. Okay, so right. Let me just say a couple more quick things and then I'll stop. Quick things and then I'll stop. So, type spaces: what's the type of a tuple? It's just you should record all the values of the functions, excuse me, of the formulae when you plug in that tuple. Okay, so what's the type of A? It's a function. You give me a formula. I record the value of the formula. I get a space of complete types with a natural logic topology, right? You give me a formula and you give me an open interval. And you give me an open interval, I look at all the types which the formula according to that type lands in that interval. Okay, so logic topology, it's compact Hausdorff, like in classical logic, not totally disconnected anymore, like in classical logic. Okay. And then you could ask yourself, cool, we've got a compact Hausdorff space, Sx of T. What are the continuous real-valued functions? Turns out to be precise. Turns out to be precisely those of the form you give me a type and I plug in a T formula. So that's another reason why T formulas are the right thing to do. They correspond precisely to the continuous functions on the type space. New to continuous logic, however, is a second topology on the type space. If you have the logic Type space. You have the logic topology just like in classical logic, but we also have a metric now on the type space. You give me two types. What should their distance be? The infimum of the distances of their realization over models of the theory. How close can realizations of these types get? It's a little bit of an exercise to check that's actually a metric. The triangle inequality takes a little bit of thought, but it is. And it's easy to see that every logically open set is also metrically open. So you might wonder, maybe these are actually the same topology? No. When they are the same topology is precisely when your theory is separably categorical, a unique separable model. But this is telling us something new to continuous logic: that I have these two different topologies. That I have these two different topologies on the type space. And when they get along well with each other, in the extreme circumstance that they're actually the same topology, has model theoretic content, namely that the theory is separately categorical. One other instance of that is the last thing I'll say, namely, what does the omitting types theorem look like in continuous logic? What's the correct notion of a principal type? Based on classical logic, Based on classical logic, what is a classical logic principle type? It's a type that's actually a definable set, right? The set of realizations of the type, which is a priori type definable, is actually definable. If you make the same definition in classical continuous logic, provided you have the correct, as we do, notion of definable set, then you get the notion of a principal type. And now a type is principal. A type is principle precisely when the two topologies I just introduced agree, not globally, but on that type, around that type. And using that, actually using this slightly weaker thing I wrote down in item three, that every metric ball around P has non-empty logic interior, you can prove the omitting types theorem. That if I have a non-principal type, then there's a model omitting that type. You do a Hank and Type. You do a Hank and construction. And rather than trying to say none of the new constants realize the type, you have to say all these constants stay appreciably away from realizing the type. And that's exactly what item three would allow you to do. I don't expect you to absorb all these nuances. I just wanted you to be aware: classical logic and continuous logic are very similar in many respects, but they're all. Very similar in many respects, but there are also important differences that, you know, when you really get working in it, you have to understand. And I just wanted to give you a glimpse of some of them. Let me end with just some references. So the go-to reference is the classical, I guess by now it's classical, four-author paper by Benyakov, Bernstein, Henson, and Usfyatsov. The Benyakov-Peterson paper is I list here in case you want to see how the proof system works. If you want to know more about If you want to know more about definability in continuous logic, my article that I put up here. And then we just wrapped up a volume on the model theory of operator algebras. And Brad Hart did a tremendous job writing an introduction to continuous model theory in that book. He posted it to the archive about a week or two ago. And so what I just talked about now appears with a lot more details in this survey article. In this survey article. So, if you're really looking for like a first place to learn all this stuff, I would go there personally. I appreciate your attention and I look forward to answering your questions. Thank you. Thank you, Isaac. I'll also say that Brad Hart, I don't know if you're looking at the participant list, but Brad Hart was here today. The participant list, but Brad Hart was here to hear your compliments about his article. So, wonderfully done. Questions? So, on page 23, the definition of a definable set. If you have it in more than one variable, the condition two, does it matter what metric you put on the product space? I usually mean here the max metric. The max metric, but I don't think it's super important. Well, it shouldn't matter as long as the it's not stupid. It shouldn't, right? Like as long as it's definable, as long as your metric on tuples is actually definable in terms of the structure, it'll be equivalent. Because here it doesn't matter at all, right? When you're talking about like functions and predicates. No, it doesn't matter there. Like here it could matter as long as it has to be like reasonable. But no, no, it does, it doesn't, because you have the quantification, right? So if you can quantify over the definable set, whatever metric you define. Set, whatever metric you define, you can say what is the ephemer of the distance with regards to my DOM product metric metric on the set and network games. So they are equipped. That sense. I didn't mean to say you're not. And I should say yes, go ahead. Oh, I have a soapbox question. Box question. So you have three notions of formula. You didn't give one of them a name, right? But you have like formulas, you have your countable debt subset, which are often called restricted formulas, and then you have your t formulas. What is the point of maintaining all three of those notions when you really only ever use restricted formulas or t formulas in practice? Well, okay. So if you're going to If you're going to, like, for example, it's okay. So it's good to know that if force, I could express whatever I wanted to as a limit of restricted formula. But in practice, if you had to write all formulae down that way, right, you would drive yourself nuts, right? So it's good to know I could just plug this thing to this continuous function. Great. Do that. Now, if there comes a moment where like, oh man, now I have to put my. Like, oh man, now I have to put my computable structure theorist hat on and worry about how I effectively approximate this formula by restricted formulae. Then I have to go back and be careful and pay attention to restricted formulae. But if I'm just plain model theorist without any effective considerations hat on, then I would rather just work with arbitrary continuous connectives. And if I happen to be Um, if I happen to be working relative to some ambient theory, okay, then I want to be able to take limits of formulae and not and not worry. Um, so it really depends on what you're up to. Right, but there's really like nothing particularly special about the class of formulas defined sort of in the standard way where you where you allow arbitrary continuous connectives but don't don't allow limits. Like, it's not a semantically meaningful class in the sense. Meaningful class in the same way, say, existential formulas are. Well, I mean, but the notion of a T formula depends on a T, right? So it's not intrinsic to the language. Every T formula is equivalent to a T formula where T is the empty theory. You can do that with forces. No, no, no, no, no, no. But. No, no, no, no, no, no, no. But you, the notion of T equivalence very much depends on the ambient theory. Yes, that's true. But I'm saying that you don't get any new expressive power by talking about T formulas for a specific complete T as opposed to talking about T formulas for the empty theory. Every T formula Every T formula is T, or sorry, for every T formula, there's an empty T formula, which is T equivalent to it. That is true. But it's T equivalent. It's important that it's all relative to T. Yeah. So you can't just say, I'm only ever going to think about empty set formula. Empty set formula, right? Because that's while that's true that I could build every formula that way, whether or not two such things are equivalent very much depends on the ambient T. That's true in classical logic too, right? Like equivalence over a theory. Yeah, but you don't have to do any completing in classical logic, right? I mean, it's just. Sure, sure. Yeah. Okay. That's enough of my soapbox question. Thank you. And meeting? Yeah. So I'll move on. What can you say about the analog of PA degrees for this life? Sorry, I didn't hear the analog of what? PA degrees. So Turing degrees which can compute a consistent completion of any personally accidentized degree. But uh that that That I don't know. Oh, I think it should be the same. Maybe we'll leave that for a bit of informal discussion. At this point, we've got five minutes until the next talk. I realize we put them back to back, but I also realize that there's tea and coffee back there. So I propose that discussion continue relatively informally. Discussion continues relatively informally before we have Wesley's talk, and that we thank Isaac again.