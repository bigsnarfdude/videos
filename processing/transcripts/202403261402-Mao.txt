I'm very happy to join this event and share our work here. The title of my talk is Model Education for Risk Evaluation and Robust Optimization. This is based on joint work with Guo Tu, who is here, and Jiu, who is currently in the University of Waterloo. Here's the outline of my talk today. In the first section, I will briefly introduce why we Why we propose this motivation approach. Actually, it's a group of who came up with its idea and we studied its properties in risk evaluation and robust optimization. And in the second sector, we'll apply our method in the two common authentic set and sets. Yeah, and in the third section, I would like to study when our new method Our new method coincides with the classic methods. And at last, we'll run some numerical data of our method in Total Select. This is a very basic background of risk management. I think many of you are very familiar with this. I just briefly think about it. In the risk management, a very ideal situation is that we already know, we fully know the distribution of the risk. Distribution of the risk that exists. So the process of the risk evaluation is very straightforward. We just choose a proper risk measure to quantify and make decisions based on the specific risk model. But actually, in reality, due to various reasons such as insufficient data, the limitation of the model, we do not know the actual scripture of the risk that we face. So a very common A very common method is to use a thin setF to encompass all possible distributions based on the information we have. And we calculate the largest possible value of the risk measure within this uncertain set. This method is called the worst case risk approach. Based on this worst case risk approach, modeling Modeling paradigm is called the distributionally robust optimization. That is, we find the optimal decision A that minimizes the worst case risk measure. This A is the capital A, the set of all admissible actions. The X is the risk that you face up with. And our distribution is although we only know its distribution belongs to a certain set fx. The small f is a loss function. Small f is a noise function. If we use this F A F to represent the set of observations of F AX based on the set F, then we can write the inner supreme problem as this first case approach, based on this one. This F is the discrete set, the onset set. A very simple example of this loss function is the following selection. The loss function is a linear combination of x. Linear combination of X, A transport X. This kind of distributional robust optimization has been widely studied in the literature here in some papers, well-known papers, which study this kind of problems. For this kind of worst case risk approach, we are giving, we start by giving our author set F and a risk measure. Then we calculate the worst case risk measure within. risk measure within this thin set. F, a different F that attend this premium is called a worst case distribution. Generally, different risk measures result in different worst case distributions. So our idea is can we get a causal distribution from this autonom set directly? We call this approach as model. We call this approach as model aggregation approach. That is, we want to find a distribution, you can call it a verse-to-strip distribution directly from the often set. It's a risk measure independent model. Such model can be used for generic risk measure, not specific ones. And it can be also used for simulation analysis and calibration. So this is our. So this is our main idea. For this idea, a natural question arises: how do we define the consumptive distribution from this alternative direction? And what are the theoretical and computational properties of our method in the distribute robust optimization? And what kind of risk measure equivalent to risk evaluation for the two methods. Methods. And how is our method implemented in common settings about certainty, optimization, and real data applications? Here there are four questions that this work aims to address. Okay, before I move on, I would like to introduce the two very popular risk measures that I see. You are very familiar with this. The first is value risk that is defined as the path or quantile of the distribution R. Alpha quantile of the distribution half. The second expect shortfall is defined as the average above the alpha quantile. This is slightly different from the yesterday's talk because we treat the random variable as risk instead of profits as well. Okay, now how to define our first phase distribution based on the authentic set? We employ the idea of We employ the idea of super set. So we need an outer set. The M1 represents the set of all finite mean distributions. Of course, you can use any other distribution space. We choose for state hour result. We need a partial order on this set. We call the distribution G dominates the Distribution G dominates the set F if G dominates all distributions in this set F. We define the supreme of the set F as the smallest distribution in this set F, F1, that dominates the set F. So we have our the supreme distribution dominates all distributions in the set F and is dominated by any distribution G that dominates the set F. G that dominates that F1. But sometimes such G does not exist. That is, we couldn't find a discrete G in the F1. Don't exist at this set F. In such case, we call this set is unbounded. If such G, this, we call the G F is bounded from above with respect to this patch order. We use this notation to denote it. We will use this supreme distribution to represent the consumption distribution. You can see the supreme distribution relies heavily on the patch order. So, in this paper, we focus on the two most important orders of risk: that are first order through caste redundance and second order through cast redundance. I think you are very familiar with. You are very familiar with these notations. I want to say for the second order to custom dominance, we use increasing complex order instead of the increasing pumpkin model. The reason is, as mentioned before, we treat the random arrival as risk instead of profit. We have the well-known characterizations of those two orders. That is, G, dominance F in the FSD, the first order spectrum, is equivalent to. The second is equivalent to the valor risk of F is no larger than that of G. It's also equivalent to the distribution G is no less than the distribution F. For the second order, stochastic order sticker, it is equivalent to the expectation for F is no larger than F. And it's also equivalent to the pi function. The pi function of F is no larger than pi function of G. The pi function is defined as the expectation of the stop loss. Expectation of the stop loss. As you are familiar with this equivalent calculation of two orders, I want to mention that this is, there is a one-to-one correspondence between the distribution and its pi function. If we're giving a pi function, you can use this function to calculate its distribution. This is the right derivative of this function. Or based on the equivalent correlation, we can easily get the following result. That is, for the first order, That is, for the first order, the supreme function of the set is the informal of the distribution within the set. This is an equivalent result that is the quantile function of the supreme function in the supreme of the quantile function of the distribution with the set. I'm just trying to interpret this. This is saying that you find a function g, which is the or the circuit. Or the supremum f, and then that function g is defined point-wise by the implement over CDFs evaluated at x. What does this equality mean? Because the thing on the right. Is it function? I understand that, but that. Like, ooh, infomerial numbers, and f is a function. So this is point-wise. So this is a distributed function. This is also a distributed function. Before any x, this is the value. any x, this is the value of the distribution at the at the point x is equivalent to the this inform of all the distribution at x. Okay, sure. That's the point yes. For the second order path domains, we have the baseline parameterization in terms of the pi function. We have the pi function of the supreme function equivalent to the supreme pi function of the three leaded set. So based on the one-two-word part. So, based on the one-two-weight corresponding between the pi function and the function, we can get the supreme function is given by this formula. Okay. By examining those two formulas, you can find that if we replace this set F by its compact form, the two formulas keep the unchanged, right? So we have the following that is for the two surface orders. We have the superior of the function. The supreme of the function is equivalent to the supreme of its convex form. So, with this reduction, we find if our sensor is not complex, that will look extra difficult. This is our conserved distribution. Okay, based on our super distribution, we can define our MA risk measure. That is, for giving a risk. That is, for giving a risk measure role, we define this MA risk measure as the risk measure of this superimposition. Here we omit the stochastic orders. Compare it with the first case risk measure, you can find that we just put this up, but this is not strictly. You can, loosely speaking, we put this supreme operator inside the blankets. Blankets. But this is also a supreme operator. Generally speaking, for risk measure, if it is consistent with this stochastic order, we have our MA risk measure is no less than this must be. If we apply it for specific risk measure, say for better risk and first order supposed orders, we have those two risk measures coincide. Next, we consider the expectation flow and second-order capacity. Based on this well-known representation of the expected short flow given by Rosalind and Progressive, we can rewrite the worst case expected short flow at the supreme minimum optimization at this. And we also can write our MA risk expectation also at this mean supreme minimum. Those two, the object function. Those two the object functions are the same one, but the order of the operators are exchanged. So it's easy to say this one is always no less than this one. And if we have some actual conditions of mean-max theorem, we have those two risk measure constantly. Because you can, if you observe this object function, you'll see this object function is in your. See, this object function is linear in distribution f and convex in the variable x. So if you also assume that the f is a convex set, you will have the two risk measure. Small question on motivation. Are you interested in the worst case distribution in its own sake or just because it makes it easier? Uh uh our our first motivation is we would like to find the worst case distribution. Maybe the decision maker would like to find the model. But uh after we got their idea we would like to show our method have better capability than the worst case approach. So then we apply those two methods to robots optimization. Then we got two robust optimization problems. First is the classical one that Problems. First is the classical one that we find optimum decision A that minimize the both case risk measure. The second is our method. We find optimal A that minimize the MA risk measure. Based on the result we obtained, we think maybe WR method, the worst case method, encounter two difficulties. The first is you need to repeatedly compute the risk measure, the better risk measure for every The better risk measure for every decision vector A and every random variable X. And sometimes non-convexity of the alpha set may cause problems. But for our MA method, it is more mathematically tractable because rho is only computed once for the superdistribution. And the convexity is not a problem for the position to be, should be followed. We would like to see to compare the probability between the two methods. So consider why are you distinguishing A and F? A and F? Why are we considering this whole thing as one family of distributions? You mean the F A F? Yeah, why are you parametrizing with A? Because we need to find an optimal decision A that minimizes. Decision. Yeah, you've got a decision. I'm slightly confused because for different A's, is it not possible that I would have a different worst-case distribution? You mean for worst-case for whatever matter? Yes. Yeah, so why are we saying that row is only computed once? I don't understand what one. Yeah, for every decision vector, A is only computed once. But for this one, it's for every decision F, you need to. It is for every decision. It is for every distinction A and every random variable you need to compute the node, but this is only computed for once for every distribution vector, A. Yes, you're right. For different A, this set is different. But super function is only one. We don't need to calculate this for every x. Maybe I'm just okay. You're right. I might just be confused about what x is. x is the x in this plus object. You have the q. But it's okay. I think maybe we should just call it square q terms. Compare the tragedy of the two methods. We consider the coherence measure. This is based on the representation of a long environment coherence. Law environment coherence measure. Every coherent law environment coherence measure can be written at this point with a supreme of a witty spectral. So we take the discrete form. For this kind of risk measure, we can rewrite our MA robust observation as the follow at this problem. If the f function, loss function, is convex in the random variable A, then we have this convex problem. Anyway, it's not easy to find to see the tractability of this problem. So we consider the finite authentic set. Our MA risk robust optimization can be written at this problem. First case optimization can be also written at similar problem. You can compare those two problems. Those two problems we have there. The difference between those problems is that the w the xi x variable are different for the different distribution fi. But for our MA method, the x is the same for different fi. So the clarification. So in the first program, you're telling us how to get this program gives us the worst case distribution. And where do I see it? And where do I see it? Next, we consider the specific option set will give the robust distribution. But for this one, we would like to examine the tragedy of the two methods. So that is why later in the next section, I will consider that. For the compare the two. Compare the two problems we have, the number of our MA method is much less than this one because the Xi is a different for Fi, but Xi is the same for Fi. We draw some numerical, this is easy to, it is more easy to see if we take the risk measure X like Showfor. For the variable X, we only have. Variable X, we only have one variable X, but for the WR optimization, we have Xi. Xi is different for the different alpha alpha. We next run some numericals to compare the probability of the two methods. This is the setup of the parameters. We have the loss function that is the vendor loss function. And the file we take it as a normal distribution. We compare the We compare the computing time of the two methods. The orange line represents for the computing time of the W method and the blue line represents for the computing time of our ME method. You can see from the four fingers that as the parameter increases, that is, the model becomes much more complicated. Our method, ME method, Our method, ME method, is visible faster. For the last one, maybe you think the two methods share the same comparable computing time. That is because at the n, the parameter n is the sample size drawn from the distribution fi. At the n increases, the difference between the two models does not come back. So what is N you mean the W? N superscript W. You mean the N? Yeah. Yeah, N is the, okay, you can see from this one. The representation of the weighted number of the spike. It's a discretization. Okay, as the NW increases, the risk measure become much. The risk measure becomes much more complicated. Okay. Basically speaking, the tractability of our method is a little better than the classical worst case risk approacher. Next, we apply our method to the two common authentic steps, which will show the worst case approach. First, we consider the moment of 30 steps. We consider the moment authentic set. That is, this is a one-dimensional authentic set. This means given by me a very sigma square. I think you are very familiar with this. With this onset, we use the F1 to represent the supreme of the distribution with respect to the FST, the first-order stroke customers. This represents the supreme of this set with respect to the second-order stroke dominance. We can show the supreme function and have the explicit law. Have the explicit form. This is what you want about the worst case distribution. They have a closed form of function. And the many risk measures as the explicit formula of the MA risk measure. We consider those two, or those three risk measures, the range bar, that is defined as the average between the other and beta functional functions. The mean variance set, we figure. You fix and the variance, and you get all the possible distributions with those means and the variance. That's what it means. This is the worst-case distribution with our MA method. Yes. We consider those three risk measures. The first, red bar, the second, power-distorted risk measure, is a distortion risk measure with the power distortion function. The third is the expectile, it's defined as the unique solution to this equation. This equation, we consider those two risk measures. We can give explicit, I mean, risk measure of those risk measures. Which is to say, our method have their explicit format. We consider the high-dimensional authentic set. This is the mean covariance of the set. In those cases that you list quality and the risk of the worst case distribution. High-dimensional mean covariance authentic set based on this, the well-known projection result of the mean covariance authentic set, we have our MA method can be, optimalization can be written at this point, this problem. For this set, this is one-dimensional mean variance model of the set. So we have this closed form of this solution. If additionally, we assume the row satisfies the constitution. Assume the row satisfies the translation variance and for political homogeneous, we can rewrite our MA robust optimization problem as a second-order conic program within the quiet. This is a mean variance authentic set. For the Watson system authentic, we have similar results. This is a one-dimensional Watson system box. You are very familiar with definition. I skipped over this. This is what Watson sent. This is what was the relevance epsilon. F0 is the relevant distribution. We use F1 to represent the super distribution of this ball with respect to FSD. This F2 represents the super distribution of this ball with respect to second-order stochastic dominance. We have this result. That is, for the first Dochasidomness, we have the quantum function of the Have the quantum function of the supreme can be the unique solution to this equation. For the second-order stochastic, we have the quantile function of the supreme function is the quantile function of the reference distribution plus the Pareto quantile function. So we have the closed form of this distribution, super distributions for this univariate. For this univariate Coissant or Watson sample. For high-dimensional Watson sample, we have a similar result. That is, this is the pure sensor distance definition, and this is the, you can say this is the high-dimensional sensor ball. We use this set to denote all the distributions of linear combinations where the distribution. Where the distribution of F of C belongs to this high-dimensional Watson step. Okay, okay. We show this set is for univariate Watson step. This point is because with this result, we can give the explicit form of the worst-case distribution in this set. So without the result, we can rewrite our MA risk problem as follows. I may risk the problem as follows. I think because we gave over this result, for this third part, I would like to see this quite interesting reduction. So I want to let you know that is where, as mentioned before, we see if the row is consistent with this stochastic orders, we can show that our MA risk measure is no less than the worst case risk measure. So we are interested in when those two Interesting, when do those two risk measures coincide? We call these properties as EMA, the C represent for the convex set with assumed set convex. Or we have the with some additional standard properties of risk measures we can show satisfy this property equivalent to the row is viral risk. And for the second order, we show the is equivalent to row as we expect shortfall. has been selected also. This topic is very interesting in economics. So here are some papers which study the prioritization of those two risk measures. This is for general thin set. I should like to skip over these properties. The last part is our empirical result. We draw some numerical results to compare the the performance of two methods in all results in all examples. All examples, I think the MA approach and the worst case approach share their comparable performance with our MA approach slightly better than the worst case approach. We present some annual return, quality, as job ratio, and then transaction cost. Although that shows that our share this comparable performance with Westcase. Comparable performance with post-cases approach. But the metric of method is that our method has a better ability than the post-case approach. This is the conclusion that I think I can skip those methods. Thank you very much. One more question. So from your results, does it mean that if you take a more general Kusuka risk measure, if it's not expected shortfall, you won't have equality between the worst case risk and the risk of the worst case distribution. Yes, that is what it is, which all says only just one of the cases that is called. Otherwise, you always can find this that usually varies from one. And you know how conservative it can be? I mean that you know what makes it more conservative and the weights of the score coverage comes in? I don't think there's a clearised to higher order stochastic uh ordering? High-order stochastic ordering? Like third stochastic orders? Yeah, first order, second order, third order. Yeah, for just is there a way to generalize it? I think you can generalize those orders to any patch orders. Yes, you can, but we do not study in this paper. That for future maybe. Probably we need to a third order this matter, the server that there's a step forward with a step or this matter, but it's not a third others. I don't give us a third order risk matter, which we didn't quite prove. I think it's for prompt. By what you mean the one with the there's equality between yes, but the curve worst case risk and this for the worst case. Yes. These two they line up quite well. They line up very well. The store order startup will be here very soon. Okay, thank you very much. Third talk of the session is Johannes Bies.