Thank you so much for the kind introduction. It's been a really great workshop, and I think my talk fits in well with the earlier one this morning. So, before I start, just to acknowledge all the great people I've had a chance to work with during my time in academia. So, I just want to start out. I'm going to talk mostly about electronic health records. And although this is a conference about genetics and deep learning, you'll see that I'll be talking about generally simple statistical models and focusing more on analyzing. Models and focusing more on analyzing the electronic health record data. So, just so we're on the same page, an EHR or electronic health record is exactly what it sounds like: an electronic record of a patient's interactions with the healthcare system. So, as a patient comes in and out of the hospital or primary care provider over time, various information is recorded about them, such as diagnosis codes, why they were there, labs, procedures, medications, and a lot of rich information in clinical notes. And so, something we always need to keep in mind when we analyze EHR data is that it's a byproduct of clinical care. So, it's not necessarily designed for whatever purpose we want to use it for. And so, from a more positive angle, this is really a blessing to us. I think we've had a taste of this in our introduction to biobank data this morning. But the data itself is very extensive. We have longitudinal records on large populations. Like I just showed you, there's numerous fields measured at each encounter. Measured at each encounter. These are real-world patients, so reflective of people actually receiving treatment and care. And luckily, we have increasing EHR adoption worldwide. And so, the ultimate dream is that we can actually learn something from all this data that's just generated as a byproduct of you and I going to the hospital. And so, this was a recent article in the Wall Street Journal about this sort of using this data, this treasure trove of information to actually reach this goal. Information to actually reach this goal of learning something from the data. And so I think particularly relevant to this group is this idea of linking this rich phenotypic data in the EHR to biobanks, which we touched on this morning, that contain recycled biospecimens that we can genotype and use in all sorts of genetic research. So we've already touched on the UK Biobank, there's also the Million Veterans Program, which is a little bit different than a population biobank. It's actually generated from hospital data, as well as Hospital data, as well as the Mass General Brigham Biobank, which is the data I'll show you today. Mass General Brigham is a large hospital system in Massachusetts. It's flagship by the Mass General Hospital and it contains electronic health records on over 4 million patients with a biobank of 100,000. And so there's a lot of opportunities for these EHR-linked biobanks, which I think we've already seen this morning, so I won't spend too much time here. But essentially, we can have timely large-scale. But essentially, we can have timely, large-scale, cost-effective research. So, when these biobanks first started coming out about 10 years ago, there were a lot of replication studies. And this is an example of one that was done looking at known alleles related to rheumatoid arthritis, where they were able to essentially replicate the study, but in a fraction of the time, fraction of the cost, as well as replicate it for an African-American and Native American cohort. Another advantage of these studies is that they can actually enable research that. Studies is that they can actually enable research that couldn't be done otherwise. So these EHRs contain a lot of rich phenotypic information. So we can do studies looking at multiple phenotypes and a single SNP, such as this one we did a few years ago, which is a phenome-wide association study or PHEWAS, essentially the reverse of a GWAS. But while this is a very positive thing that we have all this data at our fingertips, there is a negative. This is maybe not a curse, but a challenge. Not a curse, but a challenge is that the data is very, very messy. It's quintessential observational data, and we have to do a lot of work to make it useful. And so, this isn't a surprise to anyone after the morning talks, but the phenotype information, which is half of this bit we need to actually leverage electronic health records for genetic research, aren't readily available in EHR. And so, when I say phenotype, I mean something like disease status. So, yes, no, a patient has rheumatoid arthritis, for example. Patient has rheumatoid arthritis, for example, which I'll focus on today, but they can be more complex phenotypes, such as someone responding to treatment or the time of disease onset. And so we can actually understand why phenotype information is so hard to get it, at least at a high level, just by understanding what the data looks like in an electronic health record. And so there's two primary forms of EHR data. The first is structured data, which has the advantage of being easy to abstract. This is just data that's To abstract. This is just data that's stored in a relational database, but it lacks a lot of context. So these are things like billing codes, medications, things that we can get out relatively easily. However, they lack a lot of context. So in particular, billing codes are known to be highly erroneous. So for instance, they're often tied to reimbursement. So we can see things like upcoding. Patients will be billed for something they don't actually have for the purposes of reimbursement. Diseases themselves can be hard to diagnose. So someone can be miscoded for a substantial period of time. Miscoded for a substantial period of time while they're getting the correct diagnosis. And even the systems that are used for coding shift over time. So, about five years ago, the International Classification for Disease or ICD-9 billing codes shifted from version 9 to version 10, and about 40,000 codes were added. So there's all sorts of challenges working with these data. Where a really rich piece of information is actually the unstructured data, primarily contained in clinical notes or pre-text. Contained in clinical notes or free text. It's very rich, it'll contain things that aren't in structured data. So, the insights a physician has about a patient, such as signs or symptoms that won't be available in structured data. However, it requires natural language processing or NLP, so harder to work with. A common way to analyze the data is to take clinical terms, so ways that physicians write different diseases, map them to what's called a concept unique identifier. So, the National Library of Medicine has essentially created Library of Medicine has essentially created a dictionary of concepts that contain synonyms for the different ways that physicians can write different diseases and then use those in all of our analyses, at least at a simple level. And so as you might imagine, a phenotype is really some aggregation of the structured and unstructured data. And there's been a lot of work in recent years. This is from the eMERGE Network, which is a large system of institutions in the US that have been working on linking EHR and bio-based. That have been working on linking EHR and Biobank for over a decade now. They've created over 80 algorithms to try to help people get out phenotype accurately and quickly. However, most of these algorithms are actually rule-based. So they're clinical decision rules you can think about. Physicians actually sit in a room, think about how they would actually code a disease, how they would actually write it in text. So these are very time-consuming to generate while they're simple to use. They can be highly accurate, but you can imagine they're just. Be highly accurate, but you can imagine they're just not scalable. So that brings us to what you might expect. We want to aggregate lots of information in the health record. Machine learning or ML might be a natural way to do this. And so this has actually been done for the last decade or so. And the basically recipe to do this is quite simple. We need to select relevant features, which I'll denote by Z, manually review some records. So we have our gold standard to train a supervised learning model, and I'll denote the phenotype step. And I'll denote the phenotype status, which I'll take as binary, so yes, no disease as y. And then we build a model. We can project that to the database and figure out who and who does not have the disease. And so this has been done for a long time. When I first started working on this about 10 years ago, there was an initial algorithm for rheumatoid arthritis, which is an autoimmune disease, and a simple machine learning model worked. So 500 records were randomly sampled from our EHR database, and it had an AUC of 0.95. And it had an AUC of 0.95. So, this was actually sort of an example where the statistician's dream, a simple model, worked. Logistic regression was a hero here, which maybe is disappointing in this conference, but there is a catch, of course. I've sort of skirted over how do we get the features? How do we get the label? And so I vividly remember reading this paper that my supervisor gave me when I first started working on this. And she told me that this project took 20 MD PhDs in two years for one disease. In two years for one disease. And so, given how the attraction of using EHR and Biobank, scalability being fast, this really goes against that. And so, what I've been thinking about for the last few years is how can we actually make phenotyping more efficient? And so, the ideas are very simple. I just want to leverage all the data I have at my fingertips. That's the underlying idea. And so, what I've just told you is that we took a small subset of labeled records, 500, to build our model. You can imagine. To build our model. You can imagine many, many records don't undergo this chart review. There's information in that data, which I'm going to call unlabeled data. And so, all of the work that I've been doing is trying to make use of both the labeled and unlabeled data. And so, I've thought about this in two different ways. The first is actually using the unlabeled data for model estimation. And so, today I'll talk about estimating a regression model. And the second is actually evaluating prediction performance evaluation. Performance evaluation. So, getting tighter confidence controls for those ROC curves, although sometimes we're missing them in some of our papers. And so, I won't talk about this work today, but here are two references of some of our work developing semi-supervised methods for ROC parameters. So, back to model estimation. So my goal again, I want to build an accurate prediction model for Y. So, let's let this be a binary phenotype with as little human information as possible. And so, as I mentioned previously, my two sources. Mentioned previously, my two sources of human input are these identification of the features and the labeling of the data. And so, just to briefly comment on the feature selection bit, this is a work of my colleague. So, instead of asking physicians to sit down, manually think about what codes, what medications, what terms do they use, the idea is to apply natural language processing to existing knowledge sources like Wikipedia, Merck, et cetera, and identify concepts that are commonly mentioned in relation. Concepts that are commonly mentioned in relation to given phenotypes so that we can obtain some starting point. As a statistician, my general belief is that if there's information that exists, I should use it. I don't want to throw everything into my model. This is one way that we tried. It actually works quite well. And so it takes burden off of our experts. We're getting towards this idea of automation, but we obviously result in some noise, which places more burden on this idea of labeling. The more features I have, the more. Labeling. The more features I have, the more labeled data I need. And so, what we noticed in doing a lot of our work is that the data itself, while it's messy, while it's noisy, it's complex, there is inherent structure in it. And some of the features that already exist in the data that can be automatically extracted are more predictive of a phenotype than others. So, for instance, this can be things like diagnosis codes for the disease, mentions positively that the patient has the disease in their record. In their record, and that are more predictive than, say, features that can help you discriminate, like signs and symptoms, but might be used for other diseases. So, our general thinking is that let's leverage this information. Let's make use of the unlabeled data through these additional features, which I'm denoting by S, and I'll call them surrogates or silver standard labels. And so, there's a lot of related work in this area, particularly, we heard about this at the end of yesterday in the context of week. Yesterday, in the context of weekly supervised learning. So, within electronic health records research, there's been a number of methods, all with nice acronyms. As statisticians, we don't usually have nice acronyms, but EXPRESS, Anchor, these are all different methods. And the basic idea is to take this surrogate, use it in place of the gold standard because it's easier to get to predict my why. I don't have to do label data. And in fact, empirically, these methods do work quite well. They allow often lack theory. They often lack theory. Why do they work? What condition do I need about my surrogate? When will I expect them to fail? Additionally, by using the surrogate as the outcome, I'm not using it in my final prediction, only by means of using it as a noisy label. So when I have some unlabeled data, I'd ideally like to use that bit in my final prediction for y. And so these were the two particular questions I was interested in looking at. As an annoying statistician, I want to know why. I'm not satisfied if the method just I'm not satisfied if the method just works well on some data sets. So I set this up as just a general semi-supervised learning problem where I have some labeled data of size n. I have my outcome Y. I'm denoting by Z the features, which include both the surrogate S and the remaining features X. And then I have unlabeled data, which has not undergone chart review. And I'm going to assume here just a logistic regression model that's parameterized by an intercept alpha zero, a regression vector beta zero. Regression vector beta zero, and the coefficient for the surrogate gamma zero. And the key assumption here is that my labeling was done randomly. This is an assumption we've weakened recently, but for the purposes of this talk, we'll assume this, and also that the unlabeled data is much, much larger than the labeled data. And so, when we were thinking about this problem, we actually found a really interesting result is that when we have this logistic regression model, if we make an assumption about the surrogate, that is. If we make an assumption about the surrogate that it only relates to the covariates through the outcome, and that my surrogate isn't terrible, it's slightly related to my outcome, then actually the conditional distribution of S given the covariates follows a single index model in that linear predictor beta zero transpose x. So why is this actually important? So there's a clever result back from 1989 that says when this is the case, when I have a single index model with just some linear predictor beta zero transpose x, I can actually fit Transpose x, I can actually fit at least squares to recover that up to a constant of proportionality, which I'm going to denote by rho. So it's a nice result. The key assumption here is this bottom one, which is essentially an assumption about the behavior of the distribution of x. One, this is satisfied when the distribution is elliptically symmetric, for instance, in the case of a multivariate normal. And so, what does this actually mean, translating this into why is this important for the purposes of phenotyping? Purposes of phenotyping is that actually by simply regressing this surrogate on X, I can get a good score in the sense that I've recovered the weights or their relative contribution to how they predict Y just using the surrogate, right? That's what we're often doing in machine learning, right? Getting the weights of how our features contribute to the outcome. Here I do so, what I lose, because I only have unlabeled data is the constant. So how do I get that back? The idea is that we can rewrite our model, replace the beta zero with that constant of proportional. The beta zero with that constant of proportionality, the regression parameter vector that I recover from that surrogate regression, and it leads me to this following procedure. I know I can estimate that phi zero with the unlabeled data completely, regress the surrogate on the covariates. Then with the labeled data, I can actually recover the intercept, constant of proportionality, and the regression parameter vector for s. And so this led to this two-step semi-supervised learning procedure. Two-step semi-supervised learning procedure where we do precisely this. First step, fit a penalize or unpenalize the unlabeled data is much bigger, so your choice to get estimates of that regression vector phi. And then we take that linear predictor, we refrit as a covariate in a logistic regression together with s. Then we can take our prediction as just the prediction from this model, the second step right here. And so the key idea. And so, the key idea or why this works is that what I've done is I've taken potentially high-dimensional feature vector that I have collected based on all the features related to my outcome, and I'm now pushing the estimation of that to the unlabeled data, which is much, much larger. I only have to estimate three parameters with this labeled data set. It's a much easier task, and ideally, I would need much less labeled data to do it. So, that's just a high-level idea of what our thinking was here. And so, I apologize. And so I apologize that these results aren't the most polished, but I really wanted to use this data set today, which it's one of the few publicly available EHR data sets. It's been released from the Mass General Brigham. So there's the mimic data set if you're familiar, but this is a newer one. It's a bit more processed. It's actually available in our package called FeeCAP. And so what it contains is 10,000 patients. This was a random selection of patients who had at least one billing code for CAD or one mention. It's called a One mentioned. It's called the data marks in the EHR literature, and 181 of those records actually went through chart review. So we have confirmed coronary artery disease status or CAD for those patients. And then we have 576 features. So these are things like billing codes for CAD, positive NLP mentions in the record, as well as symptoms, and measure of healthcare utilization. So how often the patient is actually using healthcare. And so if you want to take a look at the And so, if you want to take a look at this data set, just do note that as part of the release process, the variable names aren't available. Unfortunately, that will change over time, but I won't go over variable selection results since they're not very meaningful. And this link in my slides will take you to a workshop that introduces this package. So, just to get at this procedure that I've just described to you, the first thing I need to do is to determine the surrogate. So, here what I'm going to take is the positive NLP mentions. To take is the positive NLP mentions of the disease as well as the billing codes. So, features that I'd really imagine patients who had the disease would have a high number of them, patients who didn't would have not very many of them. And so, I'm just going to do a simple comparison here of three approaches. I'm going to take that 181 labeled data set. I'm going to do some resampling, and I'm going to show you what the results would look like under 50, 75, and 90 labeled examples, because the least amount of labeled data we have, the better. Data we have, the better. The procedures I'm going to compare are just an adaptive loss, so taking the regression of y on x, and I'm going to take the anchor method, which is common in the EHR literature, which is essentially taking our surrogate, thresholding it to create a binary variable, using it as the outcome, and then doing the regression. And then the proposed procedure with 50, 70, and 90 examples. And so, for simplicity, I'll just show you the results on AUC. And so, what we can see here. And so, what we can see here, just note that the anchor method doesn't have a box spot because it's using the entire unlabeled data set. There's no recept thing to be done there. We can see that with this two-step method, with 50 examples, I attain an AUC of almost 0.9. By only using the anchor method that uses the surrogate as the outcome, so not really used in the final prediction, the AUC is about 0.62. And adaptive lasso, you can see, is sort of slowly increasing as I increase my sample size, would require much more label data to actually achieve. Label data to actually achieve an acceptable performance. And so, just to close up, I wanted to illustrate that phenotyping is a barrier to EHR research. What I'm working on is semi-supervised learning methods that can reduce this labeling burden. And of course, there are many assumptions and methods that are all areas that I'm working on currently. I think the glaring one is obviously that the phenotype falls a logistic regression model. I've also collapsed all my data to a patient level. So, for certain phenotypes, for instance, Level. So, for certain phenotypes, for instance, like the mental health phenotypes that were mentioned this morning, accounting for time using a more complex model is obviously a necessary direction. I've also made this assumption that the labeled data is randomly sampled. We can do much more clever things with active learning, even with stratified sampling is something we've looked at. Another thing is this idea of the presence of a surrogate. That's a key assumption. So, working in situations where that's not the case. And of course, this idea that we have this binary gold standard label. Gold standard label. And so, lastly, I just want to comment that post-phenotyping inference is another area that we've worked on. So, what I've described today is how to phenotype quickly, efficiently. But now what I have available to me is just for everyone a predictive probability. This has more variability, or this has less variability if I use this imputation. How do I actually account for this in my inference? So, currently I have a student working on this where our general thought is here that the types of errors will That the types of errors we'll have in our predicted probability are going to be very complex. So, classical measurement error misclassification models from the statistical literature make strong assumptions about those models. So, our general approach is to do a joint modeling of both the predicted probability together with the labeled data. And so, hopefully, that work will be out soon. And that's it. Thank you so much.