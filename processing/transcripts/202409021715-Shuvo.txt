Some model proposals we have derived and some star some results. Yeah, sorry for the interruption. So let's so for the first limitation, let's go back to the likelihood we have just discussed on GO task. So if you look at the likelihood, this density, that actually represents the response time of the process, the GO process that has actually owned and the survival is for the process that didn't win. That didn't win. So, the existing approaches they have assumed that these two finishing times distribution are actually independent. But we have actually gone through some, we have done actually some exploratory research, as well as there are some decades of research on psychopathology domain that doesn't actually support this assumption. So, let's see what our exploratory analysis says. So, this is a box plot of response accuracy. So, by response accuracy, I'm So, by response accuracy, I am meaning the proportion of correct responses in a Go trial. So, as you can see, the proportion of correct responses are more than 80% for most of the subjects. So, these indicates actually are goal maintenance activity for correct decision making, which is not actually happening in random, at random. So, we have already discussed the proactive controls. So, the proactive control of the cognitive control is actually Of the cognitive control is actually responsible for a goal maintenance activity during any task. So, such kind of goal maintenance activity, as I've said, it's governed by the projective control and existing models, so far the best of our knowledge, they didn't consider this dynamic nature of the proactive control, which actually accounts for the performance of the GO task in SSP. So, that's limitation number one. I will come back with the proposals. The second one, the second assumption of the An assumption of these likelihoods. So, the successive trials, the successive trials, those are not independent. That means the performance of a trial is actually influenced, is not influenced by the performance preceding to its performance of its preceding trial. So, however, we can see this is an ACF of the response times with respect to trials. So, we can see that it's a time-dependent process. There is an autocorrelation of the response times over here. Response times over here. So, this time-dependent nature, this time-dependent nature, it also actually coincides with the dynamic behavior of the proactive controls. So, proactive control is actually making us maintain our goal over the task. Limitation number three. So, here, this is the box plot of again the choice accuracy by choice accuracy. I'm meaning the response accuracy. So, this is a response accuracy box plot of. Box plot of male and female. The left one is for female, and the right one is for males. So, here you can also see that the proportion of accurate responses is higher for females with respect to males. So, this is also an indication that there might be some kind of external variables which we need to consider while estimating the SSRTs. So, again, existing. SSRTs. So again, in the existing models, they are not incorporating this fact while estimating the SSRTs. So now I'll be giving some proposals. So the first one that's actually incorporating how to incorporate the time trial by trial dependence as well as the goal maintenance in cognitive control in GOTA. The first one, so let's again review this response time density that is characterized by. That is characterized by the inverse Gaussian distribution. So, as you can see, the red mark parameters, the threshold boundary, and the evidence accumulation rate, those are actually fixed in our present setup in the existing literature. So, what we are proposing that to incorporate the proactive control, to incorporate the dynamic behavior of the proactive control, we need to consider these parameters as. Parameters as some unknown time varying functions such as Bt and Ut. So these are some unknown time varying functions that we want to estimate in our exercise. And so again, so this is for the TA trial. So at TA trial, we have this kind of density and these parameters will vary with our as the trial goes on. Okay, so now we have already talked about the Now we have already talked about the trial by trial dependence. Now let's talk about goal maintenance behavior. So we have hypothesized that in a certain trial, the proactive control, so the role of proactive control is to maintain the goal so that you can do the right task, you can respond correctly. So what we are hypothesizing is that proactive control is actually That proactive control is actually penalizes the evidence accumulation rates of the incorrect decisions. So, as you can remember, we have a race between all the evidence accumulators when a stimulus is presented. So, if the evidence accumulation rate of the incorrect decision is lower than the evidence accumulation of the correct decision, the correct decision will reach its boundary first and the correct response will be made, right? So, we are hypothesizing that the proactive control is actually penalizing. Proactive control is actually penalizing the evidence accumulation rate of incorrect decision by penalizing by slowing it down so that the correct decision wins. So, to incorporate this mathematically, we have introduced a penalty function. So, that will actually decrease the evidence accumulation rate of the incorrect decision. So, to go over this mathematically, so this is an example of response time for the ith response. For the ith response, for the ith individual, where you have two kinds of go processes, left and right, and the respondents, the participants are actually presented with some stimulus, left and right. So, we are assuming we already know that this actually follows the inverse Gaussian distribution. And we assume these are actually time-dependent parameters. So, we have talked about the evidence accumulation rates. Accumulation rates. So we are actually modeling the evidence accumulation rate like this. If the response is correct, if you see here, there is no change. So this is the evidence accumulation rate of the correct response. But if it's incorrect, we are penalizing it in this way. So this is our penalty function. So this is actually the evidence accumulation rate of the response. Of the response, and this is the stimulus given. So that is how the penalization happens in our model. And now let's talk about the penalty function. So since we are simply subtracting, we want to decrease the wrong evidence equation rate of the incorrect decision, the penalty function needs to be always increasing. So we have ensured that this is an increasing function and as x or And as X or the evidence accumulation rate of the stimulus presented is increasing, the penalty function also increases. Okay, that's the part of the goal maintenance behavior. So mostly we have talked about go tasks so far. So let's talk about stop tasks. So there's something going on also in stop task. So you know about this stop task we have just discussed. So it starts with a stimulus, then With a stimulus, then after some delay, SSD, we have a stop, and the task is to inhibit, task is to withhold your response. So let's deep dive into this stop task. So if you can see over here, it starts with a go stimulus. And when you see the gold stimulus, actually these two, these two green go evidence accumulators, those actually start collecting the evidence. They start racing. Racing. But the stop evidence accumulated, it starts, it only starts the race when the stop signal is presented. So what we are hypothesizing that when we have this unexpected stop signal task, so let's go back to the proactive control task again, proactive control again. So in proactive control, when there is an unexpected event happens, then Uh, happens, then actually, proactive control comes up. So, here, this stop signal, we were actually prepared for a go task where we were presented with a go and left. But when the stop signal is presented, it actually becomes a stop task. And we are hypothesizing that these unexpected on the onset of this unexpected signal, the reactive mechanism, control mechanism is actually triggering. And what is it? Triggering and what is it's doing? It's doing two things. Firstly, it's starting the evidence accumulation for the stop signal. And the second thing, the most important thing, it's actually slowing down the evidence accumulation of the Go processes. So if you can look at this circle, you will see that before this stop signal, they were in an increasing rate. After that, it's kind of stabilizes. And at this time point, this 300, the 300, the stops, the stop evidence accumulator has actually reached its boundary. So it's actually a perfect example of response inhibition trial. So where the stop signal has the evidence accumulator of this stop process has actually reached its boundary. So how to model this? That's the question. So firstly, so far we considered the linear Weiner process for the Go stimuluses, for the Go processes go. Stimuluses for the Go processes, go tasks. But since we have an interruption over here and the evidence accumulators of the GO task are forced to slow down, we can model using the linear process, Weiner diffusion process now. So we propose to use a non-linear Weiner process as the evidence accumulators for both the left and go evidence accumulators in stop trial. So the So, the non-linear Weiner process, it looks like this where we have a piecewise constant function. So, if you look at this expression closely, you will see that u less than SSD. That means before this is the time, so from the GO stimulus and before the stop signal presented, this time is actually called the stop signal delay. So, before this stop signal delay, our model is sorry, the riff rate is actually similar. rate is actually similar to what we have in the Go process. So if we have R equals to S, it's simply the evidence accumulation rate. If R naught equals to S, that means if we have, if there is an incorrect decision, then this penalty comes in. And what if we have what if U is greater than S D? That means we have the stop signal now. So then actually this Actually, this is the panelization. We have a similar kind of panelization, but here the function is with respect to the evidence accumulation rate of the stop parameter. Okay, so we have derived the Go response time for non-linear Winer process like this. And one thing to mention over here is that if you look at this, this is actually the original inverse Gaussian distribution, but this blue part. Distribution, but this blue part actually came for came from the adjustment we did due to the non-linear winer process. Okay, proposal number three. So it's an ongoing work. So we are just proposing this, but we will implement this in future. So firstly, one thing is we were saying that the parameters, those are time dependent. So it can also So, it can also vary from individual to individual. So, the drift and boundary for one individual that can be, that will differ from one individual to one individual. So, to incorporate this, we have introduced the functional mixed effect model. And here, if mu is a time-varying function, we have these two components. dt is basically what we were basically what I was saying so far. That's the main effect. And now we have to deal with this subject. Deal with this subject-specific random effect. So, we are getting GT from the cubic spline and UIT, the random effect, we will be getting it from the Gaussian process with a mean zero and a covariance function. Okay, let's see some results. So, firstly, I will go over some simulation study, then some results from the real ABCD data. So, this is the first result. So this is the first result. So I have introduced the penalty parameter and we have some traditional, we have the existing models that doesn't actually incorporate any penalty parameter. So we were interested to see how our model performs when actually a penalty is introduced and when there is no penalty. So to do that, we didn't take any random values as our simulation seed, simulation truth. So firstly, uh simulation truth so firstly what we do did we use the abcd data we proposed our uh sorry we used our proposed model we fitted our proposed model on the abcd data and recovered uh the parameters the time varying functions and then we use those time varying function as ground truth and we did the predictive predictive sampling so uh and we also do this do the same for for the existing for an existing model where there is no penalty involved Model where there is no penalty involved. So now we have two sets of samples and we check the response accuracy in those two models. So the idea is whether, and we have also the response accuracy from the real ABCD data. One thing I forgot to mention is it's done on a subset of ABCD data. There were like 15 individuals. So if I look at the results, the response accuracy for ABCD data was like 96%. Data was like 96%. And the model we have proposed with penalty, it's actually close to the response accuracy, but the posterior, but the existing model, the results are pretty poor because there is no penalty involved over here. So it's only 50%. So one thing is our goal response accuracy of our proposed model is actually close to the truth. So second thing that we checked is Second thing that we checked is whether a very, very important question is like whether our model can recover the parameters successfully. So here again, we did something similar. We again fitted our model on the ABCD data, recovered the parameters, and then do predictive sampling using these parameters as ground truth. And then again, on this sample, we have pitted our proposed model and recovered the parameters. Model and recover the parameters again. So now we have two sets of parameters. So one is the ground truth from ABCD data, and another zero one is we have just recovered from the samples. So the objective again, whether we can recover the ground truth. So here we have three sets of plots. So first one is the boundary parameter. The second one is the evidence accumulation. And the third one is a function of these two parameters, which is called mean response. Which is called mean response. So, the left is for the left stimulus, left evidence accumulator process, and right is for the right evidence accumulator process. So, these are pretty close. And like from here, we can conclude that like we can successfully recover the parameters. So, that's all about the simulation results. So, now I will present some kind, some real data results. Again, this is on those 15 individuals. On those 15 individuals. Sorry, not those 15 individuals. So we have taken again only 15 individuals to get some results. So here I am comparing the GO accuracy or the response accuracy. So as you already know, their response accuracy is more than 80% for almost all the subjects. So we have taken two groups. One is more than 90% accuracy. And another one is we are saying that. Another one is we are saying the medium accuracy, go accuracy group, which is like 80 to 80% response accuracy. So one takeaway is like for all the graphs, you can see that the high accuracy group, they have actually more response, their response time is higher than the medium accuracy group. And they have also the boundary and the evidence accumulation rate, they also have higher than the medium accuracy group. The media microscopy. So, that is one takeaway. And so, we talked about, sorry, how to go back. So, we talked about SSRT all the time. Now, let's see some SSRT results. So, this is the stop signal reaction time by gender by male and female. So, this is the empirical distribution. So, we can see that there's a little bit difference in the central trend is the same, but there's a little But there is a little, the peak for females are a bit higher, and it's also a right tail distribution, and there is some kind of variability in the tails for both of them. So yeah, so that's all the results. So in summary, we have proposed to estimate the distribution SSRT, distribution of SSRT. So in our model, we try to incorporate the trial by trial independence. The trial by trial independence and the proactive and reactive control to formulating some using some mathematical formulation. And from the simulation study, we observed that the model can successfully recover the parameters and we kind of get the similar kind of response accuracy from the ABCD task. The traditional model failed to do so. And the preliminary analysis, those are actually consistent. Those are actually consistent with the assertion of the psychopathology literature. So, we have some future plans. Firstly, as I've said, we talked about the individual differences. We are working to get it done to incorporate the functional mixed-dependent models. And secondly, as I have already said, I mentioned in limitations, there can be some external variables that we can consider while. That we can consider while using this while estimating the SSRTs. This is our collaborator, Dr. Stacey Warren from the Behavioral and Brain Science Department of UTD. And this research is partially funded by NFRS and SPIRE. These are some references. Thank you. Yeah, I think I'm at the point. Thank you, Norris. Thank you, Norris. Thank you, Farabi. I think we can open the floor up for some questions. Just like a naive question. Like when you did a simulation, you actually simulated from the assumed analysis model, right? Yeah. Yeah, you happen to know the simulation proof. But do you think there's any is this any scope for? Scope for relaxing the assumed model? Like you assume a Brownian motion, is there any scope for doing some of our favorite non-parametric base games to replace that? Or would it be overkill? What's your judgment seeing the data and how things work? So far, like this model is doing good. Maybe we can explore this. And also, there is some issues with scalability. So maybe we'll go for like something. I have no not much. I have no not much idea, but maybe we'll go with some variational auto-encoder to make it faster and get better estimates. So, so far, the results are promising, but we are not sure how scalable is this because we have a lot of data, like 8,000, 10,000 individuals. So, still, only 15 individuals is still not very fast. So, can I take Farabi's answer? Farabi's answer is absolutely aligned. Bayesian non-parametric models are great, but they might not be as scalable as parametric approaches. But we build on, we are actually what we are doing here is some semi-parametric model. So there's a rich literature on cognitive control that where the parameters of the Weiner process has some interpretations in terms of. Has some interpretations in terms of psychopathology traits. So that's why we, in the first project, we are trying to, we are using that and do not attempt to build a vision on parametric model because we the estimate like interpreting the parameters will be of future importance because we want to we want to have interpretable parameters, identifiable parameters, if that makes sense. Any other questions? All right, again, we can definitely take the conversation off. And thank you, Farabi, again. Thank you, New York. Thank you. Thank you.