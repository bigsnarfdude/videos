The invitation first and finally for the presentation. I think I should do something before going because you started the the recording and I need to say that I got it. What's happening? Yes. Okay. Sorry. Sorry. Here. Thanks. So I'm happy to be with you. Not in Mexico, but maybe next time. So I will speak about brain data and some mathematical tools to learn from this brain data. So first, just some findings. Funding. So I'm partly mainly funded by the National Research Agency in France here and some other organizations. So first I want to show you what are these brain data. So the brain is both a structural and functional network. So here on my slide, I think you can see my mouse here somewhere. On the left part of the slide, I showed you some micro scale picture of the architecture of the brain. And on top of the slide, you can get some information on the architecture of the brain. And on the bottom part of the slide, these are recordings we can make for this brain organization. So at the micro. So, at the micro scale, what you will do usually is optical imagery, like the film you are seeing here. So, it's one way to look at the firing of the neurons and the firing of several neurons at the same time. The second scale is called mesoscale, and it's like looking at the column, cortical column of the brain. And for this, And for this, we can have recordings from microelectrodes, like what you can see on the middle bottom of the slide, where you will have signals for each sensors of the electrodes. And then the macro scale, where we will record signals from each region of the brain. And this usually will And this usually we will use like functional magnetic resonance imaging, EEG or imaging. In my talk, I will focus on functional magnetic resonance imaging. This is usually the sorts of data sets I'm using. And don't hesitate to interrupt me if you want to ask any question. So from these data sets, what my work is to model the brain as a network. So for each of these sensors, we will have time series. Source, we will have a time series, and from this time series, I want to extract the network where each node of the network will correspond to a brain region, and each edge will correspond to a link to connected regions. So, this is not new. This has been introduced now 20 years ago, I think. We called it connectivity. We called it connectivist view. In the 19th, in the 20th century, at the beginning of the 30th century, neuroscientists were seeing the brain as using a phrenologic view where you've got each region associated to a specific functioning of the region. And because of the neuroimetry, Because of the neuroimagery, now we will go to a more connectivity skew where we can record many data sets from the brain. So here my talker will focus on learning from brain data sets, but before having these networks, you have to know that it's going through a lot. That it's going through a long process of pre-processing and acquisition. So, here on this pipeline here, you can see that we are recording the signals from the brain. And then we will compute connectivity matrices, like this drawing here, where you will extract a weight to see if to detect. To detect if there will be an edge in the network or not. And this, so this connectivity matrix will go for some thresholding approach, and then we will get this network. And once we are getting this network, usually we are extracting different metrics from this network to explain differences between healthy and diseased patients. Disease patients. That's where my talk is going to explain you how we can compare this, how can we learn features from this graph with or without using this graph metrics. So from just before going into details of the learning approach, just a few details from a statistical approach. So there are three main parameters. There are three main parameters in these data sets that we should take care of when we want to extract significant knowledge about this network and these comparisons. The first is the duration of scans. It's not easy to get long data sets and sometimes it's not possible because I'm sure you've all seen an fMRI scanner and as you are Scanner, and as you are lying down in this big thing, it's not really comfortable, and you don't want to lie there very long. The second parameter is the number of signals, so the number of brain regions you want to extract. And again, this is also complicated because the threshold will be completely dependent on this number of signals. And finally, the size of the population. Of course, whenever you want to compare. Course, whenever you want to compare a group of people, you need to get samples from this group of people. And you'll see, for example, in the rest of my talk, I will focus on specific data sets on comma patients. And we don't have a lot of data sets. And it's hopeful because we don't want to have lots of people in comma. Okay, so the main challenge is. Okay, so the main challenges are a robust estimation of cognitivity matrices. So usually I'm using weblet correlations. It's hard to find the largest size of correct edges. And for this, you can use multiple testing. And finally, efficient and robust classification and comparisons are needed, of course. So these are examples of brain networks. So what you can I picked up I picked up different publications here just to show you how networks can be visualized from real data sets. So the first one, you can recognize the brain and the graph on top of the brain. For the second one, this is an example of diuretic graph extracted from, I think, EEG data sets. And the last one. Data sets. And the last one is the graph of the brain networks. You can recognize here the big part of the brain, like the lobes, but they are put in a different way as matching the anatomy of the brain. So in terms of math, so graph is an abstract representation of a set of objects. Abstract representation of a set of objects where some pairs are connected by links. So, in this talk, I will denote the graph as G. It will be a couple of two sets. A set V is the set of vertices and a set E, which is the set of edges. N will be dedicated to the number of nodes in the graph, and M will be dedicated. In the graph, and M will be dedicated to the number of edges. In my talk, G will be undirected, and I will usually call the adjacency matrix A. And this, I'm sure you all know that, the adjacency matrix is a matrix N by N and there will be a 1 in the matrix if there's an edge in the graph between I and J, the nodes. The nodes. Nice, I want to spend some time here to explain how to characterize these networks because it's really important in terms of neuroscience to get interpretation of these comparisons of networks. And in this sense, if you know what you are measuring, it's better. So, usually in neuroscience, what we are using is these different features. So, the density of the graph. Features. So the density of the graph, the efficiency, the clustering, and I will just speak, give you two words about the small graphs. So first density. So density is linked to the number of edges of the graph. So here on my two graphs, I on purpose took the same number of nodes, 11, but different number of edges. And based on this, you can define the duct. Based on this, you can define the degree. It's the number of connections that each node makes to other nodes in the graph. And the mathematical definition is given at the bottom of the slide with what I call D. If I'm keeping the same number of edges and same number of nodes, now I want to discriminate two graphs like this, like these ones, and I will use the minimum. And I will use the minimum path length or what we call efficiency. So the efficiency will be the harmonic mean of the minimum path length. And you can see that in these two graphs, the one on the left is much more efficient than the one on the right. Another feature that is really used a lot in Really is used a lot in neuroscience is the clustering or the labels of nodes. So, here again, there are two graphs with sign number of nodes, sign number of edges. And in these two graphs, we can see that the connections between the neighbors of node one in these two graphs are different. So, for the graph on the left, they are disconnected, and for the graph on the right, they are disconnected. And for the graph on the right, they are fully connected. So we will say that the clustering of node 1 on the left graph will be low, and the clustering of node 1 on the right will be high. And this is computing using again the minimum pipeline for the induced graph. Gi is the induced graph of node i. And this And this clustering is linked to the robustness of the graph. Because if you just recall the two graphs I plotted before, if I'm removing node one for these two graphs, then when the clustering is low, the robustness of the graph will be low, like on the left part of the slide. Okay, so these are the four quantities I'm looking. Four quantity I'm looking at usually to extract information from these brain data sets. So the cost, efficiency, clustering, and modularity. I did not speak about modularity, but it's the way to cluster the graph. So just to an illustration of that on the small world networks, so this model was introduced by Swat and Strogatz and it's Buzzwet and Strogats, and it's the way to go from a regular graph to a random graph. And just to make sure you are following me, you can do exactly the same plot as the Watson-Stroga's in Watson-Stroga's paper, but using the local efficiency, the clustering definition I gave you, and one minus the efficiency, you will get exactly the same sort. You will get exactly the same sort of features. And of course, there exist other graph metrics, modularity, between S, per correlation, and other things. And there are plenty of nice toolbox on our Python on MATLAB to compute all these sort of measures. So now I want to motivate. Now, I want to motivate a bit more how you can learn from this graph and what you can learn. So, again, we are back to our mathematical framework. G is equal to V the couple V E. Now, I will introduce the number of patients and the number of controls because the idea is to compare two groups of networks. So, this can be. networks. So this can be anything. Here I will speak in terms of patients and healthy controls because the application will be based on that. For each, so as I showed you on the different metrics, for each patient and for each healthy control, I can extract a vector or graph metric. So here M will represent for the patient M 1K to M. 1k to nk will correspond to a metric extracted for each node of the graph for the network of patient k. And you can do the same for the healthy controls. And in the state of the art in neuroscience, in this network, neuroscience, is to take the average matrix. So, like the formula I showed you here. So, we are Show you here. So, we are taking the average efficiency, for example, by summing over all the nodes of the graph. And so, by this way, each patient and each control will get one value for each. So, it's typically what I did here on this specific group of patients. Group of patients and control. So again, we got observations from fMRI data sets. We can partition them and extract this time series, then extract the connectivity matrices, extract the graph for each subject, and then compare our groups. And on the right plot, the bottom right on the slide, On the slide, you can see one node for each subject. So, one node is the average global efficiency and in comparison to overall functional connectivity for each subject. And what we can see here is that when you look at controls and patients, it's hard to make the difference between the controls and patients. Between the controls and patients, when you are looking at what we called non-random patients. Because the blue point here were excluded from a statistical point of view because the signal was too noisy to extract significant graphs. On the left part of the slide here, you can see the dynamic of the global efficiency in function. Efficiency in function to the density of the network. And again, we can see that the patients and the controls are really overlapping. And so this is to show you that this average matrix may be not a good way to learn from brain data sets. So now I want to show you what we discover and what we implemented to be able to discriminate these two groups. Because of course, from a medical point of view, coma patients and healthy controls are completely different. So we should be able to see it on the fMRI data. So this is another data set on again coma patients. So 20-minute acquisition and we have 20-minute acquisition, and we had 90 regions of interest, and the population size was not too high. So, these are some images you can get from these data sets, where I'm extracting one graph for each LFE volunteers and one graph. And one graph for each comma matrix. And when you look at the average matrix again, so the H tues I called them before, you cannot see any differences. But when you look, dig a bit more in the data sets and you apply linear discriminant analysis of feature selection, then we were able to discriminate the two groups. So definitely there's The two groups. So definitely there's something with the average metric we are not capturing. And this is another example we looked at for these data sets with Pedro, with Pablo, sorry. And we used his method, the non-backtracking cycles, to look at the different data sets. So the comma, the patients, and different networks. And what we can see on this plot, And what we can see on this plot also is that there are something in these data sets that says that these data are different from the classical network, at least. So from a mathematical point of view, we wanted to, yeah, what I forgot to tell you is using linear discriminant analysis or Linear discriminant analysis or random forest, it's really complicated to interpret and to extract some meaningful observations from the brain networks in terms of neuroscience. So, what we develop is a new characterization. Characterization based on our observation. I will come back on this plot here. What I'm plotting here is the network of the healthy bonuses and the network of the coma patients. And what I did not describe to you on this plot is this last column here, where you've got an image of the brain, and I put red colours or blue colours. And these red colours and blue colours. And these red colours and blue colours correspond to statistically significant differences between comma, this comma patient, and the group of healthy volunteers. And when it's read, it's showing an increase in terms of degree. Here, I extracted the metric as a degree, and in below. As a degree, and in blue is showing a decrease. So, what we can see on these plots is that for each patient, we've got at the same time increase and decrease connectivity. So, what we were thinking in terms of this metric was to look at a sort of compensation. Between the regions that can have a decreased connectivity and over that can have an increased connectivity. So we called it the hub distribution index, and it's defined like this. So this time I'm taking the average over the controls. So I is the vertex, a node of the graph, and j represents the controls. And I'm extracting. And I'm extracting an average of a metric. I can use any metric, degree, global efficiency, clustering, whatever I want. And I will extract a vector of some characteristics of the group of controls. This is contained in this H hat vector. And then what I want to do is I want to compare this group, this. This group, the metric obtained for each patient to this average of controls. And I'm doing simply a regression, so that I will extract this kappa here, which represents the way the regions can be disconnected or the way the connections can be decreased or increased. Can be decreased or increased in comparison between patients. And I can do it at the individual subject, so for each patient, or I can do it for the average of all. So in terms of graphics, that's the sort of things we will observe. So in the x-axis, I put the metric, the average metric of healthy volunteers. The average metric of healthy volunteers. And on the y-axis, I'm looking at the difference between the comma and the healthy. And what we can see here is that the hubs of the healthy volunteers, the regions here, are disconnected in the patients and the non-herbs are the the the connectivity is increased. Is increased. And of course, the nice thing about this hub distribution index is that we can point the regions that are driving this regression. So this is for the average, but we can do it for each patient. And that's what we can obtain on the average of the network. And when we did it on all patients, so each of So each of these gray lines here corresponds to one patient. Here I represent the metric called degree on the bottom part of the clustering. And what we were able to do this time was to discriminate the two groups and with a high difference. So it seems that the hub disruption index is really capturing the Is really capturing the discriminative part of these different networks. Nice, but now you can tell me, okay, you can interpret, but it's very ad hoc measure. So yes, of course. So here it's a summary of what I explained for now. So the average matrix, it's not discriminative for the coma patients, so it may be a bit. So, it may be a bit complicated to work with on the long term. The linear discriminant analysis and random forest are discriminative, but it's not easy to interpret. It's even not possible. And you have disruption index. It's an ad hoc measure for comma patients, but we can find an easy interpretation in this sense. So, in fact, the general frame. So, in fact, the general framework of all this is the graph embedding. So, this is represented in this figure here. With one of my colleagues, we published a review on that. So, again, we've got this time series here from the brain, the graph computation. And then, as I told you, this little square here are showing the computation of the graph matrix. And we've got our two classes. And we've got our two classes, and then we can train the model, and then we can extract some significant edges from the graph. And with this, we are able to go back to where the discriminative parts are happening in the brain. So, now, so this is the general framework. And the last, I just want to, yeah, I've got some time, I just want you to. Got some time. I just want you to explain a different way to tackle the problem is to do, in fact, dimension reduction method on these sort of things, but covariate constraint manifold learning. So, what I want to show you here is that as soon as you know, but one of your metric is Of your metric is discriminative, then you can use this a priori to run a manifold learning approach. Like here, the example I will show you here will be based on ISOMAP. So that you can use this a priori to better identify, discriminate the differences between your group of subjects without losing the interaction. Without losing the interpretation, the focus on interpretation. So the idea is to constrain, so to compute the manifold learning by constraining here C is fixed, and in my case, it will be the hub description index. And we are searching for alpha and the overcome. Alpha and the over coordinate to put all these in a reduced space. So the cost function here we will use will be the Euclidean. It's simply by simplicity. And we will apply a specific optimization procedure here to be able to deal with that. With so the cost function will be optimized three times with so first with the computing the Xi when alpha is fixed then we will optimize on alpha and then we will optimize for each point that has already been included. Already been included. So, our algorithms will be like that. So, we've got our input data sets, the metric, and again, you can use whatever metric you want. The results will be the reduced space representation, so a coordinate of all subjects. And the tricky things here is in the optimization of E. is in the optimization of E as I explained before and we will select so in this iterative optimization we will select the most distant sample to the already selected samples this may avoid going for local minima and it's better in terms of optimization so as a result we can obtain this sort of plot using our brain data sets Our brain data sets. So, here, this is the reduced space representation of the subject, the control and patient. So, I've got 20 controls here and 17 patients. This axis is not, it's the up description index. And this axis was found by the isomac procedure, what we call CCNL. Called CCNL, covariate constraint machine. And what we can see here, I think you don't need proof about that, but a classical superfactor machine will discriminate completely to two groups. And the nice feature also we can have from that is that we can interpolate a sort of path between a patient. Between a patient to a control on this reduced phase. So, this was done using multivariate adaptive regression splines or the interpolation. And we can plot the graph, but the vector of the matrix for each of the points. So, the metric of the graph A, B, and C. So, A for patient. Patient, I would say the most extreme patient on the reduced phase. B would be in the intermediate state and C is representing a country. Nice, and I think I've nearly finished. So I just want to tell you, but so I showed you here different way to learn from brain data sets. From brand data sets. It's really complicated when you want to extract meaningful information from the brain to be able to speak with the medical people that are in charge of the patient. Because the idea for this coma patient is really to help the practitioners in the real In the rehabilitation of these patients. And what I'm encouraging you is to use different open data sets. So I'm putting here a collection of open data sets where you can find sometimes networks directly to test any learning procedure or For the OpenFMI, for example, I think they are not providing the graph, but the raw data sets. But I'm advertising these data sets because the field now is providing more and more freely available data sets and it's very nice to make an evaluation of our methods. Nice, to conclude. So, first, method and acquisition of data have to be chosen adequately and dependently. So, I did not spend lots of time on that, but we can discuss this if you want. And statistical methods are sometimes limited in terms of interpretation. So, learning techniques for brain data are promising, and I really think we need to test our method on the available data sets as soon as we are. Data sets as soon as we want to publish. And I think networks are providing a very interesting view of consciousness for these comma patients, but more works are needed to better understand their reorganization. And I would like to thank my network of collaborators. Thanks. Thank you very much. Thank you very much, Sophie, for the very nice talk. Do we have questions from the audience? Do we have questions from the audience, please? Pablo, you have a question. So I was wondering, you know, I was working with some kind of collaborators in Italy, actually. They were kind of also doing sort of classification of brain connectivity networks using like they were comparing the gra the graph Laplacians actually using like various different distances. Different distances, uh, but I think I think they were trying on pretty small data sets. So, like, they were actually using like the Euclidean distance and also using like the Versus finders and so on. But I think the data sets they were working on was actually quite small. Uh, I'm not quite sure how big like this data sets you were talking about, um, and whether like you know that whether there's like any way to compare kind of the different approaches, maybe kind of like you know, the approach of using comparing the Rapp LaPlasen, for example. Do you have any kind of Do you have any comment? I think all these methods will suffer from the lack of patience and controls. And I think the way we can maybe avoid this problem is by introducing knowledge in the classification. So, like I showed you on this money for learning ways. Manifold learning where you know but one you already know one characteristic but will be able to discriminate the two groups and then try to find another one I did not check with the the Laplace but I think the for me these graph metrics were easy to interpret so easy to start with right right and now and afterwards you can add some more complicated features Features, but but yeah, but for me, the Laplace needs one more features, right? Right. So, I guess. So, you think like the kind of the number? Oh, sorry. I mean, we're just talking across each other across the edge. Go ahead, go ahead, Pablo. So, maybe just to do with this issue of having small data sets, do you think that there could be a data augmentation? Be a data augmentation technique that you in your work would find reasonable, or that even better, the clinicians would find reasonable? Because they are small data sets, but it would be better to be able to augment them. And this is, you know, this is very frequent in like image analysis and other kinds of machine learning areas. Yeah, I think the this, yes, good question. I think the the representation The representation using this isomab, this video space, may allow to do data augmentation because you have this all space and then you can simulate data sets from this reduce space and we are not going back to the graph. We are going back to the vector of the matrix. But in a sense, this would be, I think, I'm not sure we. I think I'm not sure we'll be able to really simulate networks, but at least we can simulate the vector of graph matrix. Or maybe just to go even further, if you were to start even before getting the matrix with the signals that were obtained from the fMRI, because then you could, those are signals, there are techniques for kind of augmenting signal data as well. So maybe that could. As well. So maybe that could. I mean, is that something that would that be too difficult? Because extracting the graph from the whole signal is, I know, is very hard work. Yeah, so these fMRI data sets, it's multivariate time series with localization in space and these connections. So I know how to simulate multivariate time series. Multi-by time series with a given correlations, given matrix of connectivity. But I don't know how, because in that augmentation you don't even want to simulate data sets, you want also to interpolate all these data sets. These data sets. So I don't know how to, for me, it's hard to interpolate in the space of graphs. But maybe, yeah, I don't know. Thanks for a very interesting direction. Well, Shantanu, Yoshi was talking about like very big data sets that they have in China, for example. Shantan Lu, do you have any comment? I don't know whether it's allowed. You have any comment? I don't know whether it's all done. But, like, he was talking about in China they have, like, I don't know, like, data sample, like millions of images, something like that. SMRI images. Shantano, are you there? Yeah, I think they are collecting those. I don't think they're out yet. I don't think they're. Oh, okay, okay, okay. But on that respect, I actually had a separate question. I'll just type it in chat, actually. Do you have any comment on multi-scale properties of brain networks and structural fMRI? That's a good question. The graph I showed you here, we are extracting using fMRI, so macro scale. And I don't have any it's a bit tricky when you are doing multi-modality things because sometimes you can bring new information, but over time. Information, but over times you can lose also information because of the noise and you're not measuring the same thing. But what I think is in terms of mixing structural and functional may add in terms of the interpretation of what you want to discriminate. Is that answers your question? So you will not get any new information from there, is what you're saying. No. What you're saying? No, you can get new information, but the problem is to mix them, to put them all in the same framework. Ah, okay. So essentially, use one to interpret the other. That's what they are doing to mix the structural and functional data sets at the moment. One method is doing like that. So they are taking the structural data sets as an a priori of the graph. An a priori of the graph, and then trying to understand what the functional data sets are, how the functional acquisition are linked to this structure. Okay. And maybe just a word in terms of the acquisition of firmware data sets. What I think we will face in a few years now is the Years now is the so the acquisition is it's not it's not complicated to have data sets from one center, but usually it's reduced in terms of number. But what people are doing is that they are acquiring data sets from multi-centric study to data sets from different centers. So I don't know for your data sets, but For your data sets. And for the moment, the classification is capturing the place where you are acquiring the data sets more than the variability between subject and patients. So it's called the harmonization of data set. And that's very complicated at the moment. Okay, thanks fascinating. Are there further questions from the audience? Well, if not, Pablo. Maybe, yeah, maybe before we go, I was going to do a little advertisement for the panel that we're going to have this evening. I know that for people in I know that for people in Asia and Europe, it's going to be kind of impossible, but we are actually going to have a machine learning researcher who works at a public institution in health called the National Institute for Respiratory Illnesses, the INER in Mexico. So she will also be able to tell us what these issues are in Mexico and how we're dealing with them. And also with Paula Villarrel, who used to work in CONACIT and helped with the COVID-19 The COVID-19 data sets and organizing them. And I'm with Eduardo Lises Moya, who's actually in the audience here. And he's also a director of AI for the government of Calisco, and he also has first-hand real-world experience with dealing with AI and its applications. So I just wanted also to thank you very much for accepting the invitation, Sophia, and for participating. 